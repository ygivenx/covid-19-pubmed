{"title": "Synthetic constructs in/for the environment: Managing the interplay between natural and engineered Biology", "body": "The issue of containment of man-made genetic constructs for limiting their possible environmental impact can be traced to the very birth of Genetic Engineering (GE). The development of the gene cloning technology by Cohen et al. in the early 1970s [1] constituted a revolution for all of the biological sciences, seeded new industries based on these sciences, and ushered in the era of Biotechnology. Recombinant DNA methods grew as a standard practice during the next decade bringing about two important and simultaneous steps in the synthesis of biological materials. One was cloning and transgenic implantation, which allowed to easily transfer DNA segments from given organisms to others and express their contents in heterologous hosts. The second one was the directed mutagenesis of given genes by either deletion or allelic replacement. This period of time witnessed the arch-famous Asilomar Conference in 1975 and its proposals for guidelines and self-regulation on the use of genetic constructs [2], [3]. Although many specific recommendations were largely ignored in subsequent years, Asilomar laid the foundation for most of the biosafety measures in place today for both biological and physical containment. Inter alia, the conference expressed that containment should be made an essential consideration in any experimental design and that the effectiveness of the containment should match, as closely as possible, the estimated risk. Since it was difficult to predict the biosafety threats of these new experiments, recommendations mentioned at that time stressed the exercise of an extra vigilance. However, they also foresaw that over the years GE practitioners would learn more about the actual risks in order to gradually decrease the (pre)caution to a realistic level. The now standard Biological Safety Levels (BSL) 1\u20134 were established and some types of experiments with highly pathogenic organisms (for humans, animals and plants) were even ruled out for the time being.\n\nAlthough the meeting put on the short run a considerable focus on safety matters, the following years witnessed a growing indifference on the issue, specially in non-medical biotechnology developments. One likely reason is the lack of accidents with this technology that could seriously back the tremendous hostility that was raised at the time by some groups that gained public audience. Yet, there has been little evidence for any serious mishap that could be directly linked to the accidental or intentional release of engineered microorganisms. In the meantime, a large number of incidents involving natural pathogenic bacteria and viruses have indeed happened. There have been also largely publicized effects of crops engineered to express the insecticidal Bacillus thuringiensis toxin (BT) on non-target species (other than the stem borer), the importance of which seems to be a matter of opinion [4]. In contrast, the overwhelming facts that stem from many studies on the environmental risks of GE organisms (in particular, bacteria) suggests that engineered constructs are not any worse than natural counterparts and most often they are less fit to survive outside the laboratory [5]. This tones down the claims on immediate dangers while it raises questions on what is to be done to avoid them in the future, should such dangers ever materialize.\n\nWhat are the conclusions from the conventional (not yet synthetic) recombinant DNA era regarding containment and biosafety? Research on introduction and acceptance of novel transformative technologies (for instance, the automobile or the aviation, let alone medicine) shows that accidents help more than any other circumstance to improve the safety of the next stage of the same technology (Fig. 1\n). In reality, after many decades of genetic engineering and massive production of recombinant bacteria it is na\u00efve to think that they have never escaped the laboratory. They often have, and massively. Even if autoclaving and other safety procedures were 99.999% efficient, a good deal of recombinant materials (i.e., live bacteria and functional DNA) have found their way into the environment through the sinks and the untreated residues of numerous laboratories worldwide. Proof of escape of transgenes and hybridization with wild relatives have been shown with the GM creeping bentgrass, used on US golf courses [6], [7] and unintended transgene survival and dispersal has been demonstrated in Mexico via the maize seed systems [8]. Yet, these are somewhat minor occurrences compared to the worst-case scenarios that were predicted during the onset of the cognate technologies 40 years ago [9]. That no engineered microbe has even been traced to any disease or has caused any detectable problem is an indicator that safety measures have so far been sufficient (or that forensic methods to this end have worked poorly). In contrast, natural pathogens occasionally escape the laboratory and cause diseases [10], [11], [12], [13]. Also, the World has witnessed in recent the emergence of numerous new epidemics, pests and environmental disasters, but none of which had any connection with genetic engineering. GE pathogens (mostly viruses) created by scientists for research purposes [14], [15], [16] have \u2013so far\u2013 not left the laboratory. Furthermore bacterial agents (unfortunately) contemplated in biological warfare up to now [17] are of natural origin. This is not surprising because the complexity of pathogen\u2013host interactions is so intricate that current knowledge makes the invention of novel virulence determinants really difficult, but not impossible for the future. A number of engineered viruses, such as the interleukin 4 enhanced mouse-pox supervirus [18], the humanized infectious bat SARS-like coronavirus [19], the reconstitution of infectious, human fossil endogenous retrovirus [20], or the recently engineered H5N1 virus, demonstrate an uncomfortable truth. As Declan Butler described in the December 20, 2011 issue of Nature: \u201c\u2026 virologists have suggested that any genetic changes that made it more transmissible would probably blunt its deadliness. The new work seems to contradict that comforting idea\u2026\u201d (http://www.nature.com/news/fears-grow-over-lab-bred-flu-1.9692). Although the engineered H5N1 virus is a very unusual example of GE, it cannot be ignored. If Synthetic Biology ever happens to fulfil its promises of systems engineering of living organism, we need to revisit the biosafety discourse of the last decades and shed some new light on it. Fortunately, the issue of endowing engineered constructs with a degree of safety beyond their mere physical containment (or regulations for their manipulation) has been dealt with before. For the sake of stocktaking, some outstanding instances are discussed below.\n\nResearch on biosafety risks over the last decades has produced a body of information that turns out to become a useful background to address current safety concerns on contemporary and future synthetic genomes and non-natural microorganisms. One special aspect of such a research on genetically engineered microorganisms (GEMs) for environmental release [21], [22], [23] dealt with the engineering of circuits for the containment of modified bacteria and their recombinant (trans)genes. The notion, pioneered by S\u00f8ren Molin [24], [25] contemplated the possibility that GEMs could be endowed with conditional lethality circuits to either program their death at a given time or once they had completed their mission [26], [27]. Note that the novelty of such active containment was altogether different from the much earlier concept borne in the Asilomar Conference that advocated the use of multi-auxotrophic strains as the recipients of recombinant DNA with the purpose of decreasing the chances of survival in the environment [2], [3]. The downside of this otherwise judicious concept was that the proposed strains were so weak that they were rendered nearly useless for biotechnological applications and even for routine laboratory work. In contrast, active containment pursued the vigorous performance of the cells of interest where and when desired, until a known signal appeared in the stage that triggered a quick death.\n\nIn the following years, the field of biological containment boomed with multiple propositions for conditional suicide circuits aimed at decreasing the viability of cells should they enter undesired situations or reach their time [26], [28], [29], [30], [31], [32]. The propositions ranged from simple schemes (for instance, making growth of the GEM dependent on an essential intermediate compound that could not be synthesized by cells: diaminopimelic acid or thymidine) all the way to intricate genetic circuits that programmed cell death once a target environmental pollutant was degraded by the engineered bacteria [33]. What is important is that cell death does not mean the disappearance of its DNA. There is a large body of evidence that the genetic material of dead recombinant organisms remain perfectly active and transferable in the environment [34]. Furthermore, meteorological phenomena such as lighting might promote DNA uptake in natural settings [35]. Along this line, smart designs were proposed for inhibiting acquisition of recombinant genes from GEMs into other bacteria whether through natural transformation or by an active process of horizontal gene transfer (HGT). Many of these schemes did work both in laboratory conditions as well as in microcosms. These mimicked to some extent the environmental conditions that GM bacteria were expected to face upon release. But the dream of perfectly contained bacteria received a serious blow in 2003 with a study showing that there was a limit to the efficiency of containment circuits of any type within the range of 10\u22126\n[29]. In other words, regardless of the sophistication of the genetic design, not less than one cell in one million can always escape the conditional lethality device or the HGT traps, and eventually survive. Given the fact that 1 ml of a grown standard bacterial culture has more than 108 cells, it is clear that such approaches to containment \u2013 beautiful as they are \u2013 do not solve the problem. The reason for this was the activity of mobile insertion sequences that populate the chromosomes of environmental bacteria and hop randomly between DNA segments of the cells in vivo. This circumstance ends up knocking out both endogenous systems of conditional viability and toxic genes that could be associated to the horizontally transferred genes.\n\nA second dividend of the intensive research of risk assessment of GEMs during the late 1980s and the whole 1990s was the development of a whole series of genetic tools tailored for designing bacteria destined for environmental release. The question at stake was how to ensure the stable maintenance of transgenes without any antibiotic selection and without being burdensome to microorganisms that have to operate in the open field. Many of these problems could be addressed with the so-called transposon vectors bearing non-antibiotic or excisable selection markers, which spectacularly upgraded the genetic engineering and manipulations that could be done with Gram-negative bacteria other than Escherichia coli\n[32], [36], [37], [38]. Such vectors permitted the multiple and stable implantation of long DNA segments in desired hosts, avoided the use of antibiotics, decreased the chances of horizontal gene transfer [39] and avoided the unpredictability of having the transgenes cloned in plasmids. One of the vectors was designed to produce GEMs that were indistinguishable from non-engineered counterparts (the so-called recombinant but quasi-natural bacteria [40]). With >3000 citations and despite recent improvements [41] the original papers describing such mini-transposon vectors remain to this day as the classical references of genetic tools for environmental release and other biotechnological applications. Yet, regardless of the abundance of assets for designing acceptably safe for bioremediation, the field of GEMs for the environment has been progressively losing steam in more recent years. The ultimate reason for this has not been safety concerns or lack of public acceptance, but the hard facts on their poor behaviour as in situ catalysts [42]. This stagnation has, however, changed recently with the onset of Systems and Synthetic Biology [43], which allows to take a multi-scale approach to the same problem and to come up with much safer and predictable methods for impeding the interplay between the natural and the man-made biological constructs.\n\nThe reliance of Genetic Engineering on synthetic nucleic acids started in 1978 with the development of oligonucleotide-based site directed mutagenesis procedures, which won the Nobel Prize for its creator, Smith [44]. The major problem of making the mutants out of a population of wild-type sequences was brilliantly solved later by Kunkel [45], what started to create a sizable demand for chemically produced oligonucleotides. But by that time, their synthesis was a complicated and expensive endeavour that had to wait until 1988 (the year of the publication of the polymerase chain reaction method, PCR) to start becoming a standard and affordable material in Molecular Biology and Biotechnology laboratories. Site-directed mutagenesis and PCR not only allowed the deliberate change of specific codons of proteins and regulatory regions, but also highlighted the need of synthetic oligonucleotides for the ultimate modification of selected genes at will. The landing of synthetic DNA in microorganisms as oligonucleotides boomed with the generalization of PCR for countless applications, including the generation of large DNA segments produced in vitro from a pre-existing template sequence. But for many years the de novo chemical synthesis of complete genes was still a fastidious, expensive and time-consuming exercise. Things started to change in the early 2000s with the development of powerful techniques for the complete synthesis of long DNA segments (e.g. in the kilobase range), along with complementary methods for the assembly of such segments into still longer pieces. The process of producing longer and longer DNA sequences \u00e1 la carte \u2013 together with hosts for their expression \u2013 continues to this day [46]. Recent landmarks include the complete synthesis of a functional chromosome of Mycoplasma mycoides of 1.08 Mo bp [47]. It is paradoxical that such an impressive ability to synthesize DNA does not match our much more limited knowledge to forward-engineer genetic devices with more than 20 genes or biological parts [48], [49]. This places the SB field in a territory where designing new-to-nature properties will still rely for some time on trial-and-error approaches where emergence of unexpected, perhaps undesirable traits might certainly occur. This type of \u2013 increasingly \u2013 synthetic bacteria could mutate in an unpredictable manner, or its DNA be captured by other microbes where it can also behave erratically. In view of past experiences with traditional GE discussed above, the ultimate biosafety challenge of SB is the implementation of absolute reliability, what has been called Certainty of Containment (CoC, [50]). CoC means that the probability of escape from containment, dissemination and unintended interaction with the environment must be virtually zero. The idea therefore is to move from Genetically Modified Organisms into Genetically Secured Organisms.\n\nSize-expanded DNAs are DNA-like molecules in which the base pairs are expanded by a process called benzo homologation. The resulting genetic helices are called xDNA (expanded DNA) and yDNA (wide DNA). When singly substituted into natural DNA, they are destabilizing because the benzo-expanded base pair size is too large for the natural helix. However, when all base pairs are expanded (Fig. 2\n), xDNA and yDNA form highly stable, sequence-matching double helices [56]. In one case (xDNA) the idea is benzo-fused design of pyrimidines that would match a theoretical geometric expansion of purines and thus allow for the combination of expanded versions of A, G, C, and T with the natural base complements and allow them to form a regular helix. This leads to the new deoxyribosides dxT and dxC, and to the complete set of expanded DNA (xDNA) nucleosides. The base pair designs for xDNA are in some respects closely analogous to those of DNA, but are also different in important ways. Natural DNA sequences are composed of one of four letters at each position, and involve purines paired with pyrimidines. In xDNA, benzopurines are paired with pyrimidines and benzopyrimidines with purines, and there are eight components of xDNA with four types of ring systems. In a second case (yDNA) the helix is widened also through a different way of increasing the size of a base pair by benzo fusion. This results in different tautomeric preferences of the resulting y bases. Despite these alterations in the resulting structure, the resulting NAs are functional [57] and thus xDNA and yDNA are candidates for components of new, operative genetic systems.\n\nAnother attempt to come up with unnatural nucleotides focuses on the backbone or the outgoing motif of the DNA. Originally this research was driven by the question of how life evolved on earth and why RNA and DNA were selected by (chemical) evolution over other possible nucleic acid structures [58]. Systematic experimental studies aiming at the diversification of the chemical structure of nucleic acids have resulted in completely novel informational biopolymers. Fig. 3\nshows examples in which the poly-P-deoxyribose polymer that holds the sequence of bases in DNA has been replaced by alternative carbohydrates with a different number of carbon atoms. Although the genetic information is still stored in the four canonical base pairs, natural DNA polymerases cannot read and duplicate this information. In other words the genetic information stored in XNA is invisible to the information-processing machineries of natural cells and therefore useless to DNA-based organisms. Alternatively (or in addition) one can incorporate one or more non-natural variants of the canonical nucleotides to the XNA sequence. These scenarios are in principle optimal to keep the distance between the engineered and the existing biological world [52]. To this end it is imperative that no natural polymerase can convert DNA into XNA, or DNA into XNA, as long as the genetic information is still encoded in 4 bases using triplet encoding. Orthogonality of information storage will likely come about through a series of sequential small steps and developments. In a nutshell, the most important challenges to be solved before a XNA-based safe organism can exist in vivo include: (i) chemical synthesis of single stranded XNA, (ii) the production of hosts that are auxotrophic for synthetic xeno nucleotides, using either variants of the four canonical bases (xAMP, xGMP, xTMP, xCMP) or altogether different bases, (iii) defining and biosynthesizing highly specific enzymes that can handle XNA replication (i.e. XNA polymerase, helicase, ligase, single strand binding proteins), transcription (RNA polymerase, XNA-binding transcription factors) and possibly also XNA-binding nucleoid-associated proteins to form large scale genome structures, (iv) replacing DNA-based genome by XNA-based counterparts and (v) possibly removing ATP, CTP and GTP from cell physiology. While the tools and the knowledge to entirely implement such an agenda maxima are not yet in sight, many advances can be recorded in the right direction. One bottleneck for implementing a biological system based on XNA is indeed the availability of XNA-dependent replicative polymerases. This requires a separate effort for developing enzymes specific for such alternative substrates, as naturally occurring counterparts incorporate xenobiotic nucleotides rather poorly [59], [60], [61]. That the entire endeavour of substituting DNA by XNA may not be impossible after all is hinted at by a recent work of Marliere\u2019s and Mutzel\u2019s Laboratory [62] on the production of an E. coli strain whose DNA is composed of canonical A, C and G nucleotides but has the synthetic thymine analogue 5-chlorouracil instead of T in the corresponding positions of the sequence. These cells are, expectedly, dependent on externally supplied 5-chlorouracil for growth, but otherwise they look and behave as normal E. coli. This approach thus sets two concomitant firewalls for any interaction with other bacteria, because the strain is auxotrophic for a non-natural chemical and it contains a form of DNA that cannot be deciphered by other organisms [50], [63].\n\nExperiments replacing or enlarging the genetic alphabet of DNA with unnatural base pairs led for example to a genetic code that instead of four bases ATGC had six bases ATGCPZ [64], [65], [66]. In another study 60 candidate bases (that means 3600 base pairs) were tested for possible incorporation in the DNA [67]. These unnatural bases are not recognized by natural polymerases, and one of the challenges is to find/create novel types of polymerases that will be able to read the unnatural constructs. At least on one occasion a modified variant of the HIV-Reverse transcriptase was found to be able to PCR-amplify an oligonucleotide containing a third type base pair [64], [66], [68], [69]. Other examples of this sophisticated chemistry include e.g. novel hydrogen-bonding patterns between analogues of canonical bases [54], [65], [67], [70], [71], [72], [73] and combination of an extended genetic code (see below; [74], [75], [76], [77]) with adequate novel polymerases [66], [78], [79] could certainly lead to the next step towards implementing an artificial genetic system in vivo [63], [64]. Although not fully functional at the moment, initial in vitro experiments demonstrate that this area of research has long left the realm of science-fiction, and systems will eventually become operative.\n\nOne of the goals of contemporary SB is the development of orthogonal systems (from molecules to entire organisms) that determine traits typical of live objects but do not interact with the existing biological frame [80]. In reality, full orthogonality in engineered biological systems (as opposed to, e.g. electronics or mechanical engineering) is not yet at hand. The most conspicuous cases of such forward-designed orthogonality are those in which otherwise unused codons are reassigned to match transfer RNA (tRNA)/aminoacyl-tRNA synthetase pairs in order to either change codon specificity or expand the number of genetically encoded amino acids towards non-natural specimens. This leads to the in vivo incorporation of the non-natural amino acids into proteins in response to non-sense codons [77], [81] (see [82] for a review). Cells endowed with such aminoacyl-tRNA synthetases are thus able to read mRNA sequences that make no sense to the existing gene expression machinery [83]. These modified genetic codes allow the design of growingly complex circuits and systems that hardly interact with the host [55], [84]. But can one organism be completely redesigned to bear new codon usages? More recently, Isaacs et al. [85] reported the replacement of all 314 TAG stop codons present in the genome of E. coli with synonymous TAA codons, thereby demonstrating that massive substitutions can be combined into higher-order strains without lethal effects. The possibility of reassigning the function of large number of triplets opens the perspective to have strains that cannot exchange productively any information with the natural biological world. Moreover, these approaches are by no means limited to bacteria and can also be implemented in yeasts and in mammalian cells in culture [86], [87]. Just to show how far the issue of expanded genetic codes can go, a wealth of current efforts are directed to develop biological alphabets based on quadruplet codons [74], [88]. The key in this case is the evolution of a specialized orthogonal ribosome that efficiently decodes a series of 4-base codons, providing an equally orthogonal messenger RNA, which it specifically translates. By creating mutually orthogonal aminoacyl-tRNA synthetase-tRNA pairs and combining them with such ribosomes one can direct the incorporation of a large number of distinct unnatural amino acids and thus bring about the synthesis and the synthetic evolution of unnatural polymers in cells [74]. By the same token, there is no reason why other codons with still more bases cannot be entertained. As mentioned above, a recent work from Benner\u2019s Laboratory [89] proposes the expansion of the 4-letter standard nucleotides (G, A, C, and T) with two additional non-standard compounds (Z and P). To this end mutant polymerases and PCR conditions were developed that amplified a wide range of GACTZP DNA sequences having multiple consecutive unnatural synthetic genetic components. The field is expected to bloom in the next few years.\n\nCould all these developments be merged in a single gene expression flow that is alien to our familiar biology? The best scenario from the point of view of containment would be that of cells in which DNA has been replaced by XNAs and endowed with an enzymatic machinery \u00e1 la carte (XNA polymerases) for replication. In a further step towards orthogonalization of non-natural microbes, one could envision combining XNAs with matching RNA polymerases that produce transcripts with expanded or alternative genetic codes that could incorporate non-natural amino acids to proteins as well. While these developments are still in a very embryonic stage, they can inspire a research agenda that merges efficacy and safety of synthetic organisms in the same lot to establish a fully functional genetic firewall [52]. The more orthogonal one system is (from parts to whole organisms), the more predictable and less risky it could be [50].\n\nVirtually all containment systems discussed in the last decades are based in what we could call familiar biochemistry. However, it may well happen that living systems (or at least biological objects) can be assembled with a different set of building blocks or with an alternative relational logic. One intriguing possibility put forward by Doron Lancet is that biological information can be stored and replicated through the lipid composition of vesicles instead of the physical order of bases in nucleic acid sequence [90], [91], [92]. Such entities may be viewed as having a sort of compositional genome which could propagate biological information without any genetic apparatus [91]. It is not impossible to entertain the future creation of life forms based on such mechanism of bearing information, which would be alien to our familiar genetics and thus unable to interact with existing living entities. A second scenario could be that of mirror life, the underlying basis being the viability of biological objects with an alternative chiral biochemistry. At the 2010 Astrobiology Science Conference in League City (Texas), for example, the session Origins of molecular asymmetry, homochirality and life detection discussed the possibility of mechanisms for chiral symmetry breaking and amplification (http://www.lpi.usra.edu/meetings/abscicon2010/pdf/sess503.pdf). Specifically, Paul Davies speculated that Earth hosts, or has hosted, more than one form of life where amino acids and sugars may have had reversed the chiral signature that is familiar to standard life. While not much hard data on this subject can be found in the scientific literature one can find a good deal of activity in the field in various Synthetic Biology laboratories (http://arep.med.harvard.edu/SBP/), let alone many references to it on the web and the press (http://www.wired.com/magazine/2010/11/ff_mirrorlife/all/1). The outcome of a possible encounter between molecular mirroring forms of life is difficult to predict, but it could well happen that they were completely unable to talk to each other and thus set a firewall to any possible risk. Finally, one could think on synthetic organisms with a genomic chassis programmed for a limited life span. This concept, pioneered by Danchin [93], [94], [95] is based on the hypothesis that cells have a sort of Maxwell\u2019s demon\u2019s genes that allow cells to discriminate between old and new proteins during division, thereby ensuring that newborn cells are formed with entirely fresh materials and the aged building blocks earmarked for destruction. These genes could include e.g. the ATP-dependent subunit of Clp prokaryotic proteases that choose the right functional objects, and destroy the rest. When these proteins are challenged by antibiotics such as acyl-depsipeptides they randomly degrade proteins and this kills the cells. Mutants defective in these functions may therefore age faster and thus be less risky from the point of view of their environmental release.\n\nThe onset of Synthetic Biology and the possibility of designing chromosomes \u00e1 la carte borne by virtually non-natural microorganisms with chemically produced genomes re-enacts many of the questions raised for the last 35\u201340 years on the environmental impact of recombinant DNA technologies. One current matter of apprehension involves the risks for health and the environment that could be foreseen from the accidental or conscious liberation of microbes that carry synthetic genomes. This article advocates the question to be framed on the already extensive history and wealth of data on the design, performance and risk studies made in the US and Europe on GEMs destined for in situ bioremediation under non-contained conditions. The behaviour of such agents provides a suitable background for tackling the uncertainties raised by the new synthetic microbes that are in sight. The homeostasis of the existing microbial communities gives modified or altogether artificial bacteria a severe disadvantage to prosper as free entities in the natural ecosystems [5]. The remaining tiny risk of GEMs escaping and establishing themselves in the environment is now tackled through xenobiological systems. Although it is early days, xenobiology might solve the ultimate challenge of providing re-liable Certainty of Containment via a genetic firewall.\n\nNew and emerging technologies, described as having a major impact on society, are frequently accompanied by a rethoric firework of hope, hype and fear. Synthetic Biology, openly declaring the pursuit of a scientific and technological agenda that tries to overcome the natural state of affairs, is no exception. But despite the new language and the fresh metaphors that surround SB, many questions on safety and security of microorganisms bearing synthetic genomes have been raised before in connection to Genetic Engineering. This provides a solid basis to leave behind those safety issues that were thoroughly addressed and answered in the past, and tackle the authentically novel safety challenges [96], [97]. Finally, we advocate responsibility of Synthetic Biologists when communicating these sensitive issues. Scientists involved in the development of engineered bacteria have frequently complained about the negative public perception and ensuing regulations that their research might be constrained by, while at the same time more or less surreptitiously fuelling and provoking such a controversy themselves. One episode of this sort is recounted in detail by his protagonist, Beckwith, in his autobiography [98]. The same day of 1969 that he published a paper in Nature describing the first physical isolation of the DNA segment of a gene, he held an extraordinary press conference in which he warned society of the risks that this type of research could embody and the danger of discrimination and control, including the genetic engineering of human beings. Needless to say that the press conference received international media coverage. We see this pattern being re-enacted over and over, to the present debate on Synthetic Biology and synthetic genomes. Raising awareness on one\u2019s research topic by playing on the hopes and fears of other people will bring attention on the short term, but could turn out to be a boomerang over the long run. GMO biotechnology in Europe, the role of scientists, industry and politics, and its reception in the European public is one of the most expensive case studies to learn from. For our current endeavour we recommend to go ahead step-by-step, with enough precaution or prudent vigilance\n[96], [99], [100] being responsive to feedback from other scientists and non-scientists alike. That way, we believe, the genetic firewall will not fire back."}