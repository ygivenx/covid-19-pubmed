{"title": "Implications of Host Genetic Variation on the Risk and Prevalence of Infectious Diseases Transmitted Through the Environment", "body": "The model developed in this study is a stochastic compartmental \u201cSLDCRS\u201d model, in which animals of a closed population of constant size (i.e., no birth, removal, or death) may progress through different disease states [susceptible (S), latent infected (L), diseased (D), asymptomatic carrier (C), recovered (R), susceptible (S)] over the time course of infection (Anderson and May 1991). For animal i in disease state X, a value TX(i) is defined, describing the expected time that animal i spends in category X given progression to X at a previous time step. Infection occurs through environmental contamination, which is quantified by the density of infectious pathogen shed by infected animals in categories D and C into the environment. Each individual has its predefined shedding rates \u03baD(i) and \u03baC(i) associated with the diseased and the carrier state, respectively, which are defined as the number of bacteria shed into the environment per unit area per day. It is assumed that the total amount of infectious pathogens shed by individual animals is cumulative and uniformly dispersed, so that all animals have equal exposure to the pathogens in the contaminated environment. Infectious bacteria are assumed to have a mean survival time in the environment of TB days.\n\nThe epidemic process is simulated as a Poisson process, i.e., as a series of random events in continuous time that occur independently of one another. The possible event types are the progression of an animal from one disease state to the next (i.e., S \u2192 L, L \u2192 D, D \u2192 C, C \u2192 R, and R \u2192 S), as well as pathogen shedding by an animal of category D or C, and the decay of environmental pathogens. Both the time between successive events (i.e., the interevent time) and the probability of a specific event to occur are determined by the set of individual transition rates rX(i)=1/TX(i) from a state X (X = S, L, D, C, or R) to the next state in the SLDCRS sequence, by the pathogen shedding rates \u03baD(i) and \u03baC(i) and by bacterial decay rates rB = 1/TB as follows: in the population as a whole we define the average rate of infection of susceptible individuals as r1=\u2211i\u2208SrS(i)E, where E is the environmental bacterial load, rS(i)=1/TS(i), and the sum is taken over all susceptible individuals. The transition events corresponding to other states (i.e., X = L, D, C, or R) occur at average rates rk=\u2211i\u2208XrX(i), for k = 2, 3, 4, 5, where rX(i)=1/TX(i) was defined above. Similarly, shedding occurs at average rates r6=\u2211i\u2208DkD(i) and r7=\u2211i\u2208CkC(i), respectively, depending on whether bacteria are shed from diseased individuals (D) or asymptomatic carriers (C), and bacterial decay occurs at an average rate r8 = rBE. Note that event rates change during the time course of the disease outbreak as individuals move between compartments and environmental contamination changes.\n\nThe simulations of the stochastic model were carried out using Gillespie\u2019s direct algorithm (Gillespie 1977). In short, the interevent times are sampled from an exponential distribution with parameter R=\u2211k=18rk (Renshaw 1991). The specific event type that then occurs is obtained by drawing a random variate from a discrete distribution with probability p(k) = rk/R corresponding to event type k, k = 1\u20138, as defined above. Once the event type has been chosen, a similar sampling process is applied to determine the affected individual on the basis of its relative rate compared to that of other individuals belonging to the same category.\n\nThe model was parameterized for footrot in sheep, with Dichelobacter nodosus bacteria as the primary infectious agent (Egerton 2000). Various studies have demonstrated that the progress of infection follows that described by a compartmental SLDRCS model (Sinclair 1957; Abott 2000). In the benchmark model there was no variation between individuals in any of the host-specific epidemiological parameters (TS, TL, TD, TC, TR, \u03baD, and \u03baC) defined above. Parameter values for the benchmark model were adopted, where possible, from a recent deterministic epidemiological model of footrot (Nieuwhof et al. 2009), as listed in Table 1.\n\nThere are no estimates of the duration of the asymptotic carrier state in the literature. However, evidence exists that only a small proportion of animals per flock become asymptomatic carriers (Depiazzi et al. 1998). Since the benchmark model allows only all or no animals to become carriers, a short duration of 1 day was assumed for this state (TC = 1 day). This conservative approach was chosen to prevent overestimation of the influence of asymptomatic carriers on epidemiological characteristics.\n\nFootrot prevalence and severity are strongly dependent on environmental conditions affecting bacterial survival (Abbot and Egerton 2003; Conington et al., 2010). Different environmental conditions were modeled using different values for the bacterial survival time TB (Table 1).\n\nField studies have shown that host genetic factors influence an animal\u2019s susceptibility to footrot as well as the speed of recovery and duration of immunity and that these traits are of complex inheritance; i.e., genetic variation is best described by continuous distributions (Raadsma et al. 1994, 1995; Nieuwhof et al. 2008). For ease of interpretation, variation between hosts was considered separately for each host-specific epidemiological trait listed in Table 1 (i.e., TS, TL, TD, TC, TR, \u03baC, or \u03baD). A heritability of 1 was assumed for all traits; i.e., it was assumed that environmental variation is fully represented by the stochastic nature of the model. Values for the host-specific epidemiological trait in question were drawn from a two-parameter continuous gamma distribution with shape parameter \u03b1 and scale parameter \u03b8 [i.e., probability density function is x\u03b1\u22121(exp(\u2212x/\u03b8)/\u0393(\u03b1)\u03b8\u03b1)]. The individual transition rates between successive disease states in the epidemiological model are then the inverse of the corresponding gamma-distributed time periods. Hence, for TX \u223c Gamma (\u03b1, \u03b8), rX = 1/TX \u223c InverseGamma (\u03b1, 1/\u03b8). Gamma distributions can adopt a variety of shapes, such as concave (\u03b1 < 1), nearly symmetric (\u03b1 large), or unimodal and strongly right skewed (\u03b1 \u2248 1). The shape of the probability density function is determined by the shape parameter \u03b1, and the coefficients of variation, dispersion, skewness, and kurtosis, are 1/\u03b1, 1/\u03b1, 2/\u03b1, and 1/\u03b1, respectively. Smaller values of \u03b1 thus indicate greater heterogeneity as well as a greater degree of asymmetry and peakedness. As an example, supershedders in a disease to which the \u201c20/80 rule\u201d (20% of the population contributes 80% of the entire transmission potential) applies (Woolhouse et al. 1997; Matthews et al. 2006) can be represented as the top 20% from the right-hand tail of a gamma distribution for \u03baD or \u03baC with shape parameter \u03b1 = 0.25. From a genetic point of view, the shape of the gamma distribution reflects the genetic architecture of the trait. For example, an infinitesimal genetic model assuming that the trait in question is affected by a large number of genes each with an infinitesimally small effect, would correspond to a symmetric distribution, whereas the presence of few loci with large effects on the trait of interest and an unbalanced allele frequency at these loci in the population, or dominance, could give rise to a skewed distribution.\n\nThe central question addressed in this study is how the source of genetic variation and the underlying genetic architecture affect disease risk and severity in the population. For this purpose, distributions for each epidemiological trait for which variation was assumed were generated with the same trait mean (\u03b1\u03b8) as that of the benchmark model for homogeneous populations (Table 1). For each of these epidemiological traits, distributions with a variety of shapes were generated by choosing \u03b1 = 0.25, 1.25, 2.5, and 12.5 (Figure 1). Hence, to assess the influence of the source of genetic variation, outputs corresponding to distributions for different epidemiological traits of the same shape were compared (i.e., the same value of \u03b1 and a different mean value \u03b1\u03b8). For assessing the role of different genetic architectures underlying a particular epidemiological trait, outputs corresponding to distributions of different shapes for the particular epidemiological trait in question were compared (i.e., different \u03b1 and the same mean value \u03b1\u03b8).\n\nIn the simulations, infection was introduced into an initially fully susceptible population through one latent infected individual. The simulations were carried out for closed populations with no birth or death or removal processes and for which disease progression was not influenced by treatment effects. The model was run for a simulated time period of 3 years, after which the infection either died out or, as indicated by preliminary results, converged to equilibrium prevalence in all simulated scenarios. Ten thousand replicates were generated for each parameter combination.\n\nThe impact of host heterogeneity (in terms of both the source of variation and the underlying genetic architecture) was assessed by four different epidemic characteristics: first, we calculated the probability of the disease to exist in the population at different time points (calculated as the proportion of replicates with at least one infected individual at the time in question, where infected individuals are individuals of categories L, D, or C), providing thus predictions for the risk of disease establishment and persistence over the duration of at least 3 years. Second, we assessed disease prevalence patterns over time. Third, we recorded disease incidence, i.e., the total number of new infections over the simulated time period, as a measure of disease severity. Finally, as the stochastic epidemiological model provides continuous information of the disease state of every individual over time, observable phenotypic infection characteristics, such as the frequency of infections (i.e., number of times that the individual moved from state S to L) and the number of days that an individual was infected (i.e., spent in categories L, D, or C) throughout the simulated 3 years, were calculated for each individual. These characteristics may constitute the estimated or observed phenotypes in genetic analyses of field data and hence form the bases of selection or management strategies. We therefore assessed how population averages and variances of these observable infection characteristics are affected by variations in epidemiological input parameters. The averages and variances were calculated as averages over all individuals, over all replicates avoiding stochastic extinction, and refer to the last year in the simulations, i.e., when the system reached a steady state.\n\nFor environmental conditions favoring bacterial survival, footrot either disappears from the population within a few weeks after introduction or becomes persistent (Figure 2, A, C, and E and supporting information, File S1). When bacterial decay is fast, the risk of footrot establishment and long-term persistence is considerably lower than when bacterial decay is slow, except for \u03b1 = 0.25. For instance, for homogeneous populations the probability of footrot persistence, estimated by the proportion of replicates predicting positive prevalence at the end of the simulated time period, was 0.86 when TB = 12 days (Figure 2, A, C, and E), 0.31 when TB = 1.5 days (Figure 2, B, D, and F), and 0.03 when TB = 1 day (not shown).\n\nPositively skewed variation in host susceptibility (TS) tends to increase the probability of footrot establishment (Figure 2, A and B). The influence is strongest for populations containing a large proportion of highly susceptible individuals (low \u03b1). For \u03b1 = 0.25, the risk of footrot persistence for \u22653 years increases from 0.86 for homogeneous populations to 0.96 when TB = 12 days (Figure 2A) and substantially more, i.e., from 0.31 to 0.95 when TB = 1.5 days (Figure 2B). As \u03b1 increases, the difference between heterogeneous and homogenous populations for the probability of disease establishment decreases, and for \u03b1 = 12.5 the difference is negligible (Figure 2, A and B).\n\nPositively skewed variation in the duration of the diseased period (TD) and in shedding rates (\u03baD) tends to decrease the risk of footrot establishment in the population. As for variation in susceptibility (TS), the influence is substantially stronger for low \u03b1 and becomes negligible for high values of \u03b1 (Figure 2, C\u2013F). However, in contrast to what is observed for host variation in susceptibility, variation in host infectivity (TD or \u03baD) can substantially affect the risk of disease establishment in all environments. For example, for TB = 12 days, the probability of disease persistence for \u22653 years decreases from 0.86 for homogeneous populations to 0.46 and 0.45, for variation in TD and \u03baD with \u03b1 = 0.25, respectively (Figure 2, C and E). For TB = 1.5 days, the risk of footrot persistence in populations with the same degree of variation in TD and \u03baD, respectively, decreases from 0.31 to 0.08 (Figure 2, D and F) and becomes virtually zero for TB = 1 day (i.e., disease persisted in <5 of 10,000 replicates; results not shown). There is little difference between the probabilities of disease establishment associated with host variation in TD or \u03baD (Figure 2, C\u2013F). Variation in other epidemiological parameters (TL, TC, \u03baC, and TR; Table 1) was, however, found to have little effect on the risk of disease establishment, regardless of the shape of variation and of the simulated environmental conditions.\n\nFigure 3 shows the predicted influence of host variation on footrot prevalence over time in populations avoiding stochastic disease extinction. Average prevalence profiles, and the impact of host variation on them, depend strongly on bacterial survival rates. If bacteria survive in the soil for a long time, average footrot prevalence rapidly increases toward its peak within a few weeks after the introduction of the infection into the population, after which it gradually declines toward an endemic equilibrium (Figure 3, A, C, E, and G). In contrast, when bacteria decay fast, prevalence in populations in which footrot could establish generally increases gradually toward an endemic equilibrium with a lower average proportion of animals infected than for slow bacterial decay (Figure 3B shows the trend toward the equilibrium).\n\nThere is little difference in the prevalence patterns of homogeneous and heterogeneous populations when underlying distributions are symmetric and dispersion is moderate (e.g., \u03b1 = 12.5 in Figure 3). However, skewed distribution with great dispersion for host susceptibility (TS) and the duration of immunity (TR) tend to increase disease prevalence (Figure 3, A, B, E, and F), whereas similarly skewed distributions for the duration of the diseased period (TD) and for the corresponding shedding rate (\u03baD) tend to decrease its prevalence (Figure 3, C, D, G, and H). The degree of influence depends strongly on the environmental conditions affecting pathogen survival. For example, if bacteria decay slowly (e.g., TB = 12 days, Figure 3A), variation in host susceptibility may affect disease progression in the population at the early stages, but has little effect on the peak prevalence or on the prevalence at the endemic equilibrium. In contrast, when bacterial decay is fast (e.g., TB = 1.5 days, Figure 3B), variation in host susceptibility affects prevalence levels at any stage of the epidemics, with greater dispersion generally leading to a more rapid increase in prevalence, higher prevalence levels, and faster convergence toward the equilibrium.\n\nVariations in the duration of the diseased period (TD) and shedding rate (\u03baD), as well as in the duration of immunity (TR), affect predicted disease prevalence only in environmental conditions favoring bacterial survival and when dispersion and skewness are sufficiently strong (Figure 3, C\u2013H). The presence of a large proportion of individuals with fast recovery (low TD) decreases prevalence levels compared to those observed for homogeneous populations with the same mean recovery rate at any stage of the epidemics (Figure 3C), whereas a large proportion of individuals with little or no immunity (low TR) affect prevalence levels only after prevalence has already peaked (Figure 3E) and variation in shedding rates (\u03baD) affects only peak prevalence (Figure 3G). Variation in the duration of the latent stage (TL) or the asymptotic carrier stage (TC), as well as in the corresponding shedding rates (\u03baC), has little impact on the simulated prevalence patterns (results not shown).\n\nThe influence of host variation on the predicted overall footrot severity, described by the total number of incidences occurring during the simulated 3-year time period, was found to depend similarly on the degree of dispersion and on environmental conditions as was observed for disease risk and prevalence profiles. In particular, when bacteria decay fast (TB low), a large proportion of highly susceptible individuals poses the greatest risk for severe outbreaks, whereas when bacterial decay is slow (TB high), a large proportion of individuals not developing immunity causes the most severe outbreaks. For example, for TB = 1.5 days, the average number of incidences increased from 4387 for homogeneous populations to 13,776 in populations with large variation in TS (\u03b1 = 0.25). The same variation in TR produced on average 5361 incidences. In contrast, for TB = 12 days, the average number of incidences for homogeneous populations and populations with the same large variation in TS were similar (i.e., 16,670 and 17,564, respectively), whereas populations with large variation TR experienced a substantially higher average number of incidences (i.e., 24,465 incidences).\n\nTable 2 shows the predicted impact of host heterogeneity, when expressed in different environments, on the frequencies and number of days that animals are infected per year. As would be expected, average number of infected days and frequency of infections were generally higher in environments with high contamination risk (high values of TB). With some exceptions (outlined below), standard deviations in these characteristics were also higher for higher values of TB, implying that genetic variation is generally more strongly expressed in more infectious environments (Table 2). Also, as would be expected, individual variation in observable infection characteristics generally increases with increasing degree of heterogeneity in the epidemiological parameters. Note that individual variation in observable infection characteristics also occurs for homogeneous population due to stochasticity.\n\nIntroducing heterogeneity shows similar footprints on individual infection characteristics as was observed for the risk of disease establishment and disease severity on a population level: positively skewed host variation in susceptibility and immunity (TS, TR) has a detrimental effect on observable infection characteristics, whereas positively skewed variation in the diseased period (TD) has the opposite effect, and the effect size generally increases with increasing heterogeneity and skewness. Also, heterogeneity in TS mainly affects averages and variances in the observable infection characteristics when environmental contamination is low, whereas heterogeneity in TD and TR shows their impact on infection characteristics in all simulated environments (Table 2). Variation in other epidemiological parameters, in particular shedding rates (\u03baD and \u03baC), was found to have no significant effect on average or variation in host infection characteristics (results not shown). The results in Table 2 also illustrate that variation in a particular epidemiological trait may affect various infection characteristics simultaneously. For example, increasing the proportion of individuals with a short expected duration of the diseased period (i.e., decreasing \u03b1 for TD in Table 2) tends to decrease not only the average number of days per year that individuals are infected, but also, given that infection is a stochastic event, the average frequency of infections per year.\n\nThe important role of host genetic heterogeneity on the spread of infectious diseases has long been recognized, but to our knowledge this study is the first to examine systematically how the type and shape of host variation influence disease risk and severity. Our model results suggest that the impact of host heterogeneity on relevant epidemiological characteristics is largely controlled by the shape of underlying distributions describing individual variation. Whereas differences between homogeneous and heterogeneous populations in predicted epidemiological characteristics are generally subtle and often negligible when heterogeneity is described by symmetric distributions in underlying epidemiological traits, they can be large when underlying distributions are skewed and leptokurtic and dispersion is high. In particular, by representing host heterogeneity with right-skewed gamma distributions, our model results predict that the influence of a large proportion of individuals with low values in a particular trait (e.g., low resistance, low shedding rate, short duration of diseased period, or short-lived immunity) on the disease epidemiology outweighs the influence of a small proportion of individuals with extremely high values for these traits. Thus, the presence of few high shedders does not necessarily increase disease risk if the majority of individuals have low pathogen shedding rates. Also, our results suggest that understanding the source of genetic variation, i.e., identifying the epidemiological trait(s) for which variation occurs, is important when making predictions for epidemiological outcomes and interpreting epidemiological data. Distributions of the same shape for different epidemiological traits often resulted in different predictions for disease risk and severity. The impact of variation in a particular trait depends strongly on the role of this trait in the progression of the disease in the population. For example, both the duration of the infectious period (TD and TC) and bacterial shedding rates (\u03baD and \u03baC) may be considered as measures of host infectivity in field studies. However, whereas shedding rates are incorporated directly into epidemiological models, time periods are usually incorporated through their inverses, as these provide the required transition rates. Hence, even if variation in shedding rates and duration of the infectious period are represented by distributions of similar shape, their impact on the epidemiology can be quite different.\n\nOur study differs from the majority of previous theoretical studies in that it focuses on an infectious disease that is transmitted through an environmental source instead of through direct contact with infectious individuals. This type of disease merits attention as it constitutes a substantial proportion of livestock diseases of major concern for animal and human health. Our results agree with those from studies on directly transmitted diseases in that host genetic heterogeneity can significantly affect the risk of disease emergence (e.g., Hethcote and Van Ark 1987; Lloyd-Smith et al. 2005; Yates et al. 2006) and persistence (e.g., Doeschl-Wilson et al. 2009) and the course of emerging epidemics (e.g., May and Anderson 1988; Springbett et al. 2003; Nath et al. 2008). However, our results provide novel evidence that, for a disease transmitted through the environment, the influence of host genetic variation on disease emergence and severity can strongly depend on environmental conditions. These were found to affect more the degree than the direction of influence. For example, heterogeneity in host susceptibility was found to increase disease risk and severity in all modeled environments but its impact was much stronger when environmental contamination was low (Figures 2, A and B, and 3, A and B). Also, except for populations with large heterogeneity in susceptibility, predicted variances in observable phenotypic infection characteristics were generally lower in these conditions (Table 2), indicating that genetic variation may not be fully observed in environments with low exposure. These results complement the analytical expressions derived by Bishop and Woolliams (2010), which predict that incomplete exposure to infection (e.g., due to low disease prevalence in the population) leads to a downward bias in estimates of genetic variation.\n\nPrevious studies often differ in their predictions for the effect of host variation owing largely to different definitions of host genetic heterogeneity (Springbett et al. 2003; Lloyd-Smith et al. 2005; Yates et al. 2006; Nath et al. 2008). The study closest to ours in terms of definition of host heterogeneity is that of Lloyd-Smith et al. (2005) who modeled host variation in infectiousness for diseases transmitted through direct contact also with continuous gamma distributions. They established that the probability of extinction increases with increasing dispersion and skewness due to the high proportion of individuals contributing very little to the spread of the disease at the early stages. Our model predicts the same trend for diseases transmitted through environmental sources. However, the models of Lloyd-Smith et al. (2005) also predict for populations avoiding extinction more severe outbreaks as dispersion and skewness increase. In contrast, our model predicts that variation in infectiousness, when represented by the duration of the infectious period or by different shedding rates, tends to decrease disease prevalence at all times. The discrepancy could be attributed to the different modes of disease transmission considered in both models. Compared to directly transmitted diseases, the influence of few highly infectious individuals on the spread of disease is diffused when transmission occurs through an environmental source, whose contamination level is the cumulative contribution of all infectious individuals.\n\nSeveral studies on directly transmitted diseases have shown that the impact of genetic variation on the risk of disease emergence depends on to the source of genetic heterogeneity. For example, assuming a finite locus model for genetic variation, with one locus controlling the transmission coefficient (representing host susceptibility to infection) and another locus controlling the recovery rate, Nath et al. (2008) found that genetic variation in the transmission coefficient resulted in higher probabilities of disease emergence, whereas variation in the recovery rate had no such impact. Defining host variation also by few (i.e., two) distinct homogeneous subgroups, Yates et al. (2006) have further shown that variation in host infectivity can greatly reduce the probability of disease emergence. Our model predicts similar trends if host susceptibility, recovery, or infectivity are controlled by multiple loci. However, in contrast to the results of Nath et al. (2008) and our results, Yates et al. (2006) predict that variation in susceptibility will not increase the risk of disease establishment, in particular when bacterial decay is fast. A likely cause for this discrepancy is that Yates et al. (2006) simulated heterogeneous populations by assigning 100 times higher susceptibility to 10% of the individuals than to the rest of the population, thus approximating a negatively skewed distribution in susceptibility rather than the positively skewed distributions considered here. Combining the results of both studies would thus suggest that compared to homogeneous populations with the same average susceptibility, heterogeneous populations with a large proportion of highly susceptible individuals have a higher risk of disease emergence, but the emergence risk will not decrease if the majority of individuals are resistant.\n\nFor diseases where a significant fraction of the population become asymptomatic carriers of the infection, such as footrot in sheep (Depiazzi et al. 1998), a significant role in the persistence of the disease in the population has been attributed to these carriers (Anderson and May 1991). We found, however, that, for the specific set of parameters and distributions used, the presence of few individuals that become asymptomatic carriers over a considerably long time period (i.e., several months) has no significant influence on the predicted epidemic outcomes.\n\nHere we have assumed that individual variation occurs for one epidemiological parameter at a time. Although this approach allowed us to systematically examine the contributions of different sources of variation, in reality genetic variation is likely to occur simultaneously in a number of different epidemiological traits. We also carried out additional simulations assuming variation in more than one parameter at a time. The results are more difficult to interpret, but, in essence, symmetric variation with small dispersion in multiple parameters still led to subtle differences in model predictions between homogenous and heterogeneous populations, whereas high dispersion generally led to larger discrepancies between both types of populations.\n\nSeveral important implications arise from our results concerning both the analysis and interpretation of disease data and the evaluation of control strategies, in particular genetic selection for increasing disease resistance. The latter has long been considered a viable alternative to conventional disease control, but its benefits in terms of reducing disease risk and severity are poorly understood. Our results suggest that accurate predictions of selection response for disease risk and severity require a thorough understanding of the structure of genetic variation, i.e., of the source of genetic variation and the shape of the corresponding distributions. Genetic control strategies in livestock currently focus on exploiting genetic variation in disease resistance as a whole to reduce disease risk and severity. Our results, however, indicate that variation in different epidemiological traits, such as infectiousness or duration of immunity, can influence very differently both disease risk and severity. The question therefore arises whether genetic variation in these alternative traits can be identified using currently available disease data and statistical methods. For example, a recent simulation study (Lipschutz-Powell et al. 2010) demonstrated that classic quantitative genetics models applied to binary incidence data cannot identify genetic variation in infectivity, as an individual\u2019s infectivity is expressed in its group members rather than in the individual itself. Further efforts in more detailed recording of disease data and in the development of adequate genetic models would be required, before this important source of genetic variation can be exploited.\n\nA novel insight emerging from our study is that the shape of genetic variation influences not only disease establishment and prevalence, but also observable infection characteristics of individuals that may constitute the phenotypes in genetic data analysis. Genetic analyses of disease data often assume an underlying normally distributed individual liability for disease prevalence (Robertson and Lerner 1949; Falconer and Mackay 1996). Although this assumption may hold for diseases for which many genes have small additive effects on epidemiological parameters, skewed leptokurtic distributions may be more appropriate for diseases where a number of genes have large effects and allele frequencies are unbalanced or in cases where the relationship between genetic effects and the epidemiological traits in consideration is not linear. Our results suggest that greater attention should be given to the assumptions concerning liability distributions in genetic analyses of disease data, as disease risk and prevalence depend strongly on the shape of the distributions. Wrong assumptions could not only lead to biased estimates of genetic parameters, but also produce wrong predictions for the selection response. Recent advances in genomic studies could provide the opportunity to accurately describe the genetic architecture that defines the shape of the distribution in epidemiological traits of interest.\n\nOur model results further emphasize that the consequences of genetic selection on both the population mean and the variation should be taken into consideration when predicting disease risk and prevalence in future generations. For example, selecting against high susceptibility would not only increase the average disease resistance in future generations, but also decrease the proportion of highly susceptible individuals in the frequency distribution for susceptibility. On the basis of our predictions, the associated changes in the shape of the distribution would decrease the future risk of disease establishment and disease severity much more than would be anticipated from classical quantitative genetics theory alone, which does not account for changes in the distributions\u2019 shapes. Finally, the results of our study elucidate that the choice of the most appropriate selection strategy would depend on the parent and offspring environment. For example, whereas our results suggest that selection against high susceptibility would be the most efficient means to reduce disease risk and prevalence in environments with low contamination risk, selection for fast recovery would be advantageous if the contamination risk is high. Given thus the results from this study, among the next steps to be taken in the genetic control of infectious disease are a more accurate description of genetic heterogeneity among individuals and the development of statistical methods to account for different types and shapes of genetic variation in the analysis of disease data and in genetic\u2013epidemiological models."}