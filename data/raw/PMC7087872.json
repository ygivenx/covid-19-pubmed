{"title": "From single-molecule detection to next-generation sequencing: microfluidic droplets for high-throughput nucleic acid analysis", "body": "Emulsions (or collections of isolated droplets surrounded by a continuous and immiscible carrier fluid) have long been used in chemical and biological experimentation, with the millions of contained droplets serving as isolated vessels in which reactions or assays may be performed (Fig. 1a) (Griffiths and Tawfik 2006). The use of bulk shear forces, although efficient in making large numbers of droplets on short timescales, generates polydisperse droplet populations that prohibit quantitative experimentation (Huebner et al. 2007; Pekin et al. 2011; Juul et al. 2012). Conversely, and as will be shown subsequently, flow-based microfluidic systems can be used to generate similarly large numbers of droplets, but with an unprecedented degree of control over droplet size. These features combined with the facility to adjust the chemical or biological payload at will make microfluidic droplets highly promising vehicles for large-scale biological experimentation.\n\n\nAn important application of droplet-based microfluidic systems is in the analysis of nucleic acids. Indeed, recent developments have seen the establishment of robust and high-throughput genotyping assays and expression analysis at the single-cell level (Macosko et al. 2015; Zeng et al. 2010; Turchaninova et al. 2013; Eastburn et al. 2013). A key feature in this respect is the ability to perform rapid DNA amplification (via the polymerase chain reaction or PCR) within millions of individual droplets in a parallel fashion (Tewhey et al. 2009; Markey et al. 2010; Hindson et al. 2011). Droplet-based PCR involves the partitioning of a large reaction volume into millions of smaller volumes, which statistically will either be empty or will contain a single copy of target DNA. Subsequent thermal cycling of all droplets within a sample yields signal only in droplets that originally contained DNA. Accordingly, quantitation is ensured via a simple process of counting. This feature combined with reduced reagent consumption and efficient heat transfer, engenders a range of experiments (such as rare mutation detection and bias-free amplification) that are simply not possible in other formats (Kalinina et al. 1997). The realisation of formats for droplet-based PCR (Griffiths and Tawfik 2006; Williams et al. 2006; Nakano et al. 2003) has had an immense impact on single-molecule PCR (Kumaresan et al. 2008; Diehl et al. 2006) and has already become a critical component of next-generation sequencing technologies (White et al. 2009; Margulies et al. 2005). At a basic level, the utility of droplet-based microfluidic systems in biological experimentation stems from the ability to control and manipulate droplets in a passive, reproducible and rapid fashion. Indeed, and unsurprisingly, such platforms have also been used to good effect in many other applications, including nanomaterial synthesis (Lignos et al. 2016), kinetic analysis (Lignos et al. 2015; Bui et al. 2011), drug delivery (Xu et al. 2009), high-throughput screening (Sjostrom et al. 2013) and single-cell analysis (Brouzes et al. 2009).\n\nIn the current review, we aim to survey recent developments in the use of droplet-based microfluidics for nucleic acid analysis, first highlighting key areas where such microfluidic tools have had significant effect and secondly proposing related applications where microfluidic technologies may have impact in the short to medium term. We also note that although essential background knowledge, such as the manner in which droplets are formed and manipulated, will be introduced, more detailed and comprehensive analyses of droplet-based microfluidic systems can be found elsewhere (Niu and deMello 2012; Oh et al. 2012; Choi et al. 2012; Baroud et al. 2010; Kelly et al. 2007; Shembekar et al. 2016; Price and Paegel 2016; Collins et al. 2015).\n\nEmulsions formed using bulk shear forces on the macroscale have long been used to good effect in areas such as polymer chemistry (Ugelstad et al. 1973), cosmetic formulations (Linn and West 1989) and complex food systems (Garti 1997). Despite their utility, the challenges associated with controlling droplet size, composition and size distributions are immense, making their use in quantitative experimentation demanding. Conversely, droplets (with volumes ranging from femtoliters to nanoliters) can be generated in a variety of ways within microfluidic systems. Critically, passive strategies that leverage geometrical variations of fluidic structures can be used to transform arbitrary volumes of fluid into defined sub-nanoliter droplets at kHz to MHz rates (Shim et al. 2013).\n\nAt a simple level, the most common strategies for droplet production involve the use of cross-flow structures (T-junctions) (Thorsen et al. 2001), flow-focusing geometries (Anna et al. 2003), co-flow structures (Umbanhowar et al. 2000; Cramer et al. 2004) and step emulsification (Sugiura et al. 2001; Kobayashi et al. 2005). In planar, chip-based systems immiscible aqueous and oil streams confined within microfluidic channels are brought together via external pressure (typically using syringe or pressure pumps),1 with droplets (or plugs) being formed at the point of confluence. Although the droplet generation mechanism is quite different in each these geometries, all involve the establishment of an interface between co-flowing, immiscible fluids, followed by self-segregation of one of the fluids into droplets that are surrounded by the other fluid. Interestingly, variations on the above strategies have been used to good effect (Ding et al. 2014; Dangla et al. 2013). For example, Dangla et al. (2013) exploited gradients of confinement to realise highly robust droplet formation (Fig. 1b). Using this method, droplets are formed due to curvature imbalance along the interface, without the need for shear associated with continuous phase flow. This means that droplet size is primarily determined by the gradient geometry and is insensitive to fluid properties. Unsurprisingly, such a \u201cpump-free\u201d droplet generation method (Fig. 1b) has wide ranging utility and potential in point-of-care or point-of-use applications.\n\nControl of droplet size is of obvious importance when performing quantitative experiments; however, the ability to \u201cload\u201d droplets with multiple reagents at user-defined concentrations is even more critical. Introduction of the dispersed phase through a branched inlet channel allows for the direct combination of multiple laminar streams just prior to droplet formation (Song et al. 2003), with the relative concentration of each reagent being defined by the associated volumetric flow-rate ratios (Guo et al. 2012). Notably, this strategy has been effective in creating droplet barcodes, in which co-encapsulation of multiple fluorophores spectrally encodes droplets and yields uniquely identifiable signatures (Ji et al. 2011; Gerver et al. 2012). The passive production of droplets is simple, quick and efficient, however, limited in its ability to independently manipulate droplets in a dynamic and bespoke manner. In this respect, active methods show clear utility in creating user-defined droplets in a \u201cdroplet-on-demand\u201d fashion. Common actuating sources for such purposes include pneumatic pressure (Unger et al. 2000; Willaime et al. 2006; Zeng et al. 2009), mechanical forces (Kim et al. 2012), electrical fields (Link et al. 2006), magnetic fields (Vekselman et al. 2015), acoustic waves (Collins et al. 2013), optical traps (Lorenz et al. 2006) and thermal gradients (Baroud et al. 2007). For example, Rane et al. (2015) used a pneumatic valve-based architecture to assemble combinational populations of enzyme-substrate droplets. Specifically, 650 unique combinations were programmed and generated in a droplet train in a highly reproducible manner. However, it should be remembered that active methods typically produce droplets at low generation frequencies and require the use of complex control equipment. Accordingly, the choice of droplet generation method should be made on the basis of the specific experimental requirements.\n\nSubsequent to their generation, droplets need to be manipulated in ways that mimic the standard analytical procedures used on the bench top. Fortunately, a wide range of (both passive and active) functional components have been presented for operations that include droplet merging (Niu et al. 2008; Deng et al. 2013; Mazutis and Griffiths 2012; Akartuna et al. 2015), dilution (Niu et al. 2011; Sun and Vanapalli 2013), dosing (Abate et al. 2010; Chen et al. 2008), splitting (Link et al. 2004; Gao et al. 2016), pairing (Ahn et al. 2011; Bai et al. 2010), sorting (Baret et al. 2009; Nam et al. 2012; Cao et al. 2013), trapping/releasing (Wang et al. 2010; Korczyk et al. 2013; Courtney et al. 2017), counting (Boybay et al. 2013; Yesiloz et al. 2015; Kim et al. 2012) and incubation (Huebner et al. 2009; Wen et al. 2015). An instructive example in this respect was reported by Hatch et al. (2011), who used successive bifurcations to split single droplets into 256 daughter droplets in a rapid and passive fashion (Fig. 1c). Using such a strategy, over one million droplets (that are either empty or contain one copy of target DNA) could be generated in 2\u20137 min. Droplet populations formed in this manner could be subsequently packed into on-chip storage chambers and thermally cycled for digital PCR analysis (Hatch et al. 2011). Conversely, Eastburn et al. (2013) reported a powerful and robust (active) method, termed picoinjection, which utilises a pressurised microchannel and periodic electric field to inject a controlled volume of reagent into a moving droplet. Picoinjection has proved to be immensely useful in a range of complex, droplet-based assays, being compatible with common biological reagents such as nucleic acids and enzymes.\n\nThe ability to link functional components within integrated and sequential workflows has been a key reason why droplet-based microfluidic systems have proved so advantageous in biological experimentation (Brouzes et al. 2009; Pan et al. 2011; Cho et al. 2013). Put simply, complex chemical and biological assays can be performed in a rapid and efficient manner. In this respect, Lan et al. (2016) assembled an elaborate workflow that leverages short-read DNA sequencing to obtain long and accurate sequence reads (Fig. 2a). Central to this process was the use of unique barcodes to label long-DNA molecules, thus allowing short-reads of breakage fragments to be accurately reassembled. Functional operations within such a workflow included droplet generation, thermal cycling, splitting, pairing/merging, incubation, triple-droplet pairing/merging, splitting, pinched-flow size sorting, and secondary thermal cycling. Significantly, such an approach enables accurate sequencing up to 10 kb, and opens up new opportunities for the identification of rare mutations inaccessible to conventional sequencing.\n\n\nIn most situations, it is desirable that droplets maintain their size and composition over extended periods of time. Long-term stability of droplets is almost exclusively facilitated by the use of appropriate surfactants, which act to inhibit droplet coalescence by stabilising the interface between the immiscible phases. Surfactant molecules are normally mixed into the continuous phase, and upon contact with the discrete phase self-organise at the interface.\n\nAn excellent review of droplet surfactants can be found elsewhere (Baret 2012), providing a comprehensive discussion of surfactant selection. However, in the current context, some key issues are worthy of discussion. First, although many oils and organic solvents can be used as carrier fluids in droplet-based microfluidic systems, when performing nucleic acid assays choices are somewhat restricted due to biocompatibility requirements and the need to exclude biological impurities. Mineral oils and perfluorinated oils (such as HFE-7500, FC-40 and FC-70) are most two frequently used. When using mineral oils droplets can be efficiently stabilised by Span 80 (sorbitan monooleate) and Abil EM 90 (a non-ionic, silicone-based emulsifier) (Williams et al. 2006; Sch\u00fctze et al. 2011; Bian et al. 2015). Nevertheless, due to the prevalence of fluorinated oils as carrier fluids (because of their excellent biocompatibility and high gas permeabilities), fluorosurfactants (perfluoropolyethers containing hydrophilic head groups), such as perfluoropolyether-polyethylenoxide triblock copolymers, have proved to offer exceptional long-term stabilisation of droplets in a range of situations. Second, droplet size plays a critical role in emulsion stability, with the existence of thermodynamically and kinetically stable regions with respect to droplet radii (Kabalnov 2001). Indeed, although fluorosurfactants can stabilise droplets (with diameters on the tens of microns scale) for weeks at room temperature (Holtze et al. 2008), unless absolutely essential droplets should be processed and assayed on the shortest appropriate timescales. Put simply, when droplets are in close proximity for long periods of time (e.g. when packed in an incubation chamber) undesirable mass transfer between droplets will occur to some extent due to phenomena such as Ostwald ripening, phase partitioning, bilayer diffusion or micelle-mediated transport (Webster and Cates 1998; Calder\u00f3 et al. 1998; Skhiri et al. 2012; Chen et al. 2012; Gruner et al. 2015; Debon et al. 2015). That said, controlled molecular transport between droplets can in fact open up new and unexpected opportunities (Gruner et al. 2016). In the current context, recent studies suggest that additives (such as Bovine Serum Albumin) can decrease diffusion rates by forming barrier layers, and can also maintain high enzymatic activities (when performing droplet PCR) through competitive adsorption on surfactant layers (Gruner et al. 2015; Courtois et al. 2009; Zhang and Xing 2007). It should also be noted that although mineral and fluorinated oils are both compatible with droplet PCR, their physical and chemical differences define particular limitations and advantages. These are compared and summarised in Table 1. Finally, it must not be forgotten that control of channel surface properties is critical in ensuring efficient generation and processing of droplets (Bashir et al. 2014). Although more detailed discussions of this issue can be found elsewhere (Debon et al. 2015), it is necessary for channels made from hydrophilic materials (such as glass) to made hydrophobic through silanisation and typical for naturally hydrophobic surfaces (such as PDMS and PMMA) to be treated with fluoroalkylsilanes prior to experimentation (K\u00f6ster et al. 2008).\n\n\nNext-generation sequencing (NGS) is a commonly used umbrella term describing ultra-high-throughput sequencing methods (Behjati and Tarpey 2013). Such methods allow nucleic acid sequencing at rates of thousands of gigabases per week and at a cost of less than a dollar per gigabase, and have revolutionised genetic and genomic science.\n\nSeveral distinct NGS platforms are commercially available (such as those offered by Illumina, Roche and Life Technologies). Although metrics such as cost per run, cost per base, error rate and throughput are important when evaluating performance, the read length and number of reads per run are perhaps most useful when judging sequencing capacity (Levy and Myers 2016). Since 2012, an annual comparison of available sequencing platforms (based on these two factors) has been presented by Lex Nederbragt at the University of Oslo, with data from July 2016 illustrated in Fig. 2b (Nederbragt 2016). Currently, Illumina\u2019s Hiseq platforms lead the field in terms of throughput and unsurprisingly dominate the sequencer market share. That said, most mainstream NGS systems make use of short-read lengths, which yields limitations in the resolution of structural mutations and ability to perform de novo sequencing (Treangen and Salzberg 2012). Accordingly, NGS technologies capable of long reads (such as those provided by Pacific Biosciences and Oxford Nanopore) are becomingly increasingly important, although still in the early stages of development. Finally, it should be noted that extended read lengths can be accessed indirectly via synthetic long-read (SLR) sequencing methods, which leverage short-read sequencing data to generate synthetic long reads via partitioning, label indexing and remapping techniques (Kuleshov et al. 2014). SLR methods are compatible with existing short-read sequencing platforms and have already shown utility in the recovery of missing sequences, haplotype phasing and transcriptome analysis (Li et al. 2015; Amini et al. 2014; Tilgner et al. 2015).\n\nA number of NGS methods make use of microtiter plates to partition samples (Amini et al. 2014; Adey et al. 2014). For example, haplotype determination can be achieved by dilution of samples into 384-well plates prior to sequencing library preparation (Fig. 2c) (Kuleshov et al. 2014). A key feature of \u201cdilution haplotyping\u201d is the fact that the low concentration of molecules per partition reduces the probability that a contained DNA molecule has an overlapping sequence with another. Unfortunately, dilution methods based on microtiter plates are instrumentally complex and limited in their partitioning capacity. To address these limitations, researchers from 10X Genomics and Stanford University have recently transformed haplotyping analysis (and many other applications) by using droplet-based microfluidics to achieve large-scale partitioning in a rapid and efficient manner (Zheng et al. 2016a). Specifically, a double-cross-junction was used to construct phased sequencing libraries from ng inputs of high molecular weight DNA. Hydrogel beads can then be used as barcode delivery reagents, to allow the controlled loading of individual barcodes into droplet partitions. This core technology platform has since been refined to enable the generation and analysis of more than one million droplet partitions using over four million barcodes and the integrated sequencing of up to 104 (single) cells (Fig. 2c).\n\nThe transition from microtiter plate to droplet-based formats has also impacted high-throughput cellular assays. For example, DeKosky et al. (2013) recently developed a method able to preserve heavy-chain (VH) and light-chain (VL) antibody pairing information when performing high-throughput immune repertoire sequencing. The authors were able to partition single B cells into spatially isolated compartments, whilst at the same time inserting poly(dT) magnetic beads as barcodes. Cells could be lysed, with mRNA captured on the magnetic beads and then reverse transcription and emulsion VH\u2013VL linkage PCR performed. After this complex sequence of operations, linked transcripts were finally subjected to NGS. Initially, four PDMS slides each containing 170,000 wells (with each well having a volume of 125 pL) were designed to concurrently accommodate and process 68,000 B cells (with a 95% probability of there being only one cell per well). In each experimental run, over 50,000 single B cells could be deposited and analysed. Subsequently, the same team replaced the well-based strategy with a droplet-based microfluidic system (DeKosky et al. 2014). This direct upgrade enabled the high-throughput processing of over one million single B cells per experiment. The schematic procedures for both workflows are shown in Fig. 3.\n\n\nAmplification is a prerequisite for the vast majority of nucleic acid assays. The polymerase chain reaction (PCR), the first in vitro nucleic acid amplification technique, was introduced by Mullis et al. (1986) over three decades ago, and is still to this day the preferred approach for most amplification-involved procedures. Conventional PCR is performed using bulk thermal cyclers, where Peltier effect thermoelectric heating is used convert electrical energy into a temperature gradient (Bell 2008). Almost all conventional thermal cyclers possess large thermal masses, which result in high power requirements and relatively slow heating and cooling rates. Unsurprisingly, a large number of microfluidic approaches have been developed for PCR over the past 20 years to address these limitations. Although, highly successful in allowing amplification to be performed in a rapid and efficient manner (Woolley et al. 1996; Kopp et al. 1998; Easley et al. 2006), batch and continuous flow approaches do not drastically change how PCR is used by experimentalists to generate biological information. Conversely, the adoption of droplet-based formats for PCR over the recent years has begun to transform the application and utility of PCR in complex biological experiments (Williams et al. 2006; Diehl et al. 2006). In addition to obvious advantages, such as reduced reaction times, minimal sample consumption and contamination-free operation, other intriguing features such as massively parallel operation, high amplification sensitivities and reduced amplification bias have begun to fundamentally change how biologists view and use the reaction (Tewhey et al. 2009; White et al. 2009; Nishikawa et al. 2015).\n\nDroplet-based PCR can be carried out in various microfluidic formats, which are broadly categorised as being either on-chip or off-chip (Kiss et al. 2008). For example, early studies by Schaerli et al. (2009) used a radial microfluidic device, containing concentric temperature zones, to perform single-copy amplification in 160 pL-volume droplets (Fig. 4A). Batch on-chip microfluidic systems can be created by fabricating integrated chambers that trap or hold large numbers of droplets subsequent to their production. As previously described, Hatch et al. (2011) showed an elegant example of such a format, where over a million droplets containing PCR mix were packed into a microfluidic chamber for both thermal cycling and real-time product detection. Interestingly, the majority of droplet-based PCR assays have incorporated off-chip amplification, whereby PCR droplets are generated on-chip using standard protocols and then collected and amplified in standard PCR reaction tubes. Such an approach is interesting since it leverages the ability of microfluidics to generate large numbers of defined droplets on short timescales and the convenience of using commercial formats or instruments for thermal cycling [rather than more involved approaches to thermal control (Sgro et al. 2007) (Hettiarachchi et al. 2012)]. The interested reader is directed to Table 2, which summarises representative droplet-based PCR studies over the past decade.\n\n\n\nIt should be remembered that nucleic acid amplification is not limited to PCR, with a large number of alternative amplification methods being developed in the intervening years (Fakruddin et al. 2013). These include the ligase chain reaction (LCR) and isothermal amplification methods such as rolling circle amplification (RCA), loop-mediated isothermal amplification (LAMP), recombinase polymerase amplification (RPA), helicase-dependent amplification (HDA), ramification amplification method (RAM), multiple displacement amplification (MDA) and nucleic acid sequence-based amplification (NASBA). Almost all of these basic techniques have been successfully transferred to droplet-based microfluidic formats (Zanoli and Spoto 2012). Isothermal amplifications are particularly attractive since they are characterised by short reaction time and require only simple thermal control architectures. These features suggest significant potential for use in point-of-care diagnostic applications. For example, LAMP has been shown to be rapid, accurate, and cost-effective in the diagnosis of infectious diseases such as severe acute respiratory syndrome (SARS), malaria and African trypanosomiasis (Mori and Notomi 2009; Poon et al. 2004; Surabattula et al. 2013; Njiru et al. 2008). Critically, LAMP analysis can be performed simply by visual inspection or through the use of a smartphone camera (Tomita et al. 2008; Damhorst et al. 2015). Recently, Rane et al. (2015) demonstrated an integrated device for digital LAMP, combining droplet generation, incubation (amplification) and real-time detection. Using such an approach, more than one million droplets could be processed in less than 2 h in a continuous manner.\n\nDroplet digital PCR (ddPCR) is quite possibly the most important microfluidic technology to have been commercialised in recent years (Fig. 4b) (Hindson et al. 2011), and refines the concept of digital PCR (dPCR) proposed in the late 1990s (Vogelstein and Kinzler 1999). Unlike conventional quantitative PCR (qPCR) methods, dPCR achieves quantitation by portioning a large sample volume into many smaller volumes that statistically contain no more than one copy of target DNA. dPCR is particularly robust for the detection of rare nucleic acid samples, the investigation of rare mutations in complex backgrounds and the identification of small differences in expression levels. That said, early embodiments dPCR were limited by the method of sample partitioning, which often involved the use of microtiter plates (Vogelstein and Kinzler 1999), bulk emulsions (using beads, emulsion, amplification and magnetics - BEAMing) (Dressman et al. 2003) or microfluidic chamber arrays (Ottesen et al. 2006). Hindson et al. (2013) have compared ddPCR with qPCR in the microRNA quantification, with results indicating that ddPCR yields significantly greater precision and improved \u201cday-to-day reproducibility\u201d over qPCR. Such superior metrics suggest that ddPCR will continue to play an important role in molecular diagnostics of genetic diseases (Debrand et al. 2015), cancers (Mehrian-Shai et al. 2016; Watanabe et al. 2015), infectious diseases (Bian et al. 2015; Trypsteen et al. 2016) and prenatal diagnosis (Orhant et al. 2016). For example, epidermal growth factor receptor (EGFR) mutation is an important target for many cancer therapies, with the status of the EGFR mutation being closely related to the therapeutic effect of EGFR inhibitors, such as monoclonal antibodies and tyrosine kinase inhibitor (Li\u00e8vre et al. 2006; Gazdar 2009). Siravegna et al. (2015) comprehensively combined BEAMing, ddPCR, NGS and bioinformatics analyses to genotype colorectal cancers and dynamically monitor clonal evolution during treatment with the EGFR-specific antibodies Cetuximab and Panitumumab. Results revealed the colorectal tumour genome adapts dynamically to intermittent drug schedules, and provides a molecular explanation for the efficacy of \u201crechallenge therapies\u201d based on the EGFR blockade. Such a methodology has significant implications for the development of personalised cancer treatments and the dynamic monitoring of disease progression and response to therapy. Put simply, it eliminates the difficulties associated with repeated sample acquisition, and removes temporal and spatial bias in sample selection.\n\nddPCR allows for the simultaneous detection of multiple targets through the use of multi-colour detection schemes, with further expansion of target numbers being achieved by varying parameters that control PCR efficiency (Zhong et al. 2011). Accordingly, in multiplex ddPCR, multiple mutations can be detected in a single experiment; a feature particularly valuable when assaying clinical samples (Taly et al. 2013). Much work has recently focused on improving ddPCR, in terms of detection sensitivity (Miotke et al. 2014) and sample volume limitation (Petriv et al. 2014), but there is little doubt that ddPCR is rapidly becoming a \u201cstandard\u201d component in highly sensitive genomic screening.\n\nCells are the elementary structural, functional, and biological units in living organisms, with the physiological functions of multicellular organisms being realised through individual cells. It is widely acknowledged that a seemingly homogeneous cell population will differ significantly in terms of size, genetic variants and expression patterns at single-cell level, resulting from the inherent stochasticity of biological processes (Elowitz et al. 2002) and stimulation by the external microenvironment (Liberali et al. 2015). Accordingly, the ability to identify cell-to-cell variations within a given population is critical in understanding clonal evolution in cancer (Greaves and Maley 2012), immune dysfunction (Proserpio and Mahata 2016) and somatic mutations (Xu et al. 2012). In this respect, single-cell genomics aims to enrich our understanding of genetics by engendering the study of genomes at the cellular level.\n\nA technical prerequisite for DNA or RNA sequencing of single cells is the efficient physical isolation of large numbers (>103) of discrete cells, in a manner that allows each cell to be interrogated on an individual basis (Gawad et al. 2016). Normally, cells obtained from blood or solid tissues are processed (via methods such as enzymatic dissociation, density gradient centrifugation and fluorescence-activated cell sorting) to yield a single-cell suspension, which is then delivered into the microfluidic system. Cell isolation in microfluidic systems can be used most easily achieved using traps, droplets or micromechanical valves. For example, the commercially available Fluidigm C1 platform provides an integrated and automated solution for single-cell genomics, leveraging control of pneumatic valves (that deflect under pressure to disrupt fluid flow within a microchannel) to perform single-cell capture, lysis, mRNA release, RT-PCR and cDNA amplification. Such an approach allows the parallel analyse up to 800 cells in an automated fashion.\n\nTwo recent studies describing single-cell RNA sequencing methods using droplet-based microfluidics [termed Drop-seq (Macosko et al. 2015) and InDrop (Klein et al. 2015)] have attracted significant attention in the biological community due to their ability to barcode RNA and analyse mRNA transcripts in an efficient, cost-effective and high-throughput fashion (Fig. 5). Unsurprisingly, these two approaches share much similarity in methodology, since they exploit droplet-based tools developed in the Weitz laboratory at Harvard University. Both utilise microfluidics to load single cells and single microbeads (containing a unique barcode) together with lysis buffer into droplets. Subsequently, released mRNAs from a given cell are labelled with a unique code prior to droplet breakup and pool amplification. mRNAs are converted to cDNAs by RT-PCR, followed by library preparation, sequencing and data analysis. Critically, all sequencing data, though carried out in batch, can be traced back to its \u201ccell-of-origin\u201d and \u201cgene-of-origin\u201d. The Drop-seq method uses solid microparticles, with oligonucleotide codes covalently linked to the particle surface, whilst InDrop technology uses hydrogel beads, with code release being driven by UV activation. For the interested reader, a more detailed comparison of the biochemical procedures (including transcript coverage) can be found elsewhere (Picelli 2016). That said, from a technical perspective, some comment on co-encapsulation efficiencies is worthwhile. Random (passive) loading of beads, cells and DNA molecules into droplets obeys Poisson statistics under normal circumstances (Collins et al. 2015). To ensure a >95% probability that a given droplet contains no more than one cell, the average occupancy should not be larger than 0.1 cells per droplets. Under such conditions, most droplets (90.5%) will be empty, with 9% containing a single cell. Accordingly, both Drop-seq and InDrop utilise dilute cell suspensions to ensure single-cell encapsulation, whilst leveraging the ability of microfluidic droplet generators to make droplets at high speed. Interestingly, the InDrop method utilises close-packed ordering (Abate et al. 2009) to beat Poisson constraints, with almost 100% droplets receiving gel beads, and over 90% of cell-loaded droplets containing exactly one cell and one bead. This approach involves the use of close packed, deformable particles to allow insertion of a controllable number of particles into every droplet. It should also be noted that the basic Drop-seq methodology could in future make use of inertial focusing and ordering to drastically increase the number of droplets containing a single cell and bead (Martel and Toner 2014). Considering current co-encapsulation efficiencies, the InDrop methodology should be well-suited for clinical applications, where cell availability is often limited. Interestingly, 10X Genomics have recently tested single-cell RNA-seq on their GemCode platform using similar workflows, and reported a cell capture efficiency of ~50% and analysis of eight samples in parallel (Zheng et al. 2016b).\n\n\nBesides global single-cell RNA-seq, the principle of bead-barcoding and droplet-isolation has also used for targeted transcriptomic sequencing. As noted, DeKosky et al. (2013, 2014) sequenced immune receptor repertoires with the preservation of pairing information (between heavy and light-chain antibodies). These chains contain variable domains and their pairing relationship controls cellular functionality. Compared to Drop-seq, an additional step of re-emulsifying mRNA-captured beads to perform RT-PCR and linkage PCR is necessary. In this respect, the authors have recently published a detailed protocol of the entire workflow (McDaniel et al. 2016).\n\nThe encapsulation and isolation of single cells in a drop-by-drop fashion has opened up new opportunities for cost-effective and ultra-high-throughput single-cell genetic studies in applications such as whole-genome amplification (Fu et al. 2015), chromatin profiling (Rotem et al. 2015) and PCR-activated cell sorting (Eastburn et al. 2014), with microfluidically produced droplets playing a key role. Finally, it is worth noting that thermosensitive hydrogel droplets are interesting vehicles for novel experimentation (Leng et al. 2010; Kumachev et al. 2011). For example, hydrogel droplets can be generated in oil at elevated temperatures and cooled to form gel particles downstream. Such gel particles can be washed and handled in aqueous buffer, allowing molecular exchange of substances through diffusion. Hence, unlike conventional aqueous droplets that require sophisticated operations to dose or remove reagents, gel droplets may be processed by immersion in appropriate media or dialysate. This innovation has opened up new possibilities for designing highly complex biological workflows in genetic analysis (Novak et al. 2011; Zhang et al. 2012; Geng et al. 2014).\n\nThe demand for rapid, accurate, inexpensive and convenient point-of-care (PoC) systems for infectious disease diagnostics and wellness monitoring is significant. Ideal diagnostics should be both simple in their structure and portable, whilst ensuring that predefined questions can be answered in a quantitative, low-cost and rapid manner. Whilst droplet-based microfluidic systems offer a direct route to such quantitative diagnostics, their implementation for PoC nucleic acid analysis is far from simple. For example, fluid manipulation is a key concern in formats, which need to be cheap, robust and small. In this respect, syringe pump-free systems [which utilise manual droplet generation (Dangla et al. 2013) or centrifugal microfluidics (Schuler et al. 2015)] are particularly interesting. Moreover, isothermal amplification methods will be preferable to more traditional thermocycling techniques (Zanoli and Spoto 2012). To this end, Schuler et al. (2015) recently demonstrated a system that utilises centrifugal step droplet generation, and is thus pump- and tubing-free (Fig. 4c). Using such an approach, the authors were able to perform isothermal ddPRA on-chip and quantify L. monocytogene DNA in food samples, reducing the time-to-result by four-fold when compared to the gold-standard tests. Moreover, Cao et al. (2016) showed a significant enhancement of fluid control in centrifugal microfluidics by introducing a novel two degrees of freedom (2-DoF) centrifugal microfluidic platform, which allows complex fluidic control in a direct manner, requiring no external components. Such an advance suggests new possibilities for the use of centrifugal microfluidics in PoC applications.\n\nWhilst PoC devices provide maximum accessibility to end-users, droplet-based microfluidic technologies have shown their true mettle in addressing comprehensive and complex biological questions. Although, and as we have seen, such systems have allowed experiments inaccessible on the macroscale to be performed in an automated and integrated manner, we have only scratched the surface in terms of their ultimate potential. For example, droplet-based platforms have already been integrated with \u201cmachine learning\u201d algorithms to allow the intelligent synthesis of a range of high quality nanomaterials for application in display and photovoltaic technologies (Maceiczyk and deMello 2014; Reizman and Jensen 2016). Such approaches leverage the ability of microfluidic systems to perform the chemistry/biology in an efficient manner and real-time detection to extract information on ms timescales. Machining learning methods will almost certainly impact biology in a similar way within the short term. As control architecture is refined, the sophisticated operations, shown for example in Fig. 2a, will no longer be the privilege of a few expert microfluidic laboratories, and droplet-based microfluidics will quickly become a basic tool used by any experimental scientist. Even though significant successes in system automation have been made (such as the Fluidigm C1 platform), the automated and large-scale control of droplet networks integrating multiple functional components remains a daunting challenge, requiring the robust understanding and harnessing of nonlinear and multi-phase fluid dynamics. In this respect, valuable progress has already been made in areas such as bubble logic (Prakash and Gershenfeld 2007), control logic (Weaver et al. 2010) and electric circuit analogy (Oh et al. 2012). Moreover, a recent study describing the \u201crandom design\u201d of microfluidic systems is of particular interest (Wang et al. 2016). In this approach, a library of thousands of random microfluidic chip designs was synthesised. The behaviour of each design was then simulated using finite element analysis, with users able to access structures suited to given tasks. We anticipate this type of interaction could form the basis of future microfluidic platform development. Indeed, through the collection and assimilation of user-generated data, machine-learning algorithms will allow the creation of entirely new microfluidic tools. Unsurprisingly, we feel that the future of droplet-based microfluidics is an exciting one."}