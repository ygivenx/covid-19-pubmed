{"title": "Sources of spatial animal and human health data: Casting the net wide to deal more effectively with increasingly complex disease problems", "body": "Over the last 30 years it has become commonplace for epidemiological studies or surveys to collect locational (spatial) attributes for disease data (Pfeiffer et al., 2008) . Although this advancement has been driven largely by the introduction of handheld global positioning systems (GPS), and more recently, smartphones and tablet computers with built-in GPS that facilitate geo-tagged data collection, it also highlights the increased awareness of the importance of the spatial aspect when developing efficacious animal disease surveillance and control strategies (Table 1) . Unfortunately, as a result of the particular challenges currently facing health workers and researchers, for spatial disease data to be able to effectively inform innovative surveillance and disease control strategies, it needs to move beyond the fundamentals of collecting georeferenced disease event data in individual studies and instead focus on an inclusive approach that Eysenbach (2001) , in his definition of eHealth, referred to as 'a state-of-mind, a way of thinking, an attitude, and a commitment for networked, global thinking, to improve health care locally, regionally, and worldwide by using information and communication technology'.\n\nThis collective, crowdsourced approach was aptly illustrated during the 2014 West Africa Ebola crisis when, faced with only a few rudimentary topographical maps of Guinea but no useful maps upon which to base control and surveillance efforts, M\u00e9decins Sans Fronti\u00e8res (MSF) personnel http://dx.doi.org/10.1016/j.sste.2015.04.003 1877-5845/\u00d3 2015 Elsevier Ltd. All rights reserved. enlisted the help of the Humanitarian OpenStreetMap Team (HOT) to map Gu\u00e9ck\u00e9dou -the main city in Guinea affected by the outbreak (Hodson, 2014) . Within 20 h of receiving the request, online volunteers had mapped three cities in Guinea based on satellite imagery of the area, populating them with over 100,000 buildings; information that proved crucial for door-to-door canvassing of inhabitants and mapping the spread of disease.\n\nIn addition to this collective approach, for spatial disease data to be effective in the 21st century, it needs to meet certain requirements. Firstly, the increasing number of transboundary disease epidemics has emphasized the need for animal and human health information systems that are no longer circumscribed by regional or national borders; transparent collection and sharing of disease data needs to occur at a global scale. Secondly globalization has substantially increased the speed and magnitude of disease spread. In the 2001 UK foot and mouth disease (FMD) outbreak it was estimated that at least 57 premises from 16 counties were infected before the first case was reported (Gibbens and Wilesmith, 2002) while in 2007, equine influenza spread rapidly throughout two Australian states as a result of infected horses attending an equestrian event (Cowled et al., 2009) ; approximately 70,000 horses on over 9000 premises were infected with most of the geographic dissemination occurring within the first ten days of the epidemic. For containment to be effective, reporting of disease events needs to be as rapid as possible. This is of particular concern in developing countries where reporting of animal disease events can be delayed by months (Karimuribo et al., 2012) while lag times for such reports as the Centers for Disease Control and Prevention (CDC) US Influenza Sentinel Provider Surveillance reports are currently in the order of 1-2 weeks (Ginsberg et al., 2009) .\n\nDuring the past decade, collecting spatial disease data has moved beyond the use of handheld GPS devices and there now exist numerous sources of crowdsourced georeferenced disease data such as that available from georeferencing Google search queries or Twitter messages. Not surprisingly, the focus so far has been on human health, Table 1 Using spatial analysis to inform risk-based animal disease surveillance and control.\n\nDisease distribution maps range from simple dot maps showing the location of disease events to predictive risk maps created using statistical algorithms that combine disease occurrence data with environmental covariates (Pigott et al., 2014) . But no matter what form they take, visualizing the spatial pattern of disease -be it at a global, national or local scale -is fundamental for informing risk-based disease surveillance and control strategies in several ways. Simple visualizations allow the extent of the disease to be delineated and disease frequency monitored, and when combined with maps of environmental factors or those highlighting the spatially heterogeneous distribution of at-risk populations, they can also be used to estimate disease burden (Hay et al., 2010; Robinson et al., 2002) and identify target populations for intervention (Tatem et al., 2011; Guerra et al., 2010 Guerra et al., , 2008 Guerra et al., , 2006 . Visualizing disease distribution can also be fundamental in directing control and elimination efforts. Clements et al. (2013) describe how measures to eliminate malaria from endemic countries have generally adopted a spatially progressive elimination approach referred to as shrinking the malaria map in which eradication efforts initially focus on the geographical perimeter of endemic areas and work inwards, effectively localizing disease distribution which allows for more efficient treatment and control (Feachem et al., 2010) Apart from the key role maps play in informing risk-led decision making, they also serve a more practical purpose such as facilitating integration and synthesis of data from a wide range of diverse sources, each possibly capturing information about disease and relevant risk factors at different scales (Bergquist and Tanner, 2012; Bennema et al., 2014) . As a result, cartographers need to decide on the most appropriate scale at which to present the data for it to be useful; data presented at administrative level 1 (province or region) inevitably cannot capture the fine-scale heterogeneity of most infection patterns and so estimates of numbers of individuals requiring treatment tend to be incorrect (Brooker et al., 2010) .\n\nCluster detection A clustered spatial arrangement of disease events suggests the presence of a contagious process or localised risk factor. Apart from the fact that spatial targeting of interventions at high-risk areas is more cost-effective than uniform resource allocation (Stark et al., 2006) and therefore such identification is essential for informing risk-based disease surveillance and control efforts. Identification of significant disease clusters can also advance our understanding of a disease in several ways including suggesting potential risk factors for further investigation either directly (Calistri et al., 2013; French et al., 2005; Sinkala et al., 2014; Kelen et al., 2012; Nogareda et al., 2013; Poljak et al., 2007; Le et al., 2012; Vigre et al., 2005; Ward and Carpenter, 2000) , or indirectly when analysis of model residuals indicates the modelled predictors do not explain fully the spatial heterogeneity in disease distribution (M\u00e9roc et al., 2014; Borba et al., 2013) , or by defining the scale of disease clustering (French et al., 2005; Le et al., 2012; French et al., 1999; Wilesmith et al., 2003; Picado et al., 2007; Picado et al., 2011; Porphyre et al., 2007; Sanchez et al., 2005; Minh et al., 2010; Xu et al., 2012; M\u00e9tras et al., 2012; Abatih and Ersb\u00f8ll, 2009) and thereby indicate likely transmission mechanisms involved in disease spread (Sinkala et al., 2014; Ward et al., 2013; Loobuyck et al., 2009; Ohlson et al., 2014; Rosendal et al., 2014; Poljak et al., 2010) . Cluster detection can also be used identify areas where vectors and hosts coincide resulting in potentially increased risk of disease transmission (Shaman, 2007; Hennebelle et al., 2013; Swirski et al., 2007) , highlight possible regional differences in disease transmission (Kelen et al., 2012) , or track the direction and geographical extent of disease spread (Wilesmith et al., 2003; Denzin et al., 2013; Lian et al., 2007) Spatial modelling Spatial modelling techniques can be divided into data-and knowledge-driven methods (Stevens and Pfeiffer, 2011) , the former characterised by the use of statistical methods for defining relationships between risk factors and disease risk, while knowledge-driven modelling approaches are based on existing knowledge about the causal relationships associated with the disease risk of interest. Statistical analysis is used to generate data-driven models from information collected through surveillance and other means. Such models generate quantitative estimates of risk and the relative weights of risk factors.\n\nThe results of such models are used for a variety of purposes including targeting areas for disease surveillance, risk management, simulating different control scenarios, or predicting what will happen under different environmental conditions such as those resulting from climate change (i.e. temporal prediction), or identifying new geographical areas suitable for the introduction of diseases (i.e. spatial prediction) driven to some extent by US President Barack Obama's Global Health Initiative (Initiative GH, 2009 ) and the World Health Organisation's (WHO) concerted efforts regarding neglected tropical diseases (Cringoli et al., 2013; The Global Network for Neglected Tropical Diseases; Brooker and Utzinger, 2007; van Lieshout and Yazdanbakhsh, 2013; Malone and Bergquist, 2012; Brooker and Smith, 2013; King et al., 2013; Brooker et al., 2006) . Also, by their very nature, citizen and internet-based health approaches lend themselves to human rather than animal-health problems and similarly, to developed countries that have the necessary internet infrastructure, rather than developing nations. As a result, internet-based animal-health initiatives currently lag behind those of human health, yet it is to these initiatives that we must look to see how such systems can be best adapted for use in animal health and in developing nations; countries that carry the highest burden of emerging and zoonotic infectious diseases in the world yet frequently have the least capacity for cost-effective risk-led decision making. This paper reviews a range of sources and features of spatial disease data currently available, discussing their advantages and limitations, and highlighting studies where they have been used. Although the focus is on animal diseases, relevant advancements in human health that could be adopted for animal health purposes are discussed.\n\nAlthough animal disease surveillance has traditionally been implemented at national and sub-national levels, the increasing number of transboundary animal disease epidemics has highlighted the need for establishing such systems at broader scales. As a result, data warehouses and disease reporting systems such as World Animal Health Information Database (WAHID) (Jebara et al., 2012) and EMPRES Global Animal Disease Information System (EMPRES-i) (FAO: EMPRES transboundary animal disease bulletin, 2011; Farnsworth et al., 2010; were launched to encourage and facilitate data collection and sharing at a global level (Table 2 ). However, in addition to their original role, such data warehouses also provide researchers with cost-effective access to regularly updated spatial disease data, potentially leading to increased knowledge gains, without the need for costly and time-consuming primary research. Moreover, integration of databases from different sources offers researchers a more extensive and comprehensive collection of information than if individual data sources were used with the possibility of better understanding issues at the population level. However, researchers using this data need to remember that although the provenance of national disease surveillance data ensures that specificity is reasonably high, sensitivity is likely to be low and undoubtedly exhibits considerable spatio-temporal heterogeneity with respect to bias and sensitivity (Perez et al., 2011) . Users should also bear in mind the limitations of using data that have generally been spatially referenced to administrative centroids rather than exact outbreak locations; in addition to the possibility of ecological fallacy, Stevens et al. (2013) showed that using outbreak data georeferenced to administrative centroids for spatial modelling purposes can be problematic when either constraining the study area or working at relatively low spatial resolutions. Farnsworth et al., 2010; EMPRES-i provides up-to-date information on global animal disease distribution and current threats at national, regional and global level. Disease events can be presented on a map and data may also be exported for further analysis EMPRES-i genetic module (Claes et al., 2014) This Table 2 contains details of the main animal disease geodata warehouses. EMPRES-i data is obtained from both formal (e.g. reports from the OIE, World Health Organization, national authorities, FAO country or regional projects, field missions and field officers, non-governmental organizations, laboratories and reference centers) and informal sources (e.g. media reports and those disseminated by the Global Public Health Intelligence Network and ProMED) (Farnsworth et al., 2010; , and all outbreaks appearing in the database are followed-up until either confirmed or denied (Farnsworth et al., 2010) . WAHID, on the other hand, comprises official information submitted by OIE member countries regarding immediate and follow-up notifications of exceptional disease events, or semester and annual reports on OIE-listed diseases together with background information on animal health and control programs. Alternatively, Disease BioPortal is an unrestricted, public web site created and maintained by the Center for Animal Disease Modeling and Surveillance (CADMS) at the University of California, Davis (Disease BioPortal) incorporating multiple streams of information including WAHID (Jebara et al., 2012) , EMPRES-i (FAO: EMPRES transboundary animal disease bulletin, 2011; Farnsworth et al., 2010; , GenBank, the World Reference Laboratory for Foot-and-Mouth Disease (WRLFMD), weekly reports on vesicular-like diseases from Centro Panamericano de Fiebre Aftosa (PANASFTOSA) and the Foot-and-Mouth disease (FMD) News and Rift Valley fever (RVF) News produced by CADMS.\n\nIn additional to global information systems, it is also useful for decision makers to have access to national data warehouses. Denmark was the first country to make all animal husbandry data from a range of sources and departments related to Danish production animals available in a single online geodata warehouse (Nielsen, 2011) . Disparate databases can be linked at the individual animal level or aggregated at farm, postcode or administrative level. However, the difficulties associated with standardizing and combining a range of data sources -each involving potentially dissimilar unique identifiers, data structures, languages and semantics -limits the development of animal health geodata warehouses and results in bias. Data adapters (applications that convert attributes of one database into attributes compatible with another database) are integral to the creation of such information systems as they can be used to relate diverse databases through the identification of fields containing equivalent information (Perez et al., 2011) .\n\nData quality is of primary concern with data warehouses and assessing the validity of the data is paramount. GLiPHA, which merges livestock health and production data from multiple sources, incorporates a system of error checking rules to help identify inaccuracies and inconsistencies in the data (Clements et al., 2002 ) while the Danish system uses a number of different tools such as continuous reporting of data, systematic control of entries and irregularities and cross-control of data with other sources (Nielsen, 2011) .\n\nAlthough the logistics involved in linking a range of disparate datasets partly hinders the development of national or global animal health data warehouses, the reluctance of certain agencies and organisations to share disease data freely, transparently and in a timely fashion (Perez et al., 2011) is an additional impediment to the development of a valuable resource that should be integral to 21st century risk-led disease management and decision making. Buy-in from all parties is therefore essential if such information systems are to be successfully established.\n\nWhile developed nations generally have access to a wide range of good quality georeferenced health-related data, collection, processing and dissemination of such data in resource-poor locations remains challenging owing to lack of the necessary technological infrastructure as well as issues with familiarity and usability (Betjeman et al., 2013) . In such countries data is generally still collected via paper-based forms despite associated inefficiencies such as data entry errors and long delays before the processed data is available to decision makers (Anokwa et al., 2009) . In fact, a survey of selected human and animal health surveillance systems in different regions of Southern Africa found obvious spatial heterogeneity in the delivery of monthly animal health reports to the central epidemiological department; wards closer to the headquarters submitted reports more regularly than those further away while delivery of all reports could be delayed by as much as six to nine months (Karimuribo et al., 2012) .\n\nThere is thus an urgent need for resource-poor settings to implement alternate surveillance systems and although lack of technological resources and infrastructure may preclude the use of novel internet-based surveillance approaches, mobile devices such as the now out-of-date personal digital assistants (PDAs) Shirima et al., 2007; Yu et al., 2009; Seebregts et al., 2009; Dale and Hagen, 2007 and more recently mobile phones (Robertson et al., 2010; Jean-Richard et al., 2014; Thinyane et al., 2010) , smartphones (Forsell et al., 2011) and tablet computers, are playing an increasingly fundamental role in the collection and processing of animal and human health surveillance data in resource-poor locations (Betjeman et al., 2013; Chretien et al., 2008; Mwabukusi et al., 2014; Istepanian et al., 2004) . This is, in part, a result of the extensive penetration of mobile phone use in developing countries over the last decade; estimated to be 63% in sub-Saharan Africa in 2013 and projected to pass 70% by 2015 (Betjeman et al., 2013) .\n\nInitially restricted to simple yet effective features such as short messaging service (SMS) and voice calling, the value of mHealth -using mobile devices to collect or distribute health-related information (Istepanian et al., 2004 ) -for health care workers in developing countries increased immeasurably following the development of smartphones. In addition to the variety of data that can be collected -text, audio, video, photographs and barcode scans -other key benefits of smartphones over non-internet based models include the built-in GPS and accelerometer which allow detailed locational data and changes in movement to be documented (Anokwa et al., 2009) . Images, in the form of photographs and videos, can also be sent which may allow remote diagnoses based on gross pathology.\n\nFurthermore, a range of open-source mobile software and tools such as EpiCollect (Aanensen et al., 2009; EpiCollect) and Open Data Kit (ODK) (Anokwa et al., 2009 ; Open Data Kit) allows for cheap, efficient and accurate collection and dissemination of data. Both EpiCollect and ODK allow for the generation of mobile-based forms and for a range of data to be collected and stored on the mobile device before being wirelessly transmitted to a central database. Together with a map-based interface, such as that located within spatialepidemiology.net (Anokwa et al., 2009) , data can be rapidly analysed, mapped and filtered using Google Maps\u2122 via both the web and mobile applications so that both office and field workers can display and analyse data in real-time (Anokwa et al., 2009; Aanensen et al., 2009 ). Real-time reporting and processing of disease data, followed by rapid transmission of information to decision makers, allows swift action to be taken against possible outbreaks. For example, use of real-time reporting and summarising of surveillance data via mobile devices allowed a potential FMD outbreak in the Ngara district of Tanzania to be rapidly contained (Mwabukusi et al., 2014) .\n\nAdditional benefits of mobile-based forms include the ability to customize voice-based questionnaires programmed into a smartphone or tablet computer which allows questions to be administered in local languages or dialects (Jandee et al., 2014) while programming the questionnaire to ask questions in sequence significantly reduces the risk of missing data. Similarly, the use of drop-down menus reduces the risk of data entry error (Jandee et al., 2014) . Moreover, time spent completing smartphone-based questionnaire surveys is significantly less than when completing paper-based surveys (King et al., 2013; Anokwa et al., 2009; Jandee et al., 2014) . However, as availability of open-source mobile software and tools increases -all differing with respect to accessibility, visualization and cost -choosing the most appropriate data-collection tool will depend largely on the type of data being collected (Madder et al., 2012) .\n\nAlthough smart devices have overtaken simple mobile phones restricted to sending voice calls or SMS messages, when combined with applications such as GeoChat, Ushahidi or RapidSMS, the technology can be highly effective. The Cambodian Ministry of Health uses GeoChat for disease reporting and to send staff alerts in response to potential outbreaks (Kamel Boulos et al., 2011) .\n\nUnfortunately, mHealth surveillance approaches are not an automatic panacea to the problems associated with data collection in resource-poor settings. The Southern African Centre for Infectious Disease Surveillance (SACIDS; Rweyemamu et al., 2013) used EpiCollect (Aanensen et al., 2009 ) and ODK (Anokwa et al., 2009) to design an mHealth surveillance strategy incorporating the human, livestock, and wildlife sectors (Karimuribo et al., 2012; Mwabukusi et al., 2014) but reported that for a mobile technology-based disease surveillance system to be effective and sustainable it required three key elements: (i) participatory epidemiological approaches; (ii) form-based reporting; and (iii) resident ICT expertise at the discovery end together with local support for database handling, customized programming, trouble-shooting, and training at the user end (Karimuribo et al., 2012; Mwabukusi et al., 2014) .\n\nAlthough developing counties are the ideal focus for mHealth data collection and disease control initiatives, there are few published examples of such enterprises, particularly in the field of animal health. Successful mHealth initiatives are more frequently documented in human (Betjeman et al., 2013; D\u00e9glise et al., 2012; Shet and Costa, 2011; Lee et al., 2011; Lozano-Fuentes et al., 2013; Lozano et al., 2012) than in animal health, and in developed (Raja et al., 2014) rather than developing countries. However, practical applications of mobile device use in animal disease control and surveillance in developing countries have been described (Robertson et al., 2010; Jean-Richard et al., 2014; Thinyane et al., 2010; Mtema, 2013) and include enhanced reporting of human rabies exposures at bite treatment centres in Tanzania where mobile phone technologies allowed for rapid communication between human and animal health sectors to ensure follow up of animal cases (Mtema, 2013) and the collection of demographic data and movement tracking of mobile pastoralists and their herds in Chad (Jean-Richard et al., 2014) . The real-time knowledge on camp location and populations provided by such a study facilitates health interventions, such as vaccination delivery to both humans and animals, highlighting the potential to develop One Health mHealth approaches. Such an approach provides added value compared to separate animal-human surveillance systems, especially for zoonotic diseases.\n\nHowever, as with any surveillance system, the success of such efforts depends largely on the extent of local buy-in at all levels, in particular those involved in feeding information back to frontline workers and community organisations; without an efficient two-way flow of information the benefits of mHealth will be limited to having made disease reporting more technologically advanced (Madon et al., 2014) .\n\nCompared with previous decades when the production of paper-based disease atlases was limited by the expense and inefficiency associated with producing something that was effectively out of date almost before it was published, the advent of interactive digital maps and virtual globes such as Google Maps\u2122 and Google Earth\u2122 allows for easy visualisation of disease data in real time as illustrated by the integration of such digital platforms into an ever-expanding number of animal and human-health projects (Table 3 ). The value of such technology in creating effective information resources for decision makers is epitomised by Nature's use of the platform to track the global spatio-temporal spread of highly pathogenic avian influenza (HPAI) H5N1 (Butler, 2006 ; Google Earth Avian Flu), a project that won the Association of Online Publishers (AOP) Use of a New Digital Platform Award in 2006. A list of all current projects using Google Earth\u2122 and Google Maps\u2122 can be found on theGoogle Earth Outreach\u2122 website. However, Google Earth\u2122 is not only a visualization tool; it can also be used to georeference spatial data in situations that fall outside the commonplace.\n\nAlthough the digital platform is useful for georeferencing remote locations, or those difficult to access, with sufficient accuracy for it to be a viable alternative when other forms of georeferenced data are unavailable - Carvalho et al. (2012) used this method to georeference livestock holdings in Brazil to within 31 m, -the real value of Google Earth\u2122 lies in its ability to georeference unconventional locations. In informal settlements or rural areas in developing countries, the lack of geolocation infrastructure such as road names or house numbers precludes the use of conventional mapping software for visualising disease data. In such instances, Google Earth\u2122 has proven invaluable; in a modern day reprise of John Snow's 1856 cholera investigation, use of the digital platform allowed Baker et al. (2011) to map the spread of a typhoid outbreak in Kathmandu -where street names are not used -and trace the cause of the epidemic to low-lying public water resources. Similarly, Wang et al. (2013) used the digital platform to highlight apparent clustering of malaria cases in rural China; information that proved useful in the targeted allocation of limited resources.\n\nDespite its usefulness as a visualisation tool, Google Earth\u2122 lacks the manipulation and analysis functions of GIS software and researchers are therefore increasingly combining the two approaches. In this way, human health assessment programs have effectively created sampling strategies and collected data on dengue fever (Chang et al., 2009) , schistosomiasis (Kun et al., 2012) and human mortality (Galway et al., 2012) in areas with limited or no geolocation infrastructure. For example, to evaluate how proximity to a hospital influenced water quality perceptions and practices in Haiti, Wampler et al. (2013) combined Google Earth\u2122 and the geographic information system software ArcGIS to generate a random sample of households stratified by distance to the hospital. Using a satellite image from EarthExplorer as a basemap in ArcGIS, concentric 1 km buffer zones were created around the hospital. The buffer polygons were then exported to Google Earth\u2122 where the high-resolution imagery allowed individual households within each polygon to be accurately identified and mapped using Google Earth\u2122 pushpins. These point locations were imported into ArcMap where latitude and longitude were added to the dataset and later uploaded into a handheld GPS which was used to locate the households in the field for conducting field surveys (Wampler et al., 2013) .\n\nHowever, remote sensing (RS) data is more often used to provide spatial risk-factor information, particularly for vector borne diseases such as Rift Valley fever (Lacaux et al., 2007; Linthicum et al., 1999; Pin-Diop et al., 2007; Sallam et al., 2013; Soti et al., 2013; Tourre et al., 2008 Tourre et al., , 2009 Vignolles et al., 2009 Vignolles et al., , 2010 , bluetongue (Klingseisen et al., 2013; Purse et al., 2004; Tatem et al., 2003; Guis et al., 2007; De La Rocque et al., 2004; Capela et al., 2003) , eastern encephalomyelitis virus (Barrera et al., 2001; Freier, 1993) and African horse sickness , where the disease vectors are sensitive to changes in specific climatic and vegetation factors that can be captured usefully by satellite technology (Rinaldi et al., 2006; Saxena et al., 2009) . To extend the usefulness of RS data for providing risk fact information, image processing software such as ImageJ, can be used to analyse the RS images. For example, to identify suitable refuges for mosquitoes during hot, dry conditions, ImageJ was used to analyse Google Earth\u2122 satellite imagery and the number of plants, total amount of vegetation around a homestead and its percentage of the total area were calculated and related to households that had reported cases of malaria. In this way, ImageJ was used to analyse freely-available Google Earth\u2122 images of malaria-endemic locations to identify potential risk factors associated with vegetation cover (Ricotta et al., 2014) . Recently, RS imagery has been used to predict areas of highest risk for schistosomiasis infections in Kenya (http://www.bbc.co.uk/news/science-environment-31 483 629). Using satellite imagery, the locations of suitable waterbodies for the snail vector have been mapped and compared with satellite imagery of human distribution, to identify area of highest risk of infection.\n\nVolunteered geographic information (VGI) Goodchild, 2007; Goodchild and Li, 2012 , also known as wikification of GIS by the masses (Kamel Boulos et al., 2011) and crowdsourced cartography, refers to 'the harnessing of tools to create, assemble, and disseminate geographic data provided voluntarily by individuals' (Goodchild, 2007) . A well-known example of VGI is OpenStreetMap (OSM), an open, online, editable map of the world being created by volunteers using a combination of local knowledge, GPS tracks and aerial imagery. As an extension of the basic community mapping effort, volunteers from HOT (Geo-Wiki Project) travel the globe to create collaborative maps of densely packed slums or remote villages, that can be used by aid and development agencies when deciding what infrastructure to build or in the event of a humanitarian crisis. As mentioned in the introduction to this paper, HOT played an important role in the 2014 West Africa Ebola outbreak, rapidly mapping Gu\u00e9ck\u00e9dou, a city of around 250,000 people in southern Guinea, thereby providing field workers with crucial information they needed to be able to assist with mapping the spatial distribution of the disease and planning and implementing control efforts (Hodson, 2014) .\n\nOther examples of crowdsourced cartography include Geo-Wiki (Geo-Wiki Project), a global network of volunteers working to improve the quality of global land-cover maps. The website allows volunteers to view land-cover maps in real time, with Google Earth\u2122 as a backdrop, and to apply their local-area knowledge to determine whether or not the classification is correct, and to amend or update it if necessary. Within the main project are subsidiaries such as Risk Geo-Wiki, AusCover Geo-Wiki and Livestock Geo-Wiki, which also hosts the updated global livestock distribution maps (Robinson et al., 2014) . The Global Geo-Referenced Field PhotoLibrary is a global repository of georeferenced field photos which allow land cover changes to be tracked over time and provide ground truthing for satellite imagery (Xiao et al., 2011) .\n\nDespite their usefulness in producing general maps, the area in which VGI has so far proved most valuable is that of crowdsourced disaster surveys where online volunteers work from satellite imagery to identify buildings which appear to be damaged or destroyed, and to create maps of the disaster area by which aid workers can navigate. This is an excellent example of 'collective intelligence' (Spielman, 2014) , the premise being that, under the right circumstances, collections of individuals are smarter than even the smartest individuals in the group. Similarly, if the collectively generated end-product is better than the best individual contribution, then the aggregated is incredibly valuable (Spielman, 2014) ; one of the reasons why the most successful mapping projects often address a pressing need (e.g. Haiti post-earthquake or Ebola outbreak in Guinea) or concentrate on areas with poor geographic coverage (e.g. slums) (Spielman, 2014) .\n\nHowever, these disaster surveys highlighted an important limitation associated with using untrained volunteers as, although the maps they created proved to be invaluable, damage assessments were poor (Zastrow, 2014) with satellite judgements by HOT personnel corresponding with a later ground survey only 36% of the time (Zastrow, 2014) . The question of VGI accuracy extends beyond that of disaster situations and is particularly important when deciding whether citizen scientists can provide information that is of high enough quality to be used in formal scientific investigations. See et al. (2013) found that when using satellite imagery to describe land cover and human impact on the Geo-Wiki website, although there was little difference between expert and non-expert responses when categorising degree of human impact, experts were better than non-experts at identifying a range of land-cover types (See et al., 2013) . However, accuracy of VGI can be improved by providing targeted training materials for volunteers (e.g. providing volunteers with pre-disaster imagery against which to compare current images (Zastrow, 2014) , guidance on what features to look for (Zastrow, 2014; See et al., 2013) and instituting a continual learning process by providing volunteers with feedback on their contributions (See et al., 2013) .\n\nUnsurprisingly, not all crowdsourced information is of equal quality; some data are of higher quality than others just as some contributors are consistently better than others (Haklay, 2010) . Given that crowd sourced data are of varying quality, when aggregating such data one has to guard against regression to the mean. That is, a few highly accurate or highly credible contributions should not be degraded by being combined with many contributions of low quality, even if these exceptional contributions are outliers (Spielman, 2014) . This is aggravated by the fact that participation in internet-based mapping systems is highly skewed with a few contributors accounting for a large proportion of contributions (Goodchild and Li, 2012; Sieber and Rahemtulla, 2010; Elwood et al., 2012) . Without effective means of aggregation maps will either be shaped by the most active contributors, or map features will simply reflect the average of contributions.\n\nOSM works by assembling all volunteer data into a patchwork map (Goodchild, 2007) which is in turn converted into a single map by aggregating the data using the following three review process; crowdsourced (other users check contributions), social (a set of elite users adjudicate problems) and geographic (features are validated based upon geographic context) (Goodchild and Li, 2012) . OSM contributions are aggregated primarily through crowdsourced review following a last-in, first-out model; users see only the most recent edit. Although this form of aggregation relies heavily on trust and does not directly leverage prior contributions, it has been shown that more edits of an OSM feature generally leads to greater positional accuracy (Haklay, 2010) . If conflicts arise, OSM uses the social review process whereby a set of elite users adjudicate problems.\n\nAs quality of VGI contributions vary, the addition of robust measures of quality would be useful to indicate the level of confidence associated with each piece of information, and although traditional statistical concepts of uncertainty and bias are hard to apply to VGI, other options are available. For example, See et al. (2013) found that when classifying land-cover, volunteer accuracy appeared to be higher when responses for a given location were more consistent and when the volunteers indicated higher confidence in their responses, suggesting that these additional pieces of information could be used to develop associated robust measures of quality (See et al., 2013) . Additional possibilities include the application of Bayesian probability or Dempster-Shafer theory (Eastman, 2009) to provide a measure of confidence.\n\nIdentification of outbreaks at their earliest stages -followed by a rapid response -can substantially reduce the impact of epidemics yet surveillance capacity for such detection can be costly. The internet however is revolutionizing how epidemic intelligence is gathered, particularly in developed countries, allowing us to detect disease outbreaks earlier than when using traditional surveillance approaches, with the added bonuses of reduced costs and increased reporting transparency. For obvious reasons these approaches have, so far, focused on important human diseases but there is potential for the development of similar tools for surveillance of key animal diseases.\n\nA huge volume of real-time information about infectious disease outbreaks is to be found in various forms of internet-based data streams Grein et al., 2000; Heymann and Rodier, 2001) . Known as internet-based big data, the term refers only partly to the volume of data available for analysis and mainly to the fact that access to the almost limitless body of internet data -Google processes, on average, over 40,000 search queries every second (Google Search Statistics) -allows us to learn things that we could not when using smaller, limited datasets (Cukier and Mayer-Schoenberger, 2013) . Big data are generally characterized by 3Vs: volume (relative magnitude of dataset), velocity (rate at which new data are generated) and variety (heterogenous structure of dataset [e.g. text, video, audio]) (Gandomi and Haider, 2015) . In addition, big data are also characterized by their ability to convert many aspects of the world that have never previously been quantified, into valuable data that can be analysed; a process known as datafication (Cukier and Mayer-Schoenberger, 2013) .\n\nHowever, there are several limitations associated with big data, of which researchers accustomed to working with smaller, conventional datasets, need to be aware (Cukier and Mayer-Schoenberger, 2013) . Firstly, when working with big data researchers need to accept that the data will not be pristine; however, working with vast quantities of data of slightly questionable quality invariably trumps using small amounts of very exact data. Secondly, big data requires a move from causation to correlation, so rather than trying to identify why something happens, big data allows us to search mammoth amounts of information about an event and anything associated with it, in order to identify patterns that might help predict future occurrences. In the words of Cukier and Mayer-Schoenberger (2013) , 'big data helps answer what, not why, and often that's good enough'.\n\nIf mined using internet-based tools, these big data are often capable of detecting the first evidence of a disease outbreak. Such systems are based on the assumption that changes in information and communication patterns on the Internet can act as early warning of changes in population health (Wilson and Brownstein, 2009 ) and comprise automated biosecurity intelligence text-mining systems that continuously query, filter, integrate and visualise infectious disease data from myriad primary or secondary data sources. Two such sites that have received a lot of attention are Google and the social-media platform, Twitter.\n\nThe immediacy of Twitter offers health officials an enormous advantage as both a surveillance and research tool. For example, emergency departments in Boston learned about the 2013 marathon bombings through Twitter before announcements from conventional sources such as the media or established emergency service communication channels (Cassa et al., 2013) . While terrorist attacks are an extreme case, the general principle also holds true for early warning of disease epidemics. Similarly, in addition to posting information about their health on social-media sites such as Twitter, data from search-queries have been found to be highly predictive of a wide range of population-level health events. For example, trends in Google and Yahoo search-queries have been used to predict influenza and dengue fever outbreaks (Chan et al., 2011) and estimate the prevalence of Lyme disease (Seifter et al., 2010) . In addition, the relative immediacy of internet-based surveillance systems also allows for much quicker targeting of infection hot-spots in pandemic situations, as was done by companies such as Google during the 2009 swine-flu pandemic (Chew and Eysenbach, 2010; Signorini et al., 2011; St Louis and Zorlu, 2012) .\n\n2.5.1.1. Search-term surveillance. Google's Flu Trends (GFT) (Ginsberg et al., 2009; Google Flu Trends) is perhaps the most well-known of the search-term surveillance systems. Combining data-mining of Google search queries and statistical modelling to provide a baseline indicator of the trend or changes in the rate of influenza, GFT provides estimates of weekly regional US influenza activity with a reporting lag of only one day compared with the 1-2 week delays associated with the CDC Influenza Sentinel Provider Surveillance reports (Ginsberg et al., 2009) . GFT has been extended to include surveillance for dengue -Google Dengue Trends (GDT) (Chan et al., 2011 ; Google Dengue Trends) -and also been used to develop early warning systems for influenza epidemics (Pervaiz et al., 2012; Dugas et al., 2013; Cook et al., 2011) .\n\nImplemented in 29 countries, with a focus on Europe (Google Flu Trends; Eurosurveillance Editorial Team, 2009; Valdivia et al., 2009) , GFT is currently best suited to track disease activity in developed countries as the system requires large populations of web-search users in order to be most effective (Carneiro and Mylonakis, 2009 ) and a robust existing surveillance system to provide data for calibration . However, even in countries where GFT is not yet officially available, such as China (Kang et al., 2013) and South Korea (Cho et al., 2013) , the system has been shown to complement the country's traditional influenza surveillance systems although an inability to search in the local languages remains a problem. In addition, analysis of the Google Trends' search frequency for the term 'Ebola' in Guinea, Liberia and Sierra Leone showed a moderate-to-high correlation with epidemic curves for the outbreak in those respective countries (Milinovich et al., 2015) suggesting that internet-based surveillance systems have the potential to form an early-warning system in developing, as well as in developed countries.\n\nHowever, opinion is divided on the accuracy of GFT; certain studies have shown its prevalence estimates to be highly correlated with actual disease risk (Cook et al., 2011; Ortiz et al., 2011; Dugas et al., 2012; Thompson et al., 2014) while others suggest GFT is not as reliable as CDC estimates (Lazer et al., 2014; Olson et al., 2013; Butler, 2013) . The fact remains that GFT has twice been caught out by the US annual flu season both under (2009) and overestimating (2013) national flu peaks (Butler, 2013) . A similar approach to GFT which uses Wikipedia searches to estimate US influenza prevalence, was recently shown to be more accurate than GFT, performing well through both the 2009 and 2013 epidemics that tripped up GFT (McIver and Brownstein, 2014) . Similarly, GDT estimates have been shown to be highly correlated with actual dengue incidence on a large (national) spatial scale (Chan et al., 2011; Althouse et al., 2011 ), yet results varied on a small (state) scale Gluskin et al., 2014 . Gluskin et al. (2014 attributed this variation to the fact that GDT appears to work best in areas with intense transmission, particularly where local climate is well suited to this.\n\n2.5.1.2. Crowdsourced tracking systems. Crowdsourced tracking surveillance systems, such as data-mining of Twitter posts, apply algorithms to filter tweets by specific keywords, assess their relevance and accuracy, geo-tag tweets and compare this information to other surveillance data. For example, NowTrending uses Twitter to track disease trends at both regional and national levels, presenting the most commonly tweeted diseases in a WordCloud. These metrics are intended to serve as an indicator of potential emerging health issues to spur further investigation and collection of direct measures of disease. In addition, recent studies demonstrate the value of combining social media with routine epidemiological data to detect or predict disease outbreaks, including influenza and cholera (Chew and Eysenbach, 2010; St Louis and Zorlu, 2012; Broniatowski et al., 2013; Chunara et al., 2012; Abrams et al., 2013) and to estimate weekly levels of influenza-like illness (Signorini et al., 2011) .\n\nAlthough one of the main advantages of crowd-sourced tracking surveillance systems is that of timeliness through the availability of real-time, georeferenced data (Stoov\u00e9 and Pedrana, 2014) , a major limitation is the large amount of unrelated 'noise' (Chew and Eysenbach, 2010; Broniatowski et al., 2013; Denecke et al., 2013) , although Broniatowski et al. (2013) appear to have developed an algorithm that can successfully distinguish relevant tweets from noise. The lack of specificity caused by noise may be less of an obstacle if the analysis is supported by trained public health officials who can investigate signals as they develop; Barboza et al. (2014) showed that systems including human moderation were found to have a 53% higher specificity after adjustment for other variables.\n\nAn additional weakness of Twitter is that its users do not represent a random sample of the population; the majority of Twitter users are aged between 18 and 50 (http://www.pewinternet.org/fact-sheets/social-networkingfact-sheet/) and therefore, drawing conclusions without considering the primary population demographic can be problematic. Furthermore, these surveillance tools appear to be most effective in developed countries; in Turkey a comparison of flu-related tweets with real world records showed no strong correlation (Bilge et al., 2012) . However, language does not appear to be an issue as attested by a Portuguese study that successfully trained a Na\u00efve Bayes classifier to identify tweets mentioning flu or flu-like illness or symptoms (Santos and Matos, 2014) .\n\nDespite limited evidence of internet-based surveillance systems to detect emerging threats before more traditional systems (Heymann and Rodier, 2001; Zeldenrust et al., 2008; Cowen et al., 2006) , their primary value currently lies in their ability to act as an early-warning system thereby lessening the consequences of an outbreak (Wilson and Brownstein, 2009; Hartley et al., 2013) . As such, although novel surveillance systems are still a long way from replacing traditional surveillance methods, they can usefully complement conventional approaches (Milinovich et al., 2014) , to the extent that they have become an important component of the influenza surveillance scene. For example, WHO's Global Outbreak Alert and Response Network uses such data as part of its day-to-day surveillance activities (Grein et al., 2000; Heymann and Rodier, 2001) and is authorized to act on this information (Wilson et al., 2008) . In addition, internet-based data sources exist outside traditional reporting channels and as such, are invaluable to public-health agencies that rely on the timely flow of information across administrative borders.\n\nHowever, search-term surveillance and crowdsource tracking systems clearly require in-depth evaluation, especially with respect to false positives and gaps in coverage and further work is necessary to determine how much of a change from baseline warrants further investigation.\n\nIn contrast to the surveillance systems that mine the primary data available through tweets and Google searches, there are also a number of surveillance systems that mine secondary data systems such as internet-based media sites; for example BioCaster (Collier et al., 2008 (Collier et al., , 2006 BioCaster) , EpiSPIDER (Keller et al., 2009; Tolentino et al., 2007) , HealthMap Wilson and Brownstein, 2009; Keller et al., 2009; Brownstein et al., 2010 Brownstein et al., , 2009 Freifeld et al., 2008; HealthMap) , ProMED-mail (Zeldenrust et al., 2008; Cowen et al., 2006; Tolentino et al., 2007; ProMED-mail) and Canada's Global Public Health Intelligence Network (GPHIN) Mykhalovskiy and Weir, 2006 .\n\nThe value of such systems for flagging potential health threats is evidenced by the fact that GPHIN identified the 2002 severe acute respiratory syndrome (SARS) outbreak in Guangdong Province, China, more than two months before the World Health Organisation's (WHO) official announcement (Mykhalovskiy and Weir, 2006) . Similarly, HealthMap identified news stories reporting a strange fever in Guinea nine days before official notification of the 2014 West Africa Ebola outbreak (Milinovich et al., 2015) . Developing country initiatives include India's Media Scanning & Verification Cell (MSVC) which scans global and national media sources and flags unusual health events, and has successfully flagged a number of outbreaks before they were identified by traditional surveillance systems (Sharma et al., 2012) .\n\nAlthough a comparison of BioCaster, EpiSPIDER and HealthMap identified significant differences in their ability to obtain relevant disease information (Lyon et al., 2012) owing mainly to differences in sources searched, languages read, regions of occurrence and types of cases (Lyon et al., 2012; Barboza et al., 2014 ) -running the three surveillance systems in parallel has been shown to enhance early detection of disease anomalies over traditional surveillance approaches (Barboza et al., 2014) . However, such automated systems are not without problems; for example, the location detection tool of all three systems assumed that the number of articles plotted for a country reflected the number of articles found about that country, which was not necessarily true (Lyon et al., 2012) .\n\nIn addition to the passive internet-based surveillance systems there are a number of active participatory surveillance systems that capture voluntarily submitted symptom data from the general public and can aggregate and communicate that data in near real-time thereby providing unique disease information that is not available through traditional surveillance sources. To date, such systems exist only for human diseases with a focus on influenza. Examples include Influenzanet, (FluTracking), Reporta, Flu Near You, Dengue na Web and SaludBoricu.\n\nThese systems show a high degree of accuracy and increased sensitivity and timeliness relative to traditional healthcare-based systems (Wojcik et al., 2014) and have proven useful for identifying risk groups, assessing disease burden, evaluating vaccination coverage and effectiveness, and informing disease transmission models (Marquet et al., 2006; Paolotti et al., 2010 Paolotti et al., , 2014 Van Noort et al., 2012; Van Noort et al., 2007; Parrella et al., 2009; Friesema et al., 2009; Brooks-Pollock et al., 2011) . In addition, they are cheaper and more flexible than traditional systems. Nevertheless, they present important challenges including, biases associated with the population that chooses to participate, difficulty in adjusting for confounders due to patients' unwillingness to complete long surveys, and limited specificity because of reliance on syndromic disease definitions (Wojcik et al., 2014) .\n\nAs a result of 21st century challenges, such as globalization and global warming, health officials and researchers are faced with increasingly complex and challenging disease problems which demand access to new and different types of data in order to inform effective risk-based disease surveillance and control strategies. The increasingly wide range of available spatial disease data may allow us to meet those challenges, assuming we see them as opportunities rather than problems; opportunities to convert aspects of the world that have never previously been quantified into valuable data that can shine a new light on health problems. Opportunities to develop timely and cost-effective online disease surveillance systems for developing nations that lack the necessary resources and infrastructure to implement traditional surveillance systems. Opportunities to transpose novel human health surveillance systems for use in animal health situations. And opportunities to respond to the increasingly complex disease problems facing us with state-of-the-art and spatially-explicit, risk-based disease surveillance and control strategies."}