{"title": "", "body": "Studies of infectious disease control efforts-such as screening, vaccination, and contact tracing (also known as partner notification)-often focus on evaluating the effectiveness of such programs. For example, Saretok and Brouwers [1] evaluated the likely epidemic impact of school closures in the event of a potential pandemic influenza outbreak; Bozzette et al. [2] evaluated the effectiveness of different vaccination strategies in mitigating the effects of a smallpox outbreak; and Hethcote [3] evaluated the impact on gonorrhea prevalence of six different control measures. Some analyses of infectious disease control efforts have focused on finding the most effective means of allocating a fixed amount of epidemic control resources across competing interventions and populations, and/or over time. For example, Zaric and Brandeau [4] determined the optimal allocation of a fixed budget among HIV prevention programs at a given point in time and over time [5] ; Longini et al. [6] determined the optimal distribution of a limited supply of vaccine in the event of an influenza pandemic; and Halloran et al. [7] performed a similar analysis in the event of a smallpox outbreak. Other analyses have evaluated the minimum cost means of achieving a desired level of epidemic control. For example, M\u00fcller [8] determined the cost-minimizing pattern of vaccination among different age groups to achieve a desired epidemic reproduction number, and Revelle et al. [9] determined the cost-minimizing use of treatment, prophylaxis, and BCG (Bacille Calmette-Guerin) vaccination to achieve a target prevalence of tuberculosis (TB) in a given time horizon.\n\nNone of these analyses considers the optimal level of investment in epidemic control. In some cases, epidemic control programs may have diminishing returns to scale: incremental investments in epidemic control can yield diminishing health benefits. Moreover, funds not spent to control one disease can often be spent to control another disease, or on other public health programs. Thus, investing the maximum available funds to control an infectious disease may not represent a costeffective use of resources. In this paper we present a methodology for determining the appropriate level of investment in contact tracing. We develop and apply a simulation model of contact tracing and the spread of an infectious disease among a network of individuals. We evaluate the cost and effectiveness of different levels of contact tracing, and show how to determine the appropriate level of investment.\n\nContact tracing is a primary means of disease control for infectious diseases with low prevalence. In the USA, contact tracing is required for TB [10] , recommended for human immunodeficiency virus (HIV) [11] , and not uncommon for other sexually transmitted diseases (STDs) [12, 13] . Contact tracing has also been used (and modeled) for severe acute respiratory syndrome (SARS) [14] , foot-and-mouth-disease [15] , smallpox [16, 17] , and avian influenza [18] .\n\nHyman et al. [19] and Armbruster and Brandeau [20] studied contact tracing using differential equation models that assume homogeneous mixing of the population. Kretzschmar [21] reviewed STD models on networks. M\u00fcller et al. [22] introduced one of the first models of contact tracing on a network and analyzed a stochastic branching process that approximates it. Subsequent work [15, 23, 24] analyzed similar models using both stochastic simulations and moment closures (also called mean-field approximations). Most of these papers study the effectiveness of contact tracing but do not consider the costs. Armbruster and Brandeau [20] and Wu et al. [18] incorporated costs in their analyses, but considered contact tracing as an all-or-nothing decision, with a fixed level of intensity.\n\nEmpirical studies of the cost effectiveness of contact tracing programs have been carried out for diseases such as TB [25, 26] , HIV [27, 28] , chlamydia [29] , syphilis [30] , and gonorrhea [31] . These studies all evaluate a single fixed level of contact tracing.\n\nArmbruster and Brandeau [20] presented a theoretical model for determining when (a fixed level of) contact tracing should supplement screening to control an endemic infectious disease. That work was based on a simple compartmental model (an SI model) with homogeneous mixing, and employed a highly stylized representation of the contact tracing process (with contact tracing yielding new identified disease cases at a constant rate, as a function of disease prevalence). A number of studies have shown that analyses based on more realistic models of disease transmission in social networks can yield significantly different projections of disease spread than projections generated by simple compartmental models [21, 32] . In addition, depending on how it is implemented (e.g., how many contacts of any index case are traced; which contacts are tracedand possibly removed-first; and other factors), a fixed level of contact tracing may yield different numbers of new index cases identified, as well as differing impact on the spread of the disease. Therefore, we use simulation to evaluate the effectiveness (and cost) of different levels of contact tracing-and thus to determine the appropriate level of investment in contact tracing.\n\nA number of researchers have analyzed disease transmission in social networks, but often with little knowledge of the actual network structure. Schneeberger et al. [33] used data on the distribution of the number of sexual partners when constructing networks of sexual contacts among individuals in Britain and Zimbabwe. A few studies have reconstructed various small social networks: for example, Klovdahl et al. [34] mapped the network of sexual contacts of injection drug users (IDUs) and prostitutes in Colorado Springs; Weeks et al. [35] described the social network of IDUs in Hartford, Connecticut; Parker et al. [36] mapped a network of sexual contacts among 154 people in London starting from a 20-year-old HIV-infected man; and Helleringer et al. [37] described the network of sexual contacts in a region of Malawi. Wylie and Jolly [38] were able to construct a larger network of sexual contacts among individuals in Manitoba, Canada by using the reported contacts of chlamydia and gonorrhea cases. For respiratory diseases, the contact network may be easier to find: Saretok and Brouwers [1] used the location of the homes and workplaces of Swedes to model the spread of an avian flu pandemic.\n\nThe following section describes our simulation model of contact tracing and the spread of an endemic infectious disease among individuals in a network. In Section 3, we simulate the contact tracing and disease process for different amounts of contact tracing. We show that contact tracing is likely to have diminishing returns to scale: incremental investments in contact tracing yield diminishing reductions in disease prevalence. Using the simulation results in conjunction with a cost-effectiveness threshold, we show how to determine the optimal level of investment in contact tracing. We extend the analysis in Section 4 to consider the optimal allocation of a budget between contact tracing and disease screening. We consider the cases of screening for exogenous infection (infected individuals entering the population) and endogenous infection (infected individuals already in the population). In Section 5 we illustrate the use of the model with an example of contact tracing for chlamydia control. We conclude in Section 6 with discussion of results and directions for future research.\n\nNetwork structure We consider an infectious disease that is endemic in a population of n individuals. We model individuals as nodes on an undirected graph where an edge between nodes i and j indicates that infection transmission can occur between i and j if one these individuals is infected (we say they are contacts of each other). We employ an SIRS epidemic model (described below) with five states: susceptible (S), infected (I), removed (R), susceptible and being traced as a contact (ST), and infected and being traced as a contact (IT).\n\nWe used random small-world graphs in our initial simulations. We chose this family of random graphs because it is the only one among the major families of random graphs (the others are Erdos-Renyi, regular, and scale-free) that allows for significant clustering and short paths between pairs of nodes. Watts and Strogatz [32] give examples of these features in many real networks and show that they significantly affect the spread of disease on a network.\n\nTo generate the graphs, we started with a cyclic regular graph of n nodes with degree 4 where node i connects to nodes i \u00b1 1, \u00b12 (mod n). For every other pair of nodes (i, j) we created a link independently with probability 1/n. This process creates a network in which each node has approximately five contacts on average. Figure 1 shows a small example of such a network with its nodes in various states. Our choice of n (500) and average degree (5) is consistent with data from the Colorado Springs study [34] , which found a main connected network with 669 individuals and a median of 5.1 risky relationships per person (11.7 relationships per person but of which 29% reported no risky behavior and 27% reported drug sharing without needle use).\n\nWe employ an SIRS epidemic model in which susceptible individuals become infectious, become removed when they are treated, and finally become susceptible after treatment. We assume that no deaths occur in the population over the simulation time horizon. Figure 2 illustrates the disease states and transitions among them. In our simulation, the sojourn time in each state was exponentially distributed for all states except for states ST and IT, where the sojourn time was a constant.\n\nWe assume that the rate of endogenous infection (transition from S \u2192 I) of node i (or, more precisely, the individual represented by node i) is proportional to d i , the number of infected neighbors of node i: specifically, the transition rate is d i /t 1 , where t 1 is a time constant. This stochastic process on a network is called a contact process. Individuals can also become infected from exogenous sources. This could be through international travel, for example, or by healthy people leaving the system and being replaced by infected immigrants. We assume that the rate at which exogenous infection occurs is given by a constant, \u03b7 1 among susceptible individuals and \u03b7 2 among removed individuals. The combined rate at which susceptible individuals become infected is then (d i /t 1 ) + \u03b7 1 .\n\nTo model contact tracing, Eames and Keeling [24] and Kiss et al. [15] extended the contact process so that infected nodes are found and cured at a rate proportional to the number of index case neighbors a node has (in our model, this would be individuals in state R), analogous to the infection process. This model of contact tracing does not allow us to compare different contact tracing budgets. Thus, we use a discrete-event simulation.\n\nWhen an infected individual seeks treatment for symptoms of the disease (and thus becomes known to the public health system), he or she becomes an index case. This corresponds to a transition in disease state from I \u2192 R. We assume that this transition happens at rate 1/t 2 , where t 2 is a time constant. When a new index case occurs, we apply our contact tracing policy to decide (based on only the graph structure and the removed nodes) which nodes to trace. Nodes selected for tracing then transition to state ST or state IT, depending on whether the individual is susceptible or infected, respectively. We assume that contact tracing requires a fixed amount of time, t 4 for state ST and t 5 for state IT.\n\nAfter tracing is completed, a node in state ST returns to state S, whereas a node in state IT transitions to state R and becomes a new index case. We assume that the contact tracing capacity, K, is expressed in terms of the maximum allowable contact tracing rate: at any point in time, at most K individuals in total can be in states ST or IT. We can think of the capacity K as being proportional to the manpower available for contact tracing.\n\nContact tracing process In contact tracing, every index case is asked to name his or her contacts (graph neighbors who may be infected). Then public health officials seek out these contacts (as time and resources permit) to test whether they are infected and treat them if so. Who to trace is an important tactical decision since the contact tracing capacity limits the number of individuals who can be traced at any point in time.\n\nIn our simulation we keep a prioritized list of contacts who have not yet been traced (nodes in state S or I that are neighbors of removed nodes). Every time a new index case is identified, we update this list and decide on additional nodes to trace. For our analyses in Sections 3 and 4 we assume that each index case names all of his/her contacts; for our example of chlamydia contact tracing in Section 5, we assume that individuals name only a fraction of their contacts. We let k be the number of contacts we would like to trace each time a new index case arrives. Since the list is prioritized, we trace the k nodes of highest priority, provided we have not exhausted the budget.\n\nBased on a simulation study of the effectiveness of alternative contact tracing strategies [39] , we selected the following scheme for prioritizing contacts for tracing. We assign each contact a score intended to reflect the likelihood that the contact is infected (the higher the score, the more likely that a contact is infected). The contact's score is the number of index cases who name that person. We set k, the number of contacts we trace each time a new index case arrives (assuming we still have resources), equal to 5.\n\nCosts and health outcomes We compare alternative levels of contact tracing based on the resulting effectiveness and annual costs in steady state. We consider both the cost of the contact tracing and the cost of treating disease cases. We assume a cost of c for treating a case of disease-this cost is incurred each time an individual transitions from disease state I to R-and an annual cost of C for each unit of contact tracing capacity (hence an annual contact tracing cost of KC for K units of capacity).\n\nWe consider two measures of contact tracing effectiveness: steady-state disease prevalence, and annual steady-state quality-adjusted life years (QALYs) experienced in the population. The steady-state disease prevalence is a simple measure of the effectiveness of a particular disease control strategy. Following standard cost-effectiveness practice [40] , we also measure the total QALYs experienced in the population each year. To do so, we assign quality multipliers (on a scale of 0 to 1, where 0 represents death and 1 represents perfect health) to health states. We assign a quality multiplier of 1 to uninfected individuals (those in states S, ST, and R), and a quality multiplier of q < 1 to infected individuals (those in states I and IT). Given a steadystate disease prevalence p, the steady-state number of QALYs experienced in the population during one year is n(1 \u2212 pq).\n\nSimulation runs We used the simulation model to estimate for various cases-e.g., for different levels of the contact tracing budget, for different amounts of contact tracing and screening, etc.-the annual steady-state treatment costs (adding the annual cost of the contact tracing, KC, yields the total cost for a year in steady state) and the steady-state prevalence of the disease (or, equivalently, the steady-state annual QALYs experienced in the population). To measure the steady state for a particular case, we performed 1,600 runs. For each run, we generated a random smallworld graph and infected a single random node. Then we simulated the network for five years in one-day time increments (1,825 days in total), taking the daily average prevalence (per capita frequency of states I and IT) starting with day 181. (Visual inspection of sample runs indicated that the system is in steady state by this time.) We averaged over all the runs and set our error bars to the 95% confidence intervals. Table 1 shows the values of all parameters we used in our simulations. These parameters are roughly consistent with the transmission and control of gonorrhea. The speed of disease transmission, t 1 , is consistent with unprotected sexual contact every 45 days, and a 50% chance of transmission per unprotected sexual contact [41] . Choosing a 30-day average duration between infection and treatment (t 3 ) is consistent with all women and 50% of men being asymptomatic, symptoms otherwise appearing after 4 days, and then a 3-day delay in obtaining treatment [41, 42] . Gonorrhea is treated with one dose of antibiotics which costs approximately $50 [43] . We estimated that individuals would refrain from risky behavior for an average of 3 months after treatment. Our contact tracing cost, C, ($120 per case, figuring a week t 4 = t 5 = 5 per case and 50 work weeks per year) is similar to that reported by Dasgupta et al. [25] . Quality multipliers for STDs are not well researched [44] . We chose a quality multiplier of 0.9, which has been used for TB, diabetes, and asymptomatic HIV infection [45] . 3 How many to trace?\n\nCost effectiveness threshold Choosing the budget for contact tracing is an important strategic decision. Funds not spent to trace a particular disease could be used for tracing other diseases, for other disease control efforts, or even for other public health efforts. Thus, we would like to determine the most \"cost-effective\" level of investment in contact tracing for a particular disease.\n\nCost-effectiveness analysis can help policy makers allocate money across different interventions for the same or different diseases [40] . In a typical cost-effectiveness analysis of alternative interventions, the analyst evaluates the total costs and health benefits-usually measured by quality-adjusted life years (QALYs) experienced-for each intervention. The analyst then identifies the non-dominated interventions (a dominated intervention is one that costs more and yields fewer QALYs than another single intervention or than a linear combination of two interventions). The undominated interventions can then be ranked in order of increasing cost to create an efficient frontier of interventions. The incremental cost-effectiveness ratio, defined as the incremental cost per QALY gained, increases as one moves along the efficient frontier. The \"best\" intervention is the one that has the highest possible incremental costeffectiveness ratio without exceeding a pre-determined cost-effectiveness threshold \u03b1. We will suppose in our analyses that a value for the cost-effectiveness threshold \u03b1 is known. This value is often determined as an implicit value given by accepted public health/medical practice [46] . In our model, the \"alternative interventions\" involve alternative levels of investment in contact tracing (or in contact tracing and screening). Because our simulation analyses focus on the steady-state disease prevalence achieved by a given level of contact tracing, we express all costs and health benefits in annual terms. Thus, the cost of each \"alternative intervention\" equals the annual cost of contact tracing (CK) plus the annual cost to treat cases of the disease. The effectiveness is measured in terms of the resulting steady-state prevalence, or equivalently as the total steady-state number of QALYs experienced in the population per year. Thus, for these analyses, the cost-effectiveness threshold \u03b1 represents the maximum amount we are willing to pay per year for one more steady-state QALY. We used the value \u03b1 = $50,000, a value commonly used in cost-effectiveness analyses of health-related interventions [46] .\n\nFinding the optimal level of contact tracing Figure 3a shows the steady-state disease prevalence as a function of the contact tracing capacity (K ranging from 0 to 10), using our baseline simulation parameters, and averaged over 1,600 runs. The convexity of the curve shows that the effectiveness of contact tracing (in terms of reductions in disease prevalence) decreases with its capacity (i.e., it has diminishing returns to scale): for each incremental increase in the contact Effect of varying the contact tracing capacity, K. Vertical bars represent 95% confidence intervals (1,600 runs). a Simulated steady-state disease prevalence as a function of contact tracing capacity, K. b Steady-state annual cost and QALYs experienced as a function of contact tracing capacity. The points correspond to capacity K ranging from 0 to 10, as in (a). At the optimal budget this curve has a slope of 1/\u03b1 (the slope of the gray line). Here \u03b1 = $50,000/QALY tracing capacity, the corresponding reduction in steadystate prevalence diminishes. This makes intuitive sense because as the contact tracing capacity increases and prevalence decreases, we trace more contacts, fewer of whom will be infected. Thus, the probability that the contacts we trace are infected decreases as the contact tracing capacity increases. Our simulation model allows us to quantify this decrease-and thus to evaluate the relative cost effectiveness of different amounts of contact tracing. Figure 3b , which is based on the same simulations as Fig. 3a , shows the total annual cost associated with each level of contact tracing (K ranging from 0 to 10), and the total annual (steady-state) QALYs experienced in the population (recall that the population size n = 500 is constant). The costs incurred include not only the capacity costs of contact tracing (C = $6,000 for each unit of capacity K) but also the costs of treating the disease (c = $50 per case treated). While the contact tracing capacity is the biggest annual expense, disease treatment costs cannot be neglected. Indeed, as contact tracing capacity (K) increases, treatment cost decreases because the prevalence of the disease decreases. Our simulation model allows us to quantify these costs and savings.\n\nThe curve connecting the 11 points in Fig. 3b is concave, reflecting the diminishing effectiveness of contact tracing with incremental investment. The optimal budget is given by the point on the curve where the tangent line has a slope of 1/\u03b1. At this point, we spend $18,000 per year to maintain a contact tracing capacity of 3 (i.e., the ability to trace 3 people at a time) and we spend approximately $7,500 per year on treatment of the disease, bringing the total cost to $25,500. At this point, the incremental cost per reduction in QALYs equals the maximum level we will tolerate, \u03b1. Above this point, increases in annual expenditure on contact tracing increase the annual number of QALYs experienced by less than 1 per $50,000 spent (i.e., less than the ratio 1/\u03b1) and are thus deemed not cost effective.\n\nSensitivity analysis Using the simulation model, we can determine how the optimal contact tracing capacity will vary as a function of various disease parameters. We performed one-way sensitivity analysis on the disease transmissibility (i.e., baseline disease incidence, which is proportional to the parameter t 1 ), the rate at which symptoms develop (and thus the rate at which individuals seek and receive treatment, which is proportional to t 2 ), the expected length of time a treated individual remains in the Removed state (proportional to the parameter t 3 ), and the expected length of time between exogenous infections (1/\u03b7 := 1/\u03b7 1 = 1/\u03b7 2 ). We varied each parameter to 10% above and below the base case.\n\nFor each case, Fig. 4 shows the steady-state disease prevalence as a function of the contact tracing capacity. Table 2 shows the effect of changes in these parameters on the optimal capacity (the capacity at which the marginal total cost per QALY gained equals the costeffectiveness threshold).\n\nFor any given contact tracing capacity, the effectiveness of contact tracing (measured by the steadystate prevalence) is affected the most by the rate at which symptoms develop. If that rate is slower than in the base case (and thus there are more asymptomatic infected people in the population), then steady-state prevalence is higher than in the base case; conversely, if the rate is faster, steady-state prevalence is lower. A 10% decrease in this parameter (slower symptom development) increased the optimal capacity by 2, and a 10% increase in this parameter (faster symptom development) decreased the optimal capacity by 2. Changes in transmissibility also had a significant impact on prevalence. For a 10% increase in transmissibility (corresponding to a 10% decrease in the parameter t 1 ), the optimal capacity increased by 1; for a 10% decrease in transmissibility, the optimal capacity decreased by 2.\n\nChanges in the rate at which treated individuals return to the susceptible population (1/t 3 ) and in the rate of exogenous infection (1/\u03b7 1 , 1/\u03b7 2 ) had little effect on steady-state prevalence, and thus little effect on the optimal contact tracing capacity.\n\nWe performed sensitivity analysis on the network structure by comparing the base case model (which uses a small-world network) to a scale-free network (also known as a preferential-attachment network). We assumed that, although one may not know many details of the contact network, one would likely have a reasonable estimate of disease prevalence in the population, so we used this value as a point of comparison. Thus, we set the average degree of the scale-free network to 2.4 so that the steady-state disease prevalence in the absence of contact tracing would be the same as that for the small-world network with no contact tracing. The degree of 2.4 is less than the value 5 used in the smallworld network because, with a scale-free network, the disease spreads more efficiently: the scale-free network has a few individuals with many more contacts than average, and these allow for relatively efficient disease transmission. Figure 5 shows steady-state annual QALYs experienced for different levels of contact tracing for the small-world network (the same curve as in Fig. 3b ) and for the scale-free network. In both cases, contact tracing has diminishing returns to scale (i.e., the curves are concave). For non-zero levels of contact tracing, disease prevalence is slightly lower in the scale-free network than in the small-world network. This is expected because the scale-free network has some individuals who are highly connected, and finding them (through contact tracing) has a large payoff. For this example, the optimal annual investment is approximately $1,000 more for the case of the scale-free network than for the case of the small-world network. This sensitivity analysis highlights the importance of good information about network structure when evaluating how much to invest in contact tracing.\n\nThe above sensitivity analyses explore how the optimal budget changes as the network structure and epidemic parameters change. We performed additional sensitivity analyses in which we varied Fig. 3b ) and on a scale-free network of the same size (n = 500) whose average degree (2.4) is chosen so that the steady-state prevalence without contact tracing is similar to that for the small-world network. The points correspond to the contact tracing capacity K ranging from 0 to 10. At the optimal budget this curve has a slope of 1/\u03b1 (the slope of the gray lines). Here \u03b1 = $50,000/QALY. Vertical bars represent 95% confidence intervals (1,200 runs) parameter values by up to 200%; multi-way sensitivity analyses in which we varied disease parameters simultaneously; and a stochastic sensitivity analysis in which all parameters were varied within \u00b110% of their base value. In all cases, contact tracing exhibited diminishing returns to scale as a function of the budget.\n\nThus far, the only form of disease control we have considered is contact tracing. Disease prevalence can also be decreased by screening. One could screen for cases of endogenous infection (cases of infection caused by transmission from individuals in the population) or for cases of exogenous infection (e.g., among immigrants, visitors from other countries, and travelers returning from vacation).\n\nIn this section, we address the problem of allocating a combined capacity K total = K + \u03bb between contact tracing and screening, and the problem of determining the optimal total capacity K total . Here K is the capacity (manpower) allocated to contact tracing (as in the previous section) and \u03bb is the capacity allocated to screening.\n\nThe benefits of contact tracing and screening are larger than the sum of the benefits of doing them separately: the cost effectiveness of contact tracing varies with the amount of screening performed and vice versa. We thus use simulation to determine the optimal mix of contact tracing and screening: we simulate different allocations of a fixed capacity to determine the effectiveness of each combination. Once we know the cost and effectiveness of each combination, we can use the threshold value \u03b1 to determine the optimal total capacity, and the corresponding optimal level of investment in each type of control.\n\nScreening for exogenous infection We first consider the case of screening for exogenous infection. Exogenous infection can be a major source of new infection for many diseases: for example, many TB index cases in the USA are individuals who have brought the infection from another country. In the USA and Canada, longterm immigrants are screened for active TB and HIV as part of the visa process [47, 48] .\n\nWe assume that with each capacity unit we can either screen 12 people or trace one contact (every t 4 = t 5 = 5 days) because the cost of tracing one contact is approximately $120 (as discussed earlier) and the cost of a gonorrhea test is $10 [43] . Without any screening, 0.056 exogenous infections occur in the population each day (calculated as n \u03b7 1 = n \u03b7 2 = 500/9,000). We assume that 0.17% of new entrants are infected on average (consistent with gonorrhea prevalence rates in some Asian and eastern European countries [49] ); thus, the rate of exogenous infection as a function of \u03bb, the amount of the capacity allocated to screening, is \u03b7 1 = \u03b7 2 = 1 n (5/90 \u2212 0.0017\u03bb12/5). Figure 6a shows the steady-state prevalence achieved as we vary \u03bb/K total for different total capacities. As one would expect, steady-state prevalence decreases as the total capacity for contact tracing and screening increases. Additionally, we see from this figure that allocating a small fraction of the total capacity to exogenous screening is better for smaller total capacities (no screening, \u03bb = 0, is optimal for K total \u2264 5), whereas for larger total capacity it is better to allocate more of the total capacity to exogenous screening.\n\nWith this information about the effects of alternative allocations of any fixed total capacity between contact tracing and screening, we can revisit the decision of how large to make the total capacity K total . Figure 6b shows the steady-state prevalence as a function of the total capacity, where the capacity is allocated between exogenous screening and tracing so as to minimize the resulting steady-state prevalence (the corresponding minimum from Fig. 6a ). Figure 6b also shows the steady-state prevalence as a function of the total optimal mix contact tracing only c Fig. 6 Effects of allocating a total capacity K total = K + \u03bb between contact tracing, K, and screening for exogenous infections, \u03bb. Vertical bars represent 95% confidence intervals (1,600 runs) . a Steady-state prevalence as a function of the fraction spent on screening \u03bb/K total for various values of the total capacity K total . b Steady-state prevalence achieved as a function of the total capacity K total for screening and contact tracing. The solid line allocates the capacity optimally while the dotted line is from Fig. 3a where we used no screening (\u03bb = 0). c Steady-state annual cost and QALYs experienced as a function of the total capacity K total for screening and contact tracing. The points arranged (more or less) vertically represent different allocations of a given total capacity, K total , between screening and contact tracing. At the optimal budget, this curve has slope equal to 1/\u03b1 (the slope of the gray line). Here \u03b1 = $50,000/QALY capacity, assuming that no screening is used. We see that for capacity K total > 5, using a mix of screening and contact tracing achieves lower disease prevalence than does contact tracing alone, and the difference increases as the total capacity increases. Figure 6c shows the total cost of each strategy, including treatment costs (for different levels of K total and different allocations of K total between contact tracing and screening), and the resulting annual steady-state QALYs experienced. The points arranged (more or less) vertically represent different allocations of a given total capacity between screening and contact tracing. The efficient frontier in Fig. 6c connects the best strategy for each total capacity level. The optimal strategy is given by the point on the curve where the tangent line has a slope of 1/\u03b1. At this point, the annual cost is $25,500 with approximately 499.1 steady-state QALYs experienced per year. This point corresponds to a capacity K total of 3, all of which is allocated to contact tracing (thus, three contacts traced at any one time), with annual disease treatment costs of approximately $7,500. This is the same solution as was found for the case of contact tracing only.\n\nScreening for endogenous infection We next consider the case of screening for endogenous infection. In our simulation this takes the form of random screening of members of the population. One could think of such screening as resulting from encounters that individuals have with health care providers (either due to symptoms of the disease or for another reason) in which screening is offered. An increase in the endogenous screening rate increases the number of individuals in the population who are screened per unit time, and could correspond to an increase in the rate at which health care providers offer screening to patients. (For example, one practical means of achieving higher rates of routine HIV screening in the USA, as recently recommended by the CDC [50], is to encourage more doctors to routinely offer HIV tests to the patients they see.)\n\nFor the case of endogenous screening, we assume that with each capacity unit we can screen 200 people per year. Thus, the mean rate at which individuals move from the infected state (I) to the recovered state (R) when there is endogenous screening at rate \u03bb is 1/t 2 = 1/30 + \u03bb200/n/365. Figure 7a shows the steadystate prevalence achieved as a function of \u03bb/K total for different total capacities K total , and Fig. 7b shows the steady-state prevalence achieved as a function of the combined capacity K total for endogenous screening and contact tracing. As for the case of exogenous screening, allowing for the possibility of screening for endogenous infection (as occurs in the optimal mix) can reduce prevalence more than contact tracing alone, and the reduction becomes more pronounced as the total capacity increases.\n\nFrom Fig. 7c we see that the cost-effectiveness threshold is reached at a point where the total cost is approximately $25,000. This corresponds to a total capacity of K total = 3 (annual cost $18,000) plus approximately $7,000 in annual treatment costs. From Fig. 7a we see that for K total = 3, approximately one-third of the capacity is allocated to screening and two-thirds is allocated to contact tracing.\n\nIn this section we illustrate the use of our model to evaluate contact tracing for control of chlamydia, a common STD. Estimated chlamydia prevalence in the general US population is about 0.3%, but among young, sexually active individuals (age 15 to 24), prevalence has been found to be 6% or higher [51, 52] . Contact tracing is commonly performed for chlamydia.\n\nWe modeled a population of size n = 500, reflecting, for example, the size of the sexually active population in a high school. We used the same SIRS model as in Fig. 2 (but with different parameter values) . We modeled heterosexual transmission of chlamydia, and assumed that the population comprised equal numbers of males and females.\n\nTo model this heterosexual population, we created a bipartite graph with equal numbers of males and females. In addition, we modeled low-risk and highrisk males and females, to reflect different levels of risk behavior. To do so, we subdivided the male and female populations into high-contact groups (20% of the total) and low-contact groups (80% of the total).\n\nWe assumed an average of three sexual contacts (partnerships) per person. We adjusted this figure upward for high-risk individuals and downward for lowrisk individuals. We assumed that the probability that any male-female pair are contacts is independent, and we assumed that these probabilities have a ratio of 7:5:1 for contacts between high-risk individuals, contacts between high-risk and low-risk individuals, and contacts between low-risk individuals, respectively.\n\nWe set t 2 = 15/0.3 = 50 days to reflect the fact that chlamydia symptoms appear within 1 to 3 weeks after infection, but in 70% of cases, the infection remains asymptomatic [53] . We set the time associated with the sufficient contact rate, t 1 , equal to 100. This yields a Fig. 7 Effects of allocating a total capacity K total = K + \u03bb between contact tracing, K, and screening for endogenous infections, \u03bb. Vertical bars represent 95% confidence intervals (1,600 runs). a Steady-state prevalence as a function of the fraction spent on screening \u03bb/K total for various values of the total capacity K total . b Steady-state prevalence achieved as a function of the total capacity K total for screening and contact tracing. The solid line allocates the capacity optimally while the dotted line is from Fig. 3a where we used no screening (\u03bb = 0). c Steady-state annual cost and QALYs experienced as a function of the total capacity K total for screening and contact tracing. The points arranged (more or less) vertically represent different allocations of a given total capacity, K total , between screening and contact tracing. At the optimal budget, this curve has slope equal to 1/\u03b1 (the slope of the gray line). Here \u03b1 = 50,000/QALY baseline chlamydia prevalence of 8%, consistent with a study of teenage girls in Philadelphia [51] (before any intervention). Zimmerman et al. [54] found that clients at STD clinics who were found to be infected with chlamydia reported an average of 1.7 contacts. Hence we set the probability that a contact is reported to 1.7/3 = 57%. We estimated the cost of treating one case of chlamydia as c = $50 + 0.3($500) = $200: in 70% of cases, an inexpensive ($50) course of antibiotics is sufficient to treat the infection, but in roughly 30% of the cases, patients develop acute pelvic inflammatory disease which must be treated at an additional cost of Vertical bars represent 95% confidence intervals (1,600 runs). a Simulated steady-state disease prevalence as a function of contact tracing capacity, K, for the case of chlamydia. b Steady-state annual cost and QALYs experienced as a function of contact tracing capacity. The points correspond to capacity K ranging from 0 to 15, as in (a). At the optimal budget, this curve has a slope of 1/\u03b1 (the slope of the gray line). Here \u03b1 = 50,000/QALY $500 [55] . For all other parameters of the model (t 3 , t 4 , t 5 , \u03b7 1 , \u03b7 2 , and C) we used the same values as shown in Table 1 . Figure 8a shows the effect of different contact tracing capacity levels (K = 0 to 15) on steady-state chlamydia prevalence in the population. In the absence of any contact tracing, steady-state disease prevalence is 8%. As contact tracing capacity increases, prevalence decreases, but with diminishing returns, as expected. For K = 5, steady-state prevalence is 4%; for K = 10, steady-state prevalence is approximately 2.7%; for K = 15, steady-state prevalence is 2.3%. Figure 8b shows annual costs and QALYs experienced for each level of contact tracing. For a costeffectiveness threshold of \u03b1 = $50,000/QALY gained, the optimal contact tracing capacity is K = 11. The annual cost is approximately $92,000, corresponding to $66,000 for contact tracing plus $26,000 for treatment. The resulting endemic level of disease prevalence is approximately 2.6% (Fig. 8a ). If the public health budget for this example constrains contact tracing to fewer than 11 individuals being traced simultaneously, Fig. 8b shows that significant health benefits can still be achieved by contact tracing. For example, a contact tracing capacity of K = 6 will still reduce steady-state prevalence from 8 to 3.7%. A simulation model such as ours allows one to quantify the tradeoffs associated with different levels of contact tracing.\n\nWe have presented a general framework for evaluating the optimal level of investment in contact tracing. This framework combines concepts from cost-effectiveness analysis with simulation of a disease and the effects of contact tracing among a network of individuals. Our simulation results suggest that contact tracing is likely to have diminishing returns to scale: incremental increases in the budget for contact tracing yield diminishing decreases in the disease prevalence. This makes intuitive sense: the larger the number of contacts traced per unit time, the less likely it is that the incremental contacts traced will be infected, and thus the smaller the number of new cases that will be identified and removed. Use of a cost-effectiveness framework, combined with the simulation results, allows one to determine the appropriate level of investment in contact tracing. We also showed how, when other interventions such as screening are available, simulation can be used to determine the best mix of interventions for any given capacity level, and then cost-effectiveness principles can be applied to determine the appropriate total capacity for screening and contact tracing.\n\nOur results are based on a limited set of simulations. Further analyses could explore the robustness of our findings under different conditions: for example, for different networks, diseases, epidemic models, disease parameters, and populations.\n\nOne useful avenue for future research is to extend our simulation model to capture more details of contact tracing and disease transmission and progression. For example, our current model stylizes the screening of infections from exogenous sources. A more realistic model could break out the various sources of exogenous infection (e.g., holiday travelers, visitors from certain countries, legal immigrants, and illegal immigrants) and the opportunities to screen them (e.g., when they request a visa or at clinics in immigrant neighborhoods). As another example, our model of contact tracing does not include the genotype information available to investigators that allows them to distinguish between new and continuing outbreaks. In practice, when a new outbreak of a disease is detected, the intensity of contact tracing is often increased until a significant level of epidemic control has been achieved. Thus, a natural extension of our work is to consider the case of dynamically changing levels of contact tracing. A more sophisticated model would show additional benefits of contact tracing not captured by our model. As contact tracing capacity increases, the average time from acquisition of infection to detection and treatment decreases. Our model captures this effect (Fig. 9) . However, our model does not capture the fact that Contact tracing capacity Days until treatment Fig. 9 Average time from acquisition of infection until treatment is initiated, as a function of contact tracing capacity, K. Vertical bars represent 95% confidence intervals (1,600 runs) the most severe symptoms and complications are in patients whose disease was untreated the longest. Extending our model to include multiple infected states (for example, states representing benign versus acute symptoms), which differ in their infectiousness, quality multipliers, and treatment costs, would model this phenomenon and thus help quantify this additional benefit of contact tracing.\n\nThe simplicity of our current model also means that the selection of contacts to trace is extremely crude and does not reflect all the considerations (such as stage of infection, strength of immune system, age, demographics, etc.) that are used in the real world. By assuming that contacts are located after a constant number of days, our model ignores the possibility of not locating a contact, the uncertainty in the time required to locate a contact, and the option of giving up the search. A model incorporating such features would be more realistic and could be used to compare different strategies for selecting who to trace.\n\nAnother useful avenue for further research would be to tailor the analysis to specific diseases of interest. A tailored model could be used to determine the appropriate level of contact tracing for a specific disease in a specific region. The simulation model we have presented here could provide the foundation for such a policy tool. As an example, we applied our SIRS model to show how one could examine different levels of contact tracing for chlamydia control. For other diseases, a different epidemic model (with appropriate adjustments to the network model) might be needed.\n\nFor example, for TB, the disease model should include latent and active infection stages, with disease progression times set appropriately. Further, the contact network should allow contacts of greater and lesser strength (e.g., family members versus coworkers in a well-ventilated office). In our simulation, a contact's priority score is an indicator of the likelihood that this contact is infected. To better model TB contact tracing, it would be useful to distinguish individuals by their potential chance of acquiring infection, as is done in practice. For example, TB contact tracing in the USA gives priority to contacts who are children or who have AIDS.\n\nTo model HIV and some STDs, the disease model should include asymptomatic and symptomatic disease stages. For HIV, it might be appropriate to incorporate several modes of transmission (e.g., heterosexual partnerships, same-sex partnerships, and needlesharing partnerships). In addition, use of a dynamic contact network would reflect the pair formation and dissolution that occurs in social networks of such diseases (see, for example, Kretzschmar [21] ).\n\nMore work is also needed to understand the network structure underlying a particular epidemic in a particular population. Some limited work has been done to characterize specific networks of infectious disease (e.g., Klovdahl et al. [34] , Weeks et al. [35] , Parker et al. [36] , Helleringer et al. [37] ). Further knowledge of network structure, and related disease transmission features such as pair formation and dissolution, is crucial to evaluating the effectiveness, and cost-effectiveness, of different investments in contact tracing in any given setting.\n\nWe have considered contact tracing in an endemic disease setting. Contact tracing is also important (perhaps more so) for containing outbreaks of epidemic diseases. Then the crucial question is whether one can find the contacts faster than they can spread the disease, or win the \"race to trace\" as Kaplan et al. [56] put it. For this problem, the decision maker's goal is likely to be that of determining the minimum level of contact tracing that is needed to reduce the reproductive rate of infection below 1, perhaps within a specified period of time. This approach may be relevant for particularly virulent diseases such as smallpox [56] , extremely drugresistant TB [57] , or a gonorrhea \"superbug\" [58] .\n\nContact tracing can be an effective means of disease control, but it is only useful up to a point because incremental increases in the level of contact tracing are likely to yield diminishing benefits. Simulation can be used to estimate the benefits of contact tracing as a function of its intensity. Then, cost-effectiveness analysis can be used to determine the optimal level of investment in contact tracing (or the optimal level of investment in contact tracing and screening). Such analysis can help public health departments make the most cost-effective use of their available funds for disease control."}