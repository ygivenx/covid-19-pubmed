{"title": "Accurate prediction of human essential genes using only nucleotide composition and association information", "body": "Catalogs of essential genes on a whole-genome scale, determined using wet-lab methods, are available for several prokaryotic (Mushegian and Koonin 1996; Luo et al. 2014; Pechter et al. 2016 ) and eukaryotic organisms (Kamath et al. 2003; Amsterdam et al. 2004; Liao and Zhang 2007; Kim et al. 2010) . Computational methods with high accuracy offer an appealing alternative method for identifying essential genes. Computational methods are broadly divided into three types: machine learning-based methods combining intrinsic and context-dependent features (Deng et al. 2011; Cheng et al. 2013) , flux balance analysis-based methods (Kuepfer et al. 2005; del Rio et al. 2009; Gatto et al. 2015) , and homology search and evolutionary analysis-based methods (Peng et al. 2012; Wei et al. 2013) . With respect to essential gene prediction in bacteria, we integrated the orthology and phylogenetic information and subsequently developed a universal tool named Geptop , which has shown the highest accuracy among all state-of-the-art algorithms.\n\nSome studies have focused on essential gene prediction in eukaryotic genomes.\n\nIn 2005, Xu et al. investigated protein dispensability in Saccharomyces cerevisiae by combining high-throughput data and machine learning-based methods (Chen and Xu 2005) . In 2006, Seringhaus et al. reported a machine learning-based method that integrated various intrinsic and predicted features to identify essential genes in yeast S. cerevisiae genomes (Seringhaus et al. 2006) . They for the first time using AUC to measure the machine learning classifier's capability in identifying essential genes, and they also translated the essentiality from yeast S. cerevisiae to yeast Saccharomyces All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint mikatae. Yuan et al. integrated informative genomic features to perform knockout lethality predictions in mice using three machine learning-based methods (Yuan et al. 2012 ). Lloyd et al. analyzed the characteristics of essential genes in the Arabidopsis thaliana genome and used A. thaliana as a machine learning-based model to transform the essentiality annotations to Oryza sativa and S. cerevisiae (Lloyd et al. 2015) .\n\nRecently, three research teams approximately identified 2,000 essential genes in human cancer cell lines using CRISPR-Cas9 and gene-trap technology (Blomen et al. 2015; Hart et al. 2015; Wang et al. 2015) . Their results showed high consistency, which further confirmed the accuracy and robustness of the essential gene sets (Fraser 2015) . These studies provided an in-depth analysis of tumor-specific essential genes and feasible methods to screen tumor-specific essential genes (Fraser 2015; Moffat 2016). The essential genes screened by these three teams provided a clear definition of the requirements for sustaining the basic cell activities of individual human tumor cell types. Practically, these genes can be regarded as targets for cancer treatment (Fraser 2015) . The data from these three groups provided a rare opportunity to theoretically study the function, sequence composition, evolution and network topology of human essential genes. One of the most important and interesting theoretical issues in modern biology is whether essential genes and non-essential genes can be accurately classified using computational methods. The models established in the aforementioned three eukaryotic organisms, S. cerevisiae (Chen and Xu 2005; Seringhaus et al. 2006) , Mus musculus (Yuan et al. 2012) , and A. thaliana All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint (Lloyd et al. 2015) , involved intrinsic features, or intrinsic and context-dependent features. These context-dependent features included those features extracted from experimental omics data. However, the features derived from experimental data are frequently unavailable; consequently, this type of machine learning model cannot be extended to a wide range of genomes. In the present study, we addressed this problem in humans by using only intrinsic features derived from sequences, from which certain features can be characterized using a \u03bb -interval Z-curve. To facilitate the use of these data by interested researchers, we have provided a user-friendly online web server, Pheg, which can be freely accessed without registration at http://cefg.uestc.edu.cn/Pheg.\n\nCross-validation Results.\n\nThe final features of this method were described by FV w,\u03bb. , a value that contains information on the composition of the adjacent w-nucleotides (Gao and Zhang 2004) and \u03bb -interval nucleotides. The association information was also captured by FV w,\u03bb .\n\nTherefore, this method achieves improved performance compared with using the original Z-curve. The following results solidly confirmed this point.\n\nWe performed a 5-fold cross-validation test with w ranging from 2 to 4 and \u03bb ranging from 0 to 5. The detailed results are provided in mitigate these disadvantages. The support vector machine (SVM)-recursive feature extraction (RFE)+correlation bias reduction (CBR) method was adopted to rank these features in descending order based on the contribution of each feature. Subsequently, All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint the top 100 features were used to constitute the initial feature subset to train and test the model, and the next 100 features were added into the feature subset, followed by prediction using the same methods. This process was repeated until the top 4,500 features had been added according to the rank order. The test results of each model were evaluated according to the AUC scores via a 5-fold cross-validation test. The AUC values for different top features are shown in Figure 1 Among all 4,545 features examined, the best AUC of 0.8814 was achieved for the top 800 selective features. The final AUC value was 8.12% higher than that for FV 2,0 . To conduct an objective evaluation of this method, we performed a rigorous jackknife test based on the top 800 selected features using the parameters determined via a 5-fold cross-validation test. We obtained an AUC value of 0.8854. As expected, excellent performance was obtained after adopting the \u03bb -interval nucleotide All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint composition and feature selection technology. Those results illustrated that the essentiality of human genes could be well reflected by only sequence information.\n\nAs we are extremely interested in the actual essential genes in the predicted results, we used the positive predictive value (PPV) to further refine. This evaluation index can be calculated using the formula TP / (TP+FP), where TP (true positive) and FP (false positive) represent the number of real essential and non-essential genes among the positive predictions. Therefore, the PPV reflects the proportion of actual essential genes among the predicted essential genes. We obtained a PPV of 73.05% (TP=515, FP=190) using the jackknife test based on the top 800 features. One of the simplest cross validation tests is the holdout method. In this procedure, the dataset is separated into two subsets, namely, training and testing datasets. We randomly sampled one-fifth of the positive and negative samples from the benchmark dataset for the training model, and the remaining samples were used as the testing dataset. To comprehensively assess the method used in the present study, we repeated the holdout method 100 times to identify differences in the composition of the training and testing samples. The mean AUC score was used as the final evaluator. A mean AUC score of 0.8537 with a variance of 1.67e-005 was obtained. Additionally, the proportions of samples in the training and predicting datasets were changed for further investigation.\n\nOne-tenth of the positive and negative samples were randomly sampled as the training dataset, and the remaining samples were used as the testing dataset. This procedure was repeated 100 times. We obtained a mean AUC score of 0.8347 with a variance of 2.77e-005. These results further confirmed that this method was robust and accurate.\n\n1 0\n\nFeatures with fixed w and \u03bb values correspond to a specific group of variables. A total of 19 special groups were obtained, namely, fv 1,0 , fv 1,1 , fv 1,2 \u2026. fv 1,5 ; fv 2,0 , fv 2,1 \u2026. fv 2,5 ; fv 3,0 , fv 3,1 \u2026. fv 3,5 , and fv 4,0 . We calculated the percentage of features in these groups, and the results are provided in Table 2 . For each group, there were two frequencies: P(A), which denotes the actual frequency of features in each group appearing in the top 800 selected features, and P(E), which denotes the expected frequency of the features in each group appearing in the original All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint 1 4,545 features. Therefore, P(A) was obtained based on the number of selected features in each group divided by 800, and P(E) was calculated by dividing the number of total features in each group by 4,545. P(A) and P(E) are listed in columns 4 and 5, respectively. If P(A) is higher than P(E), then the group makes a higher-than-average contribution to the identification of essential genes. We calculated the tendentiousness using the formula P(A)/P(E)-1 to denote how the features of each group were selected, and the results are listed in column 6 of Table 2 . We further conducted a hypergeometric distribution test for each group, and the p values are listed in column 7. Figure 1 (B) and Table 2 show that fv 2,0 (p = 1.60E-06), fv 2,1 (p = 0.02407688), and fv 3,0 (p = 7.89E-05) are preferentially selected and are statistically significant. These results demonstrated that there are strong signals for classifying essential and non-essential genes when the character interval is equal to zero or one, but the other groups did not show these strong signals. To further confirm this result, the variables fv 2,0 , fv 2,1 , fv 2,2 , fv 2,3 , fv 2,4 , fv 2,5 and fv 3,0 , fv 3,1 , fv 3,2 , fv 3,3 , fv 3,4 , and fv 3,5 were used as input features. Improved performance was obtained under fv 2,0 , fv 2,1 , and fv 3,0 compared with the other groups (Table 2, column 8). Those results demonstrated that the shorter interval association provides more information. However, longer interval association can still play an independent role. Hence, integrating the interval information into adjacent ones could significantly improve our classifier's capacity of discernment (Table 1) .\n\nA web server for predicting essential genes in human cancer cell lines.\n\nTo facilitate the use of these data by interested researchers, we constructed a All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint user-friendly online web server named Pheg (Predictor of human essential genes), which is freely accessible at http://cefg.uestc.edu.cn/Pheg. The Pheg algorithm is based on the \u03bb -interval Z-curve. Additional parameters are not necessary, making this algorithm convenient to use. Pheg can predict whether a query gene is essential using only the CDS region of a gene as input. We integrated logistic regression into the Pheg server to estimate the reliability of the predicted results. Hence, this server can output a probabilistic estimated value as a measurement of gene essentiality for the inputted coding region. Note that this is the first available server for predicting human gene essentiality. Comparatively, some computational models have been proposed for the other eukaryotes however all of them did not provide online prediction service.\n\nWe re-predicted the genes in the benchmark dataset via Pheg and obtained an AUC = 0.9249 and PPV = 83.84%. A total of 612 genes were identified as essential genes among the 1,516 positive samples, and 118 genes were predicted as essential genes among the 10,499 negative samples. To estimate how many genes among those predictions are real essential genes, we calculated precisions using 5-fold, 10-fold, 15-fold, 20-fold cross-validation, and we obtained precisions with values of 70.43%, 71.63%, 72.48%, 72.22%, which were approximately 70%. Hence, we expect that 82 (118\u00d770%) are correctly predicted essential genes. The information for these 118 genes is provided in Supplemental Information S1.\n\nCancerResource is a comprehensive knowledgebase for drug-target relationships associated with cancer and for supporting information or experimental data (Gohlke et al. 2016) . Through the CancerResource database (Gohlke et al. 2016) , these 118 genes All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. CancerResource analyses and BLAST search illustrated that at least a part of these 118 genes have higher probability to be factually essential genes and have been overlooked in the essential gene screening in previous experimental studies. Hence, Pheg sever could be used to predict essentiality for anonymous gene sequences of human and closely related species, and it is also hoped to supplement the essential All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint gene list of human by identifying novel essential gene from the submitted sequences.\n\nThe Z-curve has been widely used in the field of bioinformatics for tasks such as protein coding gene identification (Zhang and Wang 2000; Chen et al. 2003; Guo et al. 2003; Guo and Zhang 2006; Hua et al. 2015) , promoter recognition (Yang et al. 2008) , translation start recognition (Ou et al. 2004) , recombination spots recognition (Dong et al. 2016) , and nucleosome position mapping ). However, correlation and \u03bb -interval nucleotide composition have not been incorporated into the Z-curve method. In the present study, we present a \u03bb -interval Z-curve based on Z-curve theory.\n\nThe DNA sequence can be understood as an ordinary character sequence; therefore, the method proposed in the present study has the potential for applications in mining characteristics from other character sequences and can be used as a universal feature extraction method for DNA sequences.\n\nBased on the \u03bb -interval Z-curve, we obtained excellent performance in human essential gene identification. This excellent performance might be attributable to the following points: First, we introduced the concept of intervals, reflecting associated information and the \u03bb -interval nucleotide composition. Second, we used feature selection technology in the present study. Thus, noisy and redundant features could be removed from the original features. Table 2 shows the improved performance obtained under fv 2,0 , fv 2,1 , and fv 3,0 compared with the other variable groups. Further comparison of these results with other feature groups shown in Table 2 , and this comparison shows that the AUC values obtained with \u03bb -interval variables are smaller All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint than those obtained with shorter interval variables. However, the performance can be improved after adding \u03bb -interval oligonucleotide association information (see Table 1 ).\n\nHence, the \u03bb -interval Z-curve should reflect additional important information for essential genes that cannot be contained in adjacent nucleotide association information.\n\nIn 2005, Chen and Xu used a neural network and SVM to predict the dispensability of proteins in the yeast S. cerevisiae based on the protein evolution rate, protein-interaction connectivity, gene-expression cooperativity and gene-duplication data (Chen and Xu 2005 phenotype lethality and gene function, copy number, duplication, expression levels and patterns, rate of evolution, cross-species conservation, and network connectivity, and the random forest-based model used in this study achieved an AUC of 0.81, which is significantly better than that obtained by random guessing. In addition, these authors integrated the features they identified to predict essential genes in A. thaliana (Lloyd et al. 2015) . Those previous researches in three eukaryotes illustrated All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint classifiers can gave satisfactory prediction through combining sequence with other features. For human essential gene identification, we only used the sequence composition and interval association information in the present study and still obtained an AUC of 0.8854. Considering that this result is better than the results obtained in previous studies using integrated features, the gene essentiality of the human genome can be accurately reflected based on only the sequence information.\n\nThe human essential genes in cancer cell lines were identified by three different groups (Blomen et al. 2015; Hart et al. 2015; Wang et al. 2015) . The results showed high consistency (Fraser 2015) , which further confirmed the accuracy and robustness of these data. We extracted the gene essentiality data from the DEG database (http://tubic.tju.edu.cn/deg/), the updated version of which contained human gene essentiality information. These essentiality annotations serve as the basis for constructing our benchmark dataset. The flowchart shown in Figure 2 illustrates the construction of the positive and negative dataset.\n\nAll rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint (Juhas et al. 2011) . Therefore, we only retained genes that were identified as lethal genes in more than half of the investigated cell lines. When a gene appeared as essential in more than six cell lines (11/2\u22486), it was selected as one sample in the positive dataset. According to this principle, we obtained a total of 1,518 essential All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint gene annotations. We downloaded all of the protein coding gene sequences from the CCDS database (https://www.ncbi.nlm.nih.gov/CCDS/CcdsBrowse.cgi), and the annotations of protein coding genes were obtained from the HGNC database (http://www.genenames.org/cgi-bin/statistics, March 1, 2016), which contained 19,003 annotation entries. The essential gene sequences were extracted according to the annotations, and genes with no counterpart in the CCDS database were excluded.\n\nAccording to this criterion, we excluded 2 genes and obtained 1,516 essential genes.\n\nWe used the essential gene annotation in the DEG dataset, and the gene sequences were extracted from the CCDS because the former did not contain the information for non-essential genes. For human essentiality annotations in the DEG database, a number of scattered annotated essential genes aside from those in the 11 cell lines were identified. A total of 28,166 essential gene annotated entries (including conditional essential gene annotated entries) were obtained. Among these annotations, there were many repeated annotation entries; therefore, there were considerably fewer unique entries. To obtain a more reliable negative dataset, i.e., absolutely non-essential genes, we excluded all of the human essential genes annotated in the DEG database (Luo et al. 2014 ) from the list of the protein coding genes. The remaining genes were regarded as the negative dataset, and their gene sequences were extracted from the CCDS database. Genes with no counterpart in the CCDS database were also excluded. A total of 10,499 non-essential genes were obtained using this method. Ultimately, a total of 12,015 gene entries were obtained in the benchmark dataset: 1,516 essential genes and 10,499 non-essential genes. The protein coding All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint gene annotations are provided in Supplemental Information S5, and information for the benchmark dataset is provided in Supplemental Information S6.\n\nThe originally proposed Z-curve variables might reflect the composition of a single nucleotide considering the features derived from phase heterogeneity of a single nucleotide (Zhang and Zhang 1991; Zhang and Chou 1994; Zhang 1997) . Herein, we provided a summary of the Z-curve method used for gene identification (Zhang and Wang 2000) . Let us suppose that the frequencies of bases A, C, G and T occurring in an ORF or a gene fragment at positions 1, 4, 7, \u2026 , 2, 5,\n\n, and 3, 6, 9, \u2026 , are represented by a 1 , c 1 , g 1 , t 1 ; a 2 , c 2 , g 2 , t 2 ; a 3 , and c 3 , g 3 , t 3 , respectively. Those 12 symbols represent the frequencies of the bases at the 1 st , 2 nd and 3 rd codon positions, respectively. According to the symbols defined above, the universal Z-curve mathematical expression is as follows (Zhang and Wang 2000) :\n\n(1)\n\nBecause composition bias for oligonucleotides in coding DNA sequence (CDS) regions or open reading frames (ORFs) exists, the adjacent w-nucleotides Z-curve method was proposed (Guo et al. 2003; Gao and Zhang 2004) . Let us suppose that w represents the length of the adjacent nucleotide sequence. The Z-curve variables for the phase-specific adjacent w-nucleotides can be calculated as follows:\n\nAll rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint 0\n\n(2) where k equals 1, 2, or 3 to indicate that the first oligonucleotide bases are situated at the 1 st , 2 nd and 3 rd codon positions, respectively.\n\nRecent studies demonstrated the existence of long-range associations in chromosomes and showed that these associations are crucial for gene regulation (Fullwood et al. 2009; Ruan 2011) . Although the two adjacent nucleotides in the primary structure have no association in some cases, strong associations in terms of tertiary structure might exist. Therefore, we introduced the \u03bb -interval Z-curve to virtually represent the interval range association. The details of this method are described as p k (S w X), which represents the frequency of oligonucleotides S w X in genes or ORFs, where X is one of the four basic bases A, T, G and C. To facilitate this presentation, the length of the oligonucleotide S w is represented as w. According to the predetermined characters, we generated the universal equation for the \u03bb -interval Z-curve based on Z-curve theory as follows:\n\n(\n\nwhere x, y, and z represent the accumulation of the three base groups classified according to chemical bond properties. Variable k denotes the phase-specific index of the first base in the nucleotide sequence S w , and \u03bb represents the intervals between S w and X. The first base in the oligonucleotide S w was located at position k. The core part\n\nAll rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint 1 of \u03bb -interval Z-curve forms oligonucleotide S w X. A schematic diagram of the formation of these oligonucleotides is shown in Figure 3 . The oligonucleotide window S w slides along a DNA molecule sequence according to phase, forming oligonucleotide sets with base X, which is \u03bb intervals away from the last base of S w . The periodicity derived from three codons is denoted as S w X.\n\nWhen w is equal to 1 and \u03bb is equal to 0, Equation (3) can be transformed into\n\nEquation (1). When w is more than 1 and \u03bb is equal to 0, Equation (3) can be All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint transformed into Equation (2). Thus, the phase-specific single nucleotide Z-curve and phase-specific adjacent w-nucleotide Z-curve are incorporated into the \u03bb -interval Z-curve. Using the \u03bb -interval Z-curve, we can extract more features to characterize DNA sequences. According to Equations (1) and (2) \n\nFV 2,0 and FV 3,0 are the combination of adjacent phase-specific w-nucleotide Z-curve variables. We performed this prediction with w ranging from 2 to 4 and \u03bb ranging from 0 to 5. According to the discussion above, we obtained 4,545 variables for FV 4, 5 .\n\nSVMs can be classified into linear SVMs and non-linear SVMs according to complexity. SVMs can be used to transform the data into another feature space with The copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint more dimensions than the original data and determine the hyper-plane. In this super-dimensional space, the hyper-plane can readily separate the samples. Linear SVMs play a key role in solving ultra-large-scale data, reflecting the effectiveness, rapid speed and splendid generalization of this method in training and prediction. LIBLINEAR, designed by Fan et al. (Fan et al. 2008) , is an easy-to-use, freely available software tool to manage large sparse data. The new version of LIBLINEAR (version 2.1-4) supports not only classification, such as L2-loss and L1-loss linear support vector machine, but also regression, such as L2-regularized logistic regression.\n\nGiven the ultra-high-dimensional feature vectors and large samples contained in the benchmark dataset in the present study, we used the LIBLINEAR software package for prediction. The penalty parameter c was determined using 5-fold cross-validation from 2 -18 to 2 10 . The new version of LIBLINEAR can be downloaded from https://www.csie.ntu.edu.tw/~cjlin/liblinear/.\n\nFirst, the predictive power of a classifier can be influenced by the relevance and noise in the original features. Second, additional time for training and predicting tasks can be increased, reflecting the high-dimensional features. Feature selection (FS) technology is a powerful method for the removal of noise and redundant features from the original features. Hence, the dimension of the features can be reduced. Recursive feature extraction through SVM linear kernels is a powerful FS algorithm (Guyon et al. 2002) , but the correlation bias was not considered using this method. Yan and Zhang (Yan and Zhang 2015) proposed an improved method, called SVM-RFE+CBR, All rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint which incorporates the CBR. The main concept is that the ranking criterion can be directly derived from the SVM-based model. The feature with the smallest weight is excluded for each run time. The training process was repeated by incorporating CBR until the ranks of all features are obtained. We used SVM-RFE+CBR FS technology to perform feature selection and improve the performance of the classifier.\n\nCross-validation test technology is the most popular method for evaluating the performance of classifiers. N-fold cross-validation means that the samples are randomly separated into N sub-samples sets. One of the sub-sample sets is used as the testing dataset, and the remaining sub-sample set is used as the training dataset during each run. The process is executed N times until every sub-dataset is utilized as the testing dataset. If N equals the number of samples in benchmark dataset, then the N-fold cross-validation can also be called jackknife or leave-one-out cross-validation.\n\nIn the present study, we used the 5-fold cross-validation test to determine the best penalty parameter and adopted the jackknife test to further assess the predictive power of the classifier. The area under the ROC curve, the AUC, is often used to measure the performance quality of a binary classifier. An AUC of 0.5 is equivalent to random prediction, whereas an AUC of 1 represents a perfect prediction. There is no bias for evaluating the performance of the unbalanced dataset through AUC. Therefore, we adopted the AUC as a cross-validation criterion in the present study.\n\nAll rights reserved. No reuse allowed without permission.\n\nThe copyright holder for this preprint (which was not peer-reviewed) is the author/funder. . https://doi.org/10.1101/084129 doi: bioRxiv preprint\n\nSupplemental data is available at Genome Res. Online: Supplemental Information S1: The 118 predicted essential genes and their sequences; Supplemental Information S2: Details for the 17 genes with drug and molecule interactions among the 118 predicted essential genes, distinguished from non-essential genes.\n\nSupplemental Information S3: Sequences of the essential genes in the genome of M.\n\nSupplemental Information S4: The information for the 21 genes and their corresponding homologs.\n\nSupplemental Information S5: Annotation of human protein-coding genes; Supplemental Information S6: Benchmark dataset used in the present study.\n\nSupplemental files can also be downloaded from the CEFG group website: http://cefg.cn/Pheg/supplement_info.zip."}