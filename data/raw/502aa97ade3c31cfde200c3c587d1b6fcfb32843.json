{"title": "Strengthening the role of the IBC in the 21st century Introduction 217 IBC review of DURC 221 US government (USG) policy and guidelines on DURC 221 DURC 222 A case study: IBC review of the Fouchier and Kawaoka studies that triggered the 2011 influenza A/H5N1 gain-of-function controversy 225 Risk and benefit analyses of gain-of-function studies with influenza A/H5N1 virus 227 Options for an additional level of review for DURC The increasing burden of regulatory review 234 Regulatory compliance for research institutions: an unfunded mandate 236", "body": "Over the last 30 years the recombinant DNA revolution has catalyzed our discovery of fundamental principles in biology, which continue to lead to translational applications in human medicine and agriculture. This new era started in 1975 when a small group of scientists, recognizing the potential for recombinant DNA technology to work for and against humankind, met at Asilomar to discuss a way forward [1] . A self-imposed moratorium on recombinant DNA research gave way to experimentation following the principles of the Asilomar Conference, and codes of practice formulated by national bodies in various countries with significant input from scientists [2] . This resulted in the development of a transparent, best-practices system of science that assured the potential of recombinant DNA research could be achieved in a safe manner, and remains a model for regulation of science by scientists [3] . In the United States, the first iteration of the Recombinant DNA Research Guidelines (National Institutes of Health (NIH) Guidelines) were drafted with public input in 1976 by a National Institutes of Health federal advisory committee that eventually became known as the Recombinant DNA Advisory Committee (RAC) (see also Chapter 12) [4] . Initially the RAC reviewed all Principal Investigator (PI)-initiated proposals (referred to as registration documents) that involved recombinant DNA, but since 1978 the review and approval of most registration documents has been conditionally delegated to the Institutional Biosafety Committee (IBC) at research establishments (see also Chapters 2 and 12) . The drafted NIH Guidelines were not static and were modified as new technologies were developed, and as our understanding of risk evolved from intuitive to empirical based on new knowledge. Led by CDC and NIH initiatives, the NIH Guidelines were the starting point in a partnership with the life sciences community to develop a code of practices for biosafety in microbiological and biomedical laboratories. In 1984 this collaboration resulted in the publication of Biosafety in Microbiological and Biomedical Laboratories (BMBL) [5] .\n\nFor the last 40 years best practices based on the NIH Guidelines and the BMBL have helped ensure the safety of the laboratory worker and the public. The rise of nonstate-sponsored terrorism in the latter part of the last century, the recognition that biotechnology was pervasive, and that life sciences research had a dual-use nature, required new approaches to identify and evaluate the research risk. In response to this and other threats, the government enacted the Antiterrorism and Effective Death Penalty Act of 1996, the USA PATRIOT Act of 2001, and Bioterrorism Preparedness and Response Act of 2002 (see also Chapters 1 and 2) . Together this legislation made it illegal to possess biological agents or toxins for nonpeaceful purposes. With regard to the most dangerous pathogens and toxins known as biological select agents and toxins (BSAT), these acts mandated their registration, criminal background checks for users, a regulatory system for storage and transportation, and the creation of a class of \"restricted persons,\" who were prohibited from their access (see also Chapter 10) . Because implementation of this legislation could have the unintended consequence of making the United States more vulnerable to bioterrorism by slowing or blocking the development of the very diagnostics, prophylactics and therapeutics that are needed to protect the country from bioterrorism, the National Research Council of the National Academies of Sciences formed the Committee on Research Standards and Practices to Prevent the Destructive Applications of Biotechnology. The charge of this committee was: \u2026to prevent the destructive application of biotechnology research and to recommend changes in these practices that could improve U.S. capacity to prevent the destructive application of biotechnology research while still enabling legitimate research to be conducted. Ref. [6] The Committee focused on biotechnology research with the potential to cause catastrophic harm through the misuse of BSAT, as well as research that could:\n\n\u2026facilitate the creation of novel pathogens with unique properties or create entirely new classes of threat agents. Ref. [6] In 2004 the Committee published its report entitled \"Biotechnology Research in an Age of Terrorism\" (NRC 2004 Report), which provided seven recommendations (Table 13 .1) that together were designed and intended to minimize impediments to fundamental research, while identifying the research with the greatest potential for misuse for additional scrutiny.\n\nThe NRC 2004 Report recommendations concerning dual-use research (DUR) with the most dangerous pathogens and toxins would be implemented by self-governance of the scientific community and existing regulatory practice. Of its seven recommendations, recommendations 1, 2, 4, and 5 were pertinent to the IBC. Recommendation 1, arguably one of the most important, proposed comprehensive education of persons involved in life sciences research with BSAT with regard to the dual-use dilemma. The remaining three recommendations focused on the actual system of regulation of DUR with BSAT. Recommendation 2 suggested that the IBC, which already reviewed experiments involving recombinant and synthetic DNA and infectious agents, would also review DUR with BSAT that belonged to seven categories (see below). Recommendation 4 proposed the creation of a National Science Advisory Board for Biosecurity (NSABB). The NSABB was proposed to provide case-specific oversight of research and its communication and dissemination, if relevant to national security and biodefense (see also Chapter 6) . In addition, it would act in an advisory capacity to alert the government to novel findings that have national security implications. Furthermore, the NSABB would serve as a resource providing education outreach [7, 8] , advice to journals concerning DUR (e.g., 2011 influenza A/H5N1 gain-of-function controversy), and international engagement with scientists and professional organizations [9] . Furthermore, the NSABB would conduct periodic review of the implementation of current legislation and existing regulation to ensure an optimal balance is maintained between stimulating life-sciences research, and ensuring national security (recommendation 5). And finally, the NRC 2004 Report recognized the key role federal funding would play in the development of this new system: \u2026successfully implementing the system \u2026will require significant additional resources at each stage; we do not attempt to provide an estimate of those costs. Otherwise, concerns for unfunded mandates could be a significant barrier to full consideration of the proposal by the scientific community. Ref. [6] We recommend that the international policymaking and scientific communities create an International Forum on Biosecurity to develop and promote harmonized national, regional, and international measures that will provide a counterpart to the system we recommend for the United States.\n\nIn addition to proposing a detailed process for evaluation of DUR at the institutional level through an Institutional Review Entity (IRE), the report emphasized the importance of mandatory education and training of all life scientists at federally funded institutions, a code of conduct for scientists and laboratory personnel working in the life sciences and aligned fields, and public involvement in the DUR debate. Importantly, the NSABB 2007 Report reiterated a concern of the NRC 2004 Report that oversight of DUR should not become another unfunded mandate.\n\nEleven years after the NRC 2004 Report and 8 years after the NSABB 2007 Report, the US government (USG) has not fully implemented a robust system to evaluate DUR. In particular, the USG has failed to effectively fund, support, and expand the role of the local Institutional Biosafety Committee in the oversight of DUR and in the changing scientific landscape; a comprehensive education program for all in life sciences research with regard to the dual-use dilemma has not been forthcoming; and finally, there has been no systematic evaluation of the impact of USG policy, regulations, and guidance on an institution's cost structure and on scientific discovery. Current USG policy appears to diminish the role of the IBC in the oversight of DURC, whereas we believe the local IBCs should assume yet even more responsibility. We detail our judgments on current USG DUR policy and provide recommendations for future oversight of DUR from our perspective as senior administrators and laboratory scientists charged with the responsibility of conducting life-sciences research in an era of increasing regulatory requirements and decreasing federal support.\n\nThe scope of these USG policies is limited to life sciences research involving tier 1 BSAT and reconstructed 1918 influenza virus and avian influenza virus (15 BSAT) and seven categories of experiments (Table 13. 2). Because research with the 15 BSAT represents a small fraction of life sciences research, the current USG policies do not provide oversight for most DUR in the life sciences or for training and education of the involved scientists as called for in the NSABB 2007 Report. Instead, it provides additional layers of scrutiny to a small segment of research in BSL-3 and BSL-4 laboratories, and targets its personnel for additional training and education. An even smaller segment of research will receive additional review at the US Department of Health and Human Services (DHHS) prior to a funding decision based on: \"A framework for Guiding US Department of Health and Human Services (DHHS) Funding decisions about Research Proposals with the Potential for Generating Highly Pathogenic Avian Influenza H5N1 Viruses that are Transmissible among Mammals by Respiratory Droplets\" (2013 DHHS framework) [12] . Although the 2015 DURC Policy and the 2013 DHHS framework thoroughly scrutinize a small segment of life sciences research for DURC, the authors of the 2015 DURC Policy recognized that DUR exists outside of the current scope and stated:\n\nInstitutions have discretion to consider other categories of research for DURC potential. Ref. [11] However, it is unlikely that a majority of institutions will undertake an expanded review of DUR without the publication of additional USG policies. For a thorough discussion of 2015 DURC Policy, see Chapter 6.\n\nThe USG policies with regard to DURC are far too limited. In addition to BSAT research in high-containment BSL-3/BSL-4 laboratories, many other potential sources of DURC may arise from scientists working in low-containment BSL-1 or BSL-2 laboratories or amateur biologists experimenting in their garages. A wellpublicized example of DUR that can be considered DURC came from a project Category Dual use of research of concern Relevant section of NIH guidelines 1 Enhances the harmful consequences of the agent or toxin Section III-D- 3 2 Disrupts immunity or the effectiveness of an immunization against the agent or toxin without clinical or agricultural justification 3\n\nConfers to the agent or toxin resistance to clinically or agriculturally useful prophylactic or therapeutic interventions against that agent or toxin or facilitates their ability to evade detection methodologies Section III-A-1-a, Section III-D-7-d 4\n\nIncreases the stability, transmissibility, or the ability to disseminate the agent or toxin Section III-D-4 5 Alters the host range or tropism of the agent or toxin Section III-D- 3 6 Enhances the susceptibility of a host population to the agent or toxin Section III-D-4, Section III-D-3 7\n\nGenerates or reconstitutes an eradicated or extinct agent or toxin listed in Section 6.2.1 of 2015 DURC policy [11] Section III-B-1, Section III-D-7-c that used ectromelia virus to develop an immune-contraceptive vaccine for wild mice. Ectromelia virus is a risk group 1 agent that is 95% identical to variola virus at the nucleotide level and is used to model smallpox infections. Ectromelia virus was modified by recombinant DNA techniques to express a gene for murine zona pellucida glycoprotein-3 (ZP-GP-3), which is part of an extracellular matrix surrounding the developing mammalian oocyte. Upon infection of mice with this ectromelia-ZP-GP-3 recombinant virus, the immune system of the mouse recognized the ZP-GP-3 as a foreign antigen, and synthesized antibodies against it. These anti-ZP-GP-3 antibodies subsequently attacked the developing oocyte and ovaries of the mouse resulting in sterility of about 6 months duration. This is an example of DUR that arguably meets the definition of DURC as it provides a \"road-map\" for the construction of a recombinant virus based on existing knowledge and techniques that could infect and transmit efficiently in human populations resulting in sterility [13] . At the time this research was published it received little attention, underscoring how difficult it is to recognize DUR, and emphasizing the need for a comprehensive and mandatory education program for DUR and relevant policies as called for in the NSABB 2007 Report.\n\nTo increase the duration of sterility from 6 months to life-long, the same researchers expressed a second gene encoding for mouse interleukin-4 (IL-4) from the genome of ectromelia virus [14] . An unintended consequence of expression of the IL-4 gene was the induction of a profound immunosuppression in the infected mouse that resulted in lethal infections of a resistant mouse strain and mice normally protected by vaccine immunity. Publication of this study in 2001 stimulated an intense debate in the popular and scientific press as the research provided a potential second \"road-map\" for construction of a similarly \"vaccine-proof\" variola virus. This second example of DUR with the ectromelia virus was discussed in the NRC 2004 Report as one of three examples of DUR.\n\nThe 2011 influenza A/H5N1 gain-of-function controversy likely provided much needed impetus for the roll-out of the 2015 DURC Policy, which focused on the 15 BSAT including the highly pathogenic avian influenza viruses such as A/H5N1. Because of the narrow focus of the 2015 DURC Policy, one of the two studies of the 2011 influenza A/H5N1 gain-of-function controversy would not have been covered by that policy even though both studies examined gain-of-function mutations in the influenza A/H5N1 hemagglutinin (H) that contribute to transmissibility of influenza virus between ferrets. The study from the Fouchier laboratory at the Erasmus Medical Center utilized a natural isolate, influenza A/H5N1/Indonesia/5/2005 virus that was genetically modified by site-directed mutagenesis of the H surface protein, and the acquisition of additional mutations by subsequent serial passage in ferrets [15] . This study would be explicitly covered by the 2015 DURC Policy. On the other hand, Kawaoka and colleagues at the University of Madison used receptor-binding studies and animal experiments to identify four mutational changes to the same H protein when expressed in a virus containing the remaining seven gene segments from a 2009 pandemic influenza A/H1N1 virus [16] . A whole 2009 pandemic influenza A/H1N1 virus or one containing the gene for the H of the more pathogenic influenza A/H5N1 virus is not covered by the 2015 DURC Policy, even though the Kawaoka virus construct was based on a 2009 influenza A/H1N1 virus that is highly communicable and few humans are thought to have protective immunity to H of influenza A/H5N1 virus. This arbitrary exclusion of one of two studies from enhanced review when both employ a similar gain-of-function approach and seek to answer the same experimental question supports the idea that all life sciences research should be evaluated at the institutional level for DURC as proposed by the NSABB 2007 Report.\n\nThere is also inconsistency in the classification of other respiratory pathogens with regard to the 2015 DURC Policy. Influenza A/H5N1 virus is included in the 2015 DURC Policy, whereas coronaviruses (CoV) severe acute respiratory syndrome (SARS) and Middle Eastern rsespiratory syndrome (MERS) are not. SARS initiated a ~9-month global pandemic in November 2002 that resulted in 8096 infections, 774 deaths and a case-fatality rate of 9.6% [17] . MERS-CoV, emerged in 2012 and as of June, 2015, has caused 1342 infections, 513 deaths, and a case-fatality rate of 38% [18] . Both SARS-CoV and MERS-CoV have infected more individuals in a shorter period of time (cumulative ~3 years) than influenza A/H5N1 virus, which is credited with 842 clinical infections, 447 deaths, and a case-fatality rate of 53% between 2003 and 2015 [19] . Both SARS-CoV and MERS-CoV are readily transmissible in human populations, while influenza A/H5N1 virus is not. These examples underscore a problem with the current USG approach for regulation of DURC. The reliance of the 2015 DURC Policy on a list of 15 BSAT suggests other potential pathogens are of less importance, and has the inherent danger of creating a false sense of security [20] . The 15 BSAT list may be expanding as in October of 2014 the USG announced a pause in DURC gain-of-function research projects (2014 Funding Pause) that would enhance the pathogenicity and/or transmissibility of influenza virus, MERS-CoV, or SARS-CoV in mammalian species by the respiratory route of infection. This pause would remain in effect \u2026until a robust and broad deliberative process is completed that results in the adoption of a new USG gain-of-function research Policy. Ref. [21] This process would include consultation with the life sciences community, numerous stakeholders, and deliberative bodies including the NSABB. It was to be completed before October 17, 2015, the 1-year anniversary of the initiation of the 2014 Funding Pause. The 2014 Funding Pause also allowed for a future expansion of the scope of regulated research possibly to other pathogens suggesting further expansion of the \"list\" [21] .\n\nThus when viewed in total, the USG policy and guidance for overseeing DURC is arguably problematic. It calls for review of life sciences research involving DURC for only a small number of agents (15 BSAT), but its reliance on a prescriptive list of pathogens can contribute to a failure to review projects with similar DURC potential, but utilizing dissimilar approaches (e.g., Fouchier versus Kawaoka studies). Equally important, the 2015 DURC Policy fails to address the need to review the vast majority of life sciences research for DURC as proposed by the NSABB 2007 Report (see also Chapter 5) . DURC by pathogens not on the 15 BSAT has the potential to create public health concerns equal to that of the 15 BSAT.\n\nThe local IBC has become the keystone of the regulatory structure known as the NIH Guidelines (see also Chapters 5 and 11). As noted previously, in the early years of the NIH Guidelines, the RAC reviewed all registration documents that involved recombinant DNA, but in 1978 the review and approval of most registration documents was conditionally delegated to the IBC. The IBC alone reviews and approves the majority of registration documents, including those involving DURC. The NIH Guidelines recognized that certain studies with influenza virus, such as those involving influenza A/H5N1 virus, involved increased risk to personnel and the public, and therefore required increased engineering controls, enhanced PPE, and additional practices, but not additional review. The Fouchier and Kawaoka ferret transmission studies with influenza viruses were reviewed under section III-D-7 of the NIH Guidelines by the IBC. For a detailed discussion of the 2011 influenza A/H5N1 controversy see Chapter 6.\n\nBoth the Fouchier and the Kawaoka groups carefully considered the potential safety and security risks associated with their studies [15, 16] . The studies were reviewed for biosafety and biosecurity at many different levels, including peerreview by study section, and program review at the National Institute of Allergy and Infectious Diseases (NIAID). The IBCs evaluated the proposed research prior to commencement of the work, and assigned a biosafety level commensurate with the guidelines and regulations, and reflective of risk-benefit analyses. The projects were carried out in biocontainment laboratories that were designed specifically for influenza virus research, were staffed with well-trained personnel, and were operated at BSL-3 Enhanced, ABSL-3 Enhanced, or BSL-3Ag. For example, the Kawaoka ferret transmission experiments were carried out at BSL-3Ag, which differs from ABSL-4 only in the lack of an automatic chemical decontamination exit shower, and the use of respiratory protection based on external HEPA filtered air-supply rather than the Powered Air Purifying Respiratory (PAPR) system used in BSL-3 Enhanced, ABSL-3 Enhanced, or BSL-3Ag laboratories. In addition to the engineering controls, personnel protective equipment and practices, a robust occupational health program was in place at both institutions. Most importantly, all appropriate institutional and government approvals and inspections were obtained prior to commencement of the work. Based on the standards of the day, gain-of-function influenza A/H5N1 virus research required no additional level of review from the RAC or NIH/OBA prior to commencement of work (see also Chapter 6) .\n\nFrom the duration and intensity of the discussions concerning these two studies in the popular and scientific press, it is clear that some felt the IBC review to be inadequate. For this reason, it is worthwhile to examine more closely the scientific question that the research was designed to answer, and the basis of the controversy.\n\nThe first human fatality from influenza A/H5N1 virus occurred in Hong Kong in 1997. Since 2003 there have been 840 laboratory-confirmed clinical infections of influenza A/H5N1 virus and 447 fatalities, but there has been no documented, sustained human-to-human transmission. In 2006, a Blue Ribbon Panel on influenza research recommended to the NIAID that: \u2026evolutionary pressures that lead to emergence and spread of new viral subtypes -especially the factors that favor transmission from animals to humans -are urgent research priorities. Ref. [22] NIAID funded Drs Fouchier and Kawaoka to undertake this line of research. Both the Fouchier and Kawaoka groups selected a gain-of-function experimental approach to search for genetic changes in the H5 that would permit efficient transmission of an influenza A/H5N1 virus between ferrets, a model of human influenza virus transmission. Of the experimental approaches available for answering this research question, gain-of-function is the most direct and time-proven. Understanding the factors that governed the evolution of influenza A/H5N1 virus transmission between humans has important implications for public health. If the research failed to identify mutations that enhanced transmission in the ferret model, it could suggest the existence of a genetic barrier to creating a highly transmissible influenza A/H5N1 virus. Influenza A/H5N1 virus infections for humans would be destined as \"dead-end\" interactions, as are infections with hemorrhagic disease viruses such as Lassa fever and Junin viruses. In this regard, a number of influenza virus researchers believe only influenza viruses with H subtypes of H1, H2, or H3 are capable of pandemic potential. A failure to demonstrate ferret transmissibility with mutated influenza A/H5N1 virus could support a reduction of investments in pandemic preparedness and research for influenza A/H5N1 virus. For example, worldwide at least five vaccines have been licensed against influenza A/H5N1 and more are in development. Nearly 600 million dollars and 1 billion dollars have been spent on influenza A/H5N1 vaccines in Japan and the United States, respectively.\n\nAlternatively, if a small number of mutations could be identified that supported influenza A/H5N1 virus transmissibility in a ferret model, it could be argued that influenza A/H5N1 virus has potential to evolve into a pandemic virus, and the investment in vaccines and research in the interpandemic period is worthwhile. The identification of these mutations could provide the opportunity to develop antivirals and vaccines that would be efficacious against the \"predicted\" virus, and inform the influenza surveillance network. The research question was clearly important.\n\nThe gain-of-function manuscripts from the Fouchier and Kawaoka laboratories were submitted for publication and reviewed by the NSABB in late 2011 as part of its mandate to advise the USG on biosecurity issues. The NSABB recommended changes, and revised manuscripts were subsequently published in 2012 followed by a voluntary moratorium on new gain-of-function research with influenza A/H5N1 virus. The debate on the pros and cons of the Fouchier and Kawaoka studies continued unabated with the main issues concerning the risk and benefit of the research to the public, the need for an enhanced evaluation and public discussion of a small percentage of DURC, and whether public dissemination of the identity of the genetic mutations enhancing transmissibility had biosecurity implications [23] .\n\nThe risks and benefits of research are difficult to quantify, and subjective judgment plays a larger role than it should. The biosafety level assigned to an experiment is tied to the risk group of the involved agent. The greater the perceived risk of the agent to the personnel carrying out the experiment or to the public through accidental release, the greater the assigned biosafety level. Appropriate selection of the biosafety level to minimize these potential risks is important, because higher biocontainment levels are associated with increased cost and duration of the experiment (see the section, \"The increasing burden of regulatory review\"). Research involving influenza A/H1N1 and A/H3N2 viruses is carried out at BSL-2 as these influenza virus strains currently circulate in the human population, while research with influenza A/H2N2 virus is carried out at BSL-3 Enhanced as this strain has not circulated in human populations since 1968. Research with highly pathogenic influenza A/H5N1 virus is carried out at BSL-3 Enhanced, where the \"Enhanced\" indicates additional engineering controls, personnel protective equipment, and/or practices. Since the 2011 influenza A/H5N1 gain-of-function controversy there have been several publications estimating the potential public health risks associated with the study of the genetic basis of influenza A/H5N1 virus transmissibility in mammalian species. The majority of these studies based their risk estimates on data from the same publication by Henkel and colleagues, which was the first report to describe the results of the National BSAT Theft, Loss or Release reporting system for the years between 2004 and 2010 that focused on biosafety or biocontainment lapses [24] ; however, as we shall see, this study lacked the necessary background information required for use of the data in robust risk analyses.\n\nThe report documented 11 laboratory-acquired infections (LAI) of which seven and four occurred in BSL-2 and BSL-3 containment laboratories, respectively. This and other studies concluded that the majority of LAI were acquired through unrecognized aerosol exposure:\n\nAll 11 LAIs resulted from either unrecognized and/or unreported exposures, presumably through the aerosol release of the BSATs. These observations are entirely consistent with studies by Pike [25] who found no distinguishable accidents or exposure events in more than 80% of LAIs. Harding and Byers [26] also reported only a small number of recognized containment breaks in a study of LAIs.\n\nThe conclusion that the LAI occurred from aerosol exposure is important as the type of respiratory protection used by the affected personnel was not described in the Henkel report. The seven individuals infected in the BSL-2 containment laboratories may not have used respiratory protection as it is not required by the BMBL [5] , whereas the four LAI from the BSL-3 laboratories would have employed no, N-95 type, or PAPR respiratory protection. The BMBL mandates respiratory protection only for research activities in ABSL-3 facilities, whereas the need for respiratory protection for BSL-3 facilities would be determined by the IBC risk assessment [5] . Without knowing the respiratory protection of the individuals who were infected, it is difficult to use the data to calculate probabilities of a LAI occurring as a result of influenza A/H5N1 research in BSL-3 Enhanced containment laboratories where the PAPR is the universally prescribed method of respiratory protection. Importantly, PAPR provides almost 100% protection against an aerosol infection when operating properly [27] . Similarly, the Henkel publication provided the number of individuals working in all of the reporting BSL-2, BSL-3, and BSL-4 containment laboratories, but did not attribute an individual number to each biosafety level. And, finally, influenza A/H5N1 virus experiments are carried out at BSL-3 Enhanced, which contain a number of additional safety features over BSL-3 that would minimize the frequency of exposures and the probability of a LAI.\n\nUsing this data set from the Henkel report, the derived range of probabilities for a laboratory created influenza A/H5N1 virus to cause a LAI ranged from 2 \u00d7 10 \u22123 [28] to 1 \u00d7 10 \u22126 [29] per laboratory per year. The 500-fold difference between the two values was due to the authors' application of various assumptions to the calculation: some included the impact of specific practices on reducing the likelihood of a LAI; others involved differing estimations of the number of labs involved in the research; and still others assumed differences in the probabilities of LAI in BSL-3 laboratories using viruses versus bacteria. Similarly, the probability that a community LAI would lead to a pandemic, and the estimated magnitude of the pandemic were equally dependent on assumptions. For comparable reasons calculating the benefit of a research project is equally difficult, and will not be discussed here.\n\nThe lack of ancillary detail to permit stratification of the data in the Henkel report suggests the derived risk analyses may be dominated by subjective assumptions, and therefore are of reduced value in determining the probability of a LAI originating from a BSL-3 Enhanced laboratory carrying out gain-of-function research with influenza A/H5N1 virus. Further, the IBCs evaluated the Fouchier and Kawaoka studies prior to publication of the Henkel report. Thus, even these limited data may not have been available for risk-benefit analyses, and to inform decisions on whether additional biosafety or biosecurity measures were necessary and would be meaningful.\n\nThe lack of stratified exposure and LAI data could be mitigated to some degree, if reports describing high-containment biosafety laboratory exposure data described the detailed biocontainment context (e.g., detailed descriptions of engineering controls, personnel protective equipment, and practices) in which individuals worked, were inadvertently exposed to an agent, and at a low frequency became infected. In order to continue to improve best practices for biocontainment and biosecurity, it is especially important to have accidental exposure data (or the lack thereof) from the new state-of-the-art BSL-3/4 National Biodefense Analysis and Countermeasures Center and Integrated Research Facility at Frederick, MD, and from similar facilities in other countries. And, finally, along with better data, there needs to be agreement in the scientific community on the appropriateness of the assumptions that are used in calculating risk-benefit analyses for DURC. Together this will enable the IBC to evaluate better the DUR projects it reviews.\n\nCurrent life sciences research involving pathogens and recombinant DNA is overseen by a series of biosecurity laws, Executive Orders, administrative orders, and guidance documents. The NIH Guidelines, 2013 DHHS framework, and 2015 DURC Policy inform the actual manner of the review process for some, if not all, life sciences research. The NIH Guidelines delegate review and approval of registration documents conditionally to the IBC. The NIH Guidelines require certain experiments that potentially involve DURC (Table 13 .2) to require additional levels of review. For example, the deliberate transfer of a drug resistance trait to a certain microorganism could require IBC review, RAC review, and NIH Director approval prior to initiation of the study (NIH Guidelines, Section III-A). Also, the cloning of a toxin molecule with LD 50 of less than 100 ng/kg requires IBC and NIH/Office of Biotechnology Activities (NIH/OBA) approval (NIH Guidelines, Section III-B). The NIH Guidelines also recognize that certain influenza virus studies involving genes from 1918-1919 H1N1 (1918 H1N1), human H2N2 (1957-1968), and influenza A/H5N1 viruses require special consideration (enhanced biocontainment), but not additional review. Thus, as mentioned previously, the Fouchier and Kawaoka gain-of-function studies needed only to be reviewed at the level of the IBC. This hierarchical review approach for recombinant and synthetic DNA experiments has worked well for almost 40 years.\n\nThe current 2015 DURC Policy calls for the evaluation of DUR involving the 15 BSAT at the level of the Institutional Review Entity (IRE), which most often will be the IBC (for detailed discussion of the mechanics of DURC review see also Chapter 6) . The IRE uses a risk-benefit assessment analysis to determine whether proposed research is DURC. If the research is determined to be DURC, the PI, working in conjunction with the IBC, would draft a risk mitigation plan to guide the conduct and communication of the DURC. The risk mitigation plan would need to be approved by the USG funding agency. This approach raises at least three questions. First, would using the agency that funded the project to review a risk mitigation plan be a conflict of interest? Second, what individual(s) or group of individuals would carry out the review? For example, would the program officer operate in another capacity to review the risk mitigation plan or would it be someone who was trained in risk-benefit analysis? And third, how would independent funding agencies ensure consistent reviews of mitigation plans for an identical DURC?\n\nIn addition to the 2015 DURC Policy, funding proposals for influenza A/H5N1 gain-of-function studies must also be responsive to the 2013 DHHS framework. Under this framework an influenza A/H5N1 virus gain-of-function research proposal would be reviewed for Scientific Merit and dual-use issues by the funding agency. If the proposal is in the fundable range, and passes the seven-point DHHS criteria, it would be forwarded to DHHS for a further rigorous review based on scientific and public health benefits, biosafety and biosecurity risks, and risk mitigation plan. If the research project passes the DHHS review process, the funding agency is approved to send the PI a notice of award. The 2013 DHHS framework also raises numerous questions, three of which are listed here. Would the funding agencies require an IBC reviewed registration document, including a risk-benefit analysis and risk mitigation plan, to accompany the funding solicitation? This would make sense as otherwise the DHHS review committee would have to generate de novo the risk-benefit analysis and the risk management plan for each project it reviewed. Although perhaps advantageous for the DHHS review committee, this approach would increase the IBC workload as most IBCs review registration documents only after the award of funds as ~80-90% research proposals are not funded. If the DHHS review committee did generate its own risk-benefit analysis, would the IBC be able to modify a DHHSapproved risk mitigation plan or would the DHHS risk-benefit assessment replace that aspect of the IBC review? And, finally, would the DHHS risk mitigation plan replace the need for the risk mitigation plan mandated in 2015 DURC Policy, since the plans likely would end up in the same place, that is the funding agency? It is not clear how this will work in practice, but the PI and IBC will have additional responsibilities and reporting requirements when these policies and guidelines take effect. This will likely make the PI less competitive with researchers abroad who are not encumbered by this regulatory burden.\n\nThe NRC 2004 Report and NSABB 2007 Report describe seven similar categories of DURC that require enhanced oversight. The NSABB 2007 Report proposed the evaluation of all life sciences DUR by an IRE, possibly the IBC with expanded expertise, and emphasized the role of this local oversight in managing DURC. DURC would be identified by risk-benefit analysis using a suite of tools described by the NSABB [10] . Identified DURC would be managed by a risk mitigation plan developed by the IBC and PI, with no additional level of review described. The NRC 2004 Report proposed review of DUR involving BSAT at the level of the IBC. For most DURC an acceptable risk management plan would be developed between the PI and IBC, however, the authors of the NRC 2004 Report recognized that certain DURC would have a greater degree of inherent risk, and would require an additional level of review. Based on the success of the NIH Guidelines at facilitating research while protecting public safety, the NRC 2004 Report proposed a similar hierarchical review process for evaluating DURC. The vast majority of DUR involving BSAT would be handled at the level of the IBC, but some or all of the experiments in the seven categories as defined in the 2015 DURC Policy and listed in Table 13.2: \u2026would be referred to an expanded RAC and possibly for approval or denial of permission to proceed with the proposed experiment. Ref. [6] This approach has the added benefit that all DURC categories except #2 are currently required by the NIH Guidelines to be reviewed by the IBC (Table 13. 2), with certain experiments in categories 3 and 7 being further reviewed by the RAC or NIH/ OBA. Importantly, unlike 2015 DURC Policy, the NIH Guidelines require IBC review of all experiments that involve DURC in these categories not just experiments involving the 15 BSAT.\n\nThe NIH Guidelines would need updated criteria to determine which DURC would be reviewed solely by the IBC and which DURC would require secondary review by an expanded central or regional RAC. These criteria could be developed by the NSABB in consultation with the scientific community. Using an expanded RAC to review selective DURC would have a number of advantages over current approaches or that proposed in 2015 DURC Policy: 1. One national entity would review registration documents ensuring consistency of review. 2. The RAC already reviews certain category 3 experiments that involve the transfer of a drug resistance trait to certain microorganisms, so there is already an \"institutional experience\" with the review of DURC. 3. It would minimize the potential for conflict of interest. 4. It would improve the effectiveness of the IBC and improve the quality of the review of DURC. Under this scenario the DHHS review committee would review scientific and public health benefits as it pertained to its research portfolio, but biosafety and biosecurity risks and the generation of a risk mitigation plan would be left to the IBC and the expanded RAC.\n\nPrior to 2011, the IBC had the major role in the evaluation of influenza A/H5N1, SARS-Cov, and MERS-CoV gain-of-function research proposals for biosafety and biosecurity based solely on the NIH Guidelines. The reaction to the 2011 influenza A/H5N1 controversy has been an inadvertent reduction of the IBC role in the management of this class of experiment, the addition of more oversight to the review process, and the potential for expansion of the \"list\" with the addition of more pathogens and more classes of DURC (2014 Funding Pause). The current reaction to gain-of-function DURC with a quantitatively unknowable level of risk is to impose additional layers of oversight and/or layers of biosafety and biosecurity. This may be justifiable for a very small number of DURC, but in our view not the vast majority. Although harder to document, an emphasis on education and proficiency training of those involved in life-sciences research, with special attention to individuals involved in DURC, may have a greater impact at lowering risk without impacting science (see the section, \"The increasing burden of regulatory review\"). In summary, we argue that the local IBC, if provided with increased resources, is capable to competently review all registration documents for DURC and to manage the vast majority, itself, while forwarding only a select few to an expanded RAC for secondary review.\n\nSince the IBC assumed the role of primary reviewer of recombinant DNA research in 1978, it has had a dramatic increase in its responsibility to oversee biological research and to engage in biosecurity as well as biosafety [30] (see also Chapters 5 and 11). The increased responsibility is based partly on recent USG policy expanding its review role for DURC with the 15 BSAT. Our view is that IBC review of DURC should be expanded even further than mandated by the 2015 DURC Policy to include all of life sciences research. Although this would further increase the workload of the IBC, it would result in a more comprehensive picture of DURC in life sciences research, especially research carried out in laboratories that have low to no biosafety containment and biosecurity. An example of this type of research leading to DURC is the immune-contraceptive ectromelia-ZP-GP-3 recombinant and the vaccine proof ectromelia-IL-4 recombinant described above. IBC expanded review would also eliminate the gaps in current 2015 DURC Policy by regulating research with all pathogens with pandemic potential, not just pathogens currently on a \"list.\" The DUR dilemma is now at least 25 years old, and although it has not been adequately addressed, in many ways it is an old debate. The IBC is also well-positioned to carry out additional activities as warranted by the changing landscape of science.\n\nFor instance, the IBC is in an ideal position to foster outreach with the local community as there is a number of important topics with which to engage the public (see also Chapter 7) . The 2011 influenza A/H5N1 gain-of-function controversy has raised a concern that certain DURC experiments are too risky to carry out due to low probability but highly consequential event, such as an LAI leading to an influenza virus pandemic. The IBC is well suited to explain to local communities the review process, including the risk-benefit assessment that is a component in the evaluation of DURC. A series of USA Today articles have suggested that not all of the nation's biocontainment laboratories are being operated in a safe and secure manner [31] . This comes at a time when trust between the scientific establishment and the public is already strained due to the inadvertent shipment of live Bacillus anthracis [32] and live influenza A/H5N1 virus [33] from CDC laboratories; the shipment of Bacillus anthracis from the Army's Dugaway Proving grounds in Utah over a 12-year period to at least 194 laboratories in all 50 states, Washington, DC, Guam, the US Virgin Islands, Puerto Rico, and nine other countries [34] ; and the release of Burkholderia pseudomallei from a high-containment BSL-3 laboratory at the Tulane National Primate Research Center [35] . It is important to note that the majority of these mishaps occurred at USG laboratories, and not at academic institutions that receive the vast majority of USG life sciences funding. The IBC is well suited to reassure local communities of the safety and security of the life sciences enterprise, as well as its importance for homeland security and for future gains in human medicine and agriculture.\n\nRecent innovations have opened up exciting areas of research (see also Chapter 7). In the last few years, a new technology called CRISPR (Clustered Regularly Interspaced Short Palindromic Repeat) has been evolving at a tremendous pace in research laboratories around the world. The technology permits modification of DNA of humans, other animals, and plants such that transcriptional regulation can be altered or genes added or subtracted [36, 37] . Importantly, compared to previous techniques for modifying DNA, this new approach is much faster and easier, and promises to change the manner in which certain diseases are treated. There is already at least one report of Chinese researchers modifying human embryos [38] . In addition, synthetic biology offers the possibility to modify microorganisms by the addition of functional \"genetic circuits\" and metabolic pathways for such practical purposes as the production of pharmaceuticals and biofuels, to break down pollutants, and, in the longer term, to create new life-forms [39] . And finally, in agriculture the continual increase in the world's population under changing environmental conditions and decreasing water supplies will drive the plant biotechnological revolution to generate new solutions for food security (see Chapter 9 for crop-specific IBC issues) [40] . All of these new areas of research will require a certain level of oversight that the IBC is best positioned to provide.\n\nAlthough the responsibilities of the IBC should continue to expand at most institutions, it is ill-prepared for the future. As compared to the Institutional Animal Care and Use Committee and the Institutional Review Board, the IBC receives a fraction of the resources, has equal or greater regulatory burden, and is responsible for a broader swath of science [41] . The USG needs to determine a mechanism to directly fund the IBC without leaving this task to the institution (see section, \"Regulatory compliance for research institutions: an unfunded mandate\").\n\nWe propose that all of life sciences research be reviewed for dual-use, and the small percentage of studies with DURC that meets a particular criterion would require an additional level of review by an expanded RAC. For this system to work effectively, the PI that prepares the IBC registration document will self-identify DUR that may be considered DURC. For its part, the IBC will need to separate DURC into projects that can be evaluated completely at the level of the IBC, and those needing secondary review by the expanded RAC. This process must be consistent among IBCs. Ideally, projects with similar DURC would be managed by similar risk management plans, even at different IBCs. Also, it would be advantageous for the local IBC to be staffed with the required subject matter experts such that each registration document receives a thorough evaluation. This could be difficult for some smaller institutions, and it would be important to have an expanded availability of commercial, virtual IBCs and/or a national register of trained ad hoc IBC members. Equally important, the PIs and members of the IBC must receive education and training on DUR, especially risk-benefit analyses.\n\nMuch has been written about the need to improve the education and training of life scientists. Both the NRC 2004 Report and the NSABB 2007 Report highlight the importance of education of the life sciences community on DUR issues. The NSABB further published a report entitled: \"Strategic Plan for Outreach and Education on Dual-Use Research Issues\" [8]. This report described a comprehensive educational outreach strategy targeting multiple target audiences (e.g., Congress, general public, scientists, laboratory staff, students/trainees, research administrators, and institutional leadership) through multiple venues (e.g., professional associations, scientific societies, scientific journals, opinion leaders, and the popular press). The Office of Biotechnology Activities in the Office Scientific Policy at NIH sponsors a comprehensive website that provides education materials on DUR [42] . More recently, the 2015 DURC Policy directed that institutions must provide education and training for individuals conducting life sciences research with the 15 BSAT. In addition to these educational materials, members of the NSABB engage target audiences on the issue of DUR, and the Office Biotechnology Activities presents \"NIH Guidelines 101\" at the American Biological Safety Association's annual meeting and at other venues.\n\nIn spite of all of these educational efforts the impact on the life sciences researchers appears to have been modest. There is a low level of interest and knowledge of the dual-use dilemma shown by senior scientists and institutional officials [43] . The majority of IBC members, PIs, and laboratory staff have not received training on DUR [44] . And, most importantly, there is little federal funding available on a direct cost basis for education of dual-use issues [43] . It is not clear how this situation will change until USG policy requires education and training on DUR issues of all individuals involved in life sciences research at the institutional level.\n\nAs mentioned above, there are very little data addressing the burden of IBC compliance and/or review on research, although there is a small literature on the more general question of regulatory burden on faculty workload. In a 2002 paper, Skorton et al. discuss the tensions that have arisen between the two missions of academic research, namely, promoting and supporting a vigorous research agenda while regulating the very programs that these activities comprise [45] . Focusing largely on human and animal subjects' protection, the article presents a call to action, to consolidate regulatory activities, streamline duplicative and unnecessary reporting, and reduce costs. Other similar discussions in the literature are available [41, 44, [46] [47] [48] [49] [50] . Few would argue that any progress has been made in reining in regulatory burdens during the 13 years that followed this plea. In fact the problem appears to be getting worse. A 2014 National Academy of Sciences report found PIs spent almost half of the time assigned for federally funded projects on meeting the requirements imposed by those projects. As many as 23 separate administrative activities -both pre-and post-award -were identified [51] .\n\nA nationwide survey of life sciences researchers, conducted in 2009 by the Center for Biodefense, Law and Public Policy, examined the degree of anxiety over inadvertent violations of BSAT regulations [52] . The survey tested the hypothesis that BSAT should not be regulated. Despite uncertainty regarding the current structure and dissemination of the guidelines and regulations, the survey showed that scientists were overwhelmingly in favor of continued regulation. Of particular interest was the observation that many researchers were more concerned about committing a violation than they were about personal injury. These data present a disturbing picture of the kinds of pressure (both personal and regulatory) that many users of the IBC may be experiencing, although the date of the survey (2009), only 5 years into the federal BSAT program, leaves room for the possibility that IBC regulations have since been clarified and attitudes have adjusted.\n\nDias et al. examined the effects of the USA PATRIOT Act and the 2002 Bioterrorism Preparedness Act on BSAT research in the US [53] . In 2010, the most salient programmatic observation was a measurable attrition from the field of the study topic (Bacillus anthracis) compared with the control topic (Klebsiella pneumoniae). The results indicated that before 2002, the average number of papers dealing with Bacillus anthracis was 17; that number declined to three by 2006. In contrast, a similar accounting of papers focused on Klebsiella pneumoniae found a smaller decrease (from 26 to 17 research papers per millions of dollars of US funding). Thus, BSAT research activity dropped by 65%, while general pathogen research dropped by only 17%. While the authors recognized a number of problems with the analysis (so that the numbers may be \"soft\"), the data clearly indicated that there was a 2-5-fold decrease in certain BSAT research, as estimated by the number of BSAT research papers published in the United States.\n\nThere have been a number of publications discussing the efficacy of the current BSAT program and the related ability of the IBC to contribute to biosecurity and biosafety, through such avenues as dual-use review, inventory, personnel reliability, SOP evaluation, etc. [20, 48, 49, 54, 55] . The potential contribution of the IBC to these kinds of biosecurity issues is discussed in other chapters of this volume (see Chapter 11) . In a novel approach to relieving the burden of IBC-related responsibilities, Bosselman et al. [46] addressed the problems of staff turnover and training requirements, anxiety, duplication of records and risk assessments, etc., by analyzing specific issues that were causing the most burden and delay. They found major obstacles in IBC protocol management (coordinating correspondence between investigators and reviewers), in managing training information, and in recordkeeping for laboratory inspections. Their solution was to create a collaboration platform that organizes and displays data, documents, and tasks in a coordinated structure, so that the processes required for progress could be tracked simultaneously.\n\nMorse [48] describes the evolution and implementation of pathogen-related regulations and outlines the discussion within the research community of their effect on productivity, international collaboration, and cost. Morse points out that \"many of the proposed negative impacts of the[se] regulations are based on anecdotal evidence.\" For example, a number of microbial collections were destroyed by investigators when the BSAT regulations came into effect, and the BSAT Program tried to prevent further losses by moving culture collections to registered laboratories [20] . Many researchers complained that they had to use research funds to upgrade their laboratories to appropriate security standards [20] . Gaudioso and Salerno bemoan the loss of personal and institutional talent applied to BSAT research, and relate the flight from the field directly to the burdensome nature of registration and compliance; more than one high-profile researcher announced that they would change their research focus, rather than submit to the regulations [54] . BSAT requirements have grown only more restrictive and demanding in the ensuing years. Sutton [50] argues that researchers are the sole targets of current biosafety and biosecurity regulations; indeed, Sutton develops the argument that the alleged perpetrator of the anthrax attacks of 2001 would not have been detected under (then) current BSAT Program, and adds: It may be time to rethink the regulatory framework for the nation's biodefense research, focusing more on fully implementing the background investigation requirements and better perimeter security, and placing less emphasis on filling out forms, filing reports and counting units of self-replicating organisms (2009).\n\nWith the recent implementation of the Personnel Suitability Assessment program under the Federal BSAT Program (2012), a program intended to determine whether a person who intends to work with (tier 1) BSAT actually warrants access, the attendant bureaucratic responsibilities have only increased. A recent report reviewed trends in IBC practices by evaluating responses to surveys of IBCs registered with the CDC from 2002, 2007, and 2010 [44] . The data were mixed: incident training and incident reporting remained problematic, as were interactions with other regulatory committees (Institutional Animal Care and Use Committees (IACUC) and Institutional Review Boards (IRB)), while improvements in management, staffing, and compliance improved over the 8 years covered by the surveys. But Jenkins points out, the Hackney study, while useful in analyzing general trends in the development of the IBC, lacked \"quantitative data on the increasing registration and policy burdens of IBCs over time, as well as on the number of protocol reviews, administrative, resources, and financial support\" [41] .\n\nDefense against biological threats, both natural and intentional, maintains a position of high priority in basic research, public health, and the clinical enterprise. The security implications of the spread of infectious disease have not gone unnoticed at the national and global levels. Furthermore, the impact of the anthrax attacks of 2001 has been felt throughout the global community. These and other events have significantly transformed the landscape of the basic science enterprise -particularly in the field of infectious disease. This discussion of the burdens associated with BSAT and infectious disease research leaves a number of questions unanswered. How can we maintain safe and secure research facilities while conducting research efficiently? How can we factor in the impacts of research policies on research quality and productivity? How can we find a \"healthy balance between facilitating research and protecting against audit and legal concerns [51] ? Going forward, the IBC will play an increasingly central role in the increasingly complex regulatory environment associated with life sciences research. Reliable quantification of the burden of IBC requirements on the research enterprise remains unavailable. In order to exploit the expertise and other contributions offered by a well-informed IBC, the burdens to be placed on these committees warrant systematic review.\n\nAs documented in previous chapters and here, the scientific community has collaborated actively with the federal government in recognizing and minimizing the risks of DURC. It is equally obvious, however, that these measures, individually and collectively, require research institutions to commit substantial resources, for which those institutions assume full financial responsibility. The issue raised by these developments is not whether institutions should or should not assume such responsibilities if they want to conduct DURC. Instead, the issue is whether institutional resources are adequate to ensure the safe and secure conduct of DURC, especially at a time when many research institutions face increasing financial stress related to the conduct of research. While those stresses are a function of several trends that apply generally to federal funding, they have clear implications for DURC: (i) decreasing levels of funding for biomedical research over the past decade, in actual dollars and as a percentage of the federal budget, and (ii) an inexorable increase in federal regulations since 1991, the year when the current cap on administrative costs on federal awards was established, that exacts a cumulative financial burden for regulatory compliance. The following sections consider each of these issues in more detail.\n\nFederal funding for biotechnology research has long been critical to scientific advances that, in turn, have contributed to improved public health and to a growing economy [6] . That funding also has occasioned growth in the biosciences: NIH funding in 2013 supported over 300,000 researchers at over 3000 universities and research institutions [56] , reflecting the critical role of NIH funding for the biomedical science community. While a longstanding and critical support for the bioscience enterprise, federal research funding, as a proportion of the federal budget, has declined steadily over the past half-century; it now accounts for less than 2% of that budget (see Figure 13 FY2007, to $29,182 million (requested) in FY2016 [57] , a decline of ~9%. Moreover, the trend is projected to continue in the aftermath of the 2013 sequester and ongoing political squabbling over funding for domestic programs.\n\nParadoxically, this decline in funding comes at a time when the number of applications for funding has increased. An examination of applications from FY2005 to FY2014 (the most recent data for which success rates are available), shows that all-agency applications increased by 18.6%, from 43,069 to 51,073 [58] . As a consequence of the increasing applications for a decreasing pool of funds, success rates for NIH awards also have declined over that period, from 22 This increase in institutional research costs is reflected in FY2010 data collected by the National Science Foundation through their annual Higher Education Research and Development (HERD) Survey, which shows that institutional funds directly supporting research totaled $9.1 billion [59] . When combined with institutional support for associated \"indirect\" (i.e., Facilities and Administrative) expenses, institutional funding in 2012 accounted for 21.6% of all Research & Development expenditures, compared to 18.0% of R&D expenditures in 2006. Hence, institutional R&D funding in 2012 was second only to that provided by federal sources (59.5%). Moreover, escalating institutional compliance requirements, such as those associated with DURC, suggest that the financial burden on institutions to provide internal research funding will continue to increase, a trend that can threaten the financial ability of all but the strongest research institutions to support the research enterprise.\n\nAs noted above, research institutions face rising costs associated with long-term trends that place increased pressure on both direct and indirect research-related expenses. Both direct and indirect costs of research represent real institutional expenditures. Direct costs, of course, are those elements that are readily identified (and represent the bulk of an award) in a research project, such as supplies, research equipment, investigator salaries, etc. Indirect costs are not so clearly linked to a given project, but include expenses for facilities, infrastructure, and operational activities (e.g., electricity, heat, ventilation, maintenance, disposal of hazardous waste), as well as costs associated with the administration of awards (e.g., compliance with regulations, cost controls and financial reporting, information security). In the case of research involving BSAT, there are additional indirect costs involved with security systems and personnel, maintenance of specialized engineering controls such as HVAC systems, administrative panels to evaluate the suitability of research-related personnel (\"personnel assurance programs\"), and the requirement for a Responsible Official to oversee program administration (see also Chapters 3 and 6). The impetus for the federal government to set Facility and Administration (F&A) rates, of course, was to support the indirect costs associated with the conduct of federally funded research. Hence, F&A rates are negotiated by institutions that receive federal funding, typically via audits that are undertaken (at an institution's expense) every 3 years. While an institution's F&A rate is subject to change (either up or down) over time, the maximum amount of administrative expenses that an institution can claim is capped at 26% of direct costs of a federal award, a rate set in 1991.\n\n1991 was a significant year for federal oversight of research, as it not only was the year in which formulas were set for F&A rates, but also was the year when the Common Rule was adopted to provide for human subject protection across federal agencies. While the cap on administrative cost rates is unchanged since 1991, the same cannot be said of the regulatory landscape that contributes to those costs. Recently, the Council on Government Relations (COGR) compiled a list of regulations that have been implemented or amended between 1991 and 2013, as well as interpretations/ implementations that have impacted business practices [60] . That document lists 52 regulations/amendments, 20 interpretations/implementations, and nine further regulatory changes that were proposed at the time that the document was published. A partial list of regulatory requirements follows with which most investigators will be familiar, as many impact the management of federally funded awards:\n\nFaced with the ongoing need to comply with these and other regulatory and/or financial reporting requirements, research institutions have developed multiple structures to manage these recurrent functions. This is reflected to some degree in compliance committees, many involving faculty representation, that include IRB, IACUC, IBC, IRE, Conflict of Interest (COI) committees, etc. While those committees largely depend on the willingness of research faculty to volunteer time in support of committee functions, the infrastructure behind these (and other) committees cannot be supported on a voluntary basis. Indeed, that infrastructure requires expert personnel, information systems, office space, and other institutional resources.\n\nBecause of the multiplicity of these operations and the varying organizational structures that support them, it is very difficult to obtain an exact accounting of the institutional costs exacted by these compliance-related operations. That said, several recent studies provide estimates of costs associated with the array of research-related regulatory requirements associated with federal funding. The previously referenced HERD study estimated that indirect costs incurred by research institutions in FY2010 exceeded federal F&A support by $4.6 billion dollars. Another survey, undertaken by the Association of American Medical Colleges (AAMC), examined indirect costs of medical schools with research programs that received federal funding that ranged from moderate ($26 million) to very high ($751 million) levels in FY2013 [56] . That study showed that institutional expenditures for unreimbursed indirect costs of external awards averaged $0.15 per dollar of direct costs. As noted previously, other internal expenditures also were incurred (e.g., startup packages, additional salary support for research effort, bridge funding), so that the combined institutional costs amounted to $0.53 per dollar of direct costs.\n\nOf course, the costs of compliance are not only measured in dollars nor are they visited only on research institutions. Those costs are visited on investigators, as well, often in the form of reduced research productivity. Data regarding reduced productivity have been collected by the Federal Demonstration Partnership (FDP), a collaboration among 10 federal agencies and 119 federally funded institutional partners that is sponsored by the National Academies [51] . The FDP survey was completed by 13,453 Principal Investigators with active federal grants in FY2010 (a 26% response rate). Respondents identified many administrative functions that consumed their effort (see Table 13 .3), greatly reducing (to 57.7%) the estimated proportion of funded effort actually spent on active research.\n\nBiomedical research institutions are under increasing financial pressure in the face of declining levels of NIH funding and accumulating administrative costs associated with ever-increasing regulatory and financial reporting requirements. While the latter trend affects all research institutions, it might be argued that it particularly affects those engaged in DURC, as the safety and security requirements for that form of research are particularly steep. In fact, the requirements are sufficiently demanding that some research institutions have moved away from DURC research because of the costs of continuing such investigations relative to the costs of other forms of biomedical research [42] . Hence, relative to biomedical research in general and to DURC specifically, we may be approaching a \"tipping point\" [60]:\n\nA decline in the quality of research infrastructure and compliance oversight, a gradual degradation of laboratories and facilities, and ultimately, lost competitiveness\u2026to conduct research.\n\nThese strains on research institutions are of particular note in light of current bioterrorism threats, which underscore the importance of scientific breakthroughs that can mitigate the effects of such threats. Indeed, Congress has recognized both the threat and the importance of scientific breakthroughs as part of its national strategy [51] for responding to bioterrorism [43] . Unfortunately, the effectiveness of the scientific arm of the \"war on terror\" can be undercut by the converging stresses discussed above, which shift increasing costs of research onto research institutions. Although the cost shift may be inadvertent, its magnitude is substantial and its long-term effects prejudicial to the health of the US scientific enterprise at a time when the potential for truly significant breakthroughs is at historically high levels.\n\nSeveral recent reports have targeted the unfunded mandate associated with the regulatory burden as a major contributor to this tipping point. For example, one report addressed the broad challenges facing research universities [44] . Of the 10 recommended actions, several were aimed at mitigating the regulatory burden described above: (i) that federal awards fully fund the direct and indirect costs of research (recommendation 6), and (ii) that the federal government reduce costly regulations that do not substantially improve the research environment (recommendation 7). Similar themes were echoed in another report that advanced more specific solutions [45]: years, the costs of maintaining that balance have grown and are visited disproportionately on the institutions in which the scientific work occurs. For these advances to continue, a rebalancing of the historically successful university-federal partnership is needed. We believe that the changes proposed in this chapter could rebalance that partnership and allow research institutions to remain viable enterprises, accommodating both scientific discoveries that can benefit human health and the precautions necessary to ensure the safety and security of the broader society."}