{"title": "Multistep-Ahead Air Passengers Traffic Prediction with Hybrid ARIMA-SVMs Models", "body": "Forecasting is an important component of planning in any enterprise, but it is particularly critical in airline revenue management because of the direct influence forecasts have on the booking limits that determine airline profits. Unfortunately, the passengers behaviors and other complicating factors make air passengers traffic prediction extremely difficult.\n\nIn the past decades, academic researchers and practitioners have made many contributions to air passengers traffic prediction. Most of the prediction models abounded in the literature can mainly be classified into two categories, namely, econometric models (casual model) and time series models. In the econometrics community, traditional research has been focusing on the economic factors of the take-off and landing point, such as population, income, and price [1\u20135]. It should be noted that most econometric models aimed to reveal the underlying relationship between air passengers traffic flow and selected variables such as geoeconomic and service-related factors. Due to the complexity of econometric modeling in variables selection, time series approach is a promising alternative in air passengers traffic prediction [6]. However, these traditional time series methods always do not work in practice. The main reason is that the underlying assumption of these traditional time series methods is linearity and they cannot capture the nonlinear patterns hidden and recognize the irregularity well. Recent research focuses on modeling time series with complex nonlinearity, dynamic variation, and high irregularity [7\u201314].\n\nRecently, the hybrid modeling approach integrating ARIMA, the most widely used linear model, and SVM, the well-established computational intelligence technique, has been proposed and justified in various areas [15\u201317]. Pai established a hybrid ARIMA and support vector machines model for stock price forecasting. Chen proposed a hybrid methodology that exploits the unique strength of the SARIMA (seasonal autoregressive integrated moving average) model and the support vector machines model in forecasting Taiwan's machinery industry production values. And Guo developed a hybrid Seasonal Autoregression Integrated Moving Average and Least Square Support Vector Machine (SARIMA-LSSVM) model to predict the mean monthly wind speed in Hexi Corridor, which shows that the developed method is simple and quite efficient. Additionally, Zhang combined the ARIMA and feedforward neural networks models in forecasting [18].\n\nThis hybrid approach arises from the fact that real-world time series are rarely pure linear or nonlinear but containing both linear and nonlinear patterns, and neither ARIMA nor SVM can be adequate in modeling and forecasting time series since the ARIMA model cannot deal with nonlinear relationships while the SVM model alone is not able to handle both linear and nonlinear patterns equally well. As such, this hybrid approach can take full advantages of the unique strength of ARIMA and SVM models in linear and nonlinear modeling, respectively. However, an important point to note from the past studies mentioned above is their preoccupation with one-step-ahead forecasting rather than the multistep-ahead one. In one-step-ahead case, the predictor uses all or some of the observations to estimate a variable of interests for the time step immediately following the latest observation. However, it provides no information as to the long-term future behavior of air passengers traffic.\n\nIn summary, this study extends the well-established ARIMA-SVMs model into the case of multistep-ahead prediction of air passengers traffic using the two modeling strategies, including iterated strategy and direct strategy and then goes a step forward by investigating the effectiveness of data preprocessing approaches, such as deseasonalization and detrending, along with the two strategies. The monthly air passengers traffic series from four airlines are used for the purpose of validation. The experimental results are judged on the basis of the prediction accuracy and the computational cost. The first contribution of this paper is that the proposed modeling framework is capable of capturing the complex dynamic of air passenger traffic considered, resulting in a more accuracy multistep-ahead prediction. The second contribution is to provide the first strong empirical evidence within the air passengers traffic literature on whether the superiority of ARIMA-SVMs modeling framework holds consistently in the case of multistep-ahead prediction. Last but not least, the third contribution is to provide the experimental evidence on the performance of different multistep-ahead prediction strategies, which may shed lights on the selection of modeling strategy while facing the different time horizons upon prediction.\n\nThe rest of this paper is organized as follows: Section 2 describes the methodologies used in this study including ARIMA and SVM along with the proposed hybrid methodology and multistep-ahead prediction strategies used in this study.  Section 3 details the research design on data source, data preprocessing, input selection, the selected counterparts for comparison, statistical criteria, methodological implementations, and experiment procedures. Following that, in Section 4, the experimental results are discussed.  Section 5 finally concludes this paper.\n\nARIMA (p, d, q) is called Autoregressive Integrated Moving Average model. It is one of the basic models of Box-Jenkins modeling method. AR refers to autoregressive part of the model and p is the autoregressive order. MA is moving average and q is the term of moving average. d is the difference frequency before a time sequence become smooth.\n\nGive a random sequence {x, t = 0,1,\u2026}, Bx\nt = x\nt\u22121, B is called as lagging operator. \u2207 = 1 \u2212 B, \u2207 is called as difference operator. If nonnegative integer d existed,\n(1)\u03bf^(B)\u2207dxt=\u03b8(B)at,\nwhere \u03bf^(B)=1-o^1B-o^2B2-\u22ef-o^pBp, \u03b8(B) = 1 \u2212 \u03b8\n1\nB \u2212 \u03b8\n2\nB\n2 \u2212 \u22ef\u2212\u03b8\nq\nB\nq, |B| \u2264 1, and \u03bf^(B) and \u03b8(B) are coprime, \u03bf^p\u03b8p\u22600, {a\nt} is white noise sequence, Ea\nt = 0, Ea\n1\n2 = \u03c3\n2 < \u221e, then {x\nt} is called autoregressive and moving average sequence.\n\nThe basic idea of this model is to regard the time series as a random sequence and to use a certain mathematical model to describe the sequence approximately. Once the model is identified, we can use the past value to predict the future.\n\nThe basic steps are as follows.Identify smooth sequence using scatter chart, autocorrelation function, partial autocorrelation function, and ADF unit root test.Do smooth processing on nonstationary sequence such as difference.Establish the model according to the time-series model identification rules. If the partial correlation function of a smooth sequence is censored and the autocorrelation function is trailing, sequence can be concluded for the AR model. If the partial correlation of a smooth sequence is trailing and the autocorrelation function is censored, it will be determined for MA model. And if both partial correlation function and autocorrelation function are trailing, ARMA model is for the sequence.Estimate parameters and justify whether the statistical test is significant.Do hypothesis test and diagnose whether the sequence is the white noise.Predict with the model established.\n\n\nAs a new method based on structural risk minimization principle, support vector machine is superior to those methods based on experience risk minimization principle. It can be converted to a salvation of a convex quadratic programming and make sure the extreme solution is the global optimal solutions. Support vector machines can handle higher dimension data better even with a relatively low amount of training samples and has a very good generalization. The models select limit support vectors from input data and so have a fast processing speed.\n\nFor linear and regressive data set {x\ni, y\ni}, the SVM regression function is formulated as follows:\n(2)f(x)=\u03c9Tx+b.\n\n\nThe coefficients \u03c9 and b are estimated by minimizing\n(3)12\u03c9T\u03c9+C1n\u2211i=1nL\u03b5(yi,f(xi)),\nwhere L\n\u03b5 is called the \u03b5-intensive loss function and is formulated as follows:\n(4)L\u03b5(y,f(x))={0if  |y\u2212f(x)\u2264\u03b5||y\u2212f(x)|others.\n\n\nBy introducing slack variables \u03be and \u03be*, Equation (3) can be transformed to the following constrained formulation:\n(5)min\u206112\u03c9Tw+C\u2211i=1n(\u03bei+\u03bei\u2217)\nsubject to\n(6)\u03c9xi+bi\u2212yi\u2264\u03b5+\u03bei\u2217\u2212\u03c9xi\u2212bi+yi\u2264\u03b5+\u03bei\u03bei,\u03bei\u2217\u22650i=1,2,\u2026,N.\nWhen solving the above formula, we always use dual theory to transform it into a convex quadratic programming problem. Introducing Lagrange equation, (5) changed into the following form:\n(7)min\u2061  12\u2211i,j=1n(\u03b1i\u2217\u2212\u03b1i)(\u03b1j\u2217\u2212\u03b1j)xiTxj\u2212\u2211i=1n\u03b1i\u2217(yi\u2212\u03b5)\u2212\u03b1i(yi+\u03b5)\nsubject to\n(8)\u2211i=1n(\u03b1i\u2212\u03b1i\u2217)=0, \u03b1i,\u03b1i\u2217\u2208[0,C].\n\n\nWhen the data set cannot be regressed linearly, we also map them to a high dimension feature space and make linear regress. Then the formulation is as follows:\n(9)min\u2061\u206112\u2211i,j=1n(\u03b1i\u2217\u2212\u03b1i)(\u03b1j\u2217\u2212\u03b1j)\u03a6(xi)T\u03a6(xj)  \u2212\u2211i=1n\u03b1i\u2217(yi\u2212\u03b5)\u2212\u03b1i(yi+\u03b5)\nsubject to\n(10)\u2211i=1n(\u03b1i\u2212\u03b1i\u2217)=0, \u03b1i,\u03b1i\u2217\u2208[0,C].\n\n\nLet K(X\ni, X\nj) = (\u03a6(X\ni) \u00b7 \u03a6(X\nj)) = \u03a6T(X\nj)\u03a6(X\ni); K(x, x) is the inner product of feature space and is called kernel function. Any symmetric function which satisfies Mercer theory can be kernel function. There are some common kernel functions: polynomial kernel function: K(x\ni, x\nj) = (x\nT\nx\ni + 1)p, RBF kernel function: K(x\ni, x\nj) = exp\u2061(\u2212\u03b3||x\ni\u2212x\nj||2), sigmoid kernel function: K(x\ni, x\nj) = th(\u03b2\n0\nx\nT\nx + \u03b2\n1).\n\n\nAccording to Karush-Kuhn-Tucker theory, the final SVM regression function can be the following form:\n(11)f(x)=\u2211i=1n(\u03b1i\u2212\u03b1i\u2217)K(xi,x)+b.\n\n\nAir passenger traffic demand is always affected by various factors such as economy, weather, and important events. Besides seasonality and long trend, there are always some irregular fluctuations which cannot be easily captured. ARIMA and SVM both have advantages in capturing linear or nonlinear patterns. So the hybrid methodology that has both linear and nonlinear modeling capabilities can be a good strategy for practical use. By combining different models, different aspects of the underlying patterns may be captured.\n\nIt is reasonable to consider a time series to be composed of a linear autocorrelation structure and a nonlinear component. That can then be represented as follows:\n(12)yt=Lt+Nt,\nwhere L\nt denotes the linear component and N\nt denotes the nonlinear component. These two components have to be estimated from the data. First, we let ARIMA model the linear component, then the residuals from the linear model will contain only the nonlinear relationship. Let e\nt denote the residual at time t from the linear model, and L^t denotes the forecast value of the ARIMA model, then\n(13)et=yt\u2212L~t.\n\n\nZhang illustrated a linear model is not sufficient if there are still linear correlation structures left in the residuals. Moreover, even if a model has passed diagnostic checking, the model may still not be adequate in that nonlinear relationships have not been appropriately modeled [18]. So by modeling residuals using SVM, nonlinear relationships can be discovered. That can be represented as follows:\n(14)et=f(et\u22121,et\u22122,\u2026,et\u2212n)+\u03b5t,\nwhere f is a nonlinear function determined by SVM and \u03b5\nt is the random error. Therefore, the combined forecast is\n(15)yt=L~t+N~t,\nwhere N~t is the forecast value of the SVM model. In summary, the ultimate forecast value consists of linear forecast part and nonlinear forecast part.\n\nThe first is named as the iterated strategy by Chevillon [19] and is often advocated in standard time series textbooks [20, 21]. This strategy constructs a prediction model by means of minimizing the squares of the in-sample one-step-ahead residuals and then uses the predicted value as a \u201cknown\u201d input to forecast the next point and continues in this manner until reaching the horizon.\n\nIn more detail, iterated strategy first embeds the original series into an input-output format:\n(16)D={(xt,yt)\u2208(Rm\u00d7R)}t=dN,\nwhere x\nt \u2282 {\u03c6\nt,\u2026, \u03c6\nt\u2212d+1}, y\nt = \u03c6\nt+1.\n\nThen the iterated prediction strategy learns one-step-ahead prediction model:\n(17)\u03c6t+1=f(xt)+\u03c9,\nwhere \u03c9 denotes the additive noise.\n\nAfter the learning process, the estimation of the H next values is returned by\n(18)\u03c6^t+h ={f^(\u03c6t,\u03c6t\u22121,\u2026,\u03c6t\u2212d+1)if  h=1f^(\u03c6^t+h\u22121,\u03c6^t+1,\u03c6t,\u2026,\u03c6t\u2212d+h)if  h\u2208[2,\u2026,d]f^(\u03c6^t+h\u22121,\u2026,\u03c6^t+h\u2212d)if  h\u2208[d+1,\u2026,H].\n\n\nIn contrast to the iterated strategy which uses a single model, the other commonly applied strategy, namely, direct strategy first suggested by Cox [22], constructs a set of prediction models for each horizon using only its past observations, where the associated squared multistep-ahead errors are minimized. Direct strategy estimates H different models between the inputs and the outputs to predict {\u03c6\nN+h, h = 1,2,\u2026, H}, respectively, which requires a sharply increased computational expense, especially in the case of a longer prediction horizon [23]. The direct strategy first embeds the original series into H datasets\n(19)D1={(xt,yt1)\u2208(Rm\u00d7R)}t=dN,  \u22eeDH={(xt,ytH)\u2208(Rm\u00d7R)}t=dN,\nwhere x\nt \u2282 {\u03c6\nt,\u2026, \u03c6\nt\u2212d+1}, y\nth = \u03c6\nt+h.\n\nThen, the direct prediction strategy learns H direct models on D\nh = {D\n1,\u2026, D\nH}, respectively. Consider\n(20)\u03c6t+h=fh(xt)+\u03c9h, h\u2208{1,\u2026,H},\nwhere \u03c9 denotes the additive noise.\n\nAfter the learning process, the estimation of the H next values is returned by\n(21)\u03c6t+h=f^h(\u03c6t,\u03c6t\u22121,\u2026,\u03c6t\u2212d+1), h\u2208{1,\u2026,H}.\n\n\nIn this study, we consider H-step ahead forecasting by ARIMA-SVMs model. The proposed methodology of the multistep-ahead prediction hybrid system consists of two stages. In the first stage, an ARIMA model is used to analyze the linear part of the problem for each step. In the second stage, SVMs model is developed to model the residuals from the ARIMA model. Since the ARIMA model cannot capture the nonlinear structure of the data, the residuals of linear model will contain information about the nonlinearity. Assuming y\nt (t = 1,2,\u2026, n) is the original data set and according to ARIMA-SVMs model, y\nt includes the linear part L\nt and nonlinear part N\nt, namely, y\nt = L\nt + N\nt. First of all, H-step ahead prediction by ARIMA model will be employed and obtain the H-step ahead forecasted value L~t+h, then for each time t, the residuals of y\nt+h can be obtained by \u03b5t+h=yt+h-L~t+h. And for nonlinear time series \u03b5\nt (t = 1,2,\u2026, n), H-step ahead prediction by SVMs model will be employed and obtain the H-step ahead forecasted values N~t+h, the ultimate forecasted time series for y\nt+h is y~t+h, which consist of L~t+h and N~t+h. In this research, two multistep-ahead forecasting strategies will be employed, namely, iterated strategy and direct strategy, and it should be noted that if the multistep-ahead strategy has been selected for linear part, the same strategy should be employed in nonlinear part; therefore, the model should be ARIMA (iterated strategy)-SVMs(iterated strategy) or ARIMA(direct strategy)-SVMs(direct strategy).\n\nThe flowchart of proposed multistep-ahead ARIMA-SVMs modeling framework is depicted in Figure 1. As shown in Figure 1, the proposed modeling framework includes three parts: data preprocessing, multistep-ahead prediction, and PSO module for SVMs parameters optimization (details on this parameters tuning module can be found in Section 3.3). In data preprocessing procedure, linear transference, deseasonalization, and detrending are performed on the data, and then the multistep-ahead forecasting model based on ARIMA-SVMs is adopted; in addition, in nonlinear forecasting part, the parameter optimization algorithm PSO is employed to optimize the parameters of SVM.\n\nIn this study, four monthly air passenger traffic series of American airlines are chosen as experimental samples, namely, American Airlines, Delta Air Lines, Southwest Airlines, and United Airlines. The data are freely obtained from the Bureau of Transportation Statistics, U.S. Department of Transportation (http://www.bts.gov/). The main reason of selecting these four airlines is that these airlines are famous in America and they represent the development trend of air industry in America.\n\nConsidering the Severe Acute Respiratory Syndrome (SARS) which happened in 2003 and brought a heavy blow to American air industry, the four sampling data series cover a period from January 1990 to December 2001, each with a total of 144 observations. The original data of these four airlines are shown in Figures 2(a)\nto 2(d).\n\nThe descriptive statistics details of all the four data series are summarized in Table 1.\n\nNormalization is a standard requirement for time series modeling and prediction with neural network. Thus, the data sets are first scaled by linear transference to map onto a range of [0, 1]. As mentioned in Section 1, the air passengers traffic data are monthly and with strong seasonal components and trend patterns. Therefore, after the linear transference, deseasonalization and detrending are performed on the data. Deseasonalization is performed by means of the revised multiplicative seasonal decomposition presented in [24]. In addition, detrending is performed by fitting a polynomial time trend and then subtracting the estimated trend from the series when trends are detected by the Mann-Kendall test [25].\n\nFilter method was employed for input selection in this study. In the case of the filter method, the best subset of inputs is selected a priori based only on the dataset. The input subset is chosen by a predefined criterion, which measures the relationship of each subset of input variables with the output [26]. Specifically, in terms of input selection criteria, the partial mutual information [27] is used for the prediction models. Mutual information (MI) is a commonly adopted measure of dependence between variables and has been widely used for input selection [26]. However, this raises a major redundancy issue because the MI criterion does not account for the interdependency between candidate variables. To address this issue, Sharma [27] developed an improved algorithm that exploits the concept of partial mutual information (PMI), which is the nonlinear statistical analog of partial correlation. The definitions of PMI are shown as follows:\n(22)PMI=\u222cfX\u2032,Y\u2032(x\u2032,y\u2032)log\u2061e\u2061[fX\u2032,Y\u2032(x\u2032,y\u2032)fX\u2032(x\u2032)fY\u2032(y\u2032)]dx\u2032dy\u2032,\nwhere\n(23)x\u2032=x\u2212E[x \u2223 z],y\u2032=y\u2212E[y \u2223 z],\nwhere X\u2032 and Y\u2032 are generalized to represent time series x(t) and lagged time x(t \u2212 i) with time step i  (i \u2264 d) conditional on Z which is a set of remaining time-lag variables. In performing the PMI, the input variable with highest conditional PMI value at every iteration can be added to the inputs set.\n\nIn this study, RBF has been chosen as the kernel function for SVM models, and thus the parameters \u03b5, \u03b3, and C are to be optimized based on the training sets. Despite the new variants of PSO for parameter tuning such as in [28], in this study, we chose the standard PSO algorithm to tune the parameters with the lowest MAPE on training sets.\n\nPSO based operators are used to explore the search space. Particle Swarm Optimization (PSO) is a population-based metaheuristic that simulates social behavior such as birds flocking to a promising position to achieve precise objectives (e.g., finding food) in a multidimensional space by interacting among them [29]. To search for the optimal solution, each particle adjusts their flight trajectories by using the following updating equations:\n(24)vidt+1=w\u00d7vidt+c1\u00d7r1\u00d7(pid\u2212xidt)+c2\u00d7r2\u00d7(pgd\u2212xidt),xidt+1=xidt+vidt+1,\nwhere c\n1, c\n2 \u2208 \u211c are acceleration coefficients, w is inertia weight, and r\n1 and r\n2 are random numbers in the range of [0,1]. v\nid\nt and x\nid\nt denote the velocity and position of the ith particle in dth dimension at tth iteration, respectively. p\nid is the value in dimension d of the best parameters combination (a particle) found so far by particle i. p\ni = \u2329p\ni1,\u2026, p\niD\u232a is called personal best (pbest) position. p\ngd is the value in dimension d of the best parameters combination (a particle) found so far in the swarm (P); p\ng = \u2329p\ng1,\u2026, p\ngD\u232a is considered as the global best (gbest) position. Note that each particle takes individual (lbest) and social (gbest) information into account for updating its velocity and position.\n\nIn the search space, particles track the individual's best values and the best global values. The process is terminated if the number of iterations reaches the predetermined maximum number of iterations.\n\nSingle SVR and monthly ARIMA are selected as counterparts for the purpose of comparison. It should be noted the reason for selecting single SVR and monthly ARIMA is to justify the effectiveness of ARIMA-SVM modeling framework. Additionally, the performances on both one-step-ahead (prediction horizon H = 1) and multistep-ahead (prediction horizon H = 2,4, 6,8, 12,18,24) prediction are compared across all the models to provide more evidences for justification. Note that the iterated strategy and direct strategy are employed in this study for comparison.\n\nTo compare the effectiveness of the different prediction models, no single accuracy measure can capture all the distributional features of the errors when summarized across data series. For each forecast horizon h, here, we consider three forecast accuracy measures. The first is the mean absolute percentage error (MAPE) defined as (25), and the second is symmetric mean absolute percentage error (SMAPE) defined as (26), and the last accuracy measure is the mean absolute scaled error (MASE), defined as (27). MAPE has the advantage of being scale-independent and so is frequently used to compare forecast performance across different datasets. Moreover, the MAPE also has the disadvantage that it puts a heavier penalty on positive errors than on negative errors. MASE has recently been suggested by Hyndman and Koehler [30] as a means of overcoming observation and errors around zero existing in some measures. The MASE has some features which are better than the SMAPE, which has been criticized for the fact that its treatment of positive and negative errors is not symmetric [31]. However, the MAPE and SMAPE are still used in this study due to their popularity in forecasting literature. The smaller the values of MAPE, SMAPE, and MASE, the closer the predicted time series values to the actual values. Consider\n(25)MAPEh=1M\u00b7T\u2211m=1M \u2211t=1T(100|xmh(t)\u2212x^mh(t)xmh(t)|),\n(26)SMAPEh=1M\u00b7T\u2211m=1M \u2211t=1T(100|xmh(t)\u2212x^mh(t)(|xmh(t)|+|x^mh(t)|)/2|),\n(27)MASEh =1M\u00b7T\u2211m=1M \u2211t=1T|xmh(t)\u2212x^mh(t)(1/(N\u22121))\u2211i=2N|xm(i)\u2212xm(i\u22121)||,\nwhere x\nm(t) denotes the observation at period t for time series m, x^m(t) denotes the forecasted value of x\nm(t), M is the number of time series (in this case, M = 4), T is the number of observation in the hold-out sample (in this case, T = 48), and N is the number of observation in the estimation sample for time series m.\n\nIn this study, we repeat running each model fifty times for airline passengers traffic dataset to even out the fluctuations. Then each of the fifty runs will produce a SMAPE for all 4 time series. Next, the mean and standard deviation of these fifty SMAPE are calculated and listed in the tables for examining the performance of different models. Similarly, the mean and standard deviation of MASE are also computed. Note that the error measures are computed after rolling back of the preprocessing step performed, such as deseasonalization and detrending.\n\nFollowing the experimental settings in [32], we also conduct a number of statistical tests to compare each model based on the obtained fifty SMAPE and MASE at the 0.05 significance level. For each prediction horizon (H = 1 and 24) and performance measure (i.e., MAPE, SMAPE, and MASE), we perform a one-way analysis of variance (ANOVA) procedure to determine if there exists statistically significant difference among the eight models in out-of-sample forecasting. Then, to further identify the significant difference between any two models, the Tukey honestly significant difference (HSD) test [33] is employed to compare all pairwise difference simultaneously. Note that Tukey HSD test is a post-hoc test; this means that a researcher should not perform Tukey HSD test unless the results of ANOVA are positive. A rank-based performance measure termed as multiple comparisons with the best (MCB) [34] is used to test whether some models perform significantly worse than the best model.\n\nLibSVM (version 2.86) [35] is employed for SVR modeling here. We select the Radial basis function (RBF) as the kernel function in the prediction models. To determine the hyperparameters, namely, C, \u03b5, \u03b3 (in the case of RBF as the kernel function), the PSO algorithm is employed in the current study. Due to its simplicity and generality as no important modification was made for applying it to model selection, PSO has been recently established for parameter determination of SVR. In solving hyperparameter selection by the PSO, each particle is requested to represent a potential solution (C, \u03b5, \u03b3). Fortunately, several empirical and theoretical studies have been performed about the parameters of PSO from which valuable information can be obtained [36]. In this study, the parameters are determined according to the recommendations in these studies and selected based on the prediction performance and computational time in a trial-error fashion. Through experiments, the parameter values of PSO are set as follows. Both the cognitive and interaction coefficients are set to 2. The swarm size and number of iterations are set to be 10 and 50, respectively.\n\nFor monthly ARIMA estimation, the automatic model selection algorithm proposed by Hyndman and Khandakar and implemented in the R software package \u201cforecast\u201d5 is used in this study. Based on these, we develop our proposed hybrid ARIMA-SVMs programs in Matlab, which is available upon request.\n\n\nFigure 3 shows the experimental procedure using the real time series. Each series is split into the estimation sample and the hold-out sample firstly. Then, the input selection and model selection for each series are conducted using aforementioned filter method, PSO algorithm, and fivefold cross-validation with iterated and direct strategies. Finally, the attained models are tested for hold-out samples, the measures MAPEh, SMAPEh, and MASEh are computed for each prediction horizon h (in our case h = 1,2,\u2026, 24) over four time series. Furthermore, the modeling process for each series is repeated fifty times. Upon the termination of this loop, performance of the examined models with selected strategies at each prediction horizon is judged in terms of the mean, average by fifty, of the MAPEh, SMAPEh, and MASEh. Analysis of variance (ANOVA) test procedures are used to determine if the means of performance measures are statistically different among the five models for each prediction horizon and dataset. If so, Tukey's honesty significant difference (HSD) tests [37] are then employed further to identify the significantly different prediction models in multiple pairwise comparisons at 0.05 significance level.\n\nThe forecasting performance of the hybrid ARIMA-SVMs and benchmarking method on the four airlines' air passenger series covered a period from January 1990 to December 2001 are shown in Table 2.\n\nFor each accuracy measure and prediction horizon, the strategies are rank order from 1 (the best) to 6 (the worst) in Table 2.As far as the comparison between ARIMA-SVMs with others benchmark methods is concerned, the results are mixing among the prediction measures examined. In terms of MAPE and the data without deseasonalization-detrending, I-ARIMA-SVM gets the best performance when horizon = 1, and D-SVM gets the best performance when horizon = 4, 8, 12 and in the average accuracy measures over the prediction horizon 1 to 24, while D-ARIMA-SVM gets the performance when horizon = 2, 6, 18, 24.It is obvious that applying deseasonalization and detrending approach for multistep-ahead prediction obtains better performances.\n\n\nFollowing the experimental procedure presented in Figure 3, an ANOVA procedure is performed to determine if there exists a statistically significant difference among the six modeling strategies in the hold-out sample for each of the performance measures and the prediction horizon. Furthermore, to identify the significant difference between any two strategies, the Tukey's HSD test is used to compare all pairwise differences simultaneously. Tukey's HSD test is a post-hoc test, meaning that Tukey's HSD test should not be performed unless the results of the ANOVA procedure are positive.  Table 3 shows the results of the multiple comparison tests for three datasets. Several observations can be made from Table 3.As for the comparison between ARIMA-SVMs and the benchmark methods, the difference in prediction performance is significant at the 0.05 level and ARIMA-SVMs is better than SVM and ARIMA.Concerning the current two multistep-ahead forecasting strategies, the direct strategy significantly outperforms the iterated strategy for the majority of prediction horizons.\n\n\nDuring the experiments, we also found that when we try to improve the forecast accuracy of SVM, there will be an overfitting on training sets. That means, though we can get a better forecast performance of the SVM model, we cannot get a better forecast performance or even get a worse performance of the hybrid ARIMA-SVM model. Therefore, the optimal parameters of the hybrid model should be further researched.\n\nDue to the complex and dynamic pattern with uncertainty, time series prediction still remains as one of the most challenging tasks in field of time series analysis. The hybrid ARIMA-SVMs prediction models have been established recently, which has present good performance in time series prediction. In this study, the hybrid ARIMA-SVMs has been employed in multistep-ahead prediction which is more complex and difficult than one-step-ahead. The contribution of this study is employing ARIMA-SVMs in multistep-ahead prediction; moreover, air passengers traffic data is used for this study and a large scale comparative study has been conducted for validation. Quantitative and comprehensive assessments are performed with the air passengers traffic data on the basis of the several prediction accuracies. Experimental results and comparisons demonstrate the superiority of the proposed ARIMA-SVMs modeling strategy for multistep-ahead time series prediction."}