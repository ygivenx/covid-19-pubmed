{"title": "MICROBIOLOGICAL HEALTH EFFECTS ASSOCIATED WITH THE USE OF MUNICIPAL WASTEWATER FOR IRRIGATION", "body": "For centuries man has been conscious of the potential value of the application of human wastes to the land.\n\nThus, von Liebig, in his 1863 work, \"The Natural Laws of Husbandry\" (Jewell and Seabrook, 1979) wrote: \"Even the most ignorant peasant is quite aware that the rain falling upon his dung-heap washes away a great many silver dollars, and that it would be much more profitable to him to have on his fields what now poisons the air of his house and the streets of the village; but he looks on unconcerned and leaves matters to take their course, because they have always gone on in the same way.\"\n\nIn the context of present-day conventional wastewater treatment we might add \"poisons the rivers and streams\" as well.\n\nMore recently, the Committee on Water Quality Criteria of the National Academy of Sciences-National Academy of Engineering (1972) stated that: \"An expanding population requires new sources of water for irrigation of crops and development of disposal systems for municipal and other wastewaters that will not result in the contamination of streams.\n\nFor each agent of concern the types and levels commonly found in municipal wastewater and the efficiency of preapplication treatment (usually stabilization pond) are briefly re viewed.\n\nThere then follows a discussion of the levels, behavior, and survival of the agent in the medium or route of potential human exposure, i.e., aerosols, surface soil and plants, as appropriate.\n\nFor the pathogens, infective dose, risk of infection, and epidemiology are then briefly reviewed. Finally, conclusions are presented.\n\nThe pathogenic bacteria of major concern in wastewater are listed in Table I. All have symptomless infections and human carrier states, and many have important nonhuman reservoirs as well.\n\nThe pathogenic bacteria of minor concern are listed in Table II ; this list is perforce somewhat arbitrary since almost any bacterium can become an opportunistic pathogen under appro priate circumstances, e.g., in the immunologically compromised or in the debilitated.\n\nIt is now thought to be as prevalent as the commonly recognized enteric bacteria Salmonella and Shigella, having been isolated from the stools of between 4 and 8% of patients with diarrhea (MMWR, 1979) .\n\nSince the bacteria of feces are predominantly anaerobes while the environment of wastewater is aerobic, and thus toxic to the anaerobes, the bacterial composition of wastewater is drastically different from that of feces.\n\nThe composition also varies with geographic region and season of the year, higher densities being found in summer.\n\nAccording to Carnow et al. (1979) the most prominent bacteria of human origin in raw municipal wastewater are Proteus, Enterobacteria (10^/ml), fecal Streptococcus (10 3 -10 4 /ml), and Clostridium (10 2 -10 3 ). Less prominent bacteria include Salmonella and Mycobacterium tuberculosis.\n\nThe total bacterial content of raw wastewater, as recovered on standard media at 20\u00b0C (Carnow et al., 1979) , is about 10^ to 10^ organisms/ml. The presence and levels in waste water of any of the pathogens listed in Tables I and II \n\nAlthough any level of bacterial inactivation could theo retically be accomplished by disinfection with chlorine, such a practice on raw wastewater would be very costly (because of the high BOD, and thus high chlorine consumption), could produce carcinogenic halomethanes, and could cause damage to the soil biota. Thus, simpler methods of preapplication treatment should be considered, if indeed they are necessary for the protection of public health.\n\nAn important point to keep in mind when discussing the degree of pathogen removal or survival during various wastewater treatment unit processes is the health significance of the number of organisms remaining.\n\nFor example, if a wastewater contains 10^ pathogenic bacteria/liter, a superficially impres sive 99% removal, or 1% survival, will produce an effluent with 10 3 pathogenic bacteria/liter. This level may still be of great public health concern, depending on how the effluent is used.\n\nThe minimum preapplication treatment system likely to be used in land treatment is sedimentation, or conventional primary treatment.\n\nTypical degrees of bacterial removals have been summarized by Crites and Uiga (1979) and Sproul (1978) , and are presented in Table V .\n\nAs a result of the need for storage of wastewater in most land treatment systems, and the possible need for low-cost further pathogen removal, wastewater stabilization ponds are likely to be the most common preapplication treatment system. Wastewater stabilization ponds, or \"lagoons\", are large shallow ponds in which organic wastes are decomposed by the action of microorganisms, especially bacteria.\n\nThere are three types of ponds in common use, often used in a series (Feachem et al., 1978) :\n\n(1) anaerobic pretreatment ponds (2 to 4 m deep, 1 to 5 (1)\n\nIn single anaerobic ponds E. coli removals of 46 to 85% after 3.5 to 5 days at various temperatures have been reported.\n\n(2)\n\nIn single facultative and aerobic ponds E. coli removals of 80 to >99% after 10 to 37 days at various temp eratures have been reported.\n\n(3)\n\nIn single facultative and aerobic ponds fecal strep tococci removals are similar to or greater than E. coli.\n\n(4) Removals of 99.99% or greater have been reported for series of 3 or more ponds.\n\nOne or two ponds will remove between 90 to 99% of Salmonella or other pathogenic bacteria.\n\nComplete elimination of pathogenic bacteria can be achieved with 30 to 40 day retention times, particularly at high temperatures ( >25\u00b0C).\n\nA series of 5 to 7 ponds, each with a 5 day retention time, can produce an effluent with less than 100 fecal coliforms and fecal streptococci/100 ml. Aerated lagoons, i.e., ponds with mechanical aerators, have been reported to provide removal rates of 60 to 99.99% for total coliforms and 99% for fecal coliforms, total bacteria. Salmo nella typhi and Pseudomonas aeruginosa (Crites and Uiga, 1979).\n\nThus, wastewater stabilization ponds can be designed to achieve practically any degree of bacterial pathogen removal deemed necessary for the protection of public health, including complete wastewater treatment.\n\nSuch a high degree of preappli cation treatment, of course, should not be necessary for most land treatment systems.\n\nWhere wastewater is applied to the land by spray equipment of some sort, e.g., impact sprinklers, fan sprinklers, rain guns, and fixed-aperture rocker-arm sprayers, aerosols that travel beyond the wetted zone of application will be produced (Schaub et al. , 1978a) . These are suspensions of solid or liquid particles up to about 50 urn in diameter, formed, for example, by the rapid evaporation of small droplets to form droplet nuclei. Their content of microorganisms depends upon the concentration in the wastewater and the aerosolization efficiency of the spray process, a function of nozzle size, pressure, angle of spray trajectory, angle of spray entry to the wind, impact devices, etc. (Schaub et al., 1978a) .\n\nAlthough aerosols represent a means by which pathogens may be deposited upon fom\u00eetes such as clothing and tools, the major health concern with aerosols is the possibility of direct human infection through the respiratory route, i.e., by inhalation. The exact location where aerosol particles are actually depo sited upon inhalation is a function of their size.\n\nThose above about 2 urn in diameter are deposited primarily in the upper respiratory tract (including the nose for larger particles), from which they are carried by cilia into the oropharynx. They then may be swallowed, and enter the gastrointestinal tract. The smaller airways and alveoli do not possess cilia, so that pathogens deposited there would have to be combated by local mechanisms. Although the pattern of deposition is variable, the greatest alveoli deposition appears to occur in the 1 to 2 urn range, decreasing to a minimum at about 0.25 urn, and increasing below 0.25 urn (Sorber and Guter, 1975).\n\nWhen aerosols are generated, bacteria are subject to an immediate \"aerosol shock\", or \"impact factor\", which may reduce their level by 10 fold within seconds (Schaub et al., 1978a). There is some evidence that this might be caused by rapid pressure changes (Biederbeck, 1979).\n\nTheir survival is sub sequently determined primarily by relative humidity and solar radiation (Carnow et al., 1979; Teltsch and Katzenelson, 1978) . At low relative humidities rapid desiccation occurs, resulting in rapid die-off (Sorber and Guter, 1975), although concen tration of protective materials within the droplet may occur (Schaub et al., 1978a). Solar radiation, particularly the ultraviolet portion, is destructive to bacteria, and increases the rate of desiccation.\n\nTeltsch and Katzenelson (1978) have found bacterial survival at night up to ten times that during daytime in Israel.\n\nHigh temperature is another factor de creasing bacterial survival. \u03bf \u03bf I \u03bf\u03bf \u03bd\u03bf\n\n4-1 4-1 4-1 4-4 4-1 4-1 4-J 4-4\n\nCQ \u038c rrj \u039d rrj \u03c7:\n\nCN rH 03 05 03 JC 00 \u03c5 \u03c5 r*-EN en X: (1980) have recently shown that further bacteriological confirmation steps of \"Klebsiella\" isolates reveal them to be nonpathogenic bac teria, true Klebsiella dying off rapidly during the aerosolization process.\n\nBecause of the low density of aerosol bacteria normally emanating from land treatment sites, high-volume samplers, e.g., 1 m 3 /min electrostatic precipitators, are often necessary for aerosol analysis.\n\nLikewise, because of the normally low density of pathogenic bacteria compared with nonpathogens, most mea surements of aerosol bacteria have utilized traditional indi cator bacteria, e.g., standard plate count, total coliforms, and fecal coliforms.\n\nThe measurements of Johnson et al. (1980) have shown little correlation between densities of these indicator bacteria and densities of the pathogens which they are intended to indicate. This results in \"extreme underestimation of pathogen levels,\" since the pathogens which they studied, i.e., Pseudomonas, Streptococcus, and Clostridium perfringens, sur vived the aerosolization process much better than did the indicator bacteria.\n\nThey suggest that fecal streptococci might be a more appropriate indicator organism because of its similar hardiness upon impact and viability to those of pathogens. Similarly, Teltsch et al.\n\n(1980) measured densities of coli forms. Salmonella, and enteroviruses in aerosols and wastewater at an Israeli land treatment site, and from \"...the ratios of salmonellae to coliforms and enteroviruses to coliforms in the air, as compared to these ratios in the wastewater, it was concluded that the suitability of coliforms as an indication of airborne contamination caused by spray irrigation is questio nable.\"\n\nThe results of some of the most important studies of aerosol bacteria production at land treatment spray sites are summarized in Table VI. Although local environmental con ditions, e.g., wind speed, vary among and within these studies, the results give a general idea of aerosol bacteria levels to be expected at land treatment sites.\n\nThe results suggest that the aerosol bacteria are usually detected at a maximum distance less than 400 m from the spray site.\n\nExperiments in Israel (Katzenelson et al., 1977) found that Escherichia coli could be detected in aerosols 10 m from the sprinkler only when its concentration in the wastewater reached 10^/ml or more.\n\nThe Pleasanton, California data (Johnson et al. , 1978) suggest that a threshold value of 10 3 /ml might be more reasonable for wastewater bacteria.\n\nThere is some evidence (Reploh and Handloser, 1957) that the type of sprinkler and spray diameter has little effect on the distance of aerosol bacteria transport. It is generally felt, however, that downward-directed, low-pressure sprinklers (usually on centerpivot spray rigs) produce much less aerosol than the upwarddirected, high-pressure types used to obtain the data in Table  VI. The Ft. Huachuca, Arizona results indicate a much greater transport distance during night than day; likewise the 400 m measurement in Germany (Bringmann and Trolldenier, 1960) oc curred at night.\n\nThe high nighttime transport of aerosol bacteria is probably due to high humidity and absence of solar radiation. Most of the aerosols represented by the data in Table  VI are probably respirable, since Bausum et al. (1978) found that, at 30 m in Deer Creek, 75% of the particles fell in the range of 1 to 5 urn, with a median of 2.6 urn.\n\nThe human exposure to aerosol bacteria at land treatment sites can be roughly estimated from the data at Kibbutz Tzora, Israel, where raw wastewater was sprayed, thus yielding higher bacterial levels than those found at Deer Creek, Ft. Huachuca, or Pleasanton, where treated wastewater was sprayed.\n\nThus, an adult male, engaged in light work, breathing at a rate of 1.2 m 3 /hr, and exposed to 34 coliforms/m 3 (the Kibbutz Tzora average) at 100 m downwind from a sprinkler, would inhale approximately 41 coliforms/hr.\n\nSince the ratio of aerosolized Salmonella to coliforms is 1:10^ (Grunnet and Tramsen, 1974) the rate of inhalation of Salmonella would be about 10^-fold less, an extremely low rate of bacterial exposure.\n\nMore recent data from Kibbutz Tzora allows a more accurate estimate of human exposure (Teltsch et al., 1980). During a period of time in 1977-78, when the wastewater total coliforms were 2.4x10^ to 1.4xl0 7 /100 ml and Salmonella was 0 to 60/100 ml, the density of aerosol Salmonella at 40 m, the maximum distance found, was 0 to 0.054/m 3 , with a mean of 0.014/m 3 . This would result in an inhalation rate of 0.017/hr at 40 m, higher than the previous estimate, but still an extremely low rate of bacterial exposure (cf. the infective dose discussion below).\n\nThe surface soil and plants of an active land treatment site are constantly heavily laden with enteric bacteria; these are the specific locations where the actual treatment of the wastewater and inactivation of the bacteria occur.\n\n(In some situations bacteria may be deposited on plants in the environs of a land treatment site, due to aerosol drift.)\n\nThe survival time of bacteria in surface soil and on plants is only of concern when decisions must be made on how long a period of time must be allowed after last application before permitting access to people or animals, or harvesting crops. (2) Moisture-holding capacity. Survival time is shorter in sandy soils than those with greater water-holding capacity.\n\n(3) Temperature. Survival time is longer at lower temp eratures, e.g., in winter.\n\n(4) pH. Survival times are shorter in acid soils (pH 3-5) than in neutral or alkaline soils.\n\nSoil pH is thought to have its effect through control of the availability of nutrients or inhibitory agents.\n\nThe high level of fungi in acid soils may play a role.\n\n(5) Sunlight. Survival time is shorter at the surface, probably due to dessication and high temperatures, as well as ultraviolet radiation.\n\nOrganic matter. Organic matter increases survival time, in part due to its moisture-holding capacity.\n\nRegrowth of some bacteria, e.g.. Salmonella, may occur in the presence of sufficient organic matter.\n\nIn highly organic soils anaerobic conditions may increase the survival of Escherichia coli (Tate, 1978) .\n\n(7) Soil microorganisms. The competition, antagonism, and pr\u00e9dation encountered with the endemic soil microorganisms decreases survival time.\n\nProtozoa are thought to be important predators of coliform bacteria (Tate, 1978) .\n\nEnteric bacteria applied to sterilized soil survive longer than those applied to unsterilized soil.\n\nIn view of the large number of environmental factors affecting bacterial survival in soil, it is understandable that the values found in the literature vary widely.\n\nTwo useful summaries of this literature are those of Bryan (1977) and Feachem et al. (1978) . The ranges given in Table VII are extracted from these summaries, as well as other literature. \"Survival\" as used in this table, and throughout this report, denotes days of detention.\n\nIt should be noted that inactivation is a rate process and, therefore, detection depends upon the initial level of organisms, sensitivity of detection metho dology, and other factors.\n\nIf kept frozen, most of these bacteria would survive longer than indicated in Table VII , but this would not be a realistic soil situation. The survival of bacteria on plants, particularly crops, is especially important since these may be eaten raw by animals or humans, may contaminate hands of workers touching them, or may contaminate equipment contacting them. Such ingestion or contact would probably not result in an infective dose of a bacterial pathogen, but if contaminated crops are brought into the kitchen in an unprocessed state they could result in the regrowth of pathogenic bacteria, e.g.. Salmonella, in a food material affording suitable moisture, nutrients, and tempera ture (Bryan, 1977). The ranges given in Table VIII are extracted from these summaries, as well as other literature. (1) cracks and split stem ends provide protected harboring places for enteric bacteria to survive for long periods, and such portions should be cut away before consumption,\n\n(2) on normal tomatoes, without cracks, after direct application of wastewater to the surface of the fruit the residual coliform concentration decreases to or below that of uncontaminated controls by the end of 35 days or less,\n\n(3) survival of Salmonella and Shigella on tomato surfaces in the field did not exceed 7 days, even when applied with fecal organic material, and (4) if wastewater application is stopped about one month before harvest, the chances for the transmission of enteric bacterial diseases will decrease to almost nil.\n\nThey also noted that, because of regrowth in soil and on leaf crops, total coliforms and fecal streptococci bore no relationship to Salmonella levels, and are unacceptable indicators of fecal contamination; they recommended using fecal coliforms or Salmonella itself.\n\nThus, the consumption of subsurface and low-growing food crops, e.g., leafy vegetables and strawberries, harvested from an irrigation site within about six months of last application, is likely to increase the risk of disease transmission, because of contamination with soil and bacterial survival in cracks, leaf folds, leaf axils, etc.\n\nPossible approaches to avoid this problem are (1) use of the subsurface or covered drip irrigation method for aerial crops (Sadovski et al., 1978a; 1978b),\n\n(2) growth of crops the harvested portion of which does not contact the soil, e.g., grains and orchard crops, or (3) growth of crops used for animal feed only, e.g., corn (maize), soybeans, or alfalfa.\n\nThe last alternative is probably the most common and most economic.\n\nIn the situation where the harvested portion does not contact the soil nor is within splash distance, stopping wastewater application a month prior to harvest would be prudent.\n\nUpon being deposited on or in a human body a pathogen may be destroyed by purely physical factors, e.g., desiccation or decomposition.\n\nBefore it can cause an infection, and eventually disease, it must then overcome the body's natural defenses. In the first interaction with the host, whether in the lungs, in the gastrointestinal tract, or other site, the pathogen encounters nonspecific immunologic responses, i.e., inflammation and pha gocytosis.\n\nPhagocytosis is carried out primarily by neutrophils or polymorphonuclear leukocytes in the blood, and by mononuclear phagocytes, i.e., the monocytes in the blood and macrophages in the tissues (e.g., alveolar macrophages in the lungs).\n\nLater interactions with the host result in specific immunologic responses, i.e., humoral immunity via the B-lymphocytes, and cell-mediated immunity via the T-lymphocytes (Bellanti, 1978).\n\nSome representative oral infective dose data for enteric bacteria, based upon numerous studies using nonuniform techniques, are presented in Table IX (adapted from  Bryan, 1977) .\n\nAlthough the terms, \"infective dose,\" \"minimal infectious dose,\" etc., are used in the literature, it is obvious from Table IX that these are misnomers, and that we are really dealing with dose-response relationships, where the dose is the number of cells to which the human is exposed, and the response is lack of infection, infections without illness, and infection with ill ness (in an increasing proportion of the test subjects). The response is affected by many factors, making it highly variable. Some of the most important factors are briefly discussed below.\n\n(1)\n\nThe site of exposure determines what types of defense mechanisms are available, e.g., alveolar macrophages and leuko cytes in the lungs, and acidity and digestive enzymes in the stomach.\n\nThe effect of acidity is clearly shown by the cholera (Vibrio cholerae) data in Table IX, \n\nfound 10 tuleremia organisms injected to be comparable to 10^ by mouth.\n\n(2) Previous exposure to a given pathogen often produces varying degrees of immunity to that pathogen, through the induction of specific immune responses.\n\nA study in Bangladesh showed that repeated ingestion of small inocula (\u0399\u039f^-\u0399\u039f 4 orga nisms) of Vibrio cholerae produced subclinical or mild diarrheal infection followed by specific antibody production.\n\nFor this reason the peak incidence of endemic cholera occurs in the one to four-year-old age group, and decreases with age thereafter as immunity developed (Levine, 1980).\n\nOther host factors, such as age and general health, also affect the disease response.\n\nInfants, elderly persons, malnourished people, those with concomitant illness, and people taking antiinflammatory, cytotoxic, and immunosuppressant drugs would be more susceptable to pathogens.\n\nAn example of human variability (possibly genetic) is the following response of men orally challenged with several different doses of Salmonella typhi (Hornick et al., 1970):\n\nTwenty-eight percent of the men came down with typhoid fever after 10^ organisms, while 5% were still resistant to 10^ organisms, four orders of magnitude as many.\n\nThe timing of the exposure to pathogens, e.g., as a single exposure or over a long period of time, would be expected to affect the response. Table IX , the virulence, or pathogenicity, of bacteria varies among strains.\n\nThus, three different strains of Shigella flexneri have been found to have infective doses of 10 10 or higher, 10 5 to 10 8 , and 180 organisms (NRC, 1977).\n\nand Shigella spp. , bacause they are the most common bacterial pathogens in municipal wastewater. The infective dose for Salmonella is high, between 10^ and 10 8 organisms, but this dose might be reached on a contaminated foodstuff under con ditions that allow multiplication. On the other hand the infective dose for Shigella is low -as few as 10 to 100 organisms.\n\n\"Because of this miniscule inoculum it is rather simple for shigellae to spread by contact without interposition of a vehicle such as food, water or milk to amplify the infectious dose\" (Keusch, 1979).\n\nConsequently, it would be prudent for humans to maintain a minimum amount of contact with an active land treatment site, and to rely on \"time\" to reduce the bacterial survival, as discussed earlier, when growing crops for human consumption. Salmonellosis has been traced to the consumption of wastewater-irrigated celery, watercress, watermelon, lettuce, cabbage, endive, salad vegetables, and fruits; shigellosis to wastewater-irrigated pastureland; and cholera to wastewaterirr igated vegetables in Israel.\n\nThese data support the view that untreated wastewater should never be used for irrigation.\n\nTransmission of viruses by feces is the second most frequent means of spread of common viral infections, the first being the respiratory route.\n\nTransmission by urine has not been established as being of epidemiological or clinical importance, although some viruses, e.g., cytomegalovirus and measles, are excreted through this route.\n\nThe gastrointestinal tract is an important portal of entry of viruses into the body, again second to the respiratory tract (Evans, 1976).\n\nThe human enteric viruses that may be present in wastewater are listed in Table X Polioviruses cause poliomyelitis, an acute disease which may consist simply of fever, or progress to aseptic meningitis or flaccid paralysis (slight muscle weakness to complete para lysis caused by destruction of motor neurons in the spinal cord).\n\nPolio is rare in the United States, but may be fairly common in unimmunized populations in the rest of the world.\n\nNo reliable evidence of spread by wastewater exists (Benenson, 1975) .\n\nCoxsackieviruses may cause aseptic meningitis, herpangina, epidemic myalgia, myocarditis, pericarditis, pneumonia, rashes, common colds, congenital heart anomalies, fever, hepa titis, and infantile diarrhea.\n\nEchoviruses may cause aseptic meningitis, paralysis, encephalitis, fever, rashes, common colds, epidemic myalgia, pericarditis, myocarditis, and diar rhea.\n\nThe new enteroviruses may cause pneumonia, bronchiolitis, acute hemorrhagic conjunctivitis, aseptic meningitis, encepha litis, and hand-foot-and-mouth disease.\n\nThe prevalence of the diseases caused by the coxsackieviruses, echoviruses, and new enteroviruses is poorly known, but 7075 cases were reported to CDC in the years 1971-75 (Morens et al., 1979) . These enteroviruses are practically ubiquitous in the world, and may spread rapidly in silent (asymptomatic) or overt epidemics, especially in late summer and early fall in temperate regions. Because of their antigenic inexperience, children are the major target of enterovirus infections, and serve as the main vehicle for their spread.\n\nMost of these infections are asymptomatic, and natural immunity is acquired with increasing age. The poorer the sanitary conditions, the more rapidly immunity develops, so that 90% of children living under poor hygienic circumstances may be immune to the prevailing enteroviruses (of the approximately 70 types known) by the age of 5.\n\nAs sanitary conditions improve, the proportion of nonimmunized in the population increases, and infection becomes more common in older age groups, where symptomatic disease is more likely and is more serious (Melnick et al., 1979; Benenson, 1975 ). Thus, de creasing low level human exposure to the common enteric viruses through the water and food route has its disadvantages, as well as advantages.\n\nto fulminating hepatitis with jaundice.\n\nRecovery with no sequelae is normal.\n\nApproximately 40,000 to 50,000 cases are reported annually in the United States, and about half the population has antibodies to hepatitis A virus.\n\nThe epidemiological pattern is similar to that of enteroviruses, with childhood infection common and asymptomatic (Duboise et al., 1979). It may be that reovirus infection in humans is common, but associated with either mild or no clinical manifestations (Rosen, 1979).\n\nPapovaviruses have been found in urine, and may be asso ciated with progressive multifocal leukoencephalopathy (PML), but are poorly understood (Warren, 1979). They found no significant correlation between the presence of virus in groundwater and levels of bacterial indicators, i.e., total bacteria, fecal coliforms, and fecal streptococci.\n\nIt appears, therefore, that estimates of virus presence or levels in the environment will have to be made on the basis of measurements of viral indicators, e.g., vaccine poliovirus or bacteriophage, or of the viral pathogens themselves, e.g., coxsackievirus or echovirus, rather than of bacterial indicators.\n\nThe concentration in the feces of an infected person has not been widely studied.\n\nHowever, from the available data it has been estimated (Feachem et al. , 1978) to be about 10 6 /gm.\n\nNumbers tend to be higher in late summer and early fall than other times of the year because of the increase in enteric viral infections at this time, except for vaccine polioviruses, whose concentration tends to remain constant.\n\nThe concentrations reported in the literature may be as little as one-tenth to one-hundredth of the actual concen trations because of the limitations of virus recovery procedures and the use of inefficient cell-culture detection methods (Akin  et al., 1978) .\n\n(The use of several cell lines usually detects more viral types than a single cell line does, and many viruses cannot yet be detected by cell-culture methods, e.g., hepatitis A virus and Norwalk-like viruses.) Some representative levels of enteric viruses in raw wastewaters of the United States are summarized in Table XI .\n\nIt is evident that reported concen trations are highly variable; Akin and Hoff (1978) have con cluded that \"...from the reports that are available from field studies and with reasonable allowances for the known variables, it would seem extremely unlikely that the total concentration would ever exceed 10,000 virus units/liter of raw sewage and would most often contain less than 1000 virus units/liter.\" 1973 ) because of the difficulty in maintaining the more viri cidal form, free chlorine under acidic conditions. Although very high doses of chlorine will destroy viruses in wastewater, cost, production of carcinogens, and toxicity make this imprac tical . et al. (1978) .\n\nSedimentation, or conventional primary treatment, results in low rates of removal, most of which is associated with the settling of solids in which the viruses are embedded or on which they are adsorbed (Lance and Gerba, 1978).\n\nRemoval rates of up to 90% have been reported (Melnick et al. , 1978) , with 10% or less being more common (Sproul, 1978; Crites and Uiga, 1979).\n\nSome representative survival data is summarized in , 1978) . The data suggest that long retention times, of the order of 50 days, particularly in combination with ponds in series, might accomplish quite significant virus removals.\n\nAerosols have been of concern as a potential route of transmission of disease caused by enteric viruses because, as with bacteria, once they are inhaled they may be carried from the respiratory tract by cilia into the oropharynx, and then swallowed into the gastrointestinal tract.\n\nSome enteroviruses may also multiply in the respiratory tract itself (Evans, 1976). Another reason for concern is the theoretically possible trans mission of respiratory viruses through wastewater aerosols.\n\nOn the basis of actual viral sampling of wastewater, however, Johnson et al. (1980) concluded that the likelihood of finding respirable viruses in treated wastewater is very small.\n\nThe subsequent die-off, estimated to be about one log every 40 sec (Sorber, 1976), is determined primarily by solar radiation, temperature, and relative humidity (Lance and Gerba, 1978).\n\nThe effect of relative humidity appears to depend upon the lipid content of viruses, lipid-containing viruses surviving better at low humidities, and those without lipids (e.g., most of the enteric viruses) surviving better at high humidities (Carnow et al., 1979) . Sorber (1976) has estimated that, under the least desirable meteorological conditions studied, less than The results of these two studies are summarized in Table  XIII. The results obtained from these two studies are highly variable, but it appears reasonable to make use of the Pleasanton aerosol virus density, i.e., 0.014/m 3 , to make human exposure estimates, since (1) the Pleasanton wastewater virus level is similar to that in United States wastewaters in general (cf . Table XI From these data it can be concluded that an adult male, engaged in light work, breathing at a rate of 1.2 m 3 /hr, and exposed to 0.014 PFU/m 3 at 50 m downwind from a sprayer, would inhale approximately 0.13 PFU of enterovirus during an 8 hr work day. This is probably an insignificant level of exposure. However, the recovery of enteric viruses from environmental samples is not perfectly efficient.\n\nSince isolation of viruses increases as more cell culture types are used, and some enteric viruses cannot yet be isolated on cell cultures, the actual exposure to enteric viruses may be as much as ten to a hundred times the reported level (Teltsch et al. , 1980) . Thus, it might be prudent to recommend a 100 m or 200 m minimum exposure distance of the general public to a land treatment spray source.\n\nAs is the case with bacteria, the surface soil and plants of an active land treatment site are constantly receiving enteric viruses, and the survival time of viruses is primarily of concern when decisions must be made on how long a period of time must be allowed after last application before permitting access to people or animals, or harvesting crops.\n\nThe factors affecting virus survival in soil are solar radiation, moisture, temperature, pH, and adsorption to soil particles.\n\nThe soil microorganisms appear to have a less important effect on virus degradation.\n\nAlthough it is often believed that adsorption to inorganic surfaces prolongs the survival of viruses, there is some evidence that adsorption may\n\ntemperatures decrease survival time (Sagik et al., 1978) .\n\nThe soil is a complex medium, however, with fluctuations in soil moisture, temperatures, pH, ionic strength, dissolved gas concentrations, nutrient concentra tions, etc., which may be caused by meteorological changes, by the action of other soil organisms, or by the activities of metazoans including humans (Duboise et al., 1979) , and under standing of the behavior of viruses in soil will be slow developing.\n\nforces are minimal.\n\nThe persistance of virus particles that survive surface forces and enter the soil matrix is not well studied. Table XIV. Although dose dependent, approximately 100 days appears to be the maximum survival time of enteric viruses in soil, unless subject to very low temperatures, which prolong survival beyond this time. Exposure to sunlight, high temperatures, and drying greatly reduce survival times.\n\nThus, Yeager and O'Brien (1979) could recover no infectivity of poliovirus and coxsackievirus from dried soil regardless of temperature, soil type, or type of liquid amendment, and suggested that the main effect of temper ature on virus survival in the field may be its influence on evaporation rates.\n\nThey suggest that enterovirus contamination of soil, and possible migration to underlying groundwater, might be reduced or eliminated by allowing the soil to dry between wastewater applications.\n\n(1977) , who observed a decrease in poliovirus titer of greater than three orders of magnitude when the solids content of sludge was increased from 65% to 83%. This loss of infectivity was due to irreversible inactivation of poliovirus because viral particles were found to have released their RNA molecules which were extensively degraded.\n\npathogens. The intact surfaces of vegetables are probably impenetrable for enteroviruses (Bagdasaryan, 1964) .\n\nOn the surface of aerial crops virus survival would be expected to be shorter than in soil because of the exposure to deleterious environmental effects, especially sunlight, high temperature, drying, and washing off by rainfall (USEPA, 1977) . Table  XV (Feachem et al., 1978) .\n\nThe data are similar to those for bacteria (cf . Table VIII) , and likewise appear to support a one month waiting period after last wastewater application before harvest. Table XVI '(modified from National Research Council, 1977) . The results are highly variable, and may reflect differences in experimental conditions as well as states of the hosts.\n\nThe recent data does suggest, however, that the infective dose of enteroviruses to man is low, possibly on the order of 10 virus particles or less.\n\nThe same factors, discussed earlier, that affect bacteria affect the virus dose-response relationship. If this were to be the case, extreme care should be taken to avoid human exposure to enteric viruses through aerosols or crops grown on land treat ment sites.\n\nOn the other hand, the concept that a single virus particle often constitutes an infective dose in the real world has been argued against (Lennette, 1976) on the basis of the oral poliovaccine studies, nonimmunologic barriers, human immuno logic responses, and probabilistic factors.\n\nViruses do not regrow on foods or other environmental media, as bacteria sometimes do. Therefore, the risk of infection is completely dependent upon being exposed to an infective dose (which may be very low) in the material applied. In any event, as is the case with bacteria, it would seem prudent for human beings to maintain a minimum amount of contact with an active land treatment site, and to rely on the viral survival data discussed earlier for limiting the hazard from crops grown for human consumption on wastewater-amended soils.\n\nFecally-polluted vegetable-garden irrigation water in Bra zil has been found to contain polioviruses and coxsackieviruses, and has been associated with earlier epidemics (Christovao et al., 1967a; 1967b), but current epidemiological techniques are probably not sufficiently sensitive to detect the low level of viral disease transmission that might occur from a modern land treatment site (Melnick, 1978; WHO 1979) .\n\nThe protozoa and helminths (or worms) are often grouped together under the term, \"parasites,\" although in reality all the pathogens are biologically parasites.\n\nBecause of the large size of protozoan cysts and helminth eggs, compared with bacteria and viruses, it is extremely unlikely that they will find their way into either aerosols or groundwater at irrigation sites.\n\nHowever, because of the increasing recognition of parasitic infections in the United States, the return of military personnel and trav elers from abroad, the level of recent immigration and food imports from countries with a high parasitic disease prevalence, and the existance of resistant stages of the organisms, a consideration of parasites is warranted.\n\nThe most common protozoa which may be found in wastewater are listed in Table XVII. Of these, only three species are of major significance for transmission of disease to humans through wastewater:\n\nEntamoeba histolytica, Giardia lamblia, and Balantidium coli.\n\nToxoplasma gondii also causes significant human disease, but the wastewater route is probably not of importance. Eimeria spp. are often identified in human fecal samples, but are considered to be spurious parasites, entering the gastro intestinal tract from ingested fish.\n\nEntamoeba histolytica causes amebiasis, or amebic dysen tery, an acute enteritis, whose symptoms may range from mild abdominal discomfort with diarrhea to fulminating dysentery with fever, chills, and bloody or mucoid diarrhea.\n\nMost infections are asymptomatic, but in severe cases dissemination may occur, producing liver, lung, or brain abscesses, and death may result. Amebiasis is rare in the United States (Krogstad et al., 1978) , and is transmitted by cysts contaminating water or food.\n\nThe carrier rate in different areas of the United States may range between 1.5 and 20% (Benenson, 1975) , and it is transmitted by cysts contami nating water or food, and by person-to-person contact.\n\nBalantidium coli causes balantidiasis, a disease of the colon, characterized by diarrhea or dysentery.\n\nInfections are often asymptomatic, and the incidence of disease in man is very low (Benenson, 1975) .\n\nBalantidiasis is transmitted by cysts contaminating water, particularly from swine. rH rd rrj Ou \n\nApproximately 50% of the population of the United States is thought to be infected (Krick and Remington, 1978), but the infection is probably transmitted by oocysts in cat feces or the consumption of cyst-contaminated, inadequately-cooked meat of infected animals (Teutsch et al. 1979), rather than through wastewater.\n\nThe trophozoites, after a period of reproduction, may round up to form precysts, which secrete tough membranes to become environmentally-resis tant cysts, in which form they are excreted in the feces (Brown, 1969) .\n\nThe number of cysts excreted by a carrier of Entamoeba histolytica has been estimated to be 1.5xl0 7 /day (Chang and Kabler, 1956), and by an adult infected with Giardia lamblia at 2.1-7.Ixl0 8 /day (Jakubowski and Ericksen, 1979). The concen tration of Entamoeba histolytica cysts in the feces of infected individuals has been estimated to be 1.5xl0 5 /gm (Feachem et al.,  1978) . et al., 1978) , up to 2.2xl0 6 /gm in infected children, and up to 9.6xl0 7 /gm in asymptomatic adult carriers (Akin et al., 1978) .\n\nThe types and levels of protozoan cysts actually present in wastewater depend on the levels of disease in the contributing human population, and the degree of animal contribution to the system. Some estimates are presented in Table XVIII .\n\nEntamoeba histolytica and Giardia are very chlorine resis tant. Entamoeba being one of the most chlorine-resistant patho gens known (Hoff, 1979) . Sedimentation, or conventional primary treatment, appears to result in poor removals of protozoan cysts from wastewater, as indicated by data on Entamoeba histolytica. Thus, Cram (1943) reported lack of removal, Foster and Engelbrecht (1973) 15% removal, Sproul (1978) lack of to incomplete removal, and Crites and Uiga (1979) 10 to 50% removal. These authors reported very poor secondary treatment removals as well.\n\nWastewater stabilization ponds may accomplish much better removals of protozoan cysts.\n\nThus, 100% reduction of protozoan cysts from the effluent was accomplished by a series of 3 ponds, with a 7-day retention time, in India (Arceivala et al., 1970) , \n\nEntamoeba cysts have been found to survive several months in water at 0\u00b0C, 3 days at 30\u00b0C, 30 min at 45\u00b0C, and 5 min at 50\u00b0C (Freeman, 1979) .\n\nGiardia cysts can survive up to about 77 days in water at 8\u00b0C, 5 to 24 days at 21\u00b0C, and 4 days or less at 37\u00b0C (Bingham et al., 1979) .\n\nProtozoan cysts are highly sensitive to drying. Rudolfs et al.\n\n(1951b) has reported survival times for Entamoeba histo lytica of 18 to 24 hr in dry soil and 42 to 72 hr in moist soil. Somewhat longer times, i.e., 8 to 10 days, have been reported by Beaver and Deschamps (1949) in damp loam and sand at 28 to 34\u00b0C.\n\nBecause of their exposure to the air, protozoan cysts deposited on plant surfaces would also be expected to die off rapidly.\n\nThus, Rudolfs et al.\n\nfound contaminated tomatoes and lettuce to be free from viable Entamoeba cysts within 3 days, and the survival rate to be unaffected by the presence of organic matter in the form of fecal suspensions. They concluded that field-grown crops \"...consumed raw and subject to contamination with cysts of E. histolytica are considered safe in the temperate zone one week after contami nation has stopped and after two weeks in wetter tropical regions.\" Therefore, if the recommendations, based on bacteria, for harvesting human food crops are followed, it is extremely unlikely than any public health risk will ensue.\n\nHuman infections with Giardia lamblia and the nonpathogenic Entamoeba coli have been produced with ten cysts administered in a gelatin capsule (Rendtorff, 1954a; 1954b) . Infections have been produced with single cysts of Entamoeba coli, and there is no biological reason why single cysts of Giardia would not also be infectious (Rendtorff, 1979) . This is probably true for E. histolytica as well (Beaver et al., 1956). The pathogenicity of protozoa is highly variable among strains, and human responses likewise are variable.\n\nThus, many infections are asymptomatic. \n\ncontact with an active land treatment site. However, if the recommended waiting periods for crop harvest are followed, the risk of infection should be minimal, because of the cysts' sensitivity to drying.\n\nA few epidemiological reports have linked the transmission of amebiasis to vegetables irrigated with raw wastewater or fertilized with night soil (Bryan, 1977; Geldreich and Bordner, 1971 ).\n\nThe pathogenic helminths whose eggs are of major concern in wastewater are listed in Table XIX. They are taxonomically divided into the nematodes, or roundworms, and cestodes, or tapeworms.\n\nThe trematodes, or flukes, are not included since they require aquatic conditions and intermediate hosts, usually snails, to complete their life cycles, and thus are unlikely to be of concern at land treatment sites.\n\nSome common helminths, pathogenic to domestic or wild animals, but not to humans, are listed in Table XX (after Reimers et al., 1980) , since their eggs are likely to be identified in wastewater.\n\nSeveral of the human pathogens listed in Table XIX , e.g., Toxocara spp., are actually animal parasites, rather than human parasites, infesting man only incidentally, and not completing their life cycle in man.\n\nEnterobius vermicular is, the pinworm, causes itching and discomfort in the perianal area, particularly at night when the female lays her eggs on the skin.\n\nA 1972 estimate of the prevalence of pinworm infections in the United States was 42 million (Warren, 1974).\n\nAlthough it is by far the most common helminth infection, the eggs are not usually found in feces, are spread by direct transfer, and live for only a few days.\n\nAscaris lumbricoides, the large roundworm, produces nu merous eggs, which require 1 to 3 weeks for embryonation.\n\nAfter the embryonated eggs are ingested, they hatch in the intestine, enter the intestinal wall, migrate through the circulatory system to the lungs, enter the alveoli, and migrate up to the pharynx.\n\nDuring their passage through the lungs they may produce ascaris pneumonitis, or Loeffler's syndrome, consisting of coughing, chest pain, shortness of breath, fever, and eosinophilia, which can be especially severe in children.\n\nThe larval worms are then swallowed, to complete their maturation in Death due to migration of adult worms into the liver, gallbladder, peritoneal cavity, or appendix occurs infre quently.\n\nThe prevalence of ascariasis in the United States was estimated to be about 4 million in 1972 (Warren, 1974).\n\nAscaris suum, the swine roundworm, may produce Loeffler's syndrome, but probably does not complete its life cycle in man (Phills et al., 1972) .\n\nTrichuris trichiura, the human whipworm, lives in the large intestine with the anterior portion of its body threaded superficially through the mucosa. Eggs are passed in the feces, and develop to the infective stage after about 4 weeks in the soil (Reimers, et al. 1980) , and direct infections of the cecum and proximal colon result from the ingestion of infective eggs. Light infections are often asymptomatic, but heavy infections may cause intermittent abdominal pain, bloody stools, diarrhea, anemia, loss of weight, or rectal prolapse in very heavy infections.\n\nHuman infections with T. suis, the swine whipworm, and T. vulpis, the dog whipworm, have been reported, but are uncommon (Reimers et al., 1980) . The prevalence of trichuriasis in the United States was estimated to be about 2.2 million in 1972 (Warren, 1974) . Reimers et al. (1980) have found Ascaris, Trichuris, and Toxocara to be the most frequently recovered helminth eggs in municipal wastewater sludge in southeastern United States.\n\nNecator americanus and Ancylostoma duodenale, the human hookworms, live in the small intestine attached to the intes tinal wall.\n\nEggs are passed in the feces, and develop to the infective stage in 7 to 10 days in warm, moist soil. Larvae penetrate bare skin, usually of the foot (although Ancylostoma may also be acquired by the oral route) , pass through the lymphatics and blood stream to the lungs, enter the alveoli, migrate up the pharynx, are swallowed, and reach the small intestine.\n\nDuring lung migration, a pneumonitis, similar to that produced by Ascaris, may occur (Benenson, 1975) .\n\nLight infections usually result in few clinical effects, but heavy infections may result in iron-deficiency anemia (because of the secreted anticoagulant causing bleeding at the site of attach ment) and debility, especially children and pregnant women. The prevalence of hookworm in the United States (usually due to Necator) was estimated to be about 700,000 in 1972 (Warren, 1974) .\n\nLarvae from eggs in cat and dog feces penetrate bare skin, particularly feet and legs on beaches, and burrow aimlessly intracutaneously, producing \"cutaneous larva migrans\" or \"creeping eruption.\" After several weeks or months the larva dies without completing its life cycle.\n\nStrongyloides stercoralis, the threadworm, lives in the mucosa of the upper small intestine.\n\nEggs hatch within the intestine, and reinfection may occur, but usually noninfective larvae pass out in the feces.\n\nThe larva in the soil may develop into an infective stage or a free-living adult, which can produce infective larvae.\n\nThe infective larvae penetrate the skin, usually of the foot, and complete their life cycle similarly to hookworms.\n\nIntestinal symptoms include abdominal pain, nausea, weight loss, vomiting, diarrhea, weakness, and constipation.\n\nMassive infection and autoinfection may lead to wasting and death in patients receiving immunosuppressive medi cation (Benenson, 1975) .\n\nThe prevalence of strongyloidiasis in the United States was estimated to be about 400,000 in 1972 (Warren, 1974) .\n\nDog feces is another source of threadworm larvae.\n\nToxocara canis and T. cati, the dog and cat roundworms, do not live in the human intestinal tract.\n\nWhen eggs from animal feces are ingested by man, particularly children, the larvae hatch in the intestine and enter the intestinal wall, similarly to Ascaris.\n\nHowever, since Toxocara cannot complete its life cycle, the larvae do not migrate to the pharynx, but, instead, wander aimlessly through the tissues, producing \"visceral larva migrans,\" until they die in several months to a year. The disease may cause fever, appetite loss, cough, asthmatic epi sodes, abdominal discomfort, muscle aches, or neurological symp toms, and may be particularly serious if the liver, lungs, eyes (often resulting in blindness), brain, heart, or kidneys become involved (Fiennes, 1978) . The infection rate of T. canis is more than 50% in puppies and about 20% in older dogs in the United States (Gunby, 1979), and Toxocara is one of the most common helminth eggs in wastewater sludge (Reimers et al., 1980) . Taenia saginata and T. solium, the beef and pork tapeworms, live in the intestinal tract, where they may cause nervousness, insomnia, anorexia, loss of weight, abdominal pain, and diges tive disturbances, or be asymptomatic.\n\nThe infection arises from eating incompletely cooked meat (of the intermediate host) containing the larval stage of the tapeworm, the cysticercus, however, rather than from a wastewater-contaminated material. Man serves as the definitive host, harboring the self-fertile adult.\n\nThe eggs (contained in proglottids) are passed in the feces, ingested by cattle and pigs (the intermediate hosts), hatch, and the larvae migrate into tissues, where they develop to the cysticercus stage.\n\nThe hazard, then, is principally to livestock grazing on land-treatment sites.\n\nThe major direct hazard to man is the possibility of him acting as the inter mediate host.\n\nWhile Taenia saginata eggs are not infective for man, those of T. solium are infective for man, in which they can produce cysticerci.\n\nCysticercosis can present serious symptoms when the larvae localize in the ear, eye, central nervous system, or heart. Taeniasis with Taenia solium is rare in the United States, and with T. saginata is only occasionally found. However, human infections with these tapeworms are fairly common in some other areas of the world. Eggs in animal feces are usually ingested by an herbivore, in which they hatch into larval forms, which migrate into tissues, where they develop into hydatid cysts.\n\nWhen the herbivore is eaten by a carnivore the cysts develop into adult tapeworms in the carni vore's intestinal tract.\n\nIf man ingests an egg, he can play the role of the herbivore, just as in cysticercosis.\n\nA hydatid cyst can develop in the liver, lungs, or other organs, where serious symptoms can be produced as the cyst grows in size or ruptures. The disease is rare in the United States, but has been reported from the western states, Alaska, and Canada, particularly where dogs are used to herd grazing animals, and where dogs are fed animal offal. \n\nSince helminth eggs are denser than water, ordinary sedi mentation, or conventional primary treatment, is a fairly efficient method of removal. German sanitary engineers have found 1 to 2 hr of sedimentation detention time to be sufficient to remove most helminth eggs (Sepp, 1971). Newton et al. (1949) showed 98% removal of Taenia saginata eggs by 2 hr sedimentation in the laboratory, but lower removals under field conditions. Conventional secondary treatment, i.e., activated sludge or trickling filter, results in very poor helminth egg removal rates (Sproul, 1978) .\n\nWastewater stabilization ponds accomplish excellent de grees of helminth egg removal, as indicated in Table XXI (after  Feachem et It should be kept in mind that the sludge or pond sediment resulting from these processes will have high densities of viable helminth eggs, and will require proper treatment before utilization.\n\nHelminth eggs and larvae, in contrast to protozoan cysts, live for long periods of time when applied to the land, probably because soil is the transmission medium for which they have evolved, while protozoa have evolved toward water transmission. Because of the growth of crops and presence of people at irrigation sites, and the longevity of helminth eggs, it would be advisable to select a preapplication treatment method, e.g., stabilization ponds, which will completely remove helminth eggs at these land treatment sites. \n\nSingle eggs of helminths are infectious to man, although, since the symptoms of helminth infections are dose-related, many light infections are asymptomatic.\n\nHowever, Ascaris infection may sensitize individuals so that the passage of a single larval stage through the lungs may result in allergic symptoms, i.e., asthma and urticaria (Muller, 1953) .\n\nBecause of the low infective doses of helminth eggs, and their longevity, it would be prudent for humans to maintain a minimum amount of contact with an active or inactive land treatment site, unless the wastewater has been pretreated to remove helminths.\n\nA few epidemiological reports have linked the transmission of Ascaris and hookworm to the use of night soil on gardens and small farms in Europe and the Orient (Geldreich and Bordner, 1971).\n\nThe types and levels in wastewater of most pathogens are fairly well understood.\n\nRecent reviews of the subject include those by Benarde ( Since only onetenth to one-hundredth of the total viruses in wastewater and other environmental samples may actually be detected, the development of methods to recover and detect viruses continues to be a research need.\n\nThe occurrence of virus in an environ mental setting should probably be based on viral tests rather than bacterial indicators since failures in this indicator system have been reported.\n\nIn 1978, the U.S. General Accounting Office (1978) found that concern over adverse health effects has been a major obstacle to the development of land treatment of municipal wastewater, resulting in the establishment of state pretreatment requirements which may be overly stringent.\n\nThese requirements often make land treatment expensive compared to conventional treatment and surface-water discharge. This review suggests that the level of preapplication treatment required for the protection of public health may be as little as properly-designed sedimentation at land treatment sites with limited public access, where crops are protected by appropriate crop choice and waiting periods. Because of potential contamination of crops and infection of animals, slowrate irrigation systems should have complete removal of helminth eggs.\n\nThese relatively simple pretreatment requirements would be appropriate for many land treatment systems.\n\nThe recommendations made in the paragraphs below assume only a minimum level of preapplication treatment, i.e., pro perly-designed sedimentation.\n\nIn situations with greater public access (e.g., water disposal on golf courses) or shorter waiting periods before grazing or harvest of crops (e.g., agriculture in arid areas), more extensive preapplication treatment will be required.\n\nThis treatment may consist of wastewater stabili zation ponds, conventional treatment unit processes, or even disinfection.\n\nThe exact degree of pretreatment required for these situations is site-specific, and recommendations should be determined separately for each system (Lance and Gerba, 1978).\n\nBecause of the potential exposure to aerosolized viruses at land treatment sites, it would be prudent to limit public access to between 100 and 200 m from a spray source.\n\nAt this distance bacteria are also unlikely to pose a significant risk. Human exposure to pathogenic protozoa or helminth eggs through aero sols is extremely unlikely. Suppression of aerosol formation by the use of downwarddirected, low-pressure nozzles, ridge-and-furrow irrigation, or drip irrigation is recommended where these application tech niques are feasible.\n\nThe survival times of pathogens on soil and plants are summarized in Table Aerial crops with little chance for contact with soil should not be harvested for human consumption for at least one month after the last wastewater application; subsurface and lowgrowing crops for human consumption should not be grown at a land treatment site for at least six months after last application. These waiting periods need not apply to the growth of crops for animal feed, however.\n\nBecause of the possibility of picking up an infection, it would be wise for humans to maintain a minimum amount of contact with an active land treatment site.\n\nUntreated wastewater should never be used for irrigation.\n\nThe comparison of the respiratory infective dose of enteric viruses with the oral infective dose is a significant research need.\n\nPerhaps the largest epidemiological study of the health effects of land treatment was a retrospective study of 77 kibbutzim (agricultural cooperative settlements) in Israel practicing slow-rate land treatment with nondisinfected oxi dation pond effluent, and 130 control kibbutzim (Katzenelson et  al., 1976) .\n\nThe incidence of typhoid fever, salmonellosis, shigellosis, and infectious hepatitis was 2 to 4 times higher in the land-treatment kibbutzim than the controls.\n\nThe study, however, did not rule out a number of pathways of infection other than aerosols, e.g., direct contact via clothing or bodies of sewage irrigation workers, and there were problems with the data reporting methods.\n\nConsequently, it is generally felt that no Other epidemiological reports on the health effects of land treatment have been more superficial.\n\nExamination of the workers on sewer farms in Berlin and Memmingen in Germany has not shown them to have a higher rate of infectious diseases or worm infestation than the rest of the population (Sepp, 1971) . At land treatment sites near Paris, grain for cattle, beef cattle, and vegetables (e.g., beans, onions, and celeriac) are raised (Dean, 1978) .\n\nThe vegetables are checked for Salmonella, with none having been found, and no disease has been traced to the farms.\n\nDuring a cholera outbreak, no cholera bacteria were found on the vegetables.\n\nAt Werribee Farm in Melbourne, Aus tralia, there has never been a reported epidemic or outbreak of disease among employees or residents, although no precautions other then normal hygiene practices have been taken, and the general health of employees and residents is no different from that of the community in general (Croxford, 1978) .\n\nAlthough these retrospective studies are reassuring, a better measure of the health effects of land treatment will come from well-planned prospective epidemiological studies. Two such studies are currently underway, one at Lubbock, Texas, and another in Israel.\n\nThe results of these two projects may well modify the conclusions and recommendations of this paper in the future.\n\nAppreciation is extended to Lon Winchester for invaluable secretarial assistance. wastewater and their potential health hazards with regard"}