{"title": "Synonym set extraction from the biomedical literature by lexical pattern discovery", "body": "For our experiment we decided to use an existing ontology to provide a training set from which our system could develop patterns and then train a classifier. Our training set was drawn from the English section of the BioCaster ontology [15]. This ontology was put together by life scientists and its terms relate to some of the most important infectious diseases currently being tracked by epidemiologists. BioCaster was developed for the search and analysis of Internet news and research literature to enable the timely detection of possible disease outbreaks. As such it contains a range of lay terms and technical terms, and we choose 4 categories of terminology from the ontology: infectious diseases, symptoms (of infectious diseases), agents (e.g. viruses, bacteria) and hosts (e.g. mammals, birds). This was useful as it gave us an accurate basis of terminology relating to a specific biomedical domain.\n\nTo develop a test set we wished to find terms that occurred naturally and so our evaluation would represent the effectiveness of the method in actual implementations. We obtained the top 150 PubMed abstracts containing the term \"infectious diseases\", and manually extracted all the terms in the same four categories. We then grouped these words into synsets attempting to follow the same guidelines as used by the BioCaster ontology. Synonymy can be quite difficult to decide, with granularity being a major problem, in fact even established thesauri can differ in their definitions of synonymy as investigated by Burgun & Bodenreider [16]. We found the following problems frequently occurred.\n\n1. Modifiers: Terms often appear with modifiers, such as \"acute headache,\" and although many ontologies would list this as a property of the disease or symptom we decided to list terms separately. It is important to be careful not to confuse this with modifiers which do not change the meaning of the term, for example \"mental retardation.\" More information on the effect of modifiers can be found in Bodenreider et al [17].\n\n2. Granularity: Many terms may turn out to be used interchangeably as the difference in meaning is rarely important. An example of this is \"HIV-1,\" which as the most common strain of HIV is often simply referred to as \"HIV,\" however as these subtle differences in meaning may prove to be important we decided to separate such terms\n\n3. Domain-specific:\n\n(a) Method of Transmission: We decided to list terms differing in method of transmission separately, for example \"hospital-acquired MRSA.\"\n\n(b) Point of Infection: Terms that differ in only the point of infection were decided not to be synonymous, for example \"syphilis\" and \"neurosyphilis.\"\n\n(c) Progression of Disease: It was decided that terms used to describe a disease at a more advanced stage were not synonymous with the disease, for example \"dengue fever\" may develop into the more serious \"dengue shock syndrome.\"\n\nIn total the BioCaster database consisted of 450 terms, grouped into 244 synsets, and containing 477 synonym pairs, similarly the test set consisted of 301 terms, 221 synsets and 101 synonym pairs, of which it shared 35 terms and 16 synonym pairs with the BioCaster database. We also had a second annotator group our 301 test terms into synonym groups and this list showed \u03ba = 68.6% Cohen's agreement with our list, which illustrates that this problem is complicated by very subtle differences in meaning between our terms. We then extracted a large corpus from which we can build patterns and statistics. We simply did this by querying every term in both our training and test sets in the PubMed database. We chose the top 250 abstracts for each term ranked according to Entrez search's ranking, discarding any duplicate documents. This gave us a corpus consisting of 83,492 documents, and 1,506,042 sentences of which term pairs in our training set were found in 46,216 sentences.\n\nOur method for pattern generation is as follows we start with base patterns which consist of three basic operators *, #, and (space), where * represents a sequence of word characters, (space) represents any white-space/punctuation between words, and # represents a term (matched case-invariantly). We generate new patterns by expanding a current pattern in the following ways: * may be replaced by any sequence of alphanumeric characters and (space) may be replaced by any sequence of non-word characters. By starting with a set of seed patterns consisting of only #s, *s and (space)s these patterns can be expanded to give us a search as in Figure 1. To expand a pattern we search the corpus by taking each synonym pair and replacing the #s with the terms. By considering each possible single expansion of a * or (space) our problem can be viewed as a tree search. Obviously this search space is huge and so we search it heuristically by best-first search. The algorithm is as follows:\n\nInput:A set of base patterns P, a set of training term pairs S={(t1,t2),...(tn,t\u2032n)}, a corpus C.\n\nOutput:A set of patterns P sorted by a scoring metric\n\n1. Add all the base patterns with a heuristic score to a heap H.\n\n2. For a fixed number of iterations\n\n(a) Select a pattern, p, with maximal score in H.\n\n(b) Find all matches of this pattern using all term pairs in S in the corpus C\n\n(c) For each * or (space) in p, find all matching strings in the corpus\n\n(d) For each match to each * or (space), add a new pattern to H\n\n3. Output patterns H sorted by score\n\nWe experimented with a number of heuristics scoring metrics, including the number of pattern matches, however we found that this gave too strong a bias for terms which occurred very frequently, in particular we noted that many of the patterns contained terms from our training set, which is quite undesirable.\n\nh1=#sentences matching pattern#sentences in corpus\n\nWe also report a pseudo-F-Measure given as below, where we judged a pattern match to be correct if it corresponded to a term in our training set. Note that as we can only consider a pattern match to be correct if it contains terms in our training set, the precision is under-estimated so this does not represent the true F-Measure associated with this pattern.\n\nh2=(1+\u03b1)p^r^\u03b1p^+r^p^=#correct pattern matches#all pattern matchesr^=#synonym pairs found#synonym pairs\n\nAlthough this gave a good result it significantly increased the search time. In the end we decided that the best answer would be to use the number of synonym pairs found as this was readily computed and did not bias towards more common terminology. In many pattern generation methods, including the WHISK system, significant effort is made to develop a strong set of patterns, we designed this algorithm just to produce lots of reasonable results. The rationale behind this was that we wanted to give the synonym classifier the most information we could, and that too much complexity in the pattern generation duplicated the effort of the statistical classifier. The heuristic we used is as follows.\n\nh3=#synonym pairs found#synonym pairs\n\nWe let the algorithm run for a fixed number of iterations, we chose this number to find approximately 10 times the number of patterns we plan to use in our classification.\n\nTo generate the feature vectors we found that many of the patterns were inflexible and matched very rarely, to combat this we simply allowed * and (space) to match \u03b5, the empty string, by which we mean that allow a match with any of the *s or (space)s omitted. This allowed patterns such as # (* #) to match not only \"term (or abbrev),\" but also \"term (abbrev).\" We found this greatly improved the recall and the precision of the result.\n\nWe would expect some of the patterns generated to produce reasonably good precision and some relatively good recall, however by combining these all together we should be able to get a much better overall result. For this reason we view the problem as a statistical classification problem.\n\nOnce we have generated a number of patterns we simple use these to generate a feature vector for each synonym pair, this is simply given by the number of matches of pattern with the #s matching the terms in the candidate synonym pair. Although this in theory would require n2(n - 1) feature vectors for n terms, in fact most pairs of terms do not occur close to each other at all in the corpus so they can be dropped (and their probabilities gained by classifying the zero vector). For our experiment we used the top 6000 patterns from the pattern generation algorithm. This then becomes a standard statistical classification problem, to which any statistical classification algorithm can be applied. We experimented with na\u00efve Bayes, logistic regression, C4.5 and support vector machines with a number of kernel choices. We found that only logistic regression gave a useful spread of probabilities that could be used for synset formation so we built the synsets first with these probabilities and then we used logistic regression on the output of the strongest classifier (SVM).\n\nThe results we gained from the statistical classification procedure gave only the probability of a particular term pair being synonymous. However we would expect every pair of terms in a synset to be synonymous and these binary classification results do not guarantee that such a transitivity relation exists. As such we shall assume that every pair of terms in a synset are synonymous and no pairs of terms in different synsets are synonymous (although this is technically incorrect as some words may be polysemous). This clearly leads to the result that synsets are complete graphs, so we can consider our goal as that of finding the closest set of complete sub-graphs to our random graph. As an example consider Figure 2, which shows a graph representation of the output on top, where the nodes represents terms, and they are connected if the classifier predicts the terms are synonymous. It is clear that the graph above should give two synsets as shown in the bottom graph.\n\nLet {1..N} correspond to our terms, then our goal is to find a set I\u2217={I1\u2217,...In\u2217} where I* exactly covers {1..N} and I* maximises c(I*) given by\n\nc({I1,...,IK})=\u2211k=1..Kc(IK)c(Ik)=\u2211i\u2208Ik\u2211j\u2208Ik(log\u2061(Pij)+\u2211j\u2209Iklog\u2061(1\u2212Pij))\n\nwhere Pij is the probability of the terms corresponding to i and j being synonymous as given by some statistical classification method (and Pii = 1). Note this logarithmic form is used as it is easier for further calculations and many of the output probabilities from our classifier were near 0 or 1. We also define the inter-node cost, cij as\n\ncij = log(Pij) + log(Pji) - log(1 - Pij) - log(1 - Pji)\n\nAs the number of potential synsets is 2N, we can greatly reduce this problem by finding a small set of potential synsets I such that I* \u2282 I \u2282 P({1..N}). Fortunately the majority of the output probabilities Pij are very small, so we can hope to have significant success by generating the set I by a branch and bound algorithm. We find a condition when a set and all of its super-sets are not optimal by observing that J \u2209 I* if c(J \u222a {k}) <c(J) + c({k}) for some k \u2208 I.\n\nLemma 1: Let J \u2282 {1..N} and k \u2209 J and V \u2282 {1..N} such that J \u2229 V = \u2205 and k \u2209 V . Then there is no set K such that J \u222a {k} \u2282 K \u2282 J \u222a {k} \u222a V, K \u2209 I* if\n\n1. \u2211i\u2208J cik < 0\n\n2. -\u2211i\u2208J cik > maxV'\u2282V\u2211i\u2208V' cik\n\nProof: Follows directly from the inequality c(K) <c(K\\{k}) + c({k})\n\nThis is very useful as the set V' is simply the set for which cik is positive.\n\nWe also notice that it is possible to divide the problem by using the following lemma\n\nLemma 2: If J {1..N} and K \u2282 {1..N} (J \u2229 K = \u2205) are such that \u2200j\u2208J\u2200k\u2208Kcjk < 0, then there does not exist \u2205 \u2282 J' \u2286 J and \u2205 \u2282 K' \u2286 K such that J' \u222a K' \u2208 I*.\n\nProof: Follows from c(J' \u222a K') <c(J') + c(K')\n\nThis means that only connected components are optimal so we only consider connected sets when generating I (where we define i and j to be connected if cij \u2265 0). Also when a term has been removed from the search space (i.e. is no longer in J \u222a V), this may result in previously connected components becoming disconnected. For this reason we also search for the connected components every time we remove an element from the search space. Table 1 shows the number of synsets generated by by using the branch and bound heuristic (lemma 1), the connected components heuristic (lemma 2), finding the connected components then using branch and bound and the final algorithm as follows (shift(X) returns and removes the first element of X):\n\n1. J = \u2205\n\n2. For each connected component V in {1... N}\n\n(a) Sort V by \u2211k = 1...Ncik\n\n(b) generate_matrix(J, V)\n\nfunction generate_matrix(J, V)\n\n1. While length(V) > 0\n\n(a) k = shift(V)\n\n(b) if(|J| = 0 or \u2211i\u2208Jcik > 0 or -\u2211i\u2208Jcik <maxV'\u2282V \u2211i\u2208V', cik)\n\ni. add J \u222a {k} to I\n\nii. generate_matrix(J \u222a {k}, V)\n\n(c) if(J \u222a V has more than one connected component)\n\ni. for each connected component, C, with |C \u2229 J| > 0 and |C \u2229 V| > 0: generate_matrix(J, C \u2229 V)\n\nii. end\n\nThis problem is in fact the set covering problem, which is NP-complete, however we found exact methods to be suffcient for our problem. To solve this problem, after we have generated I = {I1,... In} we formed a matrix A = (aij), given by\n\naij={1if j\u2208Ii;0otherwise\n\nWe also form a vector c where ck = c(Ik) and now finding an exact cover is equivalent to finding the integer vector, x, which maximises cTx subject to\n\nAx = 1\n\nThis is a well studied problem, and finding this vector x can be viewed as a branch and bound problem and attacked through the Dancing Links algorithm [18]. This algorithm uses a sparse matrix formation to efficiently remove infeasible row choices, and combined with a branch and bound algorithm can very effectively find optimal solutions. The branch and bound simply discards any partial solution when the most it costs is guaranteed to be less than the best solution found so far. This maximum cost can be found by simply tabulating the best clique available for each unsatisfied column in the matrix. More advanced algorithm use either linear relaxation (that is allowing x to take non-integer values and solving with the simplex algorithm), or Lagrangian relaxation with sub-gradient optimisation [19] to estimate this upper bound, however we did not find this necessary for our data sets.\n\nRegardless of the implementation of the solver one general purpose improvement is to attack each of the sub-problems (i.e. each of the connected components) separately, and then combine the results. The number of ways of partitioning a set is given by Bell's number\n\nBn=\u2211k=0n\u22121(nk)Bk\n\nBell's number gives the worst case search space size for the dancing links algorithm. If we can divide the problem into connected components, given by S = {S1,... Sn} then the size of the search space is\n\n\u220fSi\u2208SB|Si|\n\nHowever by attacking each of the problems separately we get the size of the search space as\n\n\u2211Si\u2208SB|Si|\n\nTherefore it now follows that the complexity of this problem is primarily dependent on the size of the largest connected component. In a standard Erd\u0151s-R\u00e9nyi random graph this grows logarithmically if the probability of an edge existing between a pair of nodes is less than n-1 as in our problem. Although our problem is not truly equivalent to an Erd\u0151s-R\u00e9nyi random graph, the false positive links between different synsets can be reasonably modelled by an Erd\u0151s-R\u00e9nyi graph. Still the complexity of the largest connected component Bc log(n) is not polynomial so it is still possible that we may not be able to find the optimal solution for larger test sets, however this would only affect the largest connected components, which as the test set grows larger would represent fewer of the terms and so fewer of the links. In this case we can solve most of the problem exactly, and some of it approximately to give a good overall solution.\n\nTo provide a comparison of our results we used encyclopedia and thesauri. We chose to use 4 sources: these sources were Wikipedia following the method suggested in [20], WordNet [21] the widely used machine readable thesaurus and two domain specific resource Medline Encyclopedia [22] which contains a specialised thesaurus on the subject of human diseases, MeSH [23], a very large controlled vocabulary containing synonyms for a huge variety of medical terminology and UMLS [24], a meta-thesaurus. MeSH, UMLS and WordNet were queried using their built in tools so differences in spelling, pluralisation were considered. As Wikipedia and Medline encyclopedia were not originally intended to be used as thesauri we had to adapt them as follow\n\n1. Wikipedia (redirect): Wikipedia uses a redirect table for terms which should direct to the same article, if this is used the text \"(Redirected from ...)\" appears beneath the title. Hence we classified two terms as synonyms if they redirected to the same page. For example (example works as of 7 November 2007) the Wikipedia page for \"WNV\" is the same as the page for \"West Nile virus\"\n\n2. Wikipedia (search): An alternative way to use Wikipedia is to take the top search term using Wikipedia's search engine. Again we classified two terms as the same if they had the same top search result.\n\n3. Medline Encyclopedia: Many articles in the Medline Encyclopedia, had an \"Alternative Names\" section, which combined with the title of the article gave us synsets. This gave us a good set of medical terminology however it did not cover animals and animal diseases.\n\nAll of our test synonym sets, which we found from the 150 abstracts containing the term \"infectious diseases\", were queried against these resources and the number of links correctly found are presented in Table 2. We found that these encyclopedia and thesauri made a number of mistakes\n\n1. Failing to find a term\n\n2. Producing an entry for the disease caused by the virus, bacteria etc. or vica versa\n\n3. Producing a match that was totally irrelevant e.g. FPV, an abbreviation of \"feline parvovirus\" in bio-medicine, but Wikipedia instead produces a page on \"Ford Performance Vehicles\" (as of 24 July 2007, after our data was obtained, a disambiguation page is shown).\n\n4. Matching a term that was broader or narrower than the one requested e.g. vCJD was matched as CJD\n\n5. Some disagreement about synonymy of terms, see the guidelines at the beginning of the data collection section.\n\nFirst for our evaluation we used a na\u00efve classifier we will call the occurrence classifier, which simply decided that two terms were synonymous if they co-occurred in any of our patterns. This unsurprisingly gives a very low precision but also the recall is only 63.4%, which will limit the recall of any method we try to use on this data. We found that 78.3% of these non-occurring term pairs, involved one or more term which matched less than 100 articles on PubMed. This suggests that this value may be very close to a limit of the recall of the method.\n\nTable 3 lists the performance of some single patterns and gives us a baseline for our method and Figure 3 shows the spread of recall and precision for all generated patterns. It can be seen that most of these are variation on parentheses apposition patterns also suggested in Yu et al [12]. We also listed a number of patterns that were domain-specific, to show the value of generating patterns for each specific domain. Finally we examined three more patterns from Yu et al which did not perform so well in our experiments, this was partly due to our syntax-free approach matching sub-terms.\n\nWe then tried several statistical pattern recognition algorithms (we used the WEKA implementations [25] in all cases), finally grouping the results into synsets based on the outputted probability of the logistic regression and SVM regression classifier by the method described above. The pattern generation took 51 hours on an Dual Core 1.66 GHz processor with 512 MB of memory, then a further 33 hours to generate feature vectors, but we feel that this could be vastly improved by the use of some kind of indexing of the corpus. The classification took 85 seconds and the synset formation about 5 seconds when based on the SVM Regression data and about 127 seconds when based on the logistic regression probabilities, this was due to more positive links in this data set, hence a larger maximum connected component.\n\nIn Figure 4 we calculated the experimental results that would be outputted if we stopped the synonym set solver before we had found the optimal solution. This illustrates that the theoretical cost c(...) is correlated to the experimental F-Measure, so better theoretical solutions produce better actual results. Looking at the main results (Table 4) we see the grouping the results in synsets improves the results in terms of totally synonymous results, and although the standard error is large we find the difference is significant at a 99% level using the p-test as described in Yeh [26]. Also the results after synset grouping appeared to be closer, so we analysed the results according to the degree of relation\n\n\u2022 Modified: Terms differed by inclusion of a modifier\n\n\u2022 Variant: We defined this precisely as any organism term referring to organisms of the same species, for example \"feline infectious peritonitis virus\" which is a mutation of \"feline enteric coronavirus\". Also we defined this as diseases caused by the same agent but at different stages of progression, for example \"dengue fever\", which may develop into \"dengue hemorrhagic fever\"\n\n\u2022 Method of Transmission/Point of Infection (MoT/PoI): These are diseases (and agents causing them) which differ only in method of transmission, for example \"hospital-acquired\", or point of infection.\n\n\u2022 Agent/Disease: One term refers to a disease and the second to an agent causing that disease\n\n\u2022 Hypernym: The terms showed a clear hypernym/hyponym relation that was not covered by the above groups.\n\nWe then analysed the logistic regression result (see Table 5) as the high precision of the SVM based result made this analysis less informative. We can clearly see that although both methods had similar precisions, the term pairs produced by the binary classification were more likely to be unrelated. This is not surprising as erroneous links between terms are likely to be caused by artifacts in the data and method and so be nearly totally at random. In contrast we would expect the pattern-based classification system to be more likely to mistake close synonyms for true synonyms, and so these results should be more consistent in relation to our definition of synset formation. So the reduction in error and an increase in number of near synonyms provides evidence for the validity of our model.\n\nTable 6 illustrates some sample output from the process.\n\nIn this work we decided to use terms that we found from actual texts as opposed to using a taxonomic thesaurus such as WordNet. This meant that it was difficult to create a test set and as we only found 101 synonymous term pairs we ended up with very large standard errors. It should be noted that for results with low recall the number of results found was so small that we would require a very large test set to accurately estimate the precision. However we found that our results are in line with previous work on both manual and automatic pattern discovery and this indicates that a good result is still obtainable without the use of syntactic knowledge or very large test sets.\n\nOur pattern generation method developed a number of interesting patterns and identified parentheses as the strongest indicator of synonymy (which by itself scored 22.9% F-Measure, see Table 2). It also found several domain specific patterns suggesting the effectiveness of generating separate patterns for specific domains. Syntax-free patterns were also generated allowing application to resource-poor languages, however this does rely on the terms which we wish to find having already been identified. In our experimental setup we inputted terminology that had been manually extracted from PubMed abstracts. There exist many named entity extractors which would allow us to completely automate the whole process and develop a large set of noun phrases for our method to classify, hence we could automatically develop a large thesaurus which would be easy to manually check for errors. One of the more surprising results we saw was that most of the patterns we generated seemed to be of quite high precision, which was unexpected as the scoring metric we chose to use in our search was biased towards patterns with a high recall. The question of how we could generate patterns with higher recall is an interesting one and deserves further study.\n\nWe conclude that for domains with a large amount of specific vocabulary most of the resources we studied perform worse than the automatic method we have developed here. Also given the amount of effort required to manually construct a resource, automatic thesaurus construction may prove more useful in many situations, either to aid construction or in replacement of manual construction. More importantly we have shown that we can easily automatically find patterns and we do not require any prior knowledge of the language's grammar in order to do this. Even though the patterns we generated were weak by themselves we showed that by statistically combining them we can get a much stronger result. We have also shown that we do not need to know a large number of synsets to develop an accurate classifier; this implies most importantly that this method can be used quickly on a different language. We tested our method on only a limited domain but we feel it would likely generalize well to other domains. Our novel synset grouping method not only converted the result to something more applicable, but also improved on the results for both a strict definition of synonymy, and a more relaxed definition.\n\nWe think given the logical/probabilistic framework for synset formation that the model we described should be applicable to other relations (for example hypernymy) and even combinations of relations. However solving this model efficiently and whether this will improve the result for actual data is an issue for future work.\n\nJM participated in the design and conception and carried out the data collection, implementation, analysis of data and drafted the manuscript. NC participated in the design and conception and helped to draft the manuscript. Both authors read and approved the final manuscript"}