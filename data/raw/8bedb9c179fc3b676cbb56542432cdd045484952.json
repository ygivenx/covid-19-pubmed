{"title": "Facing the possibility of bioterrorism", "body": "In 2002, in the aftermath of anthrax attacks on politicians and the media the previous year that affected 22 people (five fatally) [1] , the US government signed legislation providing $2.9 billion to enhance bioterrorism preparedness, including public health and medical strategies [2] . Unsurprisingly, therefore, there is a vast and burgeoning academic literature on all imaginable aspects of bioterrorism: ranging from the identification of potential agents and how to counteract them, through syndromic surveillance and diagnosis, to consequence management including treatment, isolation, risk communication and psychological intervention [1, 3] . Several specialist publications have been launched and numerous conferences held to discuss these issues.\n\nMany experts expressed the hope that, after years of neglect, by capitalizing on political concerns, fear of bioterrorism would allow the field of public health to come of age [4] [5] [6] [7] . Health tracking systems designed to deal with terrorist attacks are expected to also be of use in monitoring emerging infectious diseases more broadly and for identifying the roots of chronic illnesses [4] . This may be true, but it is also an indictment of scientific and political leaders that they only appear willing to develop a sense of common purpose in the aftermath of adversity. What is more, it remains to be determined whether it is as straightforward to reorient systems and staff developed and trained to target specific agents, to having to deal with more general ailments, as it would be the other way round.\n\nDuring this period, an outbreak of severe acute respiratory syndrome (SARS) developed in South-East Asia and was transported to a few other locations worldwide. Researchers appear to have used this episode to confirm their own prejudices, either warning of a possible apocalypse yet to come or using it as evidence of the need for, or efficiency of, the new health alert mechanisms put into place as a consequence of the focus on bioterrorism [7] [8] [9] . A less salutory interpretation of these events might suggest the very opposite -an over-reaction to a minor and predictable condition that, through the prism of the newly inflated sense of risk and warning systems, led to society inflicting considerable, yet unnecessary, damage on several regional economies and airlines.\n\nBioterrorism is defined as the release of biological agents or toxins that impact upon human beings, animals or plants with the intent to harm or intimidate [10] [11] [12] [13] . Those pathogens perceived to be the most threatening, on the basis of infectivity, virulence, lethality, pathogenicity, incubation period, contagiousness and stability, are known by Centres for Disease Control as category A agents [14] [15] [16] and are smallpox, anthrax, plague, botulism, tularemia and viral haemorrhagic fevers. Category B agents, which include the toxin ricin, are considered to be less easy to disseminate, have lower morbidity and mortality rates, and are less likely to challenge the public health system. Emerging pathogens are defined as category C agents.\n\nA lot of articles have outlined the properties of the prime suspects, focusing on dose, transmission, diagnosis and treatment. These reviews encompassed numerous journals and books, as many professions are considered to be in the front-line of having to identify or deal with bioterrorism [10, 14, 15, 17] . Few writers, however, point to the difficulties in developing, producing and deploying biological agents [18] , as evidenced by the failures of the Japanese cult, Aum Shinrikyo, with biological agents almost a decade ago [11] . In fact, such agents have rarely been used and there is a limited list of such incidents, dating back to the throwing of people infected with bubonic plague over the walls of Kaffa by the Black Sea in the mid-fourteenth century, through the purported use of smallpox infested blankets by Lord Amherst against native American tribes in the mid-eighteenth century, to a growing number of incidents across the world over the course of the twentieth century [11, 12, 18] .\n\nThe effective use of chemical and biological weapons awaited proper scientific understanding and technical capabilities that only emerged from the late nineteenth century onwards. But, it is the advent of biotechnology over the past 50 years, and in particular the more recent, if overstated, possibility of genetically engineering agents to target specific biological systems at the molecular level, that is held to pose a new and significant challenge for the future [19] . Accordingly, there is an increasing amount of literature on the need to reaffirm and strengthen existing counter-proliferation protocols, such as the Biological Weapons Convention, to monitor the use and deployment of so-called dual-use technologies, which can mean almost anything, and to ensure greater scrutiny of scientists and the communication of scientific methodologies and data [11, 12, 19, 20] .\n\nAnother area presumed to be of concern to the management of such incidents is that of dealing with their psychological impact [3,21,22 ,23-25] . Weapons of mass destruction in general, and chemical and biological weapons in particular, are considered to be likely to produce adverse psychosocial consequences upon targeted populations [26] , despite a paucity of data in this regard [22 ] . Limited, hurried and fairly superficial surveys conducted in the aftermath of the 11 September 2001 attacks purport to show significant levels of post-traumatic stress disorder, affecting both those who were immediately present, as well as those more indirectly exposed through the medium of television [21, 23, 24, 27 ] . As a consequence, numerous strategy documents have been, or are being, prepared aimed at ensuring that politicians and emergency responders are aware of, and prepared to deal with, these broader phenomena [28, 29] . This article goes on to deconstruct some of the key concepts and assumptions within this debate.\n\nMuch of this discussion takes at face-value the notion of an impending threat posed by (usually) external malefactors [19] , bent on undermining western democracies, as well as the extreme vulnerability of these societies to such attacks and the assumed fragility of their members [23, 24] . There is little attempt to identify possible internal sources of discontent, in view of the fact that the West has greater access to, and capabilities in developing, such weapons [18] . Nor is there any general recognition that advanced economies are better placed to deal with the consequences and contain the potential of bioterrorism, a fact that significantly undermines their purpose to outsiders. More importantly, there is little understanding that our exaggerated sense of vulnerability and frailty is both historically contingent, predating 9/11 quite significantly, and culturally determining, giving shape to and driving much of the bioterrorism agenda [30 ,31 ] .\n\nA notable exception to this trend is presented by King, a medical historian and epidemiologist, who identifies one of the casualties of these times as being 'a proper sense of history' [30 ] . He notes that 'experts were using the threat of novel diseases' as a rationale for change long before the recent attacks, and that contemporary responses draw on 'a repertoire of metaphors, images and values' shaped by even older, more complex forces. He goes on to suggest that 'American concerns about global social change are refracted through the lens of infectious disease', signifying a more broadly perceived 'loss of control' over contemporary society. This important essay, shows that a major contribution to our proper understanding of these purportedly narrowly scientific or military issues will come from some unexpected directions.\n\nAnother of these is sociology. In his latest book, Furedi, explores the roots of a growing sense of social and individual vulnerability in what he coins 'therapeutic culture' [32 ] . By increasingly framing problems through the prism of their emotions, people are actively incited to feel powerless and ill. Accordingly, 'the spirit of stoicism and sacrifice', along with 'a sense of common purpose, unity or a commitment to fight' are now rarely in evidence. A powerful consequence of this, along with distorted perceptions [33] and an increase in reported rates of depression, is provided by the phenomenon of mass psychogenic (or sociogenic) illness [22 ,27 ] , numerous instances of which became evident in the aftermath of the anthrax attacks [31 ,34] .\n\nEssentially, psychogenic illness occurs when members of a group exhibit a rapid spread of the signs and symptoms of an illness, but the physical complaints have no corresponding organic aetiology [22 ] . In extreme situations such cases can rapidly overwhelm existing healthcare resources, undermining the treatment of those directly affected or contaminated [21] . The arrival of television cameras or emergency workers wearing decontamination suits can act as the confirming trigger for this spread [27 ,31 ] . So too can psychological interventions, such as debriefing, which also undermine constructive, prosocial and rational responses, including the expression of strong emotions such as anger [35] [36] [37] [38] .\n\nThus, it is evident that social and cultural expectations as to behaviour shape professional interventions in an emergency or the aftermath of disaster, and that these are significant determining factors as to outcomes [39 ] . Accordingly, political and media presumptions that the public will panic, despite a categorical lack of evidence in this regard, are both false and ultimately debilitating [22 ,35,39 ,40] . Although trying to be helpful in this regard, a forthcoming World Health Organisation document displays a confused outlook, arguing for the development of long-term professional psychosocial frameworks of support, but conceding that these cannot be imposed [28] . In an incisive critique Pupavac has exposed the limitations of, and false assumptions lying behind, such interventions [41 ] .\n\nIn their study of Gulf War veterans, Stuart et al. [42] report a significant reinforcement of false beliefs in exposure to toxins among veterans receiving primary diagnoses of mental disorder. This points to the fact that psychiatrists can end up becoming complicit in shaping and creating individual and social ills [42] [43] [44] . Despite good intentions, it is difficult for the latter not to reflect the broader social outlook that emphasizes vulnerability and human frailty. The extent to which this script is culturally constructed is made evident by Bleich [45 ] : an Israeli population habituated to living with terrorist attacks displayed lower reported rates of post-traumatic stress disorder than those observed in the US post 9/11. All manner of technological fixes for dealing with the presumed problem of bioterrorism, from new vaccines to regulations regarding the conduct and communication of science, are being proposed and examined. But, none of these address our corrosive, culturally determined concerns. Indeed, by suggesting the primacy of objectivescientific problems over subjective, social and political ones -an emphasis on technical responses ensues that tends to push people further apart, thereby encouraging them to be more suspicious of one another [46] . This separation can promote a preponderance of rumours and hoaxes, as well as reinforcing passive notions of susceptibility to apparently inevitable threats [21, 26, 33, 47] . Real resilience requires bringing people together with a sense of common purpose [48 ] .\n\nIn this regard, numerous well-meaning contributions, emanating from several directions including the emergency planning community and risk managers and communicators, suggest the need to provide more or better information as a necessary building-block for restoring public trust and confidence [8, 11, 21, 35, [49] [50] [51] and uncritically accept the supposed threats and fears. Information is necessary [26] , but not sufficient to fundamentally address or assuage concerns; it cannot compensate for the demise of a more confident and purposeful culture. Indeed, if it fails to address the 'credibility gap', as Glass puts it [39 ] , or fulfil the 'need to find meaning' referred to by Hassett in his important contribution [27 ] , then information can readily become part of the problem rather than being a cure.\n\nMany responses to the perceived threat of bioterrorism fail to address the social, cultural and historical context shaping such concerns [30 ,34] . Accordingly, there has been a tendency to seek quick technical fixes to assumed problems, rather than addressing more profound political and perceptual issues. Yet, developed societies had increasingly been living in fear of the consequences of social and technological change well before the recent terrorist attacks, and politicians had busily been reinventing themselves as risk managers accordingly. Ironically, attempts to control or contain change, often for purportedly environmental or moral reasons such as the US ban on stem-cell research, could end up exposing us to even greater risks [35] .\n\nAs the public are the real first responders in any emergency or disaster, it is vital that they be fully integrated into, and engaged by, a set of broader social aims and values [48 ] . The confidence derived from having a sense of purpose or mission, developed over a long-term, active, political engagement in society, cannot be short-circuited by technical means or information campaigns. Hence, although specialist simulations and exercises for dealing with bioterrorism incidents may be of benefit to emergency responders and political leaders [13, 16, 52, 53] , they are unlikely to achieve any broader resilience across society. Worse, by failing to address the cultural presumptions and concerns that underlie the emergence of such issues, they may serve to truly corrode society from within. Restoring an appropriate and robust sense of confidence to deal with these matters will need to be a political, not a technical, project."}