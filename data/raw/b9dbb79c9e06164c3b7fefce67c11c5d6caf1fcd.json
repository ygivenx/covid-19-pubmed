{"title": "Interpretable detection of novel human viruses from genome sequencing data", "body": "Within a globally interconnected and densely populated world, pathogens can spread more easily than they ever did before. As the recent outbreaks of Ebola and Zika viruses have shown, the risks posed even by these previously known agents remain unpredictable and their expansion hard to control (Calvignac-Spencer et al., 2014) . What is more, it is almost certain that more unknown pathogen species and strains are yet to be discovered, given their constant, extremely fast-paced evolution and unexplored biodiversity, as well as increasing human exposure (Vouga & Greub, 2016; Trappe et al., 2016) . Some of those novel pathogens may cause epidemics (similar to the SARS and MERS coronavirus outbreaks in 2002 and 2012) or even pandemics (e.g. SARS-CoV-2 and the \"swine flu\" H1N1/09 strain). Many have more than one host or vector, which makes assessing and predicting the risks even more difficult. For example, Ebola has its natural reservoir most likely in fruit bats (Leendertz et al., 2016) , but causes deadly epidemics in both humans and chimpanzees. As the state-of-the art approach for the open-view detection of pathogens is genome sequencing (Lecuit & Eloit, 2014; Calistri & Pal\u00f9, 2015) , it is crucial to develop automated pipelines for characterizing the infectious potential of currently unidentifiable sequences.\n\nScreening against potentially dangerous subsequences before their synthesis may also be used as a way of ensuring responsible research in synthetic biology. While potentially useful in some applications, engineering of viral genomes could also pose a biosecurity and biosafety threat. Two controversial studies modified the influenza A/H5N1 (\"bird flu\") virus to be airborne transmissible in mammals (Herfst et al., 2012; Imai et al., 2012) . A possibility of modifying coronaviruses to enhance their virulence triggered calls for a moratorium on this kind of research (Lipsitch & Inglesby, 2014) . Synthesis of an infectious horsepox virus closely related to the smallpox-causing Variola virus (Noyce et al., 2018) caused a public uproar and calls for intensified discussion on risk control in synthetic biology (Thiel, 2018) .\n\nSeveral computational, genome-based methods exist that allow to predict the host-range of a bacteriophage (a bacteriainfecting virus). A selection of composition-based and alignment-based approaches has been presented in an extensive review by Edwards et al. (2016) . Prediction of eukariotic host tropism (including humans) based on known protein sequences was shown for the influenza A virus (Eng et al., 2014) . Two recent studies employ k-mer based, k-NN classifiers and deep learning (Mock et al., 2019) to predict host range for a small set of three well-studied species directly from viral sequences. While those approaches are limited to those particular species and do not scale to viral host-range prediction in general, the Host Taxon Predictor (HTP) (Ga\u0142an et al., 2019) uses logistic regression and support vector machines to predict if a novel virus infects bacteria, plants, vertebrates or arthropods. Yet, the authors argue that it is not possible to use HTP in a read-based manner; it requires long sequences of at least 3,000 nucleotides. This is incompatible with modern metagenomic next-generation sequencing workflows, where the DNA reads obtained are at least 10-20 times shorter. Another study used gradient boosting machines to predict reservoir hosts and transmission via arthropod vectors for known human-infecting viruses (Babayan et al., 2018) . Zhang et al. (2019) designed several classifiers explicitly predicting whether a new virus can potentially infect humans. Their best model, a k-NN classifier, uses k-mer frequencies as features representing the query sequence and can yield predictions for sequences as short as 500 base pairs (bp). It worked also with 150bp-long reads from real DNA sequencing runs, although in this case the reads originated also from the viruses present in the training set (and were therefore not \"novel\").\n\nWhile DNA sequences mapped to a reference genome may be represented as images (Poplin et al., 2018) , a majority of studies uses a distributed orthographic representation, where each nucleotide {A, C, G, T } in a sequence is represented by a one-hot encoded vector of length 4. An \"unknown\" nucleotide (N ) can be represented as an all-zero vector. CNNs and LSTMs have been successfully used for a variety of DNA-based prediction tasks. Early works focused mainly on regulation of gene expression in humans (Alipanahi et al., 2015; Zhou & Troyanskaya, 2015; Zeng et al., 2016; Quang & Xie, 2016; Kelley et al., 2016) , which is still an area of active research (Greenside et al., 2018; Nair et al., 2019; Avsec et al., 2019) . In the field of pathogen genomics, deep learning models trained directly on DNA sequences were developed to predict host ranges of three multi-host viral species (Mock et al., 2019) and to predict pathogenic potentials of novel bacteria (Bartoszewicz et al., 2019) . DeepVirFinder (Ren et al., 2018) and ViraMiner (Tampuu et al., 2019) can detect viral sequences in metagenomic samples, but they cannot predict the host and focus on previously known species. For a broader view on deep learning in genomics we refer to a recent review by Eraslan et al. (2019) .\n\nInterpretability and explainability of deep learning models for genomics is crucial for their wide-spread adoption, as it is necessary for delivering trustworthy and actionable results. Convolutional filters can be visualized by forward-passing multiple sequences through the network and extracting the most-activating subsequences (Alipanahi et al., 2015) to create a position weight matrix (PWM) which can be visualized as a sequence logo (Schneider & Stephens, 1990; Crooks et al., 2004) . Direct optimization of input sequences is problematic, as it results in generating a dense matrix even though the input sequences are one-hot encoded (Lanchantin et al., 2016; . This problem can be alleviated with Integrated Gradients (Sundararajan et al., 2016; Jha et al., 2019) or DeepLIFT, which propagates activation differences relative to a selected reference back to the input, reducing the computational overhead of obtaining accurate gradients (Shrikumar et al., 2019a) . If a reference of all-zeros is used, the method is analogous to Layer-wise Relevance Propagation (Bach et al., 2015) . DeepLIFT is an additive feature attribution method, and may used to approximate Shapley values if the input features are independent (Lundberg & Lee, 2017) . TF-MoDISco (Shrikumar et al., 2019b) uses DeepLIFT to discover consolidated, biologically meaningful DNA motifs (transcription factor binding sites).\n\nIn this paper, we first improve the performance of read-based predictions of the viral host (human or non-human) from next-generation sequencing reads. We show that reversecomplement (RC) neural networks (Bartoszewicz et al., 2019) significantly outperform both the previous state-ofthe-art (Zhang et al., 2019) and the traditional, alignmentbased algorithm -BLAST (Altschul et al., 1990) , which constitutes a gold standard in homology-based bioinformatics analyses. We show that defining the negative (nonhuman) class is non-trivial and compare different ways of constructing the training set. Strikingly, a model trained to distinguish between viruses infecting humans and viruses infecting other chordates (a phylum of animals including vertebrates) generalizes well to evolutionarily distant nonhuman hosts, including even bacteria. This suggests that the host-related signal is strong and the learned decision boundary separates human viruses from other DNA sequences surprisingly well.\n\nNext, we propose a new approach for convolutional filter visualization using partial Shapley values to differentiate between simple nucleotide information content and the contribution of each sequence position to the final classification score. To test the biological plausibility of our models, we generate genome-wide maps of \"infectious potential\" and nucleotide contributions. We show that those maps can be used to visualize and detect virulence-related regions of interest (e.g. genes) in novel genomes. Finally, we analyze a recently discovered SARS-CoV-2 coronavirus, which caused a pandemic in 2020 (Wu et al., 2020) .\n\nWe accessed the Virus-Host Database (Mihara et al., 2016) on July 31, 2019 and downloaded all the available data. The original dataset contained 14,380 records comprising Ref-Seq IDs for viral sequences and associated metadata. Some viruses are divided into discontiguous segments, which are represented as separate records in VHDB; in those cases the segments were treated as contigs of a single genome in the further analysis. We removed records with unspecified host information and those confusing the highly pathogenic Variola virus with a similarly named genus of fish. Further, we filtered out viroids and satellites. Human-infecting viruses were extracted by searching for records containing \"Homo sapiens\" in the \"host name\" field. Note that VHDB contains information about multiple possible hosts for a given virus where appropriate. Any virus infecting humans was assigned to the positive class, also if other, non-human hosts exist. In total, the dataset contained 9,496 viruses, including 1,309 human viruses. We considered both DNA and RNA viruses; RNA sequences were encoded in the DNA alphabet, as in RefSeq.\n\nWhile defining a human-infecting class is relatively straightforward, the reference negative class may be conceptualized in a variety of ways. The broadest definition takes all non-human viruses into account, including bacteriophages (bacterial viruses). This is especially important, as most of known bacteriophages are DNA viruses, while many important human (and animal) viruses are RNA viruses. One could expect that the multitude of available bacteriophage genomes dominating the negative class could lower the prediction performance on viruses similar to those infecting humans. This offers an open-view approach covering a wider part of the sequence space, but may lead to misclassification of potentially dangerous mammalian or avian viruses. As they are often involved in clinically relevant host-switching events, a stricter approach must also be considered. In this case, the negative class comprises only viruses infecting Chordata (a group containing vertebrates and closely related taxa). Two intermediate approaches consider all eukaryotic viruses (including plant and fungi viruses), or only animal-infecting viruses. This amounts to four nested host sets: \"All\" (8,187 non-human viruses), \"Eukaryota\" (5,114 viruses), \"Metazoa\" (2,942 viruses) and \"Chordata\" (2,078 viruses). Auxiliary sets containing only non-eukaryotic viruses (\"non-Eukaryota\"), non-animal eukaryotic viruses (\"non-Metazoa Eukaryota\") etc. can be easily constructed by set subtraction.\n\nFor the positive class, we generated a training set containing 80% of the genomes, and validation and test sets with 10% of the genomes each. Importantly, the nested structure was kept also during the training-validation-test split: for example, the species assigned to the smallest test set (\"Chordata\") were also present in all the bigger test sets. The same applied to other taxonomic levels, as well as the training and validation sets wherever applicable.\n\nWe simulated 250bp long Illumina reads following a modification of a previously described protocol (Bartoszewicz et al., 2019) and using the Mason read simulator (Holtgrewe, 2010). First, we only generated the reads from the genomes of human-infecting viruses. Then, the same steps were applied to each of the four negative class sets. Finally, we also generated a fifth set, \"Stratified\", containing an equal number of reads drawn from genomes of the following disjunct host classes: \"Chordata\" (25%), \"non-Chordata Metazoa\" (25%), \"non-Metazoa Eukaryota\" (25%) and \"non-Eukaryota\" (25%).\n\nIn each of the evaluated settings, we used a total of 20 million (80%) reads for training, 2.5 million (10%) reads for validation and 2.5 million (10%) paired reads as the heldout test set. Read number per genome was proportional to genome length, keeping the coverage uniform on average. While the original datasets are heavily imbalanced, we generated the same number of negative and positive data points (reads) regardless of the negative class definition used.\n\nThis protocol allowed us to test the impact of defining the negative class, while using the exactly same data as representatives of the positive class. We used three training and validation sets (\"All\", \"Stratified\", and \"Chordata\"), representing the fully open-view setting, a setting more balanced with regard to the host taxonomy, and a setting focused on cases most likely to be clinically relevant. In each setting, the validation set matched the composition of the training set. The evaluation was performed using all five test sets to gain a more detailed insight on the effects of negative class definition on the prediction performance.\n\nSimilarily to Zhang et al. (2019) , we used the human blood DNA virome dataset (Moustafa et al., 2017) to test the selected classifiers on real data. We obtained 14,242,329 reads of 150bp and searched all of VHDB using blastn (with default parameters) to obtain high-quality reference labels. If a read's best hit was a human-infecting virus, we assigned it to a positive class; the negative class was assigned if this was not the case. This procedure yielded 14,012,665 \"positive\" and 229,664 \"negative\" reads.\n\n. CC-BY-ND 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.29.925354 doi: bioRxiv preprint\n\nWe used the DeePaC package (Bartoszewicz et al., 2019) to investigate RC-CNN and RC-LSTM architectures, previously shown to accurately predict bacterial pathogenicity. Here, we employ a CNN with two convolutional layers with 512 filters of size 15 each, average pooling and 2 fully connected layers with 256 units each. The LSTM used has 384 units. We use dropout regularization in both cases, together with aggressive input dropout at the rate of 0.2 or 0.25 (tuned for each model). Input dropout may be interpreted as a special case of noise injection, where a fraction of input nucleotides is turned to N s. Representations of forward and reverse-complement strands are summed before the fully connected layers. As two mates in a read pair should originate from the same virus, predictions obtained for them can be averaged for a boost in performance. If a contig or genome is available, averaging predictions for constituting reads yields a prediction for the whole sequence. We used Tesla P100 GPUs for training and an RTX 2080 Ti for visualizations.\n\nWe wanted the networks to yield accurate predictions for both 250bp (our data, modelling a sequencing run of an Illumina MiSeq device) and 150bp long reads (as in the Human Blood Virome dataset). As shorter reads are padded with zeros, we expected the CNNs trained using average pooling to misclassify many of them. Therefore, we prepared a modified version of the \"Stratified\" dataset, in which the last 100bp of each read were turned to zeros, mocking a shorter sequencing run while preserving the error model. Then, we retrained the CNN which had performed best on the original dataset. Since in principle, the Human Blood Virome dataset should not contain viruses infecting non-human Chordata, a \"Chordata\"-trained classifier was not used in this setting.\n\nWe compare the networks to the k-NN classifier proposed by Zhang et al. (2019) and used by them for read-based predictions. We trained the classifier on the \"All\" dataset as described by the authors, i.e. using non-overlapping, 500bplong contigs generated from the training genomes (retraining on simulated reads is computationally prohibitive). We also tested the performance of using BLAST to search against an indexed database of labeled genomes. We constructed the database from the \"All\" training set and used discontiguous megablast to achieve high inter-species sensitivity.\n\nNote that both BLAST and k-NN can yield conflicting predictions for the individual mates in a read pair. What is more, BLAST yields no prediction at all if no match is found. Therefore, similarly to Bartoszewicz et al. (2019) , we used the accept anything operator to integrate binary predictions for read pairs and genomes. At least one match is needed to predict a label, and conflicting predictions are treated as if no match was found at all. Missing predictions lower both true positive and true negative rates.\n\nIn order to visualize the learned convolutional filters, we downsample a matching test set to 125,000 reads and pass it through the network. This is modelled after the method presented by Alipanahi et al. (2015) . For each filter and each input sequence, the authors extracted a subsequence leading to the highest activation, and created sequence logos from the obtained sequence sets (\"max-activation\"). We used DeepLIFT (Shrikumar et al., 2019a) to extract scoreweighted subsequences with the highest contribution score (\"max-contrib\") or all subsequences with non-zero contributions (\"all-contrib\"). Computing the latter was costly and did not yield better quality logos.\n\nWe use an all-zero reference. As reads from real sequencing runs are usually not equally long, shorter reads must be padded with N s; the \"unknown\" nucleotide is also called whenever there is not enough evidence to assign any other to the raw sequencing signal. Therefore, N s are \"null\" nucleotides and are a natural candidate for the reference input. We do not consider alternative solutions based on GC content or dinucleotide shuffling, as the input reads originate from multiple different species, and the sequence composition may itself be a strong marker of both virus and host taxonomy.\n\nHowever, some of the training sequences contain N s themselves. It is therefore possible that a filter will learn only negative weights at a given position, even though there is no biological justification for that. This may lead to assigning only negative contributions to all four possible nucleotides at a given position if the filter's contribution is positive (and positive nucleotide contributions if the filter's contribution is negative). To solve the problem, we first normalize the weight matrices position-wise, as described by Shrikumar et al. (2019a) . Finally, we calculate average filter contributions to obtain a crude ranking of feature importance with regard to both the positive and negative class.\n\nBuilding sequence logos involves calculating information content (IC) of each nucleotide at each position in a prospective DNA motif. This can be then interpreted as measure of evolutionary sequence conservation. However, high IC does not necessarily imply that a given nucleotide is relevant in terms of its contribution to the classifier's output. Some sub-motifs may be present in the sequences used to build the logo, even if they do not contribute to the final prediction (or even a given filter's activation).\n\n. CC-BY-ND 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10. 1101 /2020 To test this hypothesis, we use partial Shapley values. Intuitively speaking, we capture the contributions of a nucleotide to the network's output, but only in the context of a given intermediate neuron of the convolutional layer. More precisely, for any given feature x i , intermediate neuron y j and the output neuron z, we aim to measure how x i contributes to z while regarding only the fraction of the total contribution of x i that influences how y j contributes to z.\n\nUsing the formalism of DeepLIFT's multipliers (Shrikumar et al., 2019a) and their reinterpretation in SHAP (Lundberg & Lee, 2017) , we backpropagate the activation differences only along the paths \"passing through\" y j . In Eq. 1, we define partial multipliers \u00b5 (yj ) xiz and express them in terms of Shapley values \u03c6 and activation differences w.r.t. the expected activation values (reference activation). Calculating partial multipliers is equivalent to zeroing out the multipliers m y k z for all k = j before backpropagating m yj z further.\n\nWe define partial Shapley values \u03d5 (yj ) i (z, x) analogously to how Shapley values can be approximated by a product of multipliers and input differences w.r.t. the reference (Eq. 2):\n\nFrom the chain rule for multipliers (Shrikumar et al., 2019a) , it follows that standard multipliers are a sum over all partial multipliers for a given layer y. Therefore, Shapley values as approximated by DeepLIFT are a sum of partial Shapley values for the layer y.\n\nOnce we calculate the contributions of convolutional filters for the first layer, \u03d5 (yj ) i (z, x) for the first convolutional layer of a network with one-hot encoded inputs and an all-zero reference can be efficiently calculated using weight matrices and filter activation differences (Eq. 3-4). First, in this case we do not traverse any non-linearities and can directly use the linear rule (Shrikumar et al., 2019a) to calculate the contributions of x i to y j as a product of the weight w i and the input x i . Second, the input values may only be 0 or 1.\n\nResulting partial contributions can be visualized along the IC of each nucleotide of a convolutional kernel. To this end, we design extended sequence logos, where each nucleotide is colored according to its contribution. Positive contributions are shown in red, negative contributions are blue, and near-zero contributions are gray. Therefore, no information is lost compared to standard sequence logos, but the relevance of individual nucleotides and the filter as a whole can be easily seen.\n\nWe create genome-wide phenotype analysis (GWPA) plots to analyse which parts of a viral genome are associated with the infectious phenotype. We scramble the genome into overlapping, 250bp long subsequences (pseudo-reads) without adding any sequencing noise. For the highest resolution, we use a stride of one nucleotide. We predict the infectious potential of each pseudo-read and average the obtained values at each position of the genome. Analogously, we calculate average contributions of each nucleotide to the final prediction of the convolutional network. We visualize the resulting nucleotide-resolution maps with IGV (Thorvaldsd\u00f3ttir et al., 2013) .\n\nFor well-annotated genomes, we compile a ranking of genes (or other genomic features) sorted by the average infectious potential within a given region. In addition to that, we scan the genome with the learned filters of the first convolutional layer to find genes enriched in subsequences yielding nonzero filter activations. We use Gene Ontology to connect the identified genes of interest with their molecular functions and biological processes they are engaged in.\n\nAs a proof of concept, we analyze one of the viruses randomly assigned to the test set -the Ta\u00ef Forest ebolavirus, which has a history of host-switching and can cause a serious disease. To show that the method can also be used for other biological problems, we investigated the networks trained by Bartoszewicz et al. (2019) and their predictions on a genome of a pathogenic bacterium Staphylococcus aureus. The authors used this particular species to assess the performance of their method on real sequencing data. In this case, we used a stride of 125bp to generate the pseudo-reads. Finally, we analyzed the SARS-CoV-2 coronavirus, which emerged in December 2019, causing a pneumonia outbreak (Wu et al., 2020) .\n\nChoosing which viruses should constitute the negative class is application dependent and influences the performance of the trained models. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.29.925354 doi: bioRxiv preprint set composition. The models trained only on human and Chordata-infecting viruses maintain similar, or even better performance when evaluated on viruses infecting a much broader host range, including bacteria. This suggests that the learned decision boundary separates human viruses from all the others surprisingly well. We hypothesize that the human host signal must be relatively strong and contained within the Chordata host signal. Dropout rate of 0.2 resulted in the highest validation accuracy for CNN Str-150 and LSTM Str . A rate of 0.25 was selected for the other models.\n\nAdding more diversity to the negative class may still boost performance on more diverse test sets, as in the case of CNN trained on the \"All\" dataset (CNN All ). This model performs a bit worse on viruses infecting hosts related to humans, but achieves higher accuracy than the \"Chordata\"-trained models and the best recall overall. Rebalancing the negative class using the \"Stratified\" dataset helps to achieve higher performance on animal viruses while maintaing high overall accuracy. The LSTMs are outperformed by the CNNs, but they can be used for shorter reads without retraining (see Sections 2.2 and 3.2).\n\nWe selected LSTM All and CNN All for further evaluation. Table 2 presents the results of a benchmark using the \"All\" test set. Low performance of the k-NN classifier (Zhang et al., 2019) is caused by frequent conflicting predictions for each read in a read pair (in a single-read setting it achieves 75.5% accuracy, while our best model -87.8%). Although BLAST achieves the highest precision, it yields no predictions for over 10% of the samples. CNN All is the most sensitive and accurate (Table 3) .\n\nWe benchmarked our models against the human blood virome dataset used by Zhang et al. (2019) . Our models outperform their k-NN classifier. As the positive class massively outnumbers the negative class, all models achieve over 99% precision. CNN All-150 performs best (Table. 4 ). However, the positive class is dominated by viruses which are not necessarily novel. The CNN was more accurate on training data, so we expected it to detect those viruses easily.\n\nIn the Fig. 1 we present example filters, visualized as \"maxcontrib\" sequence logos based on mean partial Shapley values for each nucelotide at each position. All nucleotides of the filters with the highest (Fig. 1a) or lowest (Fig. 1b) score have relatively strong contributions in accordance with the filters' own contributions. However, we observe that sometimes, there is a \"conserved\" nucleotide which consistently appears in the activating subsequences, but the sign of its contributions is opposite to the filter's (Fig. 1c) . Those \"counter-contributions\" may arise if a nucleotide with a negative weight forms a frequent motif with others with positive weights strong enough to activate the filter. We comment on this fact in the Section 4.2. Some filters seem to learn gapped motifs resembling a codon structure (Fig. 1d) . We extracted this filter from a network predicting bacterial pathogenicity trained by Bartoszewicz et al. (2019) , but we find similar filters in our networks as well. We scanned a genome of S. aureus with this filter and discovered that the learned motif is indeed significantly enriched in coding sequences (Fisher exact test with Benjamini-Hochberg correction, q < 10 \u221215 ). It is also enriched in a number of specific genes. The one with the most hits (sraP, q < 10 \u221215 ) is associated with virulence in endovascular infection.\n\nWe created a GWPA plot for the Ta\u00ef Forest ebolavirus genome (Fig. 2a) . Most genes can be detected by finding peaks of elevated pathogenic potential score predicted by at least one of the models. Intergenic regions are characterized by lower mean scores.\n\nWe ran a similar analysis of S. aureus using the built-in DeePaC models (Bartoszewicz et al., 2019) and our interpretation workflow. While a viral genome contains usually . CC-BY-ND 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.29.925354 doi: bioRxiv preprint only a handful of genes, by compiling a ranking of 870 annotated genes of the analyzed S. aureus strain we could test if the high-ranking regions are indeed associated with pathogenicity. Indeed, out of three top-ranking genes, sarR and sspB are directly engaged in virulence, while hupB regulates expression of virulence-involved genes in many pathogens (Stojkova et al., 2019) . Fig. 2b presents a GWPA plot for the whole genome of the SARS-CoV-2 coronavirus. We highlighted the score peaks aligning with the gene encoding the spike protein, which plays a significant role in host entry (Li, 2016) , as well as the E and N genes, which were scored the highest (apart from an unconfirmed ORF10 of just 38aa downstream of N) by the CNN and the LSTM, respectively. Fig. 2c shows the nucleotide-level contributions in a small peak within the receptor-binding domain of the S protein, crucial for recognizing the host cell. The domain location was predicted with CD-search (Marchler-Bauer et al., 2017) . While this could suggest a host adaptation, more research is needed.\n\nCompared to the previous state-of-the-art in viral host prediction directly from next-generation sequencing reads (Zhang et al., 2019) , our models drastically reduce the error rates. This holds also for novel viruses not present in the training set. In the paired read scenario, the previously described method fails, and standard, alignment-based homology testing algorithm cannot find any matches in more than 10% of the cases, resulting in relatively low accuracy. On a real human virome sample, where a main source of negative (Moustafa et al., 2017) , our method filters out non-human viruses with high specificity. In this scenario, the BLAST-derived groundtruth labels were mined using the complete database (as opposed to just a training set). In all cases, our results are only as good as the training data used; high quality labels and sequences are needed to develop trustworthy models. Ideally, sources of error should be investigated with an in-depth analysis of a model's performance on multiple genomes covering a wide selection of taxonomic units. This is especially important as the method assumes no mechanistic link between an input sequence and the phenotype of interest, and the input sequence constitutes only a small fraction of the target genome without a wider biological context. Still, it is possible to predict a label even from those small, local fragments. A similar effect was also observed for image classification with CNNs (Brendel & Bethge, 2019) .\n\nVisualizing convolutional filters may help to identify potential problems. If the input data contains N s, a ReLU network may learn only negative weights at some positions, resulting in counter-contributions for all possible nucleotides at those positions. In our case, as the filters were weight-normalized, the counter-contributions suggest that the information content and the contribution of a nucleotide are not necessarily correlated. Visualizing learned motifs by aligning the activating sequences (Alipanahi et al., 2015) would not fully . CC-BY-ND 4.0 International license author/funder. It is made available under a The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.29.925354 doi: bioRxiv preprint (a) Ta\u00ef Forest ebolavirus. Genes that can be detected by at least one model are highlighted in black.\n\n(b) Whole genome and sequences encoding the spike protein (S), envelope protein (E) and nucleocapsid protein (N).\n\n(c) Spike protein gene, a small peak (positions 22,595-22,669, dashed line in Fig. 2b ) within the receptor-binding domain (predictied by CD-search). Binding to the receptor is crucial for entry to the host cell. Local host adaptation could help switch hosts between the animal reservoir and humans. describe how the filter reacts to presented data. It seems that the assumption of nucleotide independence -which is crucial for treating DeepLIFT as a method of estimating Shapley values for input nucleotides -is broken. Indeed, k-mer distribution profiles are frequently used features for modelling DNA sequences (as shown also by the dimershuffling method of generating reference sequences proposed by Shrikumar et al. (2019a) ). However, DeepLIFT's multiple successful applications in genomics indicate that the assumption probably holds approximately. We see information content and DeepLIFT's contribution values as two complementary channels that can be jointly visualized for better interpretability and explainability of CNNs in genomics.\n\nMapping predictions back to a target genome can be used both as a way of investigating a given model's performance and as a method of genome analysis. GWPA plots of wellannotated genomes highlight the sequences with erroneous and correct phenotype predictions at both genome and gene level, and nucleotide-resolution contribution maps help track those regions down to individual amino-acids. On the other hand, once a trusted model is developed, it can be used on newly emerging pathogens, as the SARS-CoV-2 virus briefly analyzed in this work. The methods presented here may also be applied to other biological problems outside of the field of pathogen genomics. However, experimental work and traditional sequence analysis are required to truly understand the biology behind host adaptation and distinguish true hits from false positives.\n\nWe presented a new approach for predicting a host of a novel virus based on a single DNA read or a read pair, cutting the error rates in half compared to the previous state-of-the-art. For convolutional filters, we jointly visualize nucleotide contributions and information content. Finally, we use GWPA plots to gain insights into the models' behavior and analyze a recently emerged SARS-CoV-2 virus. Data is available at https://doi.org/10.5281/zenodo.3630803 and the code is submitted in Supplementary Material. The copyright holder for this preprint (which was not peer-reviewed) is the . https://doi.org/10.1101/2020.01.29.925354 doi: bioRxiv preprint"}