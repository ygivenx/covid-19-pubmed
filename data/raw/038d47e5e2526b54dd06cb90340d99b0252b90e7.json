{"title": "Association of radiologic findings with mortality of patients infected with 2019 novel coronavirus in Wuhan", "body": "index estimates for the prediction models ranged from 0.73 to 0.81 in those for the general population (reported for all 3 general population models), from 0.81 to > 0.99 in those for diagnosis (reported for 5 of the 9 diagnostic models), and from 0.90 to 0.98 in those for prognosis (reported for 4 of the 7 prognostic models). All studies were rated at high risk of bias, mostly because of non-representative selection of control patients, exclusion of patients who had not experienced the event of interest by the end of the study, and poor statistical analysis, including high risk of model overfitting. Reporting quality varied substantially between studies. A description of the study population and intended use of the models was absent in almost all reports, and calibration of predictions was rarely assessed.\n\nConclusion COVID-19 related prediction models for diagnosis and prognosis are quickly entering the academic literature through publications and preprint reports, aiming to support medical decision making in a time where this is needed urgently. Many models were poorly reported and all appraised as high risk of bias. We call for immediate sharing of the individual participant data from COVID-19 studies worldwide to support collaborative efforts in building more rigorously developed and validated COVID-19 related prediction models. The predictors identified in current studies should be considered for potential inclusion in new models. We also stress the need to adhere to methodological standards when developing and evaluating COVID-19 related predictions models, as unreliable predictions may cause more harm than benefit when used to guide clinical decisions about COVID-19 in the current pandemic.\n\nSystematic review registration osf.io/ehc47/ . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\n(which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nWhat is already known on this topic -The sharp recent increase in COVID-19 infections has put a strain on healthcare systems worldwide, necessitating efficient diagnosis of patients suspected of the infection and prognostication of COVID-19 confirmed cases.\n\n-Viral nucleic acid testing and chest CT are standard methods for diagnosing COVID-19, but are time-consuming.\n\n-Earlier reports suggest that the elderly, patients with comorbidity (COPD, cardiovascular disease, hypertension), and patients presenting with dysapnoea are vulnerable to more severe morbidity and mortality after COVID-19 infection.\n\n-We identified three models to predict hospital admission from pneumonia and other events (as a proxy for covid-19 pneumonia) in the general population.\n\n-We identified nine diagnostic models for COVID-19 detection in symptomatic patients. Seven of these were neural network models based on CT images.\n\n-We identified seven prognostic models, of which five aimed to predict mortality risk in confirmed COVID-19 patients and two aimed to predict a hospital stay of more than 10 days from admission.\n\n-All included studies were appraised at high risk of bias, suggesting concern that the models may be flawed and perform poorly when applied in practice, such that their predictions may be unreliable.\n\nThe novel coronavirus (COVID-19) presents a significant and urgent threat to global health.\n\nSince the outbreak in early December 2019 in the Hubei Province of the People's Republic of China, more than 318.000 cases have been confirmed in over 160 countries, and over 13.000 people died (up to 22 st March). 1 Despite public health responses aimed at containing the disease and delaying the spread, several countries have been confronted with a critical care crisis, and more countries may follow. 2 3 Outbreaks lead to important increases in the demand for hospital beds and shortage of medical equipment, while medical staff themselves may also get infected.\n\nTo mitigate the burden on the health care system, while also providing the best possible care for patients, efficient diagnosis and prognosis is needed. Prediction models, which combine multiple predictors (variables or features) to estimate the risk of being infected or experiencing poor outcome of the infection, could assist medical staff in triaging patients when allocating limited healthcare resources. Prediction models, ranging from rule-based scoring systems to advanced machine learning models (deep learning), have already been proposed and published in response to a call to share relevant COVID-19 research findings rapidly and openly to inform the public health response and help save lives. 4 Many of these prediction models are published in open access repositories, ahead of peer-review.\n\nWe aimed to systematically review and critically appraise currently available COVID-19 related prediction models, in particular models for diagnosis of COVID-19 in suspected cases or models for prognosis of individuals in confirmed cases. This systematic review was done in collaboration with the Cochrane Prognosis Methods group.\n\nWe searched PubMed, EMBASE via Ovid, bioRxiv, medRxiv, and arXiv for research on COVID-19 published after 3 rd January 2020. We used the publicly available publication list of the COVID-19 Living Systematic Review. 5 This list contains studies on COVID-19 published on PubMed, EMBASE via Ovid, bioRxiv, and medRxiv, and is continuously updated. We validated the list to examine whether it is fit for purpose by comparing it to relevant hits from bioRxiv and medRxiv when combining COVID-19 search terms (covid-19, sars-cov-2, \"novel corona\", 2019-ncov) with methodological search terms (diagnostic, prognostic, prediction model, machine learning, artificial intelligence, algorithm, score, deep learning, regression).\n\nAll relevant hits were found on the Living Review list. We supplemented the Living Review list with hits from PubMed searching for \"covid-19\", as this was at the moment of our search not included in the Living Review search terms for PubMed. We further supplemented the Living Review list with studies on COVID-19 retrieved from arXiv. The search strings are listed in the Supplementary Material. In addition, we reached out to authors to include studies that were not publicly available at the time of the search. 6 7 Databases were searched on 13 th March 2020. All studies were considered, regardless of language or publication status (preprint or peer reviewed articles). Studies were included if they developed or validated a multivariable model or scoring system, based on individual participant level data, to predict any COVID-19 related outcome in individuals, either to inform diagnosis or prognosis. There was no restriction on setting (e.g., in-or outpatients), prediction horizon, included predictors, or outcomes. Prediction models to detect individuals at risk of developing COVID-19 pneumonia in the general population were also included.\n\nEpidemiological studies that aimed at modelling disease transmission or case-fatality rates, diagnostic test accuracy and predictor finding studies were excluded. Titles, abstracts and full . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\n(which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint texts were screened in duplicate for eligibility by pairs of independent reviewers (from LW, BVC, MvS), and discrepancies were resolved through discussion.\n\nData extraction of included articles was done by two independent reviewers (from LW, BVC, GSC, TPAD, MCH, GH, KGM, RDR, ES, LS, EWS, KIES, CW and MvS), using a standardized data extraction form based on the CHARMS checklist 8 and Prediction model Risk Of Bias ASsessment Tool (PROBAST) 9 . We sought to extract each model's predictive performance, using whatever measures were presented, including any summaries of calibration (the extent to which predicted risks correspond to observed risks) and discrimination (the extent to which predicted risks discriminate between participants with and without the outcome), as recommended in the TRIPOD statement. 10 Any discrepancies in data extraction were resolved by LW and MvS. Details on data extraction are provided in the Supplementary Material. Reporting of the article considered aspects of PRISMA 11 and TRIPOD 10 .\n\nA total of 1914 titles were retrieved through our systematic search ( Figure 1 ). Two additional unpublished studies were made available upon request. These were not yet publicly available at the time of the search and were identified after a call on social media. Out of 1916 titles, 46 studies were retained for abstract and full text screening. Fifteen studies, describing nineteen prediction models, met the inclusion criteria and were selected for data extraction and critical appraisal. 6 7 12-24 . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\n(which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nAll of the 15 studies used data on COVID-19 cases from China (see Supplementary Table 1 ).\n\nBased on nine of the 15 studies that reported study dates, data were collected between 8 th December 2019 and 3 rd March 2020. The duration of follow-up was unclear in most studies, although one reported a median follow-up of 8.4 days. 13 Some Chinese centers provided data to multiple studies, but it was unclear how much these datasets overlapped across our 15 identified studies. One study used U.S. Medicare claims data from 2015 to 2016 to estimate COVID-19 vulnerability, 7 one study used control CT scans from the USA, 19 and one study used simulated data. 12 All but one study 18 developed prediction models for use in adults. The median age varied between studies (from 38 to 65 years, see Supplementary Table 1) , as did the percentage of men (from 35% to 61%).\n\nAmong the seven studies that developed prognostic models to predict mortality risk in individuals with confirmed or suspected COVID-19 infection, the percentage of deaths varied between 8% and 59% (See Table 1 ). This wide variation is in part due to severe sampling bias caused by studies excluding participants who had not experienced the event by the end of the study period (i.e., neither healed nor died). In addition, length of follow-up may have varied between studies, but was not reported.\n\nAmong the nine diagnostic model studies, there was only one that reported on prevalence of COVID-19 infection in those suspected of having COVID-19; the prevalence was 19% (development dataset) and 24% (validation dataset). 24 Since the seven imaging studies used either case-control sampling or the method of data collection was unclear, the prevalence in these diagnostic studies may not have been representative of their target population.\n\n. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nIn what follows, we give an overview of the 19 prediction models reported in the 15 identified studies (Table 1) . Modeling details are provided in Supplementary Table 2 , and the availability of models in a format for use in clinical practice is discussed in Box 1.\n\nThree models predicted the risk of hospital admission for COVID-19 pneumonia for individuals in the general population, but used admission due to non-tuberculosis pneumonia, influenza, acute bronchitis, or upper respiratory infections as outcomes in a dataset without any COVID-19 cases (see Table 1 ). 7 Among the predictors were age, sex, previous hospital admissions, comorbidity data, and social determinants of health. The models reported C-index estimates of 0.73, 0.81 and 0.81.\n\nOne study developed a warning score to diagnose COVID-19 in symptomatic adults based on sex, age, fever, highest body temperature between onset and admission, history of close contact with confirmed patients, signs of pneumonia on CT, neutrophil-to-lymphocyte ratio, and meaningful respiratory syndromes (see Table 1 ). 24 They report a C-index estimate of 0.97. One study developed a decision tree to diagnose severe disease in symptomatic paediatric inpatients based on direct bilirubin and alaninetransaminase. 18 They report an estimated F1 score of 1.00 (indicating 100% observed sensitivity and specificity).\n\nSeven prediction models were proposed to support the diagnosis of COVID-19 or COVID-19 pneumonia (and monitor progression) based on CT images. The reported predictive performance varied widely, with C-index estimates ranging from 0.81 to nearly 1.\n\n. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nWe identified seven prognostic models (Table 1) . Of these, five estimated mortality risk in suspected or confirmed COVID-19 patients. 12 13 15 16 25 The intended use of these models (i.e., when to use it, in whom to use it, and the prediction horizon (e.g., mortality by what time)) was not clearly described. Two models aimed to predict a hospital stay of more than 10 days from admission. 14 Predictors that were included in more than one prognostic model were age (n=3), features derived from CT-scoring (n=3), C-reactive protein (n=2), lactic dehydrogenase (n=2), and lymphocyte count (n=2) (see Table 1 ).\n\nOnly two studies predicting mortality reported a C-index; they obtained estimates of 0.90 16 and 0.98 6 . Only one study evaluated calibration. 6 When applied to new patients, their model yielded probabilities of mortality that were too high for low-risk patients and too low for highrisk patients (calibration slope >1), despite excellent discrimination. 6 One study developed two models to predict a hospital stay of >10 days and reported C-indexes of 0.92 and 0.96. 14\n\nAll models were at high risk of bias according to assessment with PROBAST (Table 1) , which suggests that their predictive performance when used in practice is likely lower than what is reported, and so gives concern that their predictions are unreliable. Details on common causes for risk of bias are given in Box 2 for each type of model.\n\nSix of the fifteen studies had a high risk of bias for the \"participants\" domain ( Table 2 ), indicating that the participants enrolled in the studies may not be representative for the models' targeted populations. Unclear reporting on the inclusion of participants prohibited a risk of bias assessment in six studies. Three out of sixteen studies had a high risk of bias for . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint the \"predictors\" domain, indicating that predictors were not available at the models' intended time of use, not clearly defined, or influenced by the outcome measurement. The diagnostic model studies that used CT imaging predictors were all scored as \"unclear\" on the \"predictors\" domain. The publications often lacked clear information on the preprocessing steps (e.g., cropping of images). Moreover, translation of CT image results to predictors is challenging, which makes the PROBAST predictors section hard to complete reliably. Most studies used outcomes that are easy to assess (e.g., death, presence of COVID-19 by laboratory confirmation). Nonetheless, there was reason to be concerned of bias induced by the outcome measurement in five studies, due to the use of subjective or proxy-outcomes.\n\nAll studies were at high risk of bias for the \"analysis\" domain ( Table 2) . Many studies had small sample sizes (Table 1) , leading to an increased risk of overfitting, particularly if complex modeling strategies were used. Two studies did not report the predictive performance of the developed model, and two studies reported the apparent performance only (that is, the performance in the exact same data as was used to develop the model, without adjustment for optimism due to potential overfitting). Only one study assessed calibration (i.e., the extent to which predicted risks corresponded to observed risks).\n\nIn this systematic review of prediction models related to the COVID-19 pandemic, we identified and critically appraised 15 studies that described 19 prediction models for detecting individuals at risk for hospital admission for COVID-19 pneumonia in the general population, for diagnosis of COVID-19 in symptomatic individuals, and for prognosis of COVID-19 infected patients. All models reported good to even excellent predictive performance, but all were appraised as high risk of bias, due to a combination of poor reporting and poor . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint methodological conduct for participant selection, predictor description and statistical methods used. As expected, in these early COVID-19 related prediction model studies, clinical data from COVID-19 patients is still scarce and limited to data from China. With few exceptions, the available sample size and number of events for the outcomes of interest were limited, which is a known problem for building prediction models, increasing the risk of overfitting the prediction model. 26 A high risk of bias implies that these models are likely to perform worse in practice than the performance that is reported by the researchers. Only two studies carried out an external validation on data from other individuals than from which the models was developed, and only one study assessed calibration (i.e., the correspondence between predicted and observed risks).\n\nWe reviewed seven studies that used advanced machine learning methodology on chest CT scans to diagnose COVID-19 disease, COVID-19 related pneumonia, or to assist in segmentation of lung images. The predictive performance measures showed a high to almost perfect ability to identify COVID-19, although these models and their evaluations also suffered from a high risk of bias, notably due to an artificial mix of COVID-19 cases and noncases.\n\nThe main aim of prediction models is to support medical decision making. It is therefore key to identify a target population in which predictions serve a clinical need, and a representative dataset on which the prediction model can be developed and validated. This target population must also be carefully described such that the performance of the developed or validated model can be appraised in context, and users know in which individuals the model can be applied to make predictions. However, the included studies in our systematic review often . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint lacked an adequate description of the study population, which leaves users of these models in doubt of the models' applicability. While we recognize that all studies were done under severe time constraints caused by urgency, we recommend that studies currently in preprint and future studies use and adhere to the TRIPOD reporting guideline 10 to improve the description of their study population as well as their modeling choices. TRIPOD translations (e.g., in\n\nChinese and Japanese) are also available.\n\nA better description of the study population may also help understand the observed variability in the reported outcomes across studies, such as COVID-19 related mortality. The variability in the relative frequencies of the predicted outcomes presents an important challenge to the prediction modeler: a prediction model applied in a setting with a different relative frequency of the outcome may produce predictions that are miscalibrated 27 and may need to be updated before it can safely be applied in that new setting. 28 Indeed, such an update may often be required when prediction models are transported to different healthcare systems, which requires COVID-19 patient data to be available from that system.\n\nData (IPD) from multiple countries and healthcare systems may facilitate better understanding of the generalizability and implementation prediction models across different settings and populations, and may greatly improve their applicability and robustness in routine care. [29] [30] [31] [32] The evidence base for the development and validation of prediction models related to COVID-19 will quickly increase over the coming months. Together with the increasing evidence from predictor finding studies 33 and open peer review initiatives for COVID-19 related publications, 34 data registries [35] [36] [37] [38] are being set up. To maximize the new opportunities and to facilitate IPD meta-analyses, the WHO has recently released a new data platform to encourage sharing of anonymized COVID-19 clinical data. 39 To leverage the full potential of . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint these evolutions, international and interdisciplinary collaboration in terms of data acquisition and model building is crucial.\n\nWith new publications on COVID-19 related prediction models that are currently quickly entering the medical literature, this systematic review cannot be viewed as an up-to-date list of all currently available COVID-19 related prediction models. Also, studies we reviewed were only available as a preprint, and they might improve after peer review, when entering the official medical literature. Finally, we have also found other prediction models which are currently implemented in clinical practice without scientific publications 40 and web risk calculators launched for use while the scientific manuscript was still under review (and unavailable upon request). 41 These unpublished models naturally fall outside the scope of this review of the literature.\n\nAll nineteen reviewed prediction models were found to have a high risk of bias and evidence from independent external validation of these models is currently lacking. However, the urgency of prediction models to assist in quick and efficient triage of patients in the COVID-19 pandemic may encourage clinicians to implement prediction models without sufficient documentation and validation. Although we cannot let perfect be the enemy of good, earlier studies have shown that models were of limited use in the context of a pandemic, 42 and they may even cause more harm than good. 43 We anticipate that more COVID-19 data on the individual participant level will soon become available. Based on the predictors included in models identified by our review, we encourage researchers to include data on age, sex, . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint comorbidities, C-reactive protein, lymphocyte markers (percentage or neutrophil-tolymphocyte ratio), lactate dehydrogenase, and potentially features derived from CT images when collecting data or building new models. By pointing to the most important methodological challenges and issues in design and reporting of the currently available models, we hope to have provided a useful starting point for further studies aiming at developing new models or validating and updating existing ones.\n\nDiagnostic and prognostic models for COVID-19 are available and they all appear to show good to excellent discriminative performance. However, their performance estimates are likely to be optimistic and thus misleading, as all identified studies were at high risk of bias.\n\nSharing data and expertise for development, validation and updating of COVID-19 related prediction models is urgently needed.\n\n. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nModels to predict hospital admission for COVID-19 pneumonia in the general population. The \"CV-19 vulnerability index\" to detect hospital admission for COVID-19 pneumonia from other respiratory infections (e.g. pneumonia, influenza), is available as an online tool. 7 44 Diagnostic models. The \"COVID-19 Early Warning Score\" to detect COVID-19 infection in adults is available as a score chart in an article. 24 A decision tree to detect severe disease for pediatric COVID-19 confirmed patients is also available in an article. 18 Diagnostic models based on CT imaging. Three of the seven AI models to assist with diagnosis based on CT results, are available via web applications. 17 20 23 45-47 Prognostic models. To assist in the prognosis of mortality, a nomogram (a graphic aid to calculate mortality risk), 6 a decision tree, 15 and a CT-based scoring rule are available in the articles. 16 All other 9 reports did not include any usable equation, format or reference for use of their prediction model. Because all models were at high risk of bias, we cannot recommend their routine use before they are properly externally validated.\n\n. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\npopulation. These models were based on Medicare claims data, and used proxy outcomes to predict hospital admission for COVID-19 pneumonia, in absence of COVID-19 cases. 7 Diagnostic models. Individuals without COVID-19 were excluded, altering the disease prevalence. 24 Predictors were dichotomized, leading to a loss of information. 18 24 Diagnostic models based on CT imaging. There was generally poor reporting on which patients CT images were obtained during clinical routine, and it was unclear whether the selection of controls was sampled from the target population (i.e., patients suspected of . 17 23 It was often unclear how regions of interest (ROIs) were annotated. Images were sometimes annotated by only one scorer without quality control, or the model output influenced annotation. 19 21 22 Careful description of model specification and subsequent estimation was lacking, challenging the transparency and reproducibility of the models. Every study used a different deep learning architecture, including established and specifically designed ones, without benchmarking the used architecture with respect to others.\n\nPrognostic models. Study participants were often simply excluded because they did not develop the outcome at the end of the study period but were still in follow-up (i.e., in the hospital and neither healed nor died), yielding a highly selected study sample. Only one study accounted for censoring by using Cox regression. 13 Other studies used highly subjective predictors, 16 or the last available predictor measurement from electronic health records was used (rather than the measurement of the predictor value at the time the model is intended to be used). 15 Dichotomization of predictors was often applied which tends to lead to loss of information. 18 24 . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint\n\nWe thank the authors who made their work available, by posting it on public registries or sharing it confidentially. C49297/A27294). The funders played no role in study design, data collection, data analysis, data interpretation, or reporting. The guarantors had full access to all the data in the study, take responsibility for the integrity of the data and the accuracy of the data analysis, and had final responsibility for the decision to submit for publication.\n\n. CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\n\nThe copyright holder for this preprint . https://doi.org/10.1101/2020.03.24.20041020 doi: medRxiv preprint High *1 Performance is given for the strongest form of validation reported. This is indicated in the column \"type of validation\". When a train-test split was used, performance on the test set is reported. Apparent performance is the performance observed in the development data. *2 Proxy events used: pneumonia (except from TB), influenza, acute bronchitis, or other specified upper respiratory infections (no COVID-19 pneumonia cases in data). *3 The development set contains scans from Chinese patients, the testing set contained scans from Chinese cases and controls, and U.S. controls. *4 Describes similarity between segmentation of the CT scan by a medical doctor and automated segmentation. *5 Outcome and pedictor data were simulated. *6 Wavelet-HLH_gldm_SmallDependenceLowGrayLevelEmphasis, wavelet-LHH_glcm_Correlation, wavelet-LHL_glszm_GrayLevelV ariance, wavelet-LLH_glszm_SizeZoneNonUniformityNormalized, wavelet-LLH_glszm_SmallAreaEmphasis, wavelet-LLH_glcm_Correlation. 3 models to identify subjects at risk in the general population 7 prognostic models (5 for mortality, 2 for length of stay) 9 diagnostic models (7 CT imaging studies)"}