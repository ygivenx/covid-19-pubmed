{"title": "Mathematical modeling of infectious disease dynamics", "body": "No doubt, the history of mankind has been shaped by the pitiless outbreaks of infectious disease pandemics. Whole nations and civilizations have been wiped off the map through the ages. The list is long: biblical pharaonic plagues that hit Ancient Egypt in the middle of Bronze Age around 1715 BC,1 the \u201c\u03bb\u03bf\u03b9\u03bc\u03cc\u03c2\u201d in Athens from 430 to 425 BC set the end of the Periclean golden era, the \u201ccocoliztli\u201d epidemics, which occurred during the 16th century, resulted in some 13 million deaths, decimating the Mesoamerican native population,2 the Black Death bubonic plague burst in Europe in 1348, and is estimated to have killed over 25 million people in just five years. The pandemic influenza virus of 1918\u20131919 swept through America, Europe, Asia, and Africa smashing the globe: the death toll was around 40 million people. Two one-year, less severe influenza pandemics followed in the next decades: the 1957 and the 1963 influenza pandemics resulted to two and one million deaths respectively (World Health Organization: http://apps.who.int/iris/handle/10665/68985). In the last decades emerging and re-emerging epidemics such as AIDS, measles, malaria, and tuberculosis cause death to millions of people each year. According to the UNAIDS report on the global AIDS epidemic, an estimated 34 million people, including 3.4 million children, were living with HIV worldwide at the end of 2010, while the related deaths and new infections were 1.8 and 2.7 million, respectively.3\n\nThe rapid technological and theoretical progress has dramatically enhanced our arsenal in fighting epidemics and we are getting better on it. The global surveillance network is growing under an intensive worldwide effort. We are now able to produce effective vaccines and antiviral drugs and knowledge goes deep in details such as the molecular structure of a variety of viruses. A large and intensive research is evolving for the design of better drugs and vaccines. Yet, studies warn us that a new pandemic\u2014influenza-type is the most worrisome one\u2014is sooner or later on the way.4 The critical question(s) is not whether but when it will arise, how it is going to spread, how deadly it will be, who should get the vaccine when not all can, how likely are multiple waves of re-emergence and what type of intervention may be applied to stop the spread. Unfortunately, even with all the advances, we still don\u2019t have robust answers.\n\nThe problem stems mainly from two reasons: (1) the continuous and ever-lasting mutations of the viruses, and (2) the complexity in the disease transmission mechanism. Unfortunately, the odds are that in a real crisis, even if researchers succeed to come up with a vaccine tailor-made for an emerged virus strain, it is doubtful that it would stop a pandemic.5\n\nThe complex multi-scale interplay between a host of factors ranging from the micro host\u2013pathogen and individual-scale host\u2013host interactions to macro-scale ecological, social, economic, and demographic conditions across the globe complicated by technical issues such as the time lag between vaccine prototype development and commercial production and distribution imposes a real impediment to our control strategy potential.\n\nMathematical, statistical models and computational engineering are playing a most valuable role in shedding light on the problem and for helping make decisions.\n\nThe very first publication addressing the mathematical modeling of epidemics dates back in 1766. In this seminal paper, Essai d'une nouvelle analyze de la mortalit\u00e9 caus\u00e9e par la petite v\u00e9role,6 Daniel Bernoulli developed a mathematical model to analyze the mortality due to smallpox in England, which at that time was one in 14 of the total mortality. Bernoulli used his model to show that inoculation against the virus would increase the life expectancy at birth by about three years. A translation in English and review of this work can be found in Sally Blower (2004),7 while a revision of the main findings and a presentation of the criticism by D\u2019Alembert appears in Dietz and Heesterbeek (2002).8 Lambert, in 1772, followed up the work of Bernoulli extending the model by incorporating age-dependent parameters.9 Laplace has also worked on the same concept.10 However this line of research has not been developed systematically until the benchmark paper of Ross in 1911, which actually establishes modern mathematical epidemiology.11 In this work, Ross addressed the mechanistic a priori modeling approach using a set of equations to approximate the discrete-time dynamics of malaria through the mosquito-borne pathogen transmission (for a discussion and a review of this model see also Smith et al. [2012]12).\n\nFollowing up the work of Ross, Kermack and McKendrick published three seminal papers which founded the deterministic compartmental epidemic modeling.13-15 In these papers, they addressed the mass\u2013action incident in disease transmission cycle, suggesting that the probability of infection of a susceptible (virgin from illness) is analogous to the number of its contacts with infected individuals. Hence, the rate at which susceptibles become infected is given by kSI where S and I represent population densities of susceptible and infected people, respectively. In this context, the rate at which infected individuals become recovered is given by \u03bbI, while the rate at which recovered individuals become again susceptible is given by \u03bcR; k, \u03bb and \u03bc are analogy constants. This mechanistic-deterministic representation holds strong analogy to the Law of Mass Action16 introduced by Guldberg and Waage in 1864 and is called the SIR model, implying homogeneous mixing of the contacts and conservation of the total mass (population) as well as relatively low rates of interaction. Forty years after the paper of Ross, MacDonald extended Ross\u2019s model to explain in depth the transmission process of malaria and propose methods for eradicating the disease on an operational level. Due to the importance of MacDonald\u2019s contribution to the field by exploiting the use of computers, mathematical models for the dynamics and the control of mosquito-transmitted pathogens are known as Ross\u2013MacDonald models.12\n\nAt this point it would be remiss of us not to mention the work of Enko,17-19 who in 1889 published a remarkable probabilistic model for describing the epidemic of measles in discrete time. With the use of the model, Enko evaluated the number of contacts between infectives and susceptibles in the population. The model of Enko is the precursor of the famous Reed-Frost chain binomial model introduced by W. H. Frost in 1928 in biostatistics lectures at Johns Hopkins University (not published then in a journal, but published in 197920). This model assumes that the infection spreads from an infected to a susceptible individual through discrete time Markov chain events. This representation set the basis of contemporary stochastic epidemic modeling.\n\nMathematical modeling and simulation allows for rapid assessment. Simulation is also used when the cost of collecting data is prohibitively expensive, or there are a large number of experimental conditions to test. Over the years, a vast number of approaches have been proposed looking at the problem from different perspectives. These encompass three general categories (see Fig. 1): (1) statistical methods for surveillance of outbreaks and identification of spatial patterns in real epidemics, (2) mathematical models within the context of dynamical systems (also called state-space models) used to forecast the evolution of a \u201chypothetical\u201d or on-going epidemic spread, and (3) machine learning/ expert methods for the forecasting of the evolution of an ongoing epidemic. For all three of these categories there are again different approaches weaving a big and diverse literature. Here, we try to draw the map of these approaches and try to describe their basic underpinning concepts.\n\nRegression models try to detect an outbreak from time-series of epidemic-free periods by monitoring a statistic of reported infected cases, say y(t). An epidemic alert is raised when a certain threshold, say k, is surpassed, defined by , (\u03bc being the mean value of the time-series distribution) within a confidence interval (usually of 95%).\n\nA basic regression model is that proposed from Serfling which was initially constructed to monitor the deaths of influenza based on the seasonal pattern of pneumonia and influenza deaths.24 Due to the seasonal behavior of the disease the following cyclic regression model has been addressed:\n\n\u03b8 is a linear function of time t while the coefficients are to be determined by a parameter identification technique. The cosine and sine terms are used to approximate cyclical seasonal patterns; e(t) is the noise (assumed that is Gaussian distributed with mean zero and variance \u03c32) which is estimated from the time-series. In the original paper of Serfling, y(t) was the expected mean value of total deaths due to pneumonia and influenza in units of 4-weeks periods. The model was fitted using data from 108 US cities for a 3 year period starting in September of 1955.\n\nUsing least squares estimation Serfling ended up to the following model:\n\nOther models including square terms, t2, to account long-term changes due to factors such as the population growth or disease assessment have been also proposed.27 Today, the above approach is used by the Centers for Disease Control in the US, Australia, France, and Italy for the detection of influenza outbreaks.\n\nWhile this approach is very popular among epidemiologists for predicting and surveillance purposes, one has to be cautious about their use as the form of the equations relies usually on ad hoc assumptions on the dependence between the dynamics of a disease and the independent factors (variables) that determine its spread. In addition, the choice of the model (linear/nonlinear), assumptions on the statistical properties (for example independence, normal distribution and fixed variance) of the unmodeled dynamics (represented by e(t)) flash a \u201cnote of caution\u201d in their use especially for the surveillance and prediction of outbreaks of new emerging epidemics.\n\nThese models relax the hypothesis of autocorrelation of regression models as well as the hypothesis of simple autoregressive models such as AR (autoregressive) and ARMA (autoregressive moving) in which past disturbances are not modeled. In this category, ARIMA models are the most commonly used. Their general form reads:\n\nwhere y(t) denotes a stationary stochastic process at time t with mean value E(y(t)) = \u03bc; z\u20131 is the backward shift operator defined by z\u2013ky(t) = y(t \u2013 k) and \u0394d is the differencing operator of order d defined by \u0394d \u2261 (1 \u2013 z\u22121)d; A(z\u22121) is the autoregressive operator defined as ; B(z\u22121) is the moving-average operator defined by ; e(t) is the residual (noise) at time t representing the part of the measurement that cannot be predicted from previous measurements. For d = 0 and na = 0 one gets the moving average model, while for d = 1, na = nb = 0 one gets the random walk with drift. Seasonal differencing enters naturally in the above framework by considering the seasonal differencing operator where k is the length of seasonal cycle and S is the degree of seasonal differencing producing series of changes from one season to the next.\n\nThe time-series is then split in two sets: one containing the times-series serving as a training set, and another one containing the remaining data serving as a test (validation) set. The Akaike Information Criterion35 is usually applied to identify the optimal model order by compromising between the goodness-of-fit and number of parameters. The fitted model is then used for the forecasting of disease evolution. The reliability of such approaches is limited mostly by (1) the statistical uncertainty related to the estimation of the values of the unknown parameters and (2) the hypotheses related to the statistical properties of the corresponding time series.\n\nCUSUM is probably the most common used technique for the detection of disease outbreaks. This is achieved by monitoring a cumulative performance measure over time. Let us consider the number of infected cases y(ti) as observed at different time instances ti, i = 1, 2, \u2026, n. In its simple representation, for a single parameter process, CUSUM is defined as\n\nor in a recursive form as\n\nwhere k is a reference value corresponding to the difference between to the in-control and the out-of-control mean. The process is considered to be in-control if CUSUM(i) < h with h denoting a threshold (its value is usually taken to be three times the standard deviation from the baseline/mean value of in-control-observations). An alarm is raised at time ti if CUSUM(i) exceeds h; the process is considered to be out-of-control. The reference value k is determined by likelihood ratio based methods.44-49 Hence, denoting by f(\u03b80) and f(\u03b81) the probability function of the in-control and out-of-control processes with parameters \u03b80 and \u03b81 respectively, the reference value reads:\n\nThe probability functions f(\u03b80) and f(\u03b81) and their parameters can be estimated using data from past periods. For Poisson distributions the above relation reads:\n\nwhere \u03bc0 and \u03bc1 are the mean values of the in-control and out-of-control Poisson distributions.\n\nFor an epidemic that involves time-varying characteristics, such as seasonality, the reference parameter is now time-varying itself, i.e., k \u2261 k(t).\n\nThe EWMA control chart method monitors infectious disease dynamics using the following recursive statistical estimator, which in its simple form reads:\n\n\u03b3 is a \u201cforgetting\u201d factor, a number between 0 and 1 which weights the significance of past values. Actually this factor reduces the importance of past observed information in estimating future. Again, an alarm is raised at time ti if z(ti) > h.\n\nOther statistical process control methods such as temporal scan statistics have been also used.46,50,51\n\nThe question that the HMMs come to answer in epidemiology is the following: how can we infer about the dynamics of a particular infectious disease and forecast its outbreak when we cannot monitor/record explicitly the characteristics of the disease but we can observe some possible indicators of the disease? For example, can we forecast the evolution of an influenza epidemic by monitoring for example the number of reported cases as recorded through a surveillance network of physicians or in hospital units?52,54 HMM models are exploited exactly under these limitations/ constraints. Within this context, let us denote by Y(t) the stochastic process of the unobserved (hidden) state, e.g., the number of cases of the disease in the population at time t and with O(t) the stochastic process of the observable states.\n\nFormally, HMMs are Markov processes, i.e., stochastic processes which satisfy the so called Markov property (here for the sake of presentation we assume discrete in time Markov processes) defined by:\n\nalong with the time-invariant transition probability between two realizations, say yi(.), yj(.):\n\nThe above relations simply state that all the necessary information for predicting the distribution of Y(t) at time Y(t) with a certain probability defined by P(.)is contained within Y(t \u2013 1); y(.)denotes a realization of the stochastic process Y(.).\n\nIn HMMs, the following conditional independence assumption holds:\n\nHere, the transition probability between an observed, say oj(.), and a hidden state, say yi(.), is defined as\n\nThere are three basic questions that have to be answered here: (1) what is the likelihood of the observed sequence, (2) what is the most likely hidden sequence given aij,bij and the observation sequence, and (3) given the observation sequence, which are the HMM parameters, i.e., aij,bij and initial distribution of observed states that maximize the likelihood of the observation sequence and/or hidden sequence. The first problem is usually tackled with the use of the forward-backward algorithm,55 the second problem with the use of the Viterbi algorithm,56 and the third problem with the use of the so-called Expectation-Maximization (EM) algorithm.57\n\nMost of the infectious diseases result to strong spatio-temporal patterns whose systematic analysis is of outmost importance for better understanding, predicting and combating outbreaks. Spatial surveillance requires the use of multivariate techniques.65 Most of the multivariate methods can be viewed as extensions of standard univariate methods\u2014as the ones described above\u2014; however, there are others such as clustering, principal component analysis (PCA) based methods that do not have a common ancestor with univariate ones.66 Kleinshmidt et al. (2000) used a two tier approach for the surveillance of malaria.67 They used regression analysis on the larger scale and kriging68 to interpolate the count data at an unobserved location in order to forecast the prevalence of the disease in the local scale. Cohen et al. (2010) exploited PCA to create a single surveillance index that can be used to summarize temporal and spatial trends of malaria in India.69 Coleman et al. (2009) used the SatScan freeware software ( http://www.satscan.org/ ) to identify malaria outbreaks to a province of South Africa by detecting time and space clusters.70 The SatScan software is based on the spatial scan statistic71,72 and the Bernoulli spatial model.73 SatScan has been also exploited by Gaudart et al. (2006) to identify spatio-temporal clusters of high risk incidence of malaria in a Mali village.74 A temporal analysis using ARIMA technique was also undertaken.\n\nTo this end, we should also mention the use of copulas75,76 (joint distribution functions used to model the dependencies between random variables based on given/known marginal distributions of the individual variables) for parametric multivariate analysis. Copulas can be integrated naturally within the HMM framework and hazard analysis approaches such as the Cox77,78 and Plackett\u2013Dale79 survival models to better understand and ultimately design more efficient intervention policies such as vaccination on targeted parts of the population and project future trends for risk assessment especially for fatal diseases such as AIDS.80-84 Such models are used to quantify the relation of demographic variables (such as age, gender, social status, spatial characteristics) on the survival rates, i.e., occurrence rates of events such as death or infection) in the population.85,86\n\nContinuum models describe the coarse-grained dynamics of the epidemics in the population.87-90 One might, for example, study a model for the evolution of the disease as a function of the age and the time since vaccination91,92 or investigate the influence of quarantine or isolation of the infected part of the population.93,94 Such models can be explored using powerful analysis techniques for ordinary or partial differential equations. However, due to the complexity and the stochasticity of the phenomena, most available continuum models are often only qualitative caricatures that cannot capture all of the details, therefore compromising epidemiological realism.\n\nWithin this context, the population is divided in compartments in accordance to the state of their health, such as susceptible (S), infected (I), and recovered (R). Other states of the population linked with control policies such as vaccinated (V) and quarantined (Q) are also used.\n\nThe compartmental SIR mass-action model of Kermack and McKendrick (1922) is the basis of such models. In this representation, it is assumed that an infected individual infects a susceptible with a probability pS\u2192I and that an infected individual recovers with a probability pI\u2192R.The systems dynamics under the mass-balance formulation can be approximated by the following three ordinary differential equations:95,96\n\nwhere Pt({S, I, R}) denotes the probability that an individual is on one of the states {S, I, R} at time t and Pt(A,B) is the pair joint probability to have states A and B communicating at time t; N(S) denotes the set of links of a susceptible individual. The above equations are not in a closed form. Assuming Markovian behavior of the underlying process, Pt(S,I) = Pt(S)Pt(I). Under the mean field approximation, assuming that the population is perfectly mixed and that every susceptible has the same probability of becoming infected the probabilities are equated to the expected (mean) values of the corresponding variables in the population. These assumptions lead to the following set of equations:\n\nwhere S, I, R denote expected (mean) values; a and 1/\u03b2 denote mean values of the disease transmission probability and length of the period for which an individual can transmit the disease before recovering. The above set of equations is the celebrated Kermack and McKendrick model. When a recovered individual becomes again susceptible after a period of time 1/\u03b3 then the SIRS mean field model becomes:\n\nIn the Kermack and McKendrick model, the disease becomes epidemic, i.e.,  if and only if . Hence, the number of infective will increase as long as . At  the number of infected cases reach a maximum and after this it decreases to zero. The threshold  is called the basic reproduction number (R0) and indicates whether the disease will become epidemic (if R0 > 1) or it will die out (if R0 < 1).\n\nGenerally speaking, R0 represents the average number of secondary infections produced from a single infected individual introduced into a completely susceptible population. A transmission potential index that relaxes the hypothesis of the fully susceptible population is the effective reproduction number defined as the average number of secondary infections produced from a single infected individual in a population which is already infected from a disease. The parameters of these models can be estimated using epidemic data from past periods. Within this context Coburn et al. (2009) give a review on simulating influenza including swine flu (H1N1) with SIR models.97 Nichol et al. (2010) used a SIR model to simulate influenza dynamics in a college campus and through this to assess the impact of various scenarios of vaccinations.98 Correia et al. (2011) used a SIR model to study the measles and hepatitis C in Portugal using data from 1996 until 2007.99\n\nSIR-type models have also been extended to incorporate demographics such as age distributions, mortality and spatial dependence of the spread to account for diffusion and migration effects as well as genetic mutations in the interacting populations, thus enhancing their realism.\n\nGaudart et al. (2009) addressed a modified McDonald\u2019s SIRS model to approximate the dynamics of malaria in the region Bancoumana of Mali that deployed from June 1996 to June 2001.100 The McDonald\u2019s model has been extended to incorporate the state of contagious children as well as the state of susceptible Anopheles and the state of contagious Anopheles. Magal et al. (2010)101 presented an age-dependent infection model with a mass action law, and analyze its stability using a Lyapunov function. Metcalf et al. (2011) developed a metapopulation SIR-based model including the probability of infection by age to predict the rubella dynamics in Peru.102 Ajelli et al. (2011) developed an SIR-based metapopulation model that incorporates a spatial contact matrix describing the mixing level between Italian regions.103 The authors used the model to predict the spatiotemporal dynamics of hepatitis A in the south regions of Italy. The model was fitted using weekly time series of reported rubella cases from 1997 to 2009. The same type of models have been also used to model nosocomial epidemics modeled both at the level of pathogen and host-host interactions (see, e.g., Webb et al., 2005104). In Gaudart et al. (2010), the Ross and McKendrik model has been extended to incorporate demographics and genetic changes in the populations to simulate the spread of malaria in Mali and the plague in the Middle Ages.105 The authors have employed the Archimedean copula approach to relate the risk of infection and biological age. In another study the authors have augmented the model by age classes and with a diffusion term to account for spatial effects in order to approximate the epidemic front wave dynamics of the Black Death between 1348 and 1350.106 In Demongeot et al. (2012) the Ross and McKendrik SIR model has been revised to incorporate demographic and spatial dynamics introducing continuous age classes and diffusion of both human and vectors species subpopulations within the infected zones.107 The model has been used to simulated the spread of malaria in Bancoumana, Mali.\n\nThese are usually individual-level models that relax the hypothesis of the mean field approximations of infinite population and perfect mixing introducing the uniqueness of the individual behavior including multiple heterogeneous characteristics. The main representative in the category is the discrete Markov chains (DMC). In DMC both time and states are defined on a discrete set of values. The states of the individuals change at every discrete time step in a probabilistic manner according to simple rules involving their own states and the states of their links satisfying the Markov property, i.e., that that the future values of the states at time t + \u0394t depend only on the values of the states at the previous time step t, i.e.\n\nFor example, for a stochastic SIRS-like model these transition rules may read:\n\n\u2022 Rule #1: An infected individual (I) infects a susceptible (S) link with a probability pS\u2192I = \u03bb if an active physical communication exists between them.\n\n\u2022 Rule #2: An infected individual (I) recovers with a probability pI\u2192R = \u03b4.\n\n\u2022 Rule #3: A recovered individual (R) becomes susceptible (S) with a probability pR\u2192S = \u03b3. This condition expresses the case of temporal immunity.\n\nWhen these transition probabilities remain constant in time, the Markov process is then called time homogenous Markov process. The links between individuals form the contact network through which the disease spreads. For simple DMC models this network is assumed to be a fully connected graph resulting to homogeneous mixing of individuals. For this case and in the limit of infinite number of individuals, the stochastic model can be regarded as a mean field deterministic model.\n\nFor a uniform distribution with z links per individual and in the limit of an infinite size population the governing equations read:\n\nHowever the above deterministic mean field approximations may impose important bias when the assumptions about infinite size population, homogeneous individuals, homogenous or random regular networks do not hold. Therefore, they may miss important quantitative and/or qualitative information at the coarse-grained/emergent (continuum) level. This situation worsens as the heterogeneity becomes stronger (e.g., interactions on more complex networks with finite size populations).\n\nWithin this context, a comparison between stochastic and the analogous deterministic models is given in Allen and Burgin (2000).111 Lekone et al. (2006) used a stochastic SEIR model (E stands for exposed to the disease individuals) to simulate the dynamics of Ebola outbreak in the Democratic Republic of Congo in 1995.112 Bishai et al. (2011) used a stochastic SIR model with age structure and two additional states (compartments) to describe heterogeneity in vaccination.113 The authors combined the epidemic model with an economic model incorporating the costs of the control disease policies to study the cost effectiveness of supplemental immunization activities for measles in Uganda. Wang et al. (2012) developed a stochastic model within the SIR concept to simulate and better understand the multi-periodic patterns in outbreaks of avian flu in North America.114 The model assumes random contact between individuals as well as environmental transmission of the virus.\n\nNon-markovian SIR-like models have been also proposed. These models incorporate \u201cmemory\u201d in transmission dynamics. For example, Streftaris and Gibson (2004) propose a non-markovian SIR model for the foot-and-mouth disease outbreaks.115 In their model they assume that individuals remain infected for a time drawn randomly from a two-parameter Weibull distribution. Randomization of classical deterministic SIR-like models, coming from the random, chemical kinetics to account for non-constant population with age classes due to birth and death processes and spatial demographics have been also been proposed.116 Within this context, Allen and Burgin (2000) compare the dynamics of deterministic and their counterparts stochastic epidemics models for populations with constant and variable.117\n\nOne of the most critical problems in epidemics concerns the dynamic effects of the contact network heterogeneity. Contacts between individuals evolve under numerous complicated and strongly heterogeneous modes that are influenced by a broad spectrum of factors, ranging from the pathogen inherent variability and host\u2013pathogen interaction stochasticity characterizing the transmission mechanisms of a particular disease, to the population-level ones complicated by environmental, seasonal, economic, and demographic conditions. Furthermore, in many situations the spread of an epidemic is shaped by the topology of the contact social network, and, vice versa, the dynamic evolution of the transmission network depends on the emergent dynamics of the epidemic. For example, in a severe epidemic outbreak, a change in the state of endemicity of a particular part of the population can cause a significant change in the characteristics of the transmission network (due to, e.g., link-cutting due hospitalization). Understanding this complex behavior is of outmost importance to public-health measures and policies for controlling diseases outbreaks. Vaccination, quarantine, and/or use of antiviral drugs on targeted parts of the population have to be carefully designed for the efficient combat of an emerged epidemic. Poor understanding of the infectious disease dynamics as these emerge due to heterogeneous contact interactions may result to serious negative consequences. Over the last years, there has been an intense effort in studying the interplay between the emergent dynamics of infectious diseases and the underlying topology of transmission network.\n\nWithin this context, Kuperman and Abramson (2001) showed how changes in the rewiring probability used to construct small-world networks influence the dynamics of a simple epidemic model.124 It was shown that there exists a critical value of the rewiring probability that marks the onset of a phase transition from stationary endemic situations to self-sustained oscillations. Hwang et al. (2005) studied the influence of the clustering coefficient and average path length on epidemic outbreaks evolving on scale free networks.125 Shirley and Rushton studied the impact of four different types of network topologies, namely Erd\u0151s\u2013R\u00e9nyi, regular lattices, small-world, and scale free on epidemic dynamics.126 Reppas et al. (2012) studied the influence of the path length of small world networks on the dynamics of a simple SIRS stochastic epidemic model.127 Studies on adaptive networks have only very recently begun to appear in the physics literature128 indicating that adaptation can trigger effects that are not present in other types of networks.\n\nRegarding real-world cases, Read et al. (2008) studied the impact of social networking to the spread of a communicable disease by constructing the underlying contact network from a diary-based survey from 49 adults who recorded 8661 encounters with 3528 different individuals over 14 non-consecutive days.123 Christakis and Fowler (2010) studied a flu outbreak at Harvard University in 2009.129 Following 744 students they mapped the transmission network following their friends and contacts and detected the critical nodes and links that were responsible for rapid spread and could be used as early warning detectors. In particular, by measuring several statistics of the underlying network topology, they quantified the centrality of individuals in the network, i.e., how much likely is for the disease to pass and transmitted from an individual to other individuals through the network. Salath\u00e8 et al. (2010) used wireless sensors to obtain close proximity interactions during a typical day at an American high school.130 Based on these measures, they constructed the transmission network and studied the potential of the disease to spread in terms of topological characteristics such as transitivity and average-path-length with respect to the duration of contact between students. Keeling et al. (2010) constructed two metapopulation networks based on information available from 2001 on the commuter movements between 10 000 wards in Great Britain.121 From the cattle trading system they also constructed the movement network between 150 000 farms. They consider four infectious diseases, namely influenza and smallpox in humans and foot-and-mouth disease or tuberculosis in cattle. Comparing simulations with actual data the authors raised the question if simple network models can eventually catch the influence of movements in an epidemic. Furthermore they showed that the identity of individuals in contrast to random-mover assumption can significantly influence the emergent infection dynamics. Rocha et al. (2011) simulated the spread of sexual transmitted infections using SI and SIR models evolving over the transmission network constructed from data extracted from a Brazilian Internet community where sex buyers rate their encounters with escorts.131 The network was extended over 12 cities. They showed that due to the high clustering and the distinct communities of the underling topology, the network slows down outbreaks.\n\nIn contemporary mathematical epidemiology, agent-based modeling represents the state-of-the-art for reasoning about and simulating complex epidemic systems. These take into account details such as the transportation infrastructure of the simulated area, the mobility of the population, demographics, and epidemiological aspects such as the evolution of the disease within a host and transmission between hosts (Fig. 2). Public-health epidemiologists, researchers, and policy makers are turning to these detailed models for reasons of ethics, cost, timeliness, and appropriateness. In epidemic systems, testing experimental conditions would put the safety of people at risk, creating an ethical problem. In other cases, real-time evaluation of an existing system may be prohibitively long. For example, in a disaster, simulation can be used to rapidly evaluate many previously unexamined alternatives. In all of these cases, since the real-world system under study is a complex system, multi-agent simulations are used as they are considered to incorporate the appropriate level of complexity. For example the Models of Infectious Disease Agent Study (MIDAS, https://www.epimodels.org/midas/pubglobamodel.do ), a network launched on May 1, 2004 and funded by the US National Institutes of Health has as its pilot effort the detailed modeling of the dynamics of a hypothetical flu pandemic.\n\nWithin this context, Eubank et al. (2004) addressed the use of EpiSims, a detailed agent-based simulator which incorporates data from population mobility based on TRANSIMS and epidemic models of host-pathogen and host-host interactions.132 EpiSims, developed at Los Alamos National Laboratory creates a synthetic population based on the Transportation Analysis and Simulation System (TRANSIMS, http://code.google.com/p/transims/ ). The authors simulated the spread of an infectious disease in the area of Portland, Oregon, US whose network involves 1.5 million people (nodes), 180 000 locations and a total of 1.6 million vertices. Ferguson et al. (2005) developed and presented the simulations results concerning the H5N1 influenza A pandemic in Southeast Asia.133 Their simulations involved 85 million agents residing in Thailand and a 100 km-wide zone of neighboring countries. Demographic data involving details about households, location of schools and workplaces, and population mobility where taken into account. Using the detailed agent-based simulations they evaluated the containment strategies with respect to the potential of preventing a pandemic and the distribution of drugs necessary to eradicate the spread. Burke et al. (2006) presented an agent-based model for the spread of smallpox. The model considered hypothetical towns of 6000- and 50 000 inhabitants.134 A distribution of households, workplaces, schools, and hospital units was constructed based on US demographic data. The authors investigated the efficiency of various contagion control scenarios such as vaccination of households, children at schools, isolation of infected persons and vaccination of medical staff in hospitals. Balcan et al. (2009) investigated how short-scale and long-scale contacts due to air travel can influence the spatiotemporal pattern of a pandemic.135 The authors made use of the GLEaM agent-based computational platform ( http://www.gleamviz.org/ ) consisting of three data layers: the demographic/population, the mobility-related, and the epidemic modeling layer. In this study, real-world data from 29 countries around the globe as well as air travel flowing from 3362 airports indexed by IATA were integrated into a spatial metapopulation epidemic model.\n\nOver the last years, machine learning using data extracted from internet-based communication platforms and search engines have been used to extract early indicators of social trends. Microblogging socializing services and web searching platforms have revolutionized the way private and publicly available information diffuses. Such emerging technology appears promising to data mining agents' personal behavior. For example, such services with the aid of search queries have been exploited as tools to stock-market prediction and movie box-office revenue.\n\nWithin this context Ginsberg et al. (2009) exploited the aid of search queries on the Google platform for early detection of influenza epidemic in the US.136 The authors used around 50 million Google web queries related to influenza symptoms between 2003 and 2008. A linear model using the log-odds of a visit of a physician in a certain region and the log-odds of a related search query submitted from the same region was fitted using publicly available data from the CDC\u2019s US Influenza Sentinel Provider Surveillance Network ( http://www.cdc.gov/flu/). This approach has now been realized as a surveillance web-based tool ( http://www.google.org/flutrends/ ).\n\nHulth et al. (2009) processed web queries submitted in a Swedish website related to influenza between 2005 and 2007.137 The authors fitted two models, one for relating web queries volume with the total number of laboratory verified influenza and the number of persons exhibiting influenza-like symptoms treated by physicians in Sweden. The models were used in turn to estimate outbreaks of the disease in time as well as to predict the influenza evolution. In Chan et al. (2011) a linear model was used to relate Google search queries related to dengue in Bolivia, Brazil, India, Indonesia, and Singapore using publicly available dengue cases between 2003 and 2010.138\n\nIn this paper, we discussed and presented key modeling methods used for the surveillance and forecasting of infectious disease outbreaks. Generally speaking, epidemiological models can be categorized in three classes: statistical, mathematical-mechanistic state space, and machine-learning based ones. Public-health organizations throughout the world use such models to evaluate and develop intervention disease outbreak policies for ever-emerging epidemics. Simulation allows for rapid assessment and decision making, providing quantification and insight into the spatio-temporal dynamics of a spread. An intensive inter- and multi-disciplinary research effort is speeding up the developments in the field integrating advances from epidemiology, molecular biology, computational engineering and science, and applied mathematics as well as sociology. Nowadays, molecular, sociological, demographic, and epidemiologic data are exploited to develop state-of-the-art detailed very large-scale bottom-up agent-based models aspiring to approximate the dynamics of real-world cases. Within this context, along with the available information ranging from the host-pathogen interaction level to the host-host, city, country, and globe level, complex network theory has provided the necessary \u201cglue\u201d for the systematic link between epidemiology demographics and sociology.\n\nOn one hand, for the bridging of the scales of modeling, one has to first find the appropriate observable variables for which deterministic or stochastic models can be expressed. To this direction, data mining techniques that have flourished over the last few years can be employed to extract such information. On the other hand, due to the complexity of the underlying multiscale interactions, such models are built on incomplete knowledge imported, e.g., as parameter, rule evolution, and contact network inaccuracies. Thus far, simple brute-force temporal simulations are used to study the behavior of very large scale detailed agent-based simulators in the presence of such inaccuracies. For example some of the rules and model\u2019s parameters, such as the virus pathogenicity\u2014as this may be expressed in terms of the reproduction number\u2014and different social network topologies, are examined in order to assess how such factors may influence the spread of an outbreak. However, such simple simulations are inefficient for the systematic analysis of the emergent epidemic in the parameter space. New rigorous computational methodologies, such as the equation-free multiscale framework,96,139-142 that can be used to address this issue have the potential to expedite novel computational modeling and analysis as well as to enhance our understanding and forecasting capability to combat epidemic outbreaks."}