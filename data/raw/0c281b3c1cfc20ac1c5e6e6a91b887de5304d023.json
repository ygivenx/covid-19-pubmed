{"title": "Clinical Applications of Quantitative Real-Time PCR in Virology 5", "body": "processes, small sample volumes and can be utilised in a wide variety of applications, making it the method of choice in today's molecular laboratories.\n\nThrough the aid of fluorescent signalling probes to measure amplification of DNA at each PCR cycle, at the point of exponential DNA accumulation, real-time PCR is able to provide broader linear dynamic ranges and increased assay performance as determined by sensitivity, specificity, precision, and reproducibility. Due to the consistency in signal intensity changes during the exponential growth phase of PCR, it is also easily adaptable for quantitative reporting. However, there are three properties that are uniquely associated with quantitative real-time PCR: quantification, standardisation, and lower limit resulting.\n\nThe accumulation of fluorescence signal is measured at each PCR cycle of the reaction and the cycle at which this signal exceeds a predetermined background fluorescence threshold during the logarithmic phase of amplification is referred to as the cycle threshold (C T ). The C T value is inversely proportional to the viral copy number in the specimen, and through comparisons of this value to an external calibration curve or an internal quantitation standard, the initial nucleic acid target concentration can be calculated (Heid, Stevens, Livak, & Williams, 1996; Livak & Schmittgen, 2001) . However, accurate quantitation within each sample is hindered when relying solely on an external standard as amplification efficiencies for each individual sample may be variable and inconsistent. By utilising a standard internal reference template, with the rationale that any variable influencing amplification efficiency should Example of amplification and detection of target nucleic acid by real-time PCR.\n\naffect both template and target similarly, inhibition and amplification effects are compensated for which allows for more accurate quantitation ( Figure 2 ). This control can be further enhanced when incorporating an internal reference that utilises the same primer sequence as the target since any potential additional effects on PCR efficiency for each of the two targets is eliminated. Thus, the competitive real-time PCR strategy is the most reliable approach for nucleic acid quantitation (Diviacco et al., 1992; Gilliland, Perrin, & Bunn, 1990; Stieger, Demolliere, Ahlborn-Laake, & Mous, 1991; Wang, Doyle, & Mark, 1989; Zentilin & Giacca, 2007) and is the basis for the majority of present-day virology assays.\n\nIt is equally important to utilise appropriate quantitation standards, when available, to ensure accurate quantitative results, inter-laboratory correlation, and overall standardisation. Standardisation of reported viral loads ensures not only interlaboratory consistency but also high clinical utility of viral load monitoring, sets the foundation for establishing clinical correlations and critical thresholds leading to better management of infections and treatments, and are critical for the development of clinical guidelines (Miller et al., 2011) . With the wide availability of assay methods, viral targets, specimen type, and lack of standard reference material (Hayden et al., 2012) , viral load variability across laboratories can range significantly, as high as 4.3 log copies/mL . Specifically, results from proficiency testing/external quality assessment programmes as well as interlaboratory specimen exchange studies have demonstrated that there is significant variability in quantitative results for assays that lack appropriate standards Quantitation of viral target using competitive quantitation standard (QS). The QS compensates for effects of inhibition and controls the preparation and amplification processes, allowing a more accurate quantitation viral target in each specimen. The competitive QS contains sequences with identical primer binding sites as the viral target to ensure equivalent amplification efficiency and a unique probe binding region that distinguishes the two amplicons. The competitive QS is added to each specimen at a known copy number and is carried through the subsequent steps of specimen preparation, reverse transcription (when applicable), simultaneous PCR amplification, and detection. Viral target concentration in the test specimens is calculated by comparing the viral target signal (solid line) to the QS signal (dashed line) for each specimen and control (A, B) . In the presence of inhibitors, both QS and viral target are equally suppressed and yield accurate viral load calculations (C). 1 Introduction (Hayden et al., 2008; Pang et al., 2009; Preiksaitis et al., 2009; Wolff, Heaney, Neuwald, Stelrecht, & Press, 2009 ). Findings such as these reinforce the fact that with this high degree of variability and discrepancy, clinicians are unable to compare test results between two different laboratories and, further, clinically relevant cut-offs set by one test would not apply to results of another . Without standardisation, the quality of patient care is dramatically impacted, preventing meaningful inter-laboratory comparison of patient results and influencing disease prevention and management programmes (Kraft, Armstrong, & Caliendo, 2012) . This is especially critical for transplant patients, who may be initially monitored at one institution and then transferred to another for longer-term follow-up receiving results that no longer correlate. Therefore, whenever possible, viral load monitoring tests must report results in IU/mL and be fully traceable to the higherorder first WHO International standard. They must generate highly accurate and reliable results based on a robust calibration methodology and have excellent reproducibility across the dynamic range of the test with demonstrated co-linearity to the WHO standard.\n\nLastly, there exist two distinct end-points with quantitative real-time PCR, which should be of consideration for result interpretation and reporting: the lower limit of detection (LLOD/LOD) and the lower limit of quantitation (LLOQ/LOQ). These two limits are assessed differently and are not equivalent in either definition or, in some cases, their assigned values. The LOD (also referred to as analytical sensitivity) represents the lowest viral load level at which !95% of tested samples are detected (CLSI EP17-A, 2004); theoretically, viral levels at or below the LOD are not detected !5% of the time. It differentiates between 'detectable' and 'undetectable' results. The LLOQ, on the other hand, is the lowest viral level that is within the linear and analytically acceptable range of the assay (CLSI EP17-A, 2004) . In other words, the LLOQ is the lowest point at which an accurate viral load can be assigned and determines which 'detectable' sample will have a reported viral load. A common misconception is that the LOD of the assay is the minimum viral level for a 'detected' result but 'undetectable' and 'detectable' viral levels are never differentiated by a single theoretical viral threshold as viral levels less than the LOD may still have a high probability of being detected. This probability spans a broad range in which the lower the viral titre, the more likely the 'undetectable' result. Ultimately, the statistical probability will favour the 'undetectable' result ( Figure 3 ). And because the LLOQ can be equal or greater than the LOD on some viral load assays, it is not unusual for 'detectable but below the LOQ' (detectable/BLOQ) result reporting (Cobb et al., 2011) . Further, the 'detectable/BLOQ' results should not be inferred that the actual viral concentration of the sample is between the LOD and LOQ.\n\nThe clinical demand has driven and shaped the evolution of PCR and continues to do so as we gain a greater understanding of the infections we monitor and treat. Through the study of the natural history and disease progression attributed to specific viral infections, the need for sensitive, accurate, precise, reproducible, and reliable quantitative measurements of viral levels has become a necessity.\n\nWith the deeper understanding of the natural history of human immunodeficiency virus (HIV) infections, it is now well understood that progressive immunosuppression and the onset and development of clinical disease are strictly associated with increasing viral burden (Furtado, Kingsley, & Wolinsky, 1995; Ho, Moudgil, & Alam, 1989; Mathez et al., 1990; Nicholson et al., 1989; Schnittman et al., 1990) . Thus, quantitative real-time PCR is critical for monitoring patients infected with HIV (Hufert et al., 1991; Mellors et al., 1995) and those undergoing antiretroviral therapy (ART) to ensure viral replication is sufficiently and effectively suppressed and to monitor potential for viral resistance to the medication (DHHS HIV, 2014) . This monitoring and maintained viral suppression is absolutely necessary not only to maintain progression-free survival of HIV-infected patients but also to reduce subsequent HIV transmission (Cohen et al., 2011; Diffenbach, 2012) . Due to the significance of viral load monitoring and maintaining viral suppression, the demand for Likelihoods of different test results given different viral concentration. When the viral concentration tends to 0, the proportion of 'Target not Detected' increases to 1 (dotted line), increasing the likelihood of 'Not Detected' results. As the concentration tends to LLOQ (dashed line), the likelihood of 'Detected but <LLOQ' results peaks. When the concentration tends to infinity, the proportion of quantitative results tends to 1 (solid line), resulting in a continual increase in the likelihood of 'Detected: Quantitative' results. At any concentration, the sum of the three types of reported results is always 100% and throughout the concentration continuum, variations in result reporting exist. As the concentration of viral levels approaches the LLOQ, near equal likelihood of 'Detected: Quantitative' and 'Detected but <LLOQ' results are possible, as are 'Not detected' results. Decreasing concentrations will further shift the likelihoods and increase the chance of 'Detected but <LLOQ' or 'Not Detected' results. Diagram assumes LLOQ \u00bc LLOD.\n\nincreasingly more sensitive assays for HIV has driven the innovation of diagnostic tests and continues to push the limits of the LOD and LOQ ever lower (Glaubitz et al., 2011; Sizmann et al., 2010) .\n\nAdditionally, the amount of viral DNA in samples from either hepatitis B virus (HBV) or cytomegalovirus (CMV) chronic carriers is indicative of active viral replication in liver cells and is correlated with liver disease progression (Chen, Lin, et al., 2006; Chen, Yang, et al., 2006; Emery et al., 2000; Humar et al., 1999; Humar, Kumar, Boivin, & Caliendo, 2002; Iloeje et al., 2006; Wursthorn, Manns, & Wedemeyer, 2008) , highlighting the need for quantitative viral levels in chronic disease monitoring. Similarly, hepatitis C virus (HCV) levels have been linked to prognosis and treatment outcomes (Trepo, 2000) , treatment response (Pearlman & Ehleben, 2014; Zeuzem et al., 2009) , as well as assessing sustained virologic response (SVR), or 'cure', following treatment (Pearlman & Traub, 2011) . More recently, as new antiviral therapies for HCV treatment rely on targeting specific biological steps of the viral replication cycle, reliable quantitative monitoring of the viral burden at specific time-points to ensure treatment compliance and resistance emergence during treatment is recommended (Au & Pockros, 2014 ; AASLD/IDSA/IAS-USA, 2014).\n\nAlongside the changing clinical needs, several instrument and manufacturing innovations have been introduced to meet these newer requirements. The first PCRbased diagnostic test and the first automated system were both introduced in the early 1990s, allowing for improved standardised technique, increased efficiency, and reduction in error and contamination. In 1996, to meet the clinical needs of monitoring patients infected with HIV, the first 'personalised healthcare' diagnostic test was FDA approved, that monitored whether ART was working and whether a patient was on optimal treatment. Entire systems were introduced in the early 2000s that automated the up-front sample preparation step, leading to further reduction of hands-on time, increased reliability and reproducibility. With the introduction of real-time PCR techniques to further enhance assay performance, the first PCR tests that allowed for simultaneous amplification and detection were introduced in 2003. Throughout the decade, improvements in assays and instrumentation began pushing the boundary for HIV-1 detection, achieving greater and greater sensitivity, a feat that has proven to be incredibly relevant in understanding risks of viral load rebound and virologic failure (Doyle et al., 2012; Estevez et al., 2013; Pascual-Pareja et al., 2010) . More recently, shortly after the release of two new higher-order standards for CMV (first WHO International Standard and NIST CMV Standard), fully automated real-time quantitative PCR assays were FDA approved to meet laboratory and clinical demands for standardisation. Not only does the instrumentation reduce assay design variability and inconsistency across laboratories, but also the assays are standardised to the WHO and report in international units, providing accuracy across the entire dynamic range that is only achieved by calibration and traceability to these higher-order standards. But it is not until these FDA-approved tests gain widespread use will inter-laboratory agreement for CMV viral load results truly improve (Kraft et al., 2012) .\n\nThe objective of test innovation is to evolve and adapt to clinical and laboratory needs. As patient outcome gaps are identified, and as we gain greater understanding and insight into the clinical progression of disease and disease management, technological achievements facilitate advancements in quality of diagnostics and patient outcomes. Life-threatening illnesses convert to chronic/manageable disease with the aid of viral load monitoring. And pressure exists to develop more precise, accurate, sensitive assays, which, in turn, drives the development of more efficacious drugs. This synergy demonstrates the value of diagnostics: it is an integral part of the patient care continuum.\n\nApplications of quantitative real-time PCR for virology are extensive. It is especially necessary when antibody seroconversion is delayed after an acute infection and early diagnosis are essential (DiBiasi & Tyler, 2004; Thomson et al., 2009) , in immunocompromised patients that may not have an optimal antibody response (Kadmon et al., 2013) , and for the diagnosis of congenital or perinatally acquired viral infections (Park, Streicher, & Rothberg, 1987; Young, Nelson, & Good, 1990) . Additionally, it is critical in maintaining a high level of patient care at each stage of the disease and infection (Figure 4 ).\n\nScreening tests are designed with one key parameter in mind: exceptional sensitivity (Herman, Gill, Eng, & Fajardo, 2002) . They must ensure that disease is not missed in a population that is generally free from risk to allow for appropriate early intervention, thereby effectively reducing mortality and morbidity. Although traditional Applications of real-time PCR in the continuum of care and patient management.\n\npopulation-based screening programmes and recommendations do not often utilise nucleic acid testing (NAT) for virology targets, relying more on immunoassays and antigen testing, NATs and real-time PCR assays are still integral components.\n\nDonor-eligibility determination ensures that a donor is eligible to donate cells or tissues to be used as human cells, tissues, and cellular and tissue-based products (HCT/Ps), which can include haematopoietic stem/progenitor cell, organ, semen, and other types of donations. In part, living and cadaveric HCT/P donor eligibility is granted if screening shows that the donor is free from risk factors for, and clinical evidence of, infection due to relevant communicable disease agents and diseases such as HIV type 1 and 2, HBV, HCV, and West Nile Virus (WNV) (FDA Testing, 2014) . The FDA testing recommendations stipulate that HCT/Ps tests for HIV and HCV may include FDA-licensed NAT blood donor screening and that, specifically, an FDA-licensed NAT should be used to assess infection with WNV. Despite the fact that these NAT tests provide qualitative as opposed to quantitative results, the molecular technology utilised is often real-time PCR due to its enhanced accurate and reliable resulting (FDA Assays, 2014) .\n\nAdditionally, pre-emptive virology screening post-transplant may reduce subsequent complications and provide a more cost-effective management strategy (Evers, 2013) . Pre-emptive therapy utilises routine viral screening to initiate therapy at the first indications of viraemia, prior to clinical manifestation of disease. This strategy reduces the overall morbidity and mortality post-transplant compared to strategies in which treatment is initiated at the onset of clinical disease (Sch\u20ac onberger et al., 2010). The pre-emptive treatment model utilising routine screening by quantitative realtime PCR of asymptomatic post-transplant patients has been shown to be especially effective for paediatric patients undergoing haematopoietic stem cell transplant, who are uniquely at a high risk of CMV and Epstein-Barr virus (EBV) infections (Evers, 2013) . This strategy and prospective screening utilising a quantitative real-time PCR assay for BK polyoma virus (BKV) is recommended as part of routine posttransplant follow-up of all kidney transplants since early identification and management of BKV infection may prevent future incidence of polyoma virus-associated nephropathy (Hirsch et al., 2005; KDIGO, 2009) .\n\nNATs and real-time PCR are also utilised as vital parts of certain screening algorithms. The Center for Disease Control (CDC) currently recommends that HCV RNA testing be utilised for anyone who may have been exposed to HCV within the preceding 6 months. In addition, NATs would identify active HCV infection among persons who have tested anti-HCV positive or those with an indeterminate antibody test indicating need for referral for further medical evaluation and care (CDC HCV, 2013).\n\nViral diagnostic tests are used to determine presence or absence of current or previous infection. Because many viral infections present with similar symptoms, accurate diagnosis is critical as each requires unique and vastly different interventions and/or management strategies. Historically, diagnosis of viral infections has relied on viral growth in cell culture, immunoassays, antigen assays (ELISA), haemagglutination testing, and electron microscopy (Krishna & Cunnion, 2012) . The introduction of molecular methods including NATs and real-time PCR assays has vastly improved viral diagnosis with their superior sensitivity, specificity, and rapid result reporting (Emmadi et al., 2011) . NAT-based infectious disease testing, providing rapid results, aids in outbreak detection (Ebola), genotype identification (HCV), and identification of possible drug resistance (HIV-1), which can lead to rapid clinical therapeutic decisions and early infection control to prevent spread of disease (Espy et al., 2006) . Viral culture was traditionally considered to be the 'gold standard' for viral diagnosis because of increased sensitivity compared to rapid antigen-testing methods. However, a significant limitation of viral culture was a long time to result, reaching 14 days for CMV (Gleaves, Smith, Shuster, & Pearson, 1985) . Improvements in culture included the introduction of shell viral culture, which reduced the result turnaround-time (TAT) for CMV to 24 h. Although considered to be a vast improvement, certain viruses require immediate treatment intervention to prevent life-threatening infection and therefore, a 24-h TAT is simply much too long. Further, reliance on viral culture for diagnostics testing introduces other pronounced drawbacks, the most noteworthy being that not all routine viruses grow in culture and that virus viability, and thus its culture ability, could be impacted by sample collection, transport conditions, or prior patient treatment. With these limitations in mind, NAT testing has tremendous advantages and has replaced culture for most viruses due to its greatly reduced TAT (typically less than 8 h) while still retaining high sensitivity.\n\nThe applications of NAT testing, and more specifically, real-time quantitative PCR technology, in viral diagnostics and confirmatory testing continue to expand. The CDC-updated recommendations for HIV testing state that specimens that are reactive on the initial antigen/antibody combination immunoassay and non-reactive or indeterminate on the HIV-1/HIV-2 antibody differentiation immunoassay should be tested with an FDA-approved HIV-1 NAT test (Branson, 2010; Branson et al., 2014) . Additionally, NATs have demonstrated utility in high-risk populations, in which antibody testing alone might miss a considerable percentage of HIV infections that are otherwise detectable by NAT virologic tests (Priddy et al. 2007; Stekler et al., 2009) . Particularly, immunoassays for HIV diagnosis are limited by the marked delay between infection and seroconversion, a time when HIV viral levels are at their peak ( Figure 5 ). For this reason, NATs are the recommended method for diagnosis of HIV during the acute phase of infection (10-50 days post-infection). HIV viral RNA is the first marker to manifest itself approximately 10 days post-infection at initiation of the acute phase of infection (Lindback et al., 2000) . Not until 4-10 days after initial detection of HIV RNA do the HIVp24 antigen levels rise to detectable levels using fourth-generation immunoassays. This marked delay and the need for rapid diagnosis by the identification of HIV RNA is especially critical when an early diagnosis is medically warranted. During the acute phase of the HIV infection, patients typically present with flu-like symptoms, which can often lead to a misdiagnosis. Misdiagnosing those at the highest risk of infection such as prison inmates, IV drug users, and men who have sex with men (MSM), also increases the risk of further disease spreading (Chu & Selwyn, 2010) . Therefore, the use of a real-time PCR testing method that is able to identify the presence of the HIV virus at this initial stage is critical.\n\nAdditionally, NATs are critical for early diagnosis of congenital or perinatally acquired viral infections, especially infants born to HIV-infected mothers (Tang & Ou, 2012) . The maternal antibodies against HIV can persist in exposed infants for up to 18 months of age, which, in turn, prevents the use of antibody-based assays for early diagnosis of infection. There is a high level of morbidity and mortality during the first 2 years of life for infected infants; therefore, it is of high importance to determine infection status quickly in the exposed infant to implement the appropriate ART therapy early (van Rossum, Fraaij, & de Groot, 2002) . In addition to HIV, additional viruses have been demonstrated to exhibit perinatal transmission including respiratory virus, CMV, herpes simplex virus (HSV), varicella zoster virus (VZV), HBV, enterovirus, rotavirus, and human papilloma virus (Prober et al., 1987) . For all these, NAT testing may serve as a valuable tool for early intervention, especially in this critical neonate population that lacks fully developed immune systems. In addition to their applications for HIV diagnosis, NAT testing is also utilised as a diagnostic for HCV. Although diagnostic HCV RNA real-time PCR tests are predominately used as confirmation of a positive HCV antibody result (CDC HCV, 2012), NAT testing is recommended for the confirmation of a non-reactive HCV antibody test for patients suspected of having recent HCV exposure. HCV RNA can be detected as early as 2 weeks post-exposure, whereas HCV antibodies are not detectable until 8-12 weeks post-infection (Ghany, Strader, Thomas, & Seeff, 2009 ). Therefore, similar to strategies outlined for HIV, NAT testing is the most suitable for detection and diagnosis during the acute phase of HCV infection. Further, immunocompromised patients, those with weakened immune systems can include those that are HIV positive, on immunosuppressive drugs, or undergoing chemotherapy, may lack the ability to generate an appropriate immune response required for an anti-HCV test. In these special circumstances, the CDC recommends considering HCV RNA testing for diagnosing viral infections.\n\nRapid diagnostic testing is also very important for viruses with high potential to create an epidemic or outbreak. Throughout history, influenza has caused many such outbreaks, the most notable being the 1918 Spanish flu thought to be responsible for an estimated 50 million deaths (Taubenberger & Morens, 2006) . In response to the possibility of future outbreaks, the World Health Organization (WHO) has established that PCR-based influenza testing is now the first-choice diagnostic test for both humans and animals (WHO Influenza, 2011) . The conversion to molecular tests was based on the fact that these assays are more sensitive and specific for detecting influenza viruses compared to other non-molecular methods. NAT methods also have a lower likelihood of false positive or false negative results, and therefore, result interpretation is less impacted by community-based influenza prevalence (CDC Influenza, 2014) . The introduction of rapid molecular testing, which can provide results in as little as 15 min, is extremely beneficial especially in hospitals, nursing homes, and chronic care facilities where early influenza identification can prevent outbreaks.\n\nThe number of molecular-based diagnostic tests has expanded even further with the introduction of tests for WNV, respiratory syncytial virus, HSV, rotavirus, and even more recently, Ebola. As development and improvements continue to be made to real-time PCR technology that will reduce both cost and time to result even further, the number of NAT diagnostic tests will continue to increase and the applications for real-time PCR in diagnostics will also expand.\n\nOnce a patient is appropriately diagnosed and linked to care, partly through the aid of quantitative real-time PCR, clinicians will often consider several baseline factors, which collectively help to guide therapeutic decision. These decisions are based on a series of questions including: Are treatment options available? What treatment regimen should be prescribed? For how long will the patient need to be on therapy? Factors influencing these therapeutic decisions include disease complications, patient predisposition, prior treatment experience, viral genotype, viral resistance profile, and host genetic profile, among others. And depending on the viral infection, a quantitative baseline viral load may also serve an important role in helping to guide treatment decision (Table 1) .\n\nOver the past 25 years, pharmaceutical development and clinical trials investigating cutting-edge antiviral treatments have relied heavily on the data generated from PCR-based-and eventually quantitative real-time PCR-based-technology to determine safe and effective drug use (Cobb et al., 2011; MacKay, Arden, & Nitsche, 2002) . Practice guidelines continue to reference registrational and nonregistrational studies utilising quantitative real-time PCR to guide both clinicians and laboratories in the proper implementation of treatment and testing in order to deliver the most effective personalised care to patients (AASLD/IDSA/IAS-USA, Kotton et al., 2013) . Because of this extensive co-utilisation of real-time PCR, it is widely accepted as the gold-standard technology for measuring a patient's quantitative viral load before, during, and after the course of treatment.\n\nOnce the decision to initiate treatment for chronic HCV infection is made, several baseline factors are routinely considered (AASLD/IDSA/IAS-USA, 2014). Among these are complications like liver fibrosis stage, co-infection with HIV, hepatocellular carcinoma, end-stage liver disease, genetic variations like HCV genotype, subtype and resistance markers, and prior treatment experience and outcome. The association of baseline viral load, measured by quantitative real-time PCR, to chronic HCV treatment outcome has been well documented with earlier therapies (pegylated-interferon plus ribavirin) but, given the lack of alternative therapeutic options, has not been recommended as a therapeutic decision factor (Jensen et al., 2006; Pawlotsky, 2012) . Historically, practice guidelines have recommended the measurement of baseline viral load to serve only as an initial time-point required for effective monitoring of treatment without any prognostic indication (Yee, Currie, Darling, & Wright, 2006) . Currently, tremendous advancements in the treatment of chronic HCV infection that employ direct-acting antivirals (DAAs) reported cure rates (as determined by SVR) of >90% for even the once most difficult to treat HCV genotype-1 patients, the most predominant in the United States. Because of this high potency of these drugs across patient populations and the greater importance of numerous other factors, including HCV genotype and prior treatment experience, in determining the appropriate course of treatment, the most recent AASLD/IDSA practice guidelines still do not recommend a baseline quantitative viral load as a therapeutic decision factor.\n\nHowever, in the rapidly evolving field of HCV treatment, the recent FDA approval of a fixed-dose combination drug consisting of two DAAs (sofosbuvir and ledipasvir) for the treatment of HCV genotype-1, the manufacturer's drug label now includes a new indication for quantitative real-time PCR. It is indicated that treatment na\u00efve and non-cirrhotic patients with a specific baseline viral load are eligible for shortened therapy, an indication with tremendous implications. According to the prescribing information, patients with a baseline viral load below 6 million IU/mL are eligible to have shorter therapy duration of 8 weeks, much shorter than the 12-or 24-week duration for other patient populations (HARVONI, 2014) . This therapeutic decision practice is the first of its kind in treatment of chronic HCV infection and is likely to be a recurring theme as DAA manufacturers strive to develop high efficacy regimens requiring shorter treatment durations. Additionally, shorter treatment durations are more favourable to patients and payers when considering the cost of achieving SVR with DAAs and may improve patient drug adherence and completion of therapy (Hep C Online, 2014) . As much as quantitative real-time PCR helped to develop this claim for this particular regimen, this technology will also be employed by numerous laboratories to aid in this part of therapeutic decision.\n\nIn contrast to chronic infection, treatment of patients presenting in the acute phase of HCV infection, within the first 6 months after exposure, is not recommended by AASLD/IDSA for patients in whom HCV infection spontaneously clears (AASLD/ IDSA/IAS-USA, 2014). Therefore, careful monitoring of HCV RNA by a sensitive nucleic acid test is required in order to confirm spontaneous clearance, defined as HCV RNA negative at two specific measurements. Quantitative and qualitative real-time PCR assays are both widely used for this purpose, given their comparable sensitivity.\n\nFactors influencing ART decision for HIV-infected patients include determination of pregnancy, AIDS-defining conditions, acute opportunistic infections, low CD4 counts, HIV-associated nephropathy, potential drug interactions, co-infection with HCV or HBV, HIV resistance testing, and prior treatment experience (DHHS HIV, 2014). Plasma HIV RNA viral load, performed widely by quantitative real-time PCR, is also recommended as a pre-ART decision factor specifically for treatment na\u00efve patients. The Department of Health and Human Services (DHHS HIV) recommends that only ART-na\u00efve patients with a plasma HIV viral load below 100,000 cp/mL can be prescribed various regimen options, which they otherwise should be restricted from taking with higher viral load. This is primarily due to inferior virologic responses in patients with higher viral loads observed in clinical studies (Sax et al., 2009) . These clinical trial studies employed quantitative real-time PCR in order to help determine this cut-off and many labs have utilised the same technology to help guide HIV-treating clinicians in this decision.\n\nIn the case of chronic HBV infection, several studies have shown that Hepatitis B 'e' antigen (HBeAg) and high levels of HBV DNA are independent risk factors for the subsequent development of cirrhosis and hepatocellular carcinoma (Chen, Lin, et al., 2006; Chen, Yang, et al., 2006; Iloeje et al., 2006) . However, due to the fluctuating nature of chronic HBV infection, the prognostic utility of one high HBV DNA level at a single time-point is limited. Thus, HBV baseline DNA viral load, along with HBeAg, alanine aminotransferase (ALT) levels, and fibrosis, collectively aids in the decision to treat with antiviral agents as well as which HBV antiviral regimen to choose and duration of treatment (Lok & McMahon, 2009 ). Typically, patients with an HBV DNA viral load >20,000 IU/mL, signs of liver disease (i.e. high ALT levels and/or significant fibrosis), and loss of HBeAg are considered for immediate treatment with antivirals, whereas patients <2000 IU/mL are closely monitored for viral load changes prior to treatment. Patients who fall in between this range are monitored for persistent viraemia and signs of liver disease before deciding to treat. Quantitative real-time PCR, therefore, plays a crucial role in the care of chronic HBV patients who, if not treated at the appropriate time with the appropriate regimen and duration, are at greater risk of liver complications.\n\nUnlike treatment guidelines for HCV, HIV, and HBV, management of CMV after solid organ transplant is not associated with specific quantitative CMV viral load cutoffs in order to make therapeutic decisions (Kotton et al., 2013) . This is partly due to the historical lack of an international standard and varying assay designs, which has led to poor inter-institutional correlation of quantitative NATs. In addition, the widespread practice of universal prophylaxis, where CMV antiviral medication is administered to patients early in the post-transplant period and continued for a finite period of time, has diminished the clinical utility of baseline viral loads for making therapeutic decisions. However, with the recent availability of the WHO CMV International Reference Standard, the establishment of viral load cut-offs that can be applied to pre-emptive monitoring of patients prior to treatment initiation may soon become more widely accepted . Until then, institutions are required to determine their own test performance characteristics and clinical cut-offs.\n\nSeveral studies have shown that a low CMV virologic threshold (e.g. detectable viraemia) using quantitative real-time PCR should be used for starting pre-emptive therapy especially in high-risk cases where the organ donor screens positive and the receptor screens negative for CMV serology (Atabani et al., 2012; Couzi et al., 2012; Sun, Cacciarelli, Wagener, & Singh, 2010) . Among a variety of baseline risk factors that may indicate longer CMV treatment duration, significant predictive value has been demonstrated with higher baseline viral loads where longer treatment duration may prevent CMV disease relapse (Kotton et al., 2013; Sia et al., 2000) . Clinical trial studies supporting the recent FDA approval of a quantitative real-time PCR CMV test calibrated to the WHO International Standard also demonstrated clinical value for baseline testing of patients with CMV disease who are undergoing treatment with the anti-CMV drugs ganciclovir or valganciclovir (Razonable et al., 2013) . Data from this study suggested that patients with a baseline CMV viral load <18,200 IU/mL are likely to resolve CMV disease more rapidly than those who have a higher baseline viral load. Further studies are needed to determine universal thresholds for pre-emptive therapy initiation and predictive value for CMV baseline viral load in defining optimal treatment duration.\n\nThere exists a clear application for quantitative real-time PCR technology in baseline determination of patients with significant viral infections, and in fact, quantitative viral load determination plays a critical role in therapeutic decision for many other viral infections. High baseline viral load has been shown to correlate with advanced disease during infection with numerous viruses such as BKV, HSV-1, EBV, and Adenovirus and may potentiate the need for longer duration therapies in certain scenarios (Cincinnati Children's Hospital Medical Center, 2012; Domingues, Lakeman, Mayo, & Whitley, 1998; Gustafson et al., 2008; Randhawa et al., 2004) .\n\nAfter the patient's baseline assessment or pre-emptive monitoring suggests if treatment is available, which treatment regimen to choose and perhaps the duration of therapy, the patient can move on to therapeutic administration. Quantitative realtime PCR has helped and continues to set the stage for decisions that potentially saves lives, reduces complications, decreases morbidity, and lessens the economic burden to both the patient and the healthcare system.\n\nSerial measures of viral load serve as an individualised map of a viral infection through the estimation of the amount of virus found within an infected person. Tracking viral load in the continuum of care is a vital tool used predominantly to monitor treatment response and its effectiveness, early signs of resistance emergence during therapy of chronic viral infections, and viral activation or reactivation in immunocompromised patients following bone marrow or solid organ transplantation.\n\nWhile the goal of treatment for chronic HCV infection is SVR, patients may fail therapy due to non-response, on-treatment breakthrough, or post-treatment relapse ( Figure 6 ). The early change in quantitative viral load over time may be predictive of treatment efficacy and a shorten therapy for patients who respond rapidly to treatment (Yee et al., 2006) . This 'response-guided therapy' (RGT) is best exemplified during treatment of chronic HCV patients. Specifically, the sooner a patient becomes HCV RNA undetectable during treatment, the lower the relapse rate when treatment is shortened. Conversely, the longer it takes for a patient to become HCV RNA undetectable, the longer they need to remain on treatment to limit relapse. However, given the poorer efficacy of earlier regimens, not all patients who received therapy achieved SVR. For this reason, 'futility rules' or 'stopping rules' were also developed, which required that failure of a patient to respond (target not detected or viral load cutoff ) by a given time-point indicated the need to immediately discontinue therapy. Monitoring HCV viral loads during treatment. Despite advances in treatment for HCV patients, failure to achieve SVR is still a reality. Patients who do not achieve SVR fall into four categories:\n\n(1) null responders (black line) achieve less than 2-log decrease in hepatitis C viral load upon treatment;\n\n(2) partial responders (red line; light grey in the print version) experiences at least a 2-log decrease in hepatitis C viral load during HCV treatment but fail to proceed to an undetectable viral load level; (3) breakthrough patients (orange line; light grey in the print version) have an undetectable HCV viral load, but the virus rebounded during treatment; (4) relapsers (blue line; dark grey in the print version) have had an undetectable HCV viral load, but the virus rebounded after they completed HCV treatment.\n\nAlthough these RGT notions were originally developed from observations made during treatment with the older therapies, peg-IFN and ribavirin, RGT was also required during treatment with the much more potent first-generation DAAs, telaprevir and boceprevir, and stopping rules were put in place during treatment with the second-generation DAA, simeprevir (AASLD/IDSA/IAS-USA, 2014; Ghany et al., 2009; Yee et al., 2006) . Newer IFN-free DAA regimens targeting HCV, which are better tolerated by patients and by virtue of the targets they inhibit, have a higher barrier to resistance, yield more rapidly declining viral kinetics, and, thus, do not contain treatment indications for RGT in their prescribing information (HARVONI, 2014; OLYSIO, 2014; SOVALDI, 2013; VIEKIRA, 2014) . While RGT was a major driver for regular viral load monitoring during antiviral therapy, it is not the only reason to monitor HCV viral load. In the interval between baseline measurement and assessment of SVR, the 2014 AASLD/IDSA guidelines also include recommendations for monitoring initial response (week 4 on treatment with a repeat at week 6 if detectable) and end of treatment in order to provide an assessment of drug compliance/early efficacy and predict treatment outcomes, respectively (AASLD/IDSA/IAS-USA, 2014). In the most recent revision to these Web-based guidelines, it is recommended that an HCV viral load increase of greater than 10-fold on repeat testing at week 6 (or thereafter) should prompt a discontinuation of HCV treatment. Many clinicians also closely monitor and report the declining viral loads to their patients in order to demonstrate treatment efficacy, motivating patients to continue treatment and remain adherent to the drug regimen until the next follow-up appointment (Fusfeld et al., 2013) . Regardless of monitoring during HCV treatment for RGT, adherence/compliance, patient motivation, early treatment efficacy, etc., quantitative real-time PCR is widely used by laboratories due to its sensitivity, accuracy, and reproducibility of each consecutive viral load test.\n\nFor patients infected with chronic viral infections, such as HIV, the lifelong regimen of highly active ART aims to suppress HIV viral levels to near undetectable levels, ensuring progression-free survival (delay or all together prevention of the progression to AIDS) and reducing potential transmission. Alongside monitoring immune function and immunologic efficacy through CD4 T-cell count, HIV viral levels are critical in the clinical evaluation and assessment of HIV-infected patients undergoing ART. Determining a patient's HIV viral load is indicated prior to entry into care, at the initiation of ART, at 2-8 weeks after ART initiation, and then typically every 3-4 months while on treatment: (1) to establish a baseline level of HIV viral load; (2) to establish viral response to the therapy to assess the virologic efficacy of ART; and (3) to monitor for abnormalities that may be associated with antiretroviral drugs (DHHS HIV, 2014) .\n\nThe baseline HIV viral load is not only linked to treatment options (Sax et al., 2009) but also helps to establish the magnitude of viral load decline after initiation of ART and provides prognostic information about the probability of progression to AIDS or death (Marschner et al., 1998; Murray, Elashoff, Iacono-Connors, Cvetkovich, & Struble, 1999; Thiebaut et al., 2000) . Once treatment is initiated, the goal is to reach and maintain suppressed HIV replication as determined by undetected viral levels utilising highly sensitive NAT tests, which is generally achieved within 8-24 weeks after ART initiation. The need for sensitive assays to effectively assess viral suppression hinges on the need to suppress HIV replication to the extent that viral evolution and drug resistance mutations do not emerge, which typically do not occur in patients whose HIV RNA levels are maintained below the LLOD of current real-time quantitative PCR assays (Kieffer et al., 2004) .\n\nDue to the introduction of more sensitive real-time PCR assays, which can detect as few as 20 viral copies/mL, natural variability in HIV viral levels over time, even in patients with effective suppression, is much more evident (Lima, Harrigan, & Montaner, 2009; Gatanaga et al., 2009; Willig et al., 2010) . Although controversy exists between the clinical significance of viral loads between LLOD and <200 copies/ mL, there are reports suggesting that this low-level viraemia is predictive of virologic rebound (Doyle et al., 2012; Eron et al., 2013; Laprise, de Pokomandy, Baril, Dufresne, & Trottier, 2013) , virologic failure (Estevez et al., 2013) , and indication of drug resistance (Taiwo et al., 2010) , signifying the need for highly sensitive assays. Viraemic blips, a single detectable HIV viral load (<500 copies/mL) in an otherwise seemingly suppressed patient (Figure 7) , however, do not indicate subsequent virologic failure or development of resistance mutations (Castro et al., 2013; Lee, Kieffer, Siliciano, & Nettles, 2006; Nettles et al., 2005) . Blips are not unusual (Havlir et al., 2001) and appear to be more common in winter, suggesting that host-related and seasonal factors are associated with the occurrence of viraemia (van Sighem et al., 2008) . On the other hand, persistent HIV RNA levels !200 copies/mL are often evidence of viral evolution and accumulation of drug resistance On-treatment HIV patient monitoring. (A) HIV viral loads will fluctuate as patients are on treatment, and, in most instances, will remain 'undetectable' (at or below dotted line); viral 'blips' are not uncommon and will result in transient 'detectable' and even quantifiable results (above the dashed line). (B) Virologic failure will lead to a sustained high-level viral titre that, without intervention, will increase with time.\n\nmutations (Aleman, Soderbarg, Visco-Comandini, Sitbon, & Sonnerborg, 2002; Karlsson et al., 2004) . Once treatment failure is confirmed, immediate intervention is recommended to avoid progressive accumulation of resistance mutations and effective response of new regimen (DHHS HIV, 2014), which is benefited by low HIV RNA levels and/or higher CD4 cell counts (Eron et al., 2013) , and even a brief interruption in therapy may lead to a rapid increase in HIV RNA and a decrease in CD4 cell count and increases the risk of clinical progression (Deeks et al., 2001; Lawrence et al., 2003) . With the development and administration of newer drugs that target specific biological processes of HIV, routine and clinical monitoring of viral loads using a real-time quantitative PCR assay continues to be critical to predict treatment failure and early emergence of drug resistance mutations, within a timeframe that would increase subsequent treatment success.\n\nViral load monitoring is also essential when the recipient of a solid organ transplant is CMV seropositive and the decision is made to initiate treatment only once the CMV levels predictive of disease are reached. This strategy, known as pre-emptive therapy, utilises intensive monitoring for CMV activity by sensitive real-time quantitative PCR methods and short-term antiviral treatment is given only to those with significant viral counts before symptoms occur. CMV is one of the most common opportunistic pathogens that infect solid organ transplant recipients (Fishman, 2007) and is associated with increased morbidity and mortality (Sagedal et al., 2004; Schnitzler et al., 2003) . Following primary infection, the virus establishes a lifelong latent infection in several sites of the body and may reactivate in the presence of immunosuppression, such as in transplant recipients. Once reactivated, CMV is able to modulate the immune system and is known to be a potent upregulator of alloantigens (Razonable, 2008) , increasing the risk of chronic allograft dysfunction (Reischig, 2010; Sagedal et al., 2002; Smith et al., 2010) and acute rejection (Sagedal et al., 2004) . Pre-emptive therapy reduces the incidence of CMV disease (Khoury et al., 2006; Reischig et al., 2008) , which has been documented as a serious problem in randomised trials upon completion of universal antiviral prophylaxis therapy (Kalil, Levitsky, Lyden, Stoner, & Freifeld, 2005; Lowance et al., 1999; Paya et al., 2004) . Long-term studies have demonstrated that patients receiving preemptive therapy, when compared to prophylaxis therapy, were less likely to develop moderate-to-severe kidney scaring and atrophy and significantly better survival of the transplanted organ (Reischig et al., 2012) . However, challenges still exist around defining appropriate thresholds to initiate pre-emptive therapy (Humar & Snydman, 2009) . But with new standardised real-time PCR assays, widespread adoption, and utilisation of these tests, pre-emptive therapy relying in intensive viral load monitoring may become the standard for certain at-risk patients.\n\nTest of cure, or end of treatment response, is assessed following a given therapeutic regimen for signs of treatment efficacy. In few cases, a quantitative viral load measurement serves as a way to establish a cure rate, but, in others, may only be used as a confirmation of virologic suppression as clinical cure may not yet be possible with current therapies or technical limitations by real-time PCR that limits the overall sensitivity of viral detection. Regardless of the clinical utility for measuring a virologic suppression, quantitative real-time PCRs with their current limits of detection and limits of quantitation are valuable tools in measuring low-level viraemia and establishing undetectable viral loads.\n\nUtilisation of quantitative real-time PCR to assess virologic cure is perhaps best exemplified by treatment of patients with chronic HCV. According to the AASLD/ IDSA guidelines, patients who have 'undetectable' HCV RNA in the serum, when assessed by a sensitive PCR assay, 12 or more weeks after completing treatment, are deemed to have achieved a sustained virologic response . Achieving an SVR is considered a virologic cure of HCV infection since, in these patients, hepatitis C-related liver injury stops and recurrence of infection is marginal, detected in <1% of patients after 5 years post-treatment (AASLD/IDSA/IAS-USA, 2014; Manns et al., 2013) . In agreement with these guidelines, the FDA recommendation to pharmaceutical DAA manufacturers also stipulates that viral RNA clearance at SVR-12 be measured in clinical trials using an FDA-approved sensitive and specific quantitative HCV RNA assay (FDA HCV, 2013) . According to prescribing information accompanying the current DAAs, the threshold of SVR-12 is defined as a quantitative threshold of HCV RNA <25 IU/mL at 12 weeks after the end of treatment (Feld et al., 2014; Kowdley et al., 2014; Lawitz et al., 2013) . This is somewhat dissimilar to the AASLD/IDSA guidelines as 'undetected' viral levels are not equivalent to 'detected but below the limit of quantitation' (Figure 3 ). But, with the benefit of high sensitivity and reproducibility, quantitative real-time PCR has a clear established role in assessment of HCV virologic cure in both clinical trials and clinical practice and is able to meet the needs for assessing SVR.\n\nQuantitative real-time PCR may also play a critical role in the assessment of CMV disease resolution. The consensus guidelines recommend that two consecutive negative samples be obtained with a minimum treatment course of 2 weeks before treatment is discontinued, which is thought to minimise the risk for development of resistance and disease recurrence (Asberg et al., 2009; Chou, 2001; Sia et al., 2000) . Still, some transplant centres may extend treatment (secondary prophylaxis) in patients with compartmentalised disease for as long as necessary to reduce the likelihood of recurrent CMV infection (Kotton et al., 2013) . Resolving CMV disease has the long-term benefits of reducing mortality, potential allograft rejection, and the risk of bacterial, fungal, or viral opportunistic infections, among many other transplantand non-transplant-specific effects (Arthurs et al., 2008; Fishman, 2007) .\n\nAlthough there is currently no cure for HIV infection, highly sensitive quantitative and qualitative real-time PCR tests targeting total HIV DNA and RNA have been used in clinical studies for both sterilisation (elimination of HIV-infected cells) and functional (controlled HIV in the absence of ART) cures (Kibirige, 2013; Lewin & Rouzioux, 2011) . Improvements in real-time PCR technology may lead to profound increases in assay sensitivity and the ability to achieve single-copy detection (1 cp/mL) may lead us to a better understanding of HIV virology and what may be needed therapeutically to achieve a cure (Alidjinou, Bocket, & Hober, 2014) . If therapeutic strategies are one day able to achieve an HIV cure, these highly sensitive tests will no doubt play a key role in the continuum of care for patients and, most importantly, in the confirmation of cure.\n\nClinical laboratories have undergone changes to become more efficient and flexible while delivering the same high-quality results. When choosing to implement new testing, even beyond viral targets, laboratories have to consider first and foremost the performance and medical value of the test and then factors such as TAT, ease of use, and cost. Real-time PCR with its wide dynamic range, high specificity, and high sensitivity is considered the gold standard for the quantification and identification of a variety of targets including bacteria, fungi, viruses, or oncological mutations (Klein, 2002) . Furthermore, the multiplexing capability of real-time PCR increases the number of targets and information gathered from the same test, further improving laboratory workflow, TAT, and costs (Deshpande & White, 2012) . While novel technologies have entered clinical laboratories including mass spectrometry and next-generation sequencing, real-time PCR remains a staple and an attractive option for clinical laboratories aiming to create molecular laboratory-developed tests (LDTs). In addition, PCR can quickly be adapted to provide a robust test for the identification of emerging disease and molecular testing is now able to reach beyond the clinical laboratory and further enhance healthcare (Farrar & Wittwer, 2015; Foudeh, Didar, Veres, & Tabrizian, 2012) .\n\nMost molecular tests used in clinical laboratories are FDA-approved and commercially available. There are instances, however, when a test may not be available for a specific virus or the sample type and/or clinical indication used by the laboratory differs from those of the FDA-approved assay, typically leading a laboratory to design its own PCR-based test or modify existing assays. FDA defines an LDT as 'a type of in vitro diagnostic test that is designed, manufactured, and used within a single laboratory' and recognises that 'LDTs are important to the continued development of personalised medicine' (FDA LDT, 2014) . Laboratory developed tests can be grouped into three categories, FDA-cleared or approved test that have been modified, tests that are not subject to FDA clearance or approval, and tests for which no performance specifications have been provided by the manufacturer (e.g. analytespecific reagents or ASRs) (Burd, 2010; Code of Federal Regulations, 2009 ).\n\nWith alternative sample types or applications, FDA-approved tests are often modified to fit the testing needs of laboratories, including alternative collection media and sample types or expanded clinical applications. As an example, a recent gap was created in the HCV-screening algorithm for the confirmation of a positive enzyme immunoassay result following the discontinuation of the only FDAapproved confirmatory test (Alter, Kuhnert, & Finelli, 2003) . In response, the CDC published recommendation for the use of FDA-approved tests detecting HCV viraemia (CDC HCV, 2013), despite the fact that most of these assays did not have specific claims for confirmatory testing; as a result, several laboratories chose to validate these assays as LDTs to meet the screening needs for HCV. Additionally, LDTs are the only option for the identification of the aetiologic agents of viral infections that can occur in transplant patients, such as EBV, adenovirus, VZV, and BK virus, that often present with non-specific clinical manifestations (Razonable, 2011) and for which FDA-approved assay options are lacking.\n\nLDTs are an integral part of molecular laboratory testing. Whether created from the ground-up or modified from FDA-approved assays, LDTs are answering the clinicians' needs for information as an aid for diagnosis or treatment of patients. As with any clinical tests, LDTs have to meet the minimum standards set forth by CLIA prior to report patient results (Code of Federal Regulations, 2009). In July 2014, FDA informed Congress of the agency's LDT regulatory oversight framework (FDA LDT, 2014) . FDA aims to address concerns over high-risk LDTs with inadequately supported claims, lack of appropriate controls, and falsification of data that may lead to inadequate treatment, possible harm to patients, and unnecessary healthcare cost. Presently, there is still a high degree of uncertainty as to what the final regulation scope will be and the possible impact on molecular laboratories will have to be seen.\n\nPalaeopathology confirmed the truism that humanity, since its inception, has been exposed to genetic and infectious diseases with early documentation of trachoma (8000 B.C.E.), tuberculosis (7000 B.C.E.), and pneumonia (ca. 1150 B.C.E.) (Aufderheide & Rodreguez-martin, 1998; Hershkovitz et al., 2008; Roberts & Manchester, 1995; Webb, 1990) . Even today, emerging infectious diseases (EIDs) continue to appear unpredictably driven by changes in human demographic, land use, and population behaviour (Lederberg, Hamburg, & Smolinski, 2003; Sehgal, 2010; Taylor, Latham, & Woolhouse, 2001) . These infections can be classified as either newly emerging/a previously unknown disease or re-emerging infectious diseases/a previously known disease, that reappears after a significant reduction in incidence or elimination (Morens & Fauci, 2013) .\n\nEIDs are a threat not only to human health but also to global stability and economy. Efforts to monitor these EIDs are in place both at the global level spearheaded by the WHO and at the national level. In the United States, governmental agencies (Department of Health and Human Services, United States Agency for International development, Department of Defense) are supporting activities to detect, assess, and respond to potential outbreaks. Specifically, PCR and real-time PCR are easily adaptable to detect nucleic acid targets that are unique to each given pathogen, and as such, they play essential roles in the identification and detection of infectious pathogens and have been routinely used by health organisation agencies during epidemic outbreaks such as severe acute respiratory syndrome (SARS), H5N1, H1N1, and Ebola (Shuaib et al., 2014; WHO Influenza, 2011) .\n\nThe SARS epidemic appeared in November 2002, in the Chinese province of Guangdong before reaching the adjacent Hong Kong in 2003 (WHO SARS, 2003 . This SARS eventually spread to 26 countries and resulted in more than 8000 cases. In response, the CDC triggered its emergency operations centre and issued a draft genome in April 2003, 33 days after the initial WHO global alert (CDC SARS, 2013) . Soon after, real-time quantitative PCR assays were described and put in use for the diagnosis of SARS (Drosten et al., 2003; Peiris et al., 2003) . A host of measures were taken in order to contain this epidemic, and the molecular identification and diagnosis of the infectious agent by PCR played a key role in providing critical information to address the situation and contributed to the care of the patients infected. Additionally, the re-emerging 2014 Ebola epidemic (CDC Ebola, 2014; WHO Ebola, 2014) started in Guinea in March of 2014 before spreading to nearby West African countries and eventually reaching the United States and Europe (WHO Ebola, 2014) . At the height of the epidemic, FDA issued an Emergency Use Authorisation (EUA) for the use of the first real-time RT-PCR assay (FDA EUA, 2014) and less than 4 months later, five additional realtime PCR tests were authorised under an EUA (FDA EUA, 2014) to provide an early diagnosis of the Ebola viral disease (CDC Ebola, 2014).\n\nEIDs remain a constant and unpredictable threat to human health. The flexibility of real-time PCR technology continues to show how promptly it can be used for the detection of infectious agents. By providing a rapid diagnostic, real-time PCR can help in starting the appropriate treatment right away and maximise the chances of a positive outcome.\n\nThe goal of point-of-care testing (POCT) is to quickly obtain a test result that will be used to implement the appropriate treatment for an improved clinical outcome. By definition, POCT is laboratory testing that takes place at or near the site of patients (CAP POC, 2013) . The advantages of POCT are an improved TAT and result availability regardless of normal core laboratory hours, access to care in remote areas, and greater patient involvement.\n\nThe fight against AIDS largely contributed to the development of POCT devices with viral load capabilities (Hong, Studer, Hang, Anderson, & Quake, 2004; Lee et al., 2010; Marcus, Anderson, & Quake, 2006; Tanriverdi, Chen, & Chen, 2010; UNITAID, 2014; Vulto et al., 2009) . Originally developed to meet the difficult conditions associated with remote places, far from any core laboratory facility often found in the developing world, the design and convenience of a portable POCT device with fast turnaround and accurate results extends the reach of healthcare. With this in mind, these POCT systems could easily be used in developed nations at hospitals, within clinics a physicians' offices, pharmacies, correction facilities, or mobile health units, to target pathogens that benefit from immediate actionable results, for which not only accurate but also quick results are critical (Kiechle & Holland, 2009 ). Ultimately, test menu available on these platforms will drive its implementation as a complement for the clinical laboratory core testing.\n\nThe ideal molecular POCT system that includes medical value, simplicity, fast TAT, and ruggedness remains an ongoing engineering challenge. However, the latest advances in microfluidics are a great example of the potential of these devices and brings the real-time PCR lab-on-a-chip closer to mainstream diagnostic use. This is an exciting time for molecular POCT and the upcoming years should bring new systems and perhaps a paradigm change in the world of healthcare.\n\nAs the needs of the clinicians, laboratory, and patients continue to evolve, so do the applications of molecular diagnostics and PCR. Over the past decade, quantitative real-time PCR technology has been increasingly phased into clinical practice and all of the potential present-day applications of real-time PCR-based methods are enumerable. They serve to advance experimental approaches within biological fields, pushing the boundaries of what we know and what we can learn, as well as to diminish empiric medical identification and management of viral diseases.\n\nThe high sensitivity of the technology has reduced risks of the most commonly transmitted transfusion illnesses and has become an integral part of managing a variety of viral infections by providing pretreatment prognostic information, therapeutic effectiveness through monitoring, and end of treatment response assessment. Quantitative real-time PCR complements serologic testing by detecting infections within the pre-seroconversion window period and infections with immunovariant viruses and are able to predict therapeutic failures sooner than traditional methods, allowing for a more timely management response. Real-time PCR assays can be rapidly developed in cases of emerging epidemic crises involving new pathogens that may result in significant health threats. The next few years are likely to see an even further increase in the expansion of the clinical applications of nucleic acid quantification, particularly following bone marrow and solid organ transplantation for which the newest standardised assays may provide an avenue for the development of consensus management guidelines for initiating pre-emptive anti-CMV treatment. Further, with the drive towards HIV eradication and complete elimination of the virus from within cells of infected patients, innovations in quantitative real-time PCR assay design will continue to push the boundaries of detection and introduce assays with progressively lower limits of detection. Thus, quantitative real-time PCR has and will facilitate advancements in the quality of diagnostics and of what we can achieve in research, medicine, and patient outcomes."}