{"title": "Avoidable errors in the modelling of outbreaks of emerging pathogens,\nwith special reference to Ebola", "body": "The success of model-based policy in response to outbreaks of bovine spongiform\nencephelopathy [1] and\nfoot-and-mouth disease [2,3] established the utility of\nscientifically informed disease transmission models as tools in a comprehensive\nstrategy for mitigating emerging epidemics. Increasingly, the expectation is that\nreliable forecasts will be available in real time. Recent examples in which\nmodel-based forecasts were produced within weeks of the index case include severe\nacute respiratory syndrome (SARS; [4,5]), pandemic H1N1\ninfluenza [6], cholera in Haiti\nand Zimbabwe [7], Middle East\nrespiratory syndrome (MERS; [8]),\nand lately, Ebola virus disease (EBVD) in West Africa [9,10]. In the early stages of an emerging pathogen outbreak, key unknowns\ninclude its transmission potential, the likely magnitude and timing of the epidemic\npeak, total outbreak size, and the durations of the incubation and infectious\nphases. Many of these quantities can be estimated using clinical and household\ntransmission data, which are, by definition, rare in the early stages of such an\noutbreak. Much interest, therefore, centres on estimates of these quantities from\nincidence reports that accumulate as the outbreak gathers pace. Such estimates are\nobtained by fitting mathematical models of disease transmission to incidence\ndata.\n\nAs is always the case in the practice of confronting models with data, decisions must\nbe made as to the structure of fitted models and the data to which they will be fit.\nConcerning the first, in view of the urgency of policy demands and paucity of\ninformation, the simplest models are, quite reasonably, typically the first to be\nemployed. With even the simplest models, such as the classical\nsusceptible\u2013infected\u2013recovered (SIR) model, the choice of data to\nwhich the model is fit can have significant implications for science and policy.\nHere, we explored these issues using a combination of inference on simulated data\nand on actual data from an early phase of the 2013\u20132015 West Africa EBVD\noutbreak. We find that some of the standard choices of model and data can lead to\npotentially serious errors. Since, regardless of the model choice, all model-based\nconclusions hinge on the ability of the model to fit the data, we argue that it is\nimportant to seek out evidence of model misspecification. We demonstrate an approach\nbased on stochastic modelling that allows straightforward diagnosis of model\nmisspecification and proper quantification of forecast uncertainty.\n\nAn inexpensive and therefore common strategy is to formulate deterministic\ntransmission models and fit these to data using least squares or related methods.\nThese approaches seek parameters for which model trajectories pass as close to the\ndata as possible. Because, in such an exercise, the model itself is deterministic,\nall discrepancies between model prediction and data are in effect ascribed to\nmeasurement error. Implicitly, the method of least squares assumes that these errors\nare independent and normally distributed, with a constant variance. This assumption\ncan be replaced without difficulty by more realistic assumptions of non-normal\nerrors and, in particular, an error variance that depends on the mean. As for the\ndata to be fit, many have opted to fit model trajectories to cumulative case counts.\nThe incompatibility of this choice with the assumptions of the statistical error\nmodel has been pointed out previously [11\u201313]. In\nparticular, the validity of the statistical estimation procedure hinges on the\nindependence of sequential measurement errors, which is clearly violated when\nobservations are accumulated through time (see electronic supplementary material,\nappendix B). To explore the impact of this violation on inferences and projections,\nwe performed a simulation study in which we generated data using a stochastic model,\nthen fit the corresponding deterministic model to both raw and cumulative incidence\ncurves. We generated 500 sets of simulated data at each of three different levels of\nmeasurement noise. For each dataset, we estimated model parameters, including\ntransmission potential (as quantified by the basic reproduction number,\nR0) and observation error overdispersion (as\nquantified by the negative binomial overdispersion parameter, k).\nFull details of the data generation and fitting procedures are given in electronic\nsupplementary material, appendix A. The resulting parameter estimates are shown in\nfigure 1. \n\nRecognizing that quantification of uncertainty is prerequisite to reliable\nforecasting, we computed parameter estimate confidence intervals and investigated\ntheir accuracy. Figure 1a shows that, in estimating\nR0, one finds considerable error but little evidence\nfor bias, whether raw or cumulative incidence data are used. Although in general one\nexpects that violation of model assumptions will introduce some degree of bias, in\nthis case since both the raw and cumulative incidence curves generically grow\nexponentially at a rate determined by R0, estimates of\nthis parameter are fairly accurate, on average, when data are\ndrawn, as here, from the early phase of an outbreak. Figure 1b is the\ncorresponding plot of estimated overdispersion of measurement noise. Using the raw\nincidence data, one recovers the true observation variability. When fitted to\ncumulative data, however, the estimates display extreme bias: far less measurement\nnoise is needed to explain the relatively smooth cumulative incidence. The data\nsuperficially appear to be in very good agreement with the model.\n\nTo quantify the uncertainty in the parameter estimates, we examined the confidence\nintervals. The nominal 99% profile-likelihood confidence interval widths for\nR0 are shown in figure 1c. When the model is fit to\nthe simulated data, increasing levels of measurement error lead to increased\nvariance in the estimates of R0. However, the confidence\ninterval widths are far smaller when the cumulative data are used, superficially\nsuggesting a higher degree of precision. This apparent precision is an illusion,\nhowever, as figure\n1d shows. This figure plots the achieved coverage\n(probability that the true parameter value lies within the estimated confidence\ninterval) as a function of the magnitude of measurement error and the choice of data\nfitted. Given that the nominal confidence level here is 99%, it is disturbing\nthat the true coverage achieved is closer to 25% when cumulative data are\nused.\n\nWhen a deterministic model is fit to cumulative incidence data, the net result is a\npotentially quite over-optimistic estimate of precision, for three reasons. First,\nfailure to account for the non-independence of successive measurement errors leads\nto an underestimate of parameter uncertainty (figure 1c). Second, as seen in\nfigure 1b, the\nvariance of measurement noise will be substantially underestimated. Finally, because\nthe model ignores environmental and demographic stochasticity\u2014treating the\nunfolding outbreak as a deterministic process\u2014forecast uncertainty will grow\nunrealistically slowly with the forecast horizon. We elaborate on the last point in\n\u00a74.\n\nThe incorporation of demographic and/or environmental stochastic processes into\nmodels allows, on the one hand, better fits to the trends and variability in data\nand, on the other, improved ability to diagnose lack of model fit [14]. We formulated a stochastic\nversion of the susceptible\u2013exposed\u2013infectious\u2013recovered (SEIR)\nmodel as a partially observed Markov process and fit it to actual data from an early\nphase of the 2013\u20132015 West Africa EBVD outbreak. We estimated parameters by\nmaximum likelihood, using sequential Monte Carlo to compute the likelihood and\niterated filtering to maximize it over unknown parameters [15]. See electronic supplementary material,\nappendix B for details.\n\nFigure 2 shows likelihood\nprofiles over R0 for country-level data from Guinea,\nLiberia and Sierra Leone. We also wanted to explore the potential for biases\nassociated with spatial aggregation of the data. Hence, we fit our models to\nregional data, encompassing all reported cases from the three West African countries\njust mentioned. In line with the lessons of figure 1c, estimated confidence\nintervals are narrower when the cumulative reports are used. The\n\u2018true\u2019 parameters are, of course, unknown, but, as in the earlier\nexample, this higher precision is probably illusory. The somewhat, but not\ndramatically, larger confidence intervals that come with adherence to the\nindependent-errors assumption (i.e. with the use of raw incidence data) lead to a\nquite substantial increase in forecast uncertainty, as we shall see. Finally, the\nease with which the stochastic model was fit and likelihood profiles computed\ntestifies to the fact that, in the case of outbreaks of emerging infectious\ndiseases, it is not particularly difficult or time-consuming to work with stochastic\nmodels. \n\nWe took advantage of the stochastic model formulation to diagnose the fidelity of\nmodel to the data. To do so, we simulated 10 realizations of the fitted model; the\nresults are plotted in figure 3.\nWhile the overall trends appear similar, the model simulations display greater\nvariability at high frequencies than do the data. To quantify this impression, we\ncomputed the correlation between cases at weeks t and\nt \u2013 1 (i.e. the autocorrelation function at lag one\nweek, ACF(1)) for both model simulations and data. For Guinea, Liberia, and the\nregion as a whole (\u2018West Africa\u2019), the observed ACF(1) lies in the\nextreme right tail of the model-simulated distribution, confirming our suspicion.\nFor Sierra Leone, the disagreement between fitted model and data is not as great, at\nleast as measured by this criterion. These diagnostics caution against the\ninterpretation of the outbreaks in Guinea and Liberia as simple instances of SEIR\ndynamics, and call for a degree of scepticism in inferences and forecasts based on\nthis model. On the other hand, the Sierra Leone epidemic does appear, by this single\nmetric, to better conform to the SEIR assumptions when the data are aggregated to\nthe country level. \n\nFigure 4 suggests why the\npresent Ebola outbreak might not be adequately described by the well-mixed dynamics\nof the SEIR model. The erratically fluctuating mosaic of localized hotspots suggests\nspatial heterogeneity in transmission, at odds with the model's assumption of\nmass action. As an aside, this heterogeneity hints at control measures beyond the\npurview of the SEIR model. While the latter might provide more or less sound\nguidance with respect to eventual overall magnitude of the outbreak and associated\ndemands for hospital beds, treatment centres, future vaccine coverage, etc., the\nformer points to the potential efficacy of movement restrictions and spatial\ncoordination of control measures. \n\nTo summarize, we have here shown that the frequently adopted approach of fitting\ndeterministic models to cumulative incidence data can lead to bias and pronounced\nunderestimation of the uncertainty associated with model parameters. Not\nsurprisingly, forecasts based on such approaches are similarly plagued by\ndifficult-to-diagnose overconfidence as well as bias. We illustrated this using the\nSEIR model\u2014in its deterministic and stochastic incarnations\u2014fit to\ndata from the current West Africa EBVD outbreak. Emphatically, we do not here assert\nthat the SEIR model adequately captures those features of the epidemic needed to\nmake accurate forecasts. Indeed, when more severe diagnostic tests are applied\n(electronic supplementary material, figure B1), it seems less plausible that the\nSierra Leone data appear are a sample from the model distribution. Moreover, we have\nside-stepped important issues of identifiability of key parameters such as\nroute-specific transmissibility, asymptomatic ratio and effective infectious period.\nRather, we have purposefully oversimplified, both to better reflect modelling\nchoices often made in the early days of an outbreak and to better focus on issues of\nstatistical practice in the context of quantities of immediate and obvious public\nhealth importance, particularly the basic reproduction number and predicted outbreak\ntrajectory. Figure 5 shows\nprojected incidence of EBVD in Sierra Leone under both the deterministic model fit\nto cumulative incidence data (in red) and the stochastic model fit to raw incidence\ndata (in blue). The shaded ribbons indicate forecast uncertainty. In the\ndeterministic case, the latter is due to the combined effects of estimation error\nand measurement noise. As we showed above, the first contribution is unrealistically\nlow because serial autocorrelation among measurement errors have not been properly\naccounted for. The second contribution is also underestimated because of the\nsmoothing effect of data accumulation. Finally, because the model ignores all\nprocess noise, it unrealistically lacks dynamic growth of forecast uncertainty. By\ncontrast, the stochastic model fitted to the raw incidence data show much greater\nlevels of uncertainty. Because measurement errors have been properly accounted for,\nconfidence intervals more accurately reflect true uncertainty in model parameters.\nBecause the model accounts for process noise, uncertainty expands with the forecast\nhorizon. Finally, we recall once again that, because the process noise terms can to\nsome degree compensate for model misspecification, it was possible to diagnose the\nlatter, thus obtaining some additional qualitative appreciation of the uncertainty\nowing to this factor. \n\nThe increasingly high expectations placed on models as tools for public policy put an\never higher premium on the reliability of model predictions, and therefore on the\nneed for accurate quantification of the associated uncertainty. The relentless\ntrade-off between timeliness and reliability has with technological advance shifted\nsteadily in favour of more complex and realistic models. Because stochastic models\nwith greater realism, flexibility and transparency can be routinely and\nstraightforwardly fit to outbreak data, there is less and less scope for older, less\nreliable and more opaque methods. In particular, the practices of fitting\ndeterministic models and fitting models to cumulative case report data are\nprejudicial to accuracy and can no longer be justified on pragmatic grounds. We\npropose the following principles to guide modelling responses to current and future\ninfectious disease outbreaks: (1) models should be fit to raw, disaggregated data whenever\npossible and never to temporally accumulated data;(2) when model assumptions, such as independence of errors, must\nbe violated, careful checks for the effects of such violations should be\nperformed;(3) forecasts based on deterministic models, being by nature\nincapable of accurately communicating uncertainty, should be avoided;\nand(4) stochastic models should be preferred to deterministic models\nin most circumstances because they afford improved accounting for real\nvariability and increased opportunity for quantifying uncertainty.\nPost hoc comparison of simulated and actual data is\na powerful and general procedure that can be used to distinguish model\nmisspecification from real stochasticity. In closing, we are troubled that screening for lack of model fit is not a\ncompletely standard part of modelling protocol. At best, this represents a missed\nopportunity, as discrepancies between the data and off-the-shelf models may suggest\neffective control measures. At worst, this can lead to severely biased estimates\nand, worryingly, overly confident conclusions. Fortunately, effective techniques\nexist by which such errors can be diagnosed and avoided, even in circumstances\ndemanding great expedition.\n\nWeekly case reports in Guinea, Liberia and Sierra Leone were digitized from the\nWHO situation report dated from 1 October 2014 (http://www.who.int/csr/disease/ebola/situation-reports/en/) (figure 3). To compare our\npredictions to those of previous reports [16], we also aggregated those data to form a\nregional epidemic curve for \u2018West Africa\u2019. In Guinea, this\noutbreak was taken to have started in the week ending 5 January 2014 and in\nSierra Leone in that ending 8 June 2014. In Liberia, the outbreak was notified\nto WHO on 31 March 2014 (http://www.afro.who.int/en/clusters-a-programmes/dpc/epidemic-a-pandemic-alert-and-response/outbreak-news/4072-ebola-virus-disease-liberia.html),\nbut few cases were reported until June; therefore, the week ending 1 June was\ndeemed the start of the Liberian outbreak for simulation purposes. The data in\nfigure 4 was downloaded\nfrom the repository maintained by C. M. Rivers (https://github.com/cmrivers/ebola) and ultimately derived from\nreports by the health ministries of the republics of Guinea, Sierra Leone and\nLiberia.\n\nThe models used were variants on the basic SEIR model (figure 6), using the method of stages to allow\nfor a more realistic (Erlang) distribution of the incubation period [17,18]. The equations of the deterministic variant\nare\n\n\nHere, R0 represents the basic reproduction number;\n1/\u03b1, the average incubation period;\nm, the shape parameter for the incubation period\ndistribution; 1/\u03b3, the average infectious period; and\nN, the population size, assumed constant (electronic\nsupplementary material, table B1). \n\nThe stochastic variant was implemented as a continuous-time Markov process\napproximated via a multinomial modification of the\n\u03c4-leap algorithm [14] with a fixed time step\n\u0394t = 10\u22122 week.\n\nTo complete the model specification, we model the observation process. Let\n\u0394NE\u2192I(t1,t2)\ndenote the total number of transitions from latent to infectious class\n(Em to I) occurring between\ntimes t1 and t2. Between\ntimes t\u2212\u0394t and\nt, where \u0394t represents the\nreporting period, we write Ht =\n\u0394NE\u2192I(t\n\u2212 \u0394t,t) for the complete number\nof new infections during that time period. When we are fitting to cumulative\ncase counts, we change the definition accordingly to\nHt =\n\u0394NE\u2192I(0,t).\nWhen using either type of data, we modelled the corresponding case report,\nCt, as a negative binomial:\nCt \u223c\nNegBin(\u03c1Ht,1/k).\nThus,  and\n, where \u03c1 is the reporting\nprobability and k the reporting overdispersion.\n\nDescriptions of the methods used in the simulation study and in the model-based\ninferences drawn from actual data are given in the electronic supplementary\nmaterial."}