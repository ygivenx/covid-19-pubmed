{"title": "Technical Appendix", "body": "The simulation model used in this study builds on our earlier work [5-7].\n\nThe dispersion model takes as input the amount of anthrax spores released and returns as output the number of spores an individual would inhale at locations on a regular grid overlaid upon the study region. To model the dispersion of anthrax spores, we used the Hazard Prediction and Assessment Capability (HPAC) software, which is developed by the US Defence Threat Reduction Agency (DTRA) to allow modeling of threats from weapons of mass destruction [8] . This software uses meteorological and terrain data to interpolate timeintegrated exposure concentrations from a release scenario onto a spatial grid, relying on the second-order cluster integrated puff model [24] .\n\nThe HPAC software outputs a mean exposure plume for a release scenario. The mean plume is determined within HPAC by simulating many individual plumes and then calculating the mean and standard deviation of exposure at each location on an exposure grid. We did not use the mean plume directly because it smooths out the variation in spore concentration within a plume and because the mean plume tends to overestimate exposure at the margins of the plume. To address these limitations of the mean plume, we re-sampled randomly from the mean plume.\n\nThe release scenario we used for all release amounts was a stationary release in the area of the Langley Air Force Base (37.1N, 76.4W), using weather data for Veteran's Day (November 11, 5 am EST, 2 m s \u22121 NNW wind, clear skies), with a 2 m release height, and a 10 s duration. The HPAC software was set to calculate a boundary layer and large-scale variability, and to include terrain effects. We assumed that 1 kg of spores contained 10 15 spores and we modeled the dispersion of 1 kg, 0.1 kg, and 0.01 kg of spores. We calculated a mean plume at each release amount and then re-sampled 1,000 random plumes from each mean plume. The HPAC model provides the time-integrated concentration at each location and we assumed that individuals breathed at a rate of 0.0005 m 3 s \u22121 to determine the number of spores an individual would inhale at locations throughout the study region.\n\nThe infection model takes as input the number of spores an individual would inhale at locations on a regular grid (output from the dispersion model) and the population by ZIP code. The model returns the number of infected individuals by residential ZIP code. We assumed that all individuals were exposed at their home address and that each individual had a uniform probability of being exposed at any location within their home ZIP code. To determine the probability of infection following exposure to a given number of spores, we used a function corresponding to the data reported by Glassman following experimental exposure of primates [10] . This is a probit model with an LD50 of 8,600 spores and a slope of 0.67. Figure 1 shows a plot of the infection function. This function allows for infection at low dose exposure. For example, approximately 2.5% of individuals exposed to 10 spores will become infected.\n\nTo determine the number infected by home ZIP code, we first determined the probability of infection for each ZIP code Z i as the average probability of infection across the ZIP code:\n\nwhere there are n i cells from the exposure grid within ZIP code Z i and f (c j ) is the infection function that returns the probability of infection given the number of spores inhaled in cell c j . The number of individual infected within a ZIP code I Z i was then sampled from a binomial distribution:\n\nwhere N Z i is the population of ZIP code Z i .\n\nThe disease model takes as input the number of infected individuals and returns a disease path for each individual. The disease path describes the amount of time spent in each discrete disease state. We used a semi-Markov process to model progression of an individual with inhalational anthrax through three disease states: incubation, prodromal, and fulminant ( Figure 2 ) [26] .\n\nThe definition of the semi-Markov process requires identification of the states, including the holding-time functions, and specification of the transition probabilities between states. The initial state in the model was incubation, followed by certain transition to the prodromal state, and then the fulminant state. For holding-time functions, we used the lognormal distribution, which appears to describe the duration of incubation for many diseases [20, 23] , including inhalation In the prodromal state individuals experience an influenza-like illness. Finally, in the fulminant state individuals experience severe symptoms, such as shock. The holdingtime function in each state is a lognormal distribution with parameters shown in Table  1 . [25, 26] , and are shown in Table 1 .\n\nThe health-care utilization model takes as input a set of disease paths and for each path performs three tasks: (1) it identifies if and when individuals seek care in each disease state, (2) it determines the presenting syndrome for individuals that seek care, and (3) it identifies the timing and results of blood culture testing once care is sought. We used a semi-Markov process to model health-care utilization ( Figure  3) . A separate process was used to describe health-care utilization in each of the prodromal and fulminant disease states. Both processes had the same states and transitions ( Figure 3 ), but some values for holding-time functions and transition probabilities differed between the disease states (i.e., those states with a s subscript in Figure 3 ) and the values used in the simulation study are shown in Table 1 .\n\nThe transition from 'No Visit' to 'Visit' represents an individual seeking care at an ambulatory clinic or emergency department. The probability of this transition occurring (\u03b1 s ) differs between disease states (s). We set the probability of a visit in the prodromal disease state (\u03b1 p ), to approximately 0.30 because cross-sectional surveys suggest that this proportion of individuals visit a physician at some point during an episode of upper respiratory tract illness [15, 18] . For the fulminant disease state (\u03b1 f ), we estimated the probability of seeking care as approximately 95% given the severity of the symptoms in that state.\n\nThe transition from 'Visit' to 'Growth' represents an individual having a positive blood culture test after making a visit. The probability of this transition (\u03b2 s ) is the product of the probability of performing a blood culture test (\u03b2 1 s ) and\n\nthe sensitivity of the test (\u03b2 2 , i.e., \u03b2 s = \u03b2 1 s \u00d7\u03b2 2 ). The probability of performing a test in the prodromal state (\u03b2 1 p ), was estimated from the National Ambulatory Health Care Survey as approximately 0.0125 [9] . In the fulminant state, we assumed that the probability of a blood culture test (\u03b2 1 f ) was approximately 0.95. We relied on published studies of blood-culture testing to estimate the sensitivity of blood-culture testing in both symptomatic disease states (\u03b2 2 ) as approximately 0.85 [21] .\n\nThe final transition, from 'Growth' to 'Isolation', represents the decision to isolate the organism from a blood culture bottle growing gram-positive rods. We relied on data from a recent survey to estimate this value (\u03b3) as approximately 0.9 [2] .\n\nIn addition to a transition probability, each of the first three states in the health-care utilization model also requires a holding-time function. The holdingtime function for the 'No Visit' state models the distribution of time to seeking care, given that care is sought. We used a right triangular distribution fit to the time spent in the disease state. So, for example, if an individual had a prodromal disease state duration of 10 days, then the probability of seeking care at the instant of entering the disease state would be zero, and the instantaneous probability of seeking care would increase linearly to 0.2 at ten days, with a mean time to seeking care of 6.7 days. 1 This approach to modeling visits effectively limits individuals to a single visit in each disease state. The selection of a triangular distribution reflects the lack of published evidence about the timing of health-care utilization following the onset of symptoms.\n\nThe holding time function for 'Visit' reflects the distribution of times until growth occurs given that the test is positive. The holding time function for 'Growth' is the distribution of times until the organism is isolated given than a decision is made to isolate a specific organism. We modeled both these holding times as exponential with means obtained from published reviews of bloodculture testing [1, 11] .\n\nFinally, for individuals that made a health care visit, we simulated the syndrome assigned to the individual using probabilities that reflect the distribution of clinical presentations for inhalational anthrax reported in the literature [11, 12] . As we consider only respiratory syndromes for surveillance, we modeled only the probability of being assigned a respiratory syndrome in the prodromal disease state, which we estimated at approximately 0.75 (Table 1 ).\n\nWe generated 1,000 simulated outbreak signals at each release amount for a total of 3,000 signals. To generate the simulated outbreaks for each release amount, we first sampled an exposure grid and calculated the number infected by home ZIP code. The next step was to select a set of disease and health-care utilization parameters. We then generated a disease path for each infected individual, the timing of visits to physicians for symptomatic individuals, the administrative codes generated through visits, and the occurrence, timing and results of blood culture testing.\n\nDue to the large number of parameter values in the disease and healthcare utilization components of the simulation model, we used Latin hypercube sampling (LHS) to select parameter values for each simulation run of these components. LHS is an approach to sampling parameter values from a highdimensional parameter space in order to obtain estimates of output variables that are more efficient and precise than would be obtained with simple random sampling [16] . When specifying a simulation model there are K parameters. A given run of the simulation model requires a value for each parameter, or a set of parameter values X = {X 1 , . . . , X K }. Each parameter has a space of possible values S = {S 1 , . . . , S K }. In LHS the space for each parameter is partitioned into N intervals of probability size 1/N . The Cartesian product of these intervals partitions S into N K cells, which form a hypercube. To obtain X for a simulation run requires randomly sampling a partition for each parameter, and then sampling a parameter value from within that partition, assuming that values are uniformly distributed within a partition. Table 1 shows the parameter value intervals used in the simulation study.\n\nIn order to reduce the variance between scenarios, we used the same 1,000 sets of disease and health-care utilization parameters, selected through LHS, for each scenario. In other words, the first runs for each of the 3 release amounts all used the first set of sampled parameters, the second runs all used the second set of sampled parameters, and so on. Similarly, we sampled 1,000 exposure grids and all 3 scenarios used the same 1,000 re-sample exposure plumes. 2 Finally, the same random number generator with the same seed value was used for each scenario. We used a combined multiple recursive generator as proposed and implemented by L'Ecuyer with the default initial seed [14] . This sampling strategy was intended to improve the efficiency of the simulation and reduce the variance of the output variables [13] . The net result is to facilitate comparison of the results across the different scenarios.\n\nWe first defined a 330 day interval on the baseline data from January 19, 2003 until December 15, 2003 as possible starting dates for a simulated outbreak. The gap at the beginning of the year was to allow a period for the detection algorithm to initialize using test data, and the gap at the end of the year was to ensure that injected outbreaks did not run off the end of the test data. We then selected 1,000 random dates for injecting outbreaks. Each of the 330 dates was used 3 times and ten dates were sampled randomly from the 330 dates to be used a fourth time. All 100 dates were then shuffled randomly.\n\nTo inject the simulated outbreak signals for a release amount, we used the following method. \n\nIn other words, the inject series is formed by adding the values of the outbreak series to the values of the background series, after aligning the two series so that the first day of the outbreak series is added to day D j in the background series. We then applied the outbreak detection algorithm to each day in the inject series to generate alarm values from day D j \u2212 g to day D j + n \u2212 1. The lead-in gap g = 16 + (3 \u00d7 28) = 100 days, and the first 16 values were excluded from the alarm series as they were required to initialize the temporal forecast algorithm. 3 This left an 84 day (approximately 3 months) initilization period.\n\nWe used an autoregressive seasonal integrated moving average (SARIMA) model [3] to calculate one-step-ahead daily forecasts of respiratory syndrome counts and then used a cumulative sum [19] to detect positive deviations in the forecast residuals. Other researchers have used a similar approach to outbreak detection in a surveillance setting [27] .\n\nTo fit the SARIMA model, we used two years of data for respiratory syndrome visit counts (2001 to 2002, inclusive) and followed a procedure similar to that described elsewhere [22] . This entailed subtracting the overall mean, day-of-week means, month means, and holiday means from the original count data to give a series centered on zero. We used trimmed means (alpha = 0.1) for both day-of-week and month to minimize the influence of outliers. We then assessed the temporal autocorrelation in this series and fit a SARIMA model using a standard approach to model specification [3] .\n\nWe evaluated the fit of the SARIMA model to the training (2001 to 2002) and test data (2003) using the mean absolute percentage error (MAPE), defined as\n\nwhere there are m days in the training interval, \u00b5 j is the forecast value on day j and x j is the observed value. After subtracting the overall mean and means for day-of-week, month and holiday, the zero-centered series exhibited temporal autocorrelation at short lags on the order of days, and cyclical lags of order seven. We found that a SARIMA model (2,0,1) x (2,0,1)7 had the best fit to the zero-centered series. One-step-ahead forecasts from the SARIMA model resulted in a mean absolute percentage (MAPE) of 14.9% on the training data, which implies that the forecast values were, on average, within 14.9% of the true value. This fit is similar to or better than the fit reported by others using the same algorithm and similar data [22] .\n\nTo detect temporal aberrancies in the observed counts, we applied a cumulative sum to the standardized forecast residual and declared an aberrancy when the cumulative sum exceeded a threshold. We calculated the standardized residual for each day as the observed total respiratory visit count minus the one-step-ahead forecast from the SARIMA model, divided by the standard error of the forecast. Standardization, or dividing the residual by the standard error of the forecast, resulted in residuals with a standard normal distribution.\n\nWe then applied a one-sided cumulative sum to the standardized residuals to detect a positive shift in the mean of the residual series:\n\nThe cumulative sum requires four parameters: the series mean \u00b5 0 , the series standard deviation \u03c3 x , the shift k, and the decision threshold h. The shift specifies, in standard deviations, the minimum detectable change in the mean. We set the shift at 0.5 standard deviations and declared an aberrancy for a given day, j, when the cumulative sum crossed the decision threshold, or\n\nWe varied h between 0 and 10 to determine the outbreak detection performance over a range of thresholds.\n\nSensitivity is the probability of an alarm given an outbreak, or\n\nwhere n(O) is the number of outbreaks and n(A, O) is the number of outbreaks during which at least one alarm sounded. We calculated sensitivity for detecting an outbreak following a given release amount at a decision threshold h as:\n\nwhere there are n simulation runs and simulated outbreak i has m i days from onset to peak incidence. Note that sensitivity is computed using only the outbreak intervals and no distinction is made as to when in the course of an outbreak alarms occur.\n\nSpecificity is the probability of no alarm given that there is no outbreak, or\n\nwhere n(O) is the number of days in the test data and n(A, O) is the number of alarms when the algorithm is applied to the test data without any superimposed outbreaks. We calculated specificity at a decision threshold h as:\n\nwhere there are m days in the test data set. Note that specificity is calculated using only non-outbreak data. We therefore assume that any alarm in the test data without a superimposed outbreak is a false alarm.\n\nTimeliness is the time between the onset of the outbreak and the first alarm sounded during an outbreak. We calculated timeliness for a single simulated outbreak as:\n\nwhere there are m i days from the onset until the peak of an outbreak i and timeliness is not defined if m i j=1 A(h) j = 0. Note that the mean timeliness for a set of outbreaks is not necessarily monotonically increasing or decreasing with the threshold. The reason for this potential variation in timeliness is that as the thresholds changes, new outbreaks may be detected at the changed threshold. The mean is re-calculated incorporating the timeliness from these additional outbreaks, but there is no guarantee that the timeliness for these new outbreak will be less than (or greater than) the mean timeliness before their inclusion.\n\nThe effect of newly incorporated outbreaks is less at lower specificity, where there are already a considerable number of outbreaks detected. At higher specificity, though, the derivative of the mean timeliness can change from negative to positive as the threshold changes.\n\nTimeliness for detection through clinical case-finding in a simulation run was calculated as the minimum of the times to a positive blood culture diagnosis for all positive blood cultures ordered for visits occurring in either disease state which were assigned any syndrome.\n\nDetection benefit is the potential gain in time to detection from using a new detection method relative to a standard or existing method. For a new detection method A and an existing method B, the benefit of A over B for a single outbreak is calculated as the difference in the timeliness using the two methods, or\n\nThe detection benefit is always greater than or equal to zero. Note that, as with timeliness, the mean detection benefit is not necessarily monotonically increasing or decreasing with the threshold.\n\nAnother measure of detection benefit is the proportion of times that method A detects an outbreak before method B. For a single outbreak, this is equivalent to calculating a binary measure in place of the continuous detection benefit, or"}