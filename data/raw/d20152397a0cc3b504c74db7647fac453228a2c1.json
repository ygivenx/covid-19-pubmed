{"title": "Perspectives in Foodbor ne Illness", "body": "To many people in affluent nations, mere mention of the word food conjures up visions of heaped-up and sometimes well-presented piles of carbohydrate-laden and fatladen items on a plate, daring to be consumed in 1 sitting. Thoughts may drift to nutritional value and health, or perhaps to weight control, but these thoughts usually do not linger. Rarely is there a concern for the safety of the food in front of the consumer.\n\nWhen this safety is considered, myths, superstitions, and urban legends abound. In contrast, to the poor, hungry, and destitute, food is a luxury in whatever form it takes (myths, superstitions, or urban legends notwithstanding). Therefore, perhaps it is best to start this perspective on foodborne illness with a fundamental and undeniable truth: to live we must eat, for that is where it all begins. What we eat, and whether or not it sustains and promotes our growth and our health, or if it might just kill us (whether that be sooner or later), is not on the table. Where food is scarce, or famines prevail, safety is definitely not an issue, for to live one must eat.\n\nIn the articles in this issue, the causes and consequences, the diagnosis and disposition, tracking and transmission, and treatment and travel aspects of foodborne illness are presented in depth. This introductory article has a different purpose; it is intended to provide a framework for the contemporary details to rest on, a perspective over the past 100 years or so (with a touch of ancient history) as particular issues that affect the safety of the food we eat have been appreciated, have evolved or at times been successfully dealt with, or have newly emerged or reemerged, in large part because of the impact of 3 critical and rapidly changing T words: technology, trade, and travel. The intent is to provide context for the details to follow in other articles, to avoid the possibility that the forest is missed because attention is entirely focused on the individual trees within it.\n\nWithout written records, it is difficult to reconstruct what life was really like for early prehumans, except that the search for food must surely have been central to daily life and survival. What impact climate change had on the environment and the availability of things to eat, and how this affected the evolution of early hominids into Homo sapiens, dropping from the trees to the ground and over time becoming bipedal, developing functional hands capable of crafting and using tools, including those for more efficient hunting and for cultivation, can only be inferred from the anthropologic, archeological, and paleontologic evidence that has accumulated in the recent past. This evidence includes hard evidence, based on the study of fossils and the use of new biochemical and isotopic methods (for example, carbon and nitrogen stable isotope concentrations in collagen), 1 about the prehistoric diet, which consisted primarily of terrestrial animal meat, predominantly deer and to a lesser extent a few other animals. By the time Homo erectus appeared about half a million years ago, this basic diet was supplemented by edible plants, initially seeds, 2 and in coastal areas by marine life as well. 3 We can only imagine how many times decayed meat contaminated with microbial pathogens or plants that contained rapidly lethal poisons, such as the ricin in castor plant seeds or the alkaloid coniine in hemlock leaves, or the more gradually lethal constituents such as heavy metals in plants grown in soil or water containing high levels of these natural elements, were consumed, and how many times the safe practice lessons had to be learned and at what cost in health or life. The dramatic events related to consuming toxic foods were the stuff to build myths on, without the science or the ability to preserve experience in writing, as occurred later in human development. As a consequence, also as a result of nutritional deficits related to limited dietary options, unsafe food, injury, and disease, life was short 4 : for men, who also risked injury or death in hunting, and for women, who had the added burden of mortality during pregnancy and childbirth to contend with.\n\nFood historians, such as Reay Tannahill, 5 describe the discovery of cooking and preservation methods as a way of making some inedible food stuffs edible, enhancing taste (is this just a relatively modern imperative, the result of greater choice?), improving nutritive value, and preserving quality and safety. These innovations no doubt helped hunters of large animals to kill, butcher, cook, and haul the meat back to their camp or village, making it more efficient for those groups to settle down, rather than constantly moving to follow the animal herds on which they depended between winter caves in the lowlands and summer camps at the higher altitude pastures. Somewhere along the way, the discovery was made that plants could be cultivated, given the right soil and growing conditions, which further encouraged settlement rather than a migratory lifestyle. Tannahill comments on the likelihood that at some point it was known that animals regularly visited salt licks, which suggests that early human communities could have used salt to lure animals closer to their homes, especially in winter, making hunting even more efficient. With the domestication of the dog, herding of cattle and sheep undoubtedly became easier as well, and dogs proved to be more valuable alive than as a source of food. So, although populations could grow, other challenges awaited them.\n\nThe story of pigbel, the name in Papua New Guinea (PNG) Pidgin English for a necrotizing enteritis associated with traditional PNG pig feasts, and similar diseases in other societies elsewhere in the world, 6 is illustrative of some of the foodborne hazards for early humans. In PNG, common rituals and festivals involve pig feasts, in which pigs are slaughtered in a manner that often results in spillage of intestinal contents over the carcasses. These carcasses are slow cooked at low temperature using heated stones in earthen pits between layers of fruits and leaves to enhance flavor and aroma. Water is also added and the oven is then further sealed with large leaves and dirt so that the meat and offal are steamed anaerobically, essentially in the juices of the pig tissues. These events are sometimes followed after several days by acute intestinal syndromes in participants, varying from mild diarrheas to acute fulminating lethal diseases. 7 However, it is the fulminant lethal ones that really demand attention. Sharing of remnants of the pig feast over a few days to a few weeks with other clans and other villages, often the payback for previous debts or as part of celebrations like weddings, results in the geographic spread of these illnesses. The lack of consistency between feast and disease would have affected the appreciation of the relationship between the 2. From this description, it is not so surprising that soon after its clinical \"discovery\" in 1963 microbiological studies demonstrated the presence of b-toxin-producing Clostridium perfringens type C in bowel contents and stool of patients. This finding helped to make the etiologic connection with Darmbrand (gangrene of the bowel), a similar acute disease that appeared in Germany at the end of World War II, also associated with b-toxinproducing C perfringens. 8 Some have suggested that the effects of the toxin are enhanced by diets containing foods with large amounts of trypsin inhibitors, such as sweet potatoes, which limit the breakdown of the microbial toxin protein and increase the likelihood that it remain enzymatically active in those consuming the tainted feast, especially children, who are most at risk of the consequences. This syndrome is to be distinguished from food poisoning caused by type A C perfringens, which produces a-toxin, often associated with meat stews allowed to remain warm for hours or reheated after some time, although that too might have been a risk for these early human populations. Because societies in which food scarcity prevails are likely to be cultures in which all parts of an animal that can be eaten are eaten, contemporary examples of foodborne diseases suggest that primitive societies would also be subject to similar outbreaks. Given the barriers to good hygiene in primitive conditions, fecal-oral transmission of enteric pathogens introduced into foods would have been a frequent consequence of daily life.\n\nAlthough Tannahill cautions that food history is no better than informed speculation, the prevailing understanding is that the climate change associated with the end of the ice age some 10,000 to 12,000 years ago, as the Paleolithic period gave way to the Neolithic period, enhanced the growth of grains such as wheat and barley across more areas across the globe. Slowly, the gathering of wild grains morphed into the deliberate spreading of seeds to cultivate those same plants, and so modern agriculture had its beginnings. With this development, it became feasible to harvest and store grains in sufficient quantities to provide for families during lean times, serving as an additional factor in population increase. This population increase, in turn, was necessary to rapidly harvest the mature plants before the grains exploded and spread to reseed the earth. Although this event would reseed the fields, it would also waste the food that the same seeds could provide. Which came first, planting or population growth, is difficult to know for certain. Learning how to plant and harvest meant that the food supply could more effectively be moved adjacent to human habitats rather than requiring humans to go to where the food could be found. Clusters of individuals could now become fixed communities adjacent to the fields of grain, and the conditions under which they lived meant sharing not only chores and the resulting food supply but also hazards to health, including microbial pathogens. As gathering food slowly gave way to cultivating food, and gatherers became farmers, building fixed dwellings where the food was growing, the intimacies with pathogens also deepened, and toxins affecting some harvested bounty (ie, heavy metals accumulated during growth or aflatoxins during storage) became equal-opportunity poisons for the whole community. Community, let alone individual sanitation, is a recent advance.\n\nThe change to an agricultural society also meant that lessons were learned regarding the ill effects of eating raw grain, with its high and poorly digestible content of starch, along with methods for cooking the inner nutritionally valuable germ, or allowing for the sprouting of the seeds, which converted the starches into digestible sugars, increasing the content of vitamins, and partially digesting the proteins for better utilization. Cultivating and harvesting also required the creation of methods of threshing and winnowing in preparation for cooking, or otherwise preparing and storing edible foods. Along the way, the development of pottery allowed greater variation in preparation of food, because this made cooking and roasting in fires possible.\n\nAs fields were developed surrounding clusters of households, these settlements would have attracted wild ruminants also looking for food for survival. Tannahill posits that it was more effective to domesticate sheep and goats (and later on, cattle) than to constantly have to fend them off. Herding would also have allowed the addition of milk and products derived from milk into the diet, the use of animal fat for cooking (and as medicinal salves), as well as tallow for preparation of primitive candles (rush lights), the fabrication of containers from the skins for storage of solids and liquids, and the wool as well as clothing, and even the use of dried dung for fuel. Humans and animals must have lived in proximity, as they do in many rural societies to this day, sharing microbial flora and sometimes the resulting illnesses caused by commensals in one were or became pathogens in the other. As time passed, the variety of foods increased, sometimes locally depending on specific conditions, sometimes more universally, and sometimes moving with migrating people: other grains such as corn, rye, and oats, honey, pulses, olives, figs, dates, grapes, pomegranates, tomatoes, potatoes, and more. Technology improved yields, for example irrigation, the use of bulls and oxen as beasts of burden to help to till fields, thresh grain, and, with the development of the wheel, also to serve as the engines for transporting goods and materials. So, recognizable civilizations came into being, and with advances came adversities and new threats. Insects and parasites affecting plants and animals could become vectors for disease as well as direct threats to humans and domesticated animals. Ground waters used for irrigation could become contaminated with microbes and ova from the feces of infected individuals, both human and animal in origin, and so threaten the health and vitality of the whole of a village population.\n\nRecords in art and texts, in addition to archeological finds, attest to the growth of civilizations in different parts of the world. As technology improved, diets became more varied, breads, and ale produced from the same grains, became staples, and fruits with high sugar content, such as dates or figs in the Middle East and North African countries where they originated, became popular. Drying and salting (another way to dry food, in addition to the salinity per se) for preservation were introduced. Awareness of medicinal plants grew. With increased yields of grains, storage in silos began, perhaps as early as the fifth millennium BC in the fertile crescent of the Middle East. 9 In some settings, dietary laws were formulated, for example as codified in Deuteronomy, the fifth book of the Old Testament, defining what may and what may not be consumed. Although the latter are often thought of as principles in an early public health textbook, for example the separation of animals for food into clean and unclean categories, which certainly suggests a health rationale, the translation of the original terms and the context used may be misleading. Other interpretations at least as logical for prohibiting the consumption of certain items as food, even although in the case of pork, it may have been true with respect to transmission of trichinosis or the tapeworm Taenia solium, or the proscriptions against eating the flesh of animals found injured or dead of natural causes may have prevented some cases of intestinal anthrax or, from a modern perspective, the spillover of an animal infection to a human host. Specifically, social and cultural imperatives may also have played a major and perhaps even more important role in the development of Mosaic law, as the Hebrew tribes worked at creating an identity for themselves distinct from the Egyptians in as many ways as possible, including their food habits. For example, in Deuteronomy 14:21, we find the following: \"You are not to eat any animal that dies naturally; although you may let a stranger staying with you eat it, or sell it to a foreigner; because you are a holy people for Adonai your God.\" 10 It is difficult to believe that it would have been considered holy to provide something known to be potentially harmful to a stranger or foreigner, suggesting that something other than health concerns must have underlain this, and potentially other dietary laws.\n\nThe rationale for the Mosaic rules about avoiding the consumption of fish without scales or fins is more difficult to interpret in terms of health consequences, with the exception of known toxin-producing fish of the order Tetraodontiformes, such as pufferfish (also known as blowfish, or in Japan, fugu). These creatures produce a potent neurotoxin (tetrodotoxin), which acts by blocking sodium channels in nerve cell membranes, interrupting the propagation of impulses along the axon and resulting in various dramatic neurologic manifestations, essentially causing progressive muscle paralysis, which interferes with breathing, often leading to respiratory death. Where these fish are consumed, for example Japan, preparation of the flesh for eating is highly regulated, only permitted at special restaurants by chefs specifically trained for the task. As a consequence, instances of fugu poisoning have become uncommon in Japan, especially in licensed restaurants, although cases continue to occur, especially among fishermen. However, this subject is not relevant to Egypt and The Levant.\n\nThe Mosaic dietary laws also preclude consumption of bottom feeders, such as catfish and eels, filter-feeding shellfish, shrimp, even swordfish. These creatures have become increasingly unhealthy to humans because they live in pesticide-polluted river beds, or bodies of water contaminated by heavy metals released into the air from coalburning power plants and certain other industries, or concentrate the bacterial cause of cholera to an infectious dose. Such considerations would not have been so compelling when these laws were laid down, and their apparent prescient warning for our contemporary society, when several health hazards have been identified with these otherwise tasty seafoods, was at best serendipitous.\n\nOn the other hand, specific instructions to improve sanitation among inhabitants living in a community are also presented in the Old Testament, and these could be an attempt to implement practices that reduce disease incidence. In Deuteronomy 23:12-13, for example, the sanitary disposition of human feces is succinctly described: \"You shall also have a place outside the camp and go out there, and you shall have a spade among your tools, and it shall be when you sit down outside, you shall dig with it and shall turn to cover up your excrement.\" 10 Because a major source of foodborne illness is contamination of food by pathogens excreted in human feces, any means to reduce the introduction of feces into foods by improving sanitation practices reduces the incidence of such infections. However, the early Israelites did not know about microorganisms, and the relationship between illness and deposits of feces close to where food was prepared or eaten, or where water was stored in cisterns, may not have been obvious to them. However, the associated smells would have been strong and perhaps unpleasant enough to make it worth the extra effort to deposit and bury fecal material in designated spots away from the home. For the system be effective, everybody would need to follow the same practice, hence codification as a rule would have made great sense. Later on, during the Roman Empire, elaborate systems for water supply and sewage disposal were implemented, but these did not last after the Empire fell, being replaced by a simpler method: chamber pots, which were simply emptied into the streets outside the closely clustered homes of the people, to be carried away in the gutters (or more widely dispersed) by the rains, because water was too precious to use.\n\nLet us fast forward to London in the middle of the nineteenth century: a major urban center, a rapidly growing population hub, and a thriving center of the economic and industrial revolution. It was also a city of filth, covered with a veneer of human and animal feces, fetid, and lacking even a minimally effective system to remove human waste (usually just tossed out of the windows of the homes on to the streets below, and anybody unfortunate enough to be in the wrong place). Charles Dickens describes London as follows.\n\nA dirtier or more wretched place he had never seen. The street was very narrow and muddy, and the air was impregnated with filthy odors.Covered ways and yards, which here and there diverged from the main street, disclosed little knots of houses, where drunken men and women were positively wallowing in filth. 11 It was not atypical of the major cities around the world. The cholera epidemic of 1831 and 1832 drew new \"attention to the deplorable lack of sanitation in the industrial cities. It was obvious that cholera was concentrated in the poorest districts, where sanitation was most neglected and the slum housing most befouled by excremental filth and other dirt. The relationship between disease, dirt and destitution clarified the need for sanitary reform as, in the crowded and congested cities, disease could fairly readily spread from the homes of the poor to the homes of the wealthy.\" 12 In part, the realization that disease, dirt, and destitution were fellow travelers, all contributing to the pungent smells of filth and decay, supported and promoted the miasmic theory of disease, which really remained in vogue to the beginning of the twentieth century, well beyond the discovery of the microbial world as the cause of many diseases. Cholera also stimulated the movement to improve sanitation, exemplified by laws promulgated in London in 1848. During the cholera epidemic in London in 1854, Dr John Snow unraveled the role of contaminated water in cholera transmission, but it was the Great London Stink of 1858, when the polluted Thames was so foul that Parliament was forced into recess, that finally galvanized action, led by the Chief Engineer of the newly created Metropolitan Board of Works, Sir Joseph Bazalgette, to build an adequate sewage system, completed in 1875. Additional laws were put in to effect that prevented companies supplying drinking water from using the most contaminated Thames waters as a source, and requiring as well the use of some type of filtration. In conjunction with advances in the mechanics and uptake in the use of flush toilets, promoted by John Crapper, a nineteenth-century English plumbing entrepreneur and businessman who is immortalized in its common name, 13 significant improvements in environmental sanitation ultimately followed. Ultimately is the operative word, for an unintended consequence of the water closet was an initial increase in the amount of human excreta reaching the river, 14 until other reforms and administrative improvements in implementation finally had an impact and the incidence of foodborne infections could begin to diminish. This is an important point, for it is rarely the rule that single public health interventions have major impacts; more often than not, it is a combination of approaches, technical, legal, and administrative, that finally achieves the desired result.\n\nAnother technological advance affecting food safety was the development of reliable refrigeration for the storage of food (fresh or cooked) for consumption at a later date. Although earlier societies recognized the use of snow and ice as winter season refrigerants, this evolved later on (relatively recently) into the harvesting and long-term storage of ice obtained from lakes and rivers for use in domestic ice boxes year round and for shipment of goods around the globe. 15 Root cellars, as a reverse geothermal approach, became common in rural settings to safely store certain fruits, vegetables, and cooked preserves at a cool constant temperature below ground, and were especially accessible in rural settings. However, the major advance came with the identification of the microbial causes of foodborne illness and the effect of low temperature on their growth on the one hand, and on the other hand, the science behind evaporation and the development of mechanical refrigeration systems that could be used in commercial settings to produce ice. By 1882, a system to refrigerate a ship was developed by William Soltau Davidson, a Canadian entrepreneur working in Australia, leading to a global trade in refrigerated meat and dairy products, which has escalated to the present time. 16 With further innovation of the technology, home refrigerators became available around the time of World War I, but they were expensive and often used toxic chemicals as a refrigerant. 17 The development of Freon as a safer refrigerant (albeit subsequently identified as harmful to the atmospheric ozone layer and abandoned in favor of newer less harmful chemicals) and the economic boom after World War II led to a dramatic increase in the use of home refrigerators, with freezer compartments as a standard as well, after Clarence Birdseye's flash-freezing methods developed in the 1920s to preserve fish (and subsequently, a whole variety of foods) became a commercial enterprise, preserving food for later cooking and consumption that resembled the fresh item in texture and taste. 18 Refrigeration allowed longer shelf life for a variety of foods without spoilage, although freezing kept foods safe for a long time, although the desirable fresh quality was lost after variable periods of storage, depending on the food involved.\n\nThe Food Safety and Inspection Service of the US Department of Agriculture regularly publishes recommendations for the duration of safe storage of refrigerated foods kept at temperatures between 2.7 C (37 F) and 4.4 C (40 F). 19 However, it is likely that most households keep food considerably longer than recommended (for example, how many toss out cooked meat, poultry, or fish after the recommended 3-4 days is reached?); for some, the refrigerator temperature exceeds 4.4 C (40 F), and in some cases, food is kept until mold is obvious, foods smell off, or there is liquefaction or other evidence of spoilage. This situation sometimes but does not necessarily lead to intestinal illness of 1 sort or another. In addition, some organisms, such as Listeria monocytogenes, grow at refrigerator temperature, and can reach a level high enough to cause illness, particularly in highly susceptible individuals such as pregnant women, the immunocompromised, and the elderly. 20 There are no practical devices, such as the temperature indicators to monitor cold chain storage of vaccines, in use to indicate the safety of refrigerated foods. Without refrigeration, especially in warm and tropical climates, food spoilage occurs rapidly and the practical rule in such settings, but not necessarily followed, should be eat it or toss it.\n\nPasteurization, a process involving heating followed by rapid cooling to reduce microbial populations to levels that do not cause illness and delay spoilage, was developed by Louis Pasteur and subsequently has been highly successfully applied to milk and milk products, and has virtually eliminated milk transmission of infections. Implementation of pasteurization, and other initiatives to improve milk safety from \"1870 to 1940 [launched] a vigorous public health movement to prevent the bacterial contamination of milk.In this period, the market milk supply gradually became safer, with improved sanitary conditions in dairy farms, pasteurization of milk, keeping milk at low temperatures during shipping and delivery, and prohibition of the sale of loose milk (unpackaged bulk milk stored in a large canister and sold using a dipper) in grocery stores.\" 21 There are now several methods to improve milk safety. The classic method, referred to as high-temperature, short-time (HTST), involves heating to 71.7 C (161 F) for 15 to 20 seconds, and extends the refrigerated shelf life to 2 to 3 weeks. The introduction of ultrapasteurized milk, which involves heating milk to a temperature of 135 C (275 F) for a minimum of 1 second, has increased refrigerated stored life to 2 to 3 months. When ultrapasteurization is coupled with sterile handling and container technology, a shelf life of 6 to 9 months can be achieved, even without refrigeration. A third technology, referred to as extended shelf life (ESL), processes HTST milk through a microbial filtration step that further increases the useful shelf life of the product, although a lack of established standards for its production results in variable shelf life among products labeled as ESL.\n\nAnother technology, tube wells, aimed at providing clean water for developing countries, has been extensively applied since the 1970s, especially in Bangladesh and India. By obtaining water from underground rather than surface sources, microbial contamination from human or animal feces deposited in the environment and present in ground water is precluded. This development has had a major impact on the incidence of cholera and dysentery in areas where it has been used. By the early 1990s, nearly all rural populations in Bangladesh and India had access to tube well water. However, in many locales, tube well water drawn from shallow sources, 10 to 70 m below ground, contained high concentrations of arsenic, leached from sediments via biogeochemical processes that promote reducing environments. 22 An unanticipated consequence, widespread evidence of chronic arsenic poisoning, had become evident. Presenting initially with nonspecific abdominal pain, diarrhea, weight loss, and skin changes consisting of hyperpigmented areas with diffuse or nodular keratotic thickening of the palms and soles, and hepatomegaly, over time involvement of the intestinal, cardiac, respiratory, genitourinary, neurologic, endocrine, and hematologic systems became evident. A significant increase in cancers of the skin, lung, liver, kidney, and bladder has since been documented, 23 with an excess in mortality compared with populations not affected by arsenic. 24 It has been difficult to mitigate this problem, although attempts have been made to test individual tube wells, taking those with high levels of arsenic out of service, and creating shared water resources using clean wells. To find a simple, cheap, readily maintained system, the US National Academy of Engineering offered a prize, supported by the Grainger Foundation, to develop a method to remove arsenic from contaminated tube well water, allowing its continued and safe use. A winning team was identified in 2007, 25 and by 2012, the investigators reported its successful field testing in more than 200 localities in West Bengal, India, with each unit serving around 150 families, who were able to monitor and maintain the system once trained. 26 Technology may solve 1 problem and in so doing create another, as is the case with tube wells, but when the secondary problem is recognized and addressed, it is often possible to identify technological fixes to solve it.\n\nThe technological revolution not only provided options for the safe storage of perishable foods for increasing periods, but coupled with the emergence of economically viable rapid air freight systems and the development of genetic food variants that reduced damage in shipping (often at the cost of flavor and taste), has allowed a global food transportation system to develop, with an unprecedented increase in volume in the past 2 decades. The food markets, at least in affluent countries, are no longer seasonal; you can more or less consume the foods you want when you want them. Raspberries in December? Done! All sorts of tropical fruits can be obtained, at a price of course, in the dead of winter in New York, London, Paris, and most developed world centers. Bananas in January? Done! Although some warm weather fruits and vegetables can be grown in hot-house conditions in the northern hemisphere, the ability to transport them from the southern hemisphere and tropical agricultural regions to the north is now big business. The value of this global trade escalated from an estimated $50 billion in 1960, to $438 billion in 1998, to nearly $1060 billion in 2008. 27 The North American Free Trade Agreement area, the European Union, and Asia (much of it representing food exported from China to Japan) are the major destinations. All 3 regions depend on southern hemisphere countries for imports of juices and off-season fresh fruits, and on equatorial regions for bananas, the leading global fresh fruit import. 28 In the United States, in the first 10 months of 2012 alone, nearly 32 million metric tonnes of food and agricultural products were imported, legally, exclusive of wine and beer. 29 To conceptualize this statistic, to deliver a load of that magnitude would require nearly 800 Boeing 747-400 cargo planes landing every day. Now think of conducting comprehensive food safety inspections on that number of aircraft. Of course, foods are moved by every means of transportation, from ships, to railroads, to trucks, making it even more complicated as the entry points and routes of transport are so numerous and diverse; if only it were confined to the airports! Moreover, it is not just fresh foods that are involved, because increasingly, frozen and processed or at least partially processed foods are being imported. The US Food and Drug Administration (FDA) estimates that 10% to 15% of all food consumed in the United States is produced elsewhere, and that 75% of processed foods in the United States contain ingredients that originated in other countries, almost impossible to fully trace. 30 The FDA recognizes that in the future \"(p)roducts entering the U.S. will come from new and different markets and will flow through long, multi-step processes to convert globally sourced materials into finished goods. As global product flows change, many individuals will encounter the growing dangers of fraud and economic or other intentional adulteration of both foods and medical products.\" 30 It is not without reason that the international food trade network has been characterized as \"a perfect platform to spread potential contaminants with practically untraceable origins.\" 27 As we move forward in a period of climate change, which will have a variety of impacts on food production, at the same time that growth in populations will require an increasing volume of food trade, there may be additional impacts on food safety. Among the more likely of these impacts are an increase in contamination of food products by mycotoxins in a variety of crops, a likely increase in the use of pesticides at higher concentrations, and variable effects on the transfer of trace elements (some necessary and some potentially toxic). 31 On top of these effects, the deliberate adulteration of foods for animals and humans for economic gain (eg, the recent scandals in China in which melamine has been added to milk in order to boost the measurement of protein content) has resulted in an international outbreak of severe kidney disease in young children consuming the tainted products. 32 As food becomes more precious with population growth, the likelihood of adulteration of foods for economic gain will undoubtedly increase. Regulatory and inspection services will be stressed in technology, resources, and human capacity, which can only partially be ameliorated by pointof-use technology to identify pathogens or toxins in foodstuff, or drugs, pesticides, or adulterants, which, of course, is still to be developed.\n\nAnother aspect of this problem is the trade in wildlife intended for consumption. 33 The magnitude of this trade and its relationship to food safety is uncertain, especially because a considerable proportion is illicit, and some of it relates to trade in ivory, skins, hair, or parts reputed to have medicinal or aphrodisiac properties, that is, not for food. However, all the indications are that the volume is big, and potentially dangerous. 34 In an era when we recognize the global problem of emerging and reemerging infectious diseases of humans, most of which originate in animals that infect humans 35 and, in the nightmarish scenario, can successfully transmit from human to human, there is great concern about the potential of this trade to introduce new diseases, particularly when it is illicit, underground, and difficult to monitor. Given the likely origins of human immunodeficiency virus/AIDS in nonhuman primates consumed as food in the bush in Africa, 36 an agent that only subsequently became capable of transmitting from human to human, it can be thought about as an originally foodborne disease, and a dead-end infection as well. How many more like it are out there? How many can make the species jump to humans and to efficient transmission between humans? Can these possibly be monitored? And if they can, will it be possible to intervene in time to prevent the next global pandemic? The increasing global trade in bush meat for consumption suggests the virtual impossibility of accomplishing that goal, and (very) creative thinking will be essential. Another issue, generally overlooked, is the role of wildlife in the transmission of disease at the wildlife-livestock interface. 37 Because the agents that may be found are often capable of being transmitted to humans, this relationship is of potential importance. It may not be simply transmission of organisms from wildlife to livestock. In 1 study conducted in Spain, cattle were identified as the source of transmission of serovars of Salmonella to wildlife, in this case, wild boars. 38 In environments in which wildlife, livestock, and humans interact frequently and closely, disease transmission among them must be considered in developing models for disease control. The concept of One Health, the interrelated health of animals and humans, is even broader than is generally considered, and these more complex relationships will need to be considered as an appropriate and effective One Health research, surveillance, and response policy is developed. 39 As this development happens, and wildlife are increasingly seen as contributing to the risk of human disease, the message needs to be carefully crafted in order to avoid serious effects on wildlife conservation programs. 40 Two other brief comments: first, although we are paying attention to viruses and bacteria, we should not forget about parasitic infections of humans. It has recently been noted that \"(c)hanging eating habits, population growth and movements, global trade of foodstuff, changes in food production systems, climate change, increased awareness and better diagnostic tools are some of the main drivers affecting the emergence or reemergence of many foodborne parasitic diseases in recent years. In particular, the increasing demand for exotic and raw food is one of the reasons why reports of foodborne infections, and especially water-borne parasitosis, have increased in the last years.\" 41 This concern demands greater attention. Second, considered as a domestic US issue, the twin movements toward huge commercial farms and, for livestock, the crowded and often filthy conditions in which the animals are sometimes kept on the one hand, and the desire for small, local, organic, and not necessarily well-run farms on the other, may each in their own way increase the risk of marketing unsafe foods. The risks in factory farming are already known, including runoff of pathogen-containing waste, which can contaminate nearby fields of vegetables, not to mention the effect of the use of antimicrobials to promote animal growth and earlier marketing on the selection of antibiotic-resistant human pathogens, and the large-scale processing that goes along with it, which may put many people at risk when 1 contaminated carcass is processed with many clean carcasses, resulting in a lot of contaminated meat. The potential risks in small farms, struggling to compete and perhaps leading to cutting corners, is a still poorly assessed risk. 42 Proposed legislation to tighten up regulatory and inspection oversight for large farms is meeting resistance from small farmers, who fear that the costs of implementation may make them uncompetitive. 43 Travel If the discussion on trade is seen as moving potentially unsafe foods across the globe and within countries to the consumer, travel of people, especially to low-income and middle-income countries, can move them to risky foods, an equally problematic situation made potentially more risky where the regulatory environment is worse than in their home country, and access to good medical care, if necessary, may be problematic. Although not all travel-related illness is foodborne, many disease episodes, in particular diarrhea, result from eating food contaminated with bacterial, protozoal, and viral pathogens. 44 Younger travelers, on limited budgets, extended trips, and more likely to engage in risky behavior, experience frequent episodes of diarrhea and other foodborne infections. 45 Cruise ships have been the scene and source of outbreaks of diarrheal disease affecting the more affluent, often older traveler, frequently caused by norovirus but also associated with a variety of bacterial and other enteric pathogens. 46 It has been known for a long time that disease may spread along travel and trade routes. The movement of cholera is an excellent example of this. Even before the cause was identified, evidence emerged in New York in 1832 that cholera spread with human movement along the newly constructed Erie Canal. 47 It is ironic that the author of the report did not believe in his own data, succumbing to the lure of the miasmic theory of disease. Contemporary proof of this theory is provided by the origins of the cholera epidemic in Haiti, after the devastating earthquake of January 2010, introduced by Nepalese soldiers who had joined the UN Peacekeeping Force to deal with security aspects of the response to the earthquake. 48 Transport of pathogens by travelers can be an efficient way to distribute an agent widely across the Perspectives in Foodborne Illness globe. Albeit not foodborne, the severe acute respiratory syndrome outbreak of 2003 shows the rapid dissemination of an agent carried by airline passengers, which in this instance led to a pandemic within a few days of the transmission of the infection from the index case to secondary cases. 49 \n\nFoodborne illnesses have been a part of the human experience from the beginning. Over time, keen observation and deduction have identified particular illnesses associated with particular foods or food-related behavior. Such observations have stimulated attempts to mitigate these risks, often with significant success when generally applied. However, such lessons are hard won, and many have suffered and many have succumbed over time until, in a Darwinian fashion, lessons learned with positive survival value have been incorporated into cultural practices.\n\nWith the growth of science, and the ability to identify specific causes of foodborne illnesses, several technological solutions have been developed that have significantly reduced the burden of disease. Not only are they technological (eg, pasteurization), but sometimes, they are in the form of regulatory oversight to ensure good practice. However, in the real world, technology and optimal practice can break down from time to time, resulting in periodic outbreaks of illnesses that could have been avoided. The scale of contemporary food production, preservation, trade, and storage in the modern era not only makes these incidents more likely but has also introduced many new ways for foodborne disease to be transmitted, sometimes in outbreak or epidemic form. These new opportunities for transmission of illness must be identified, unraveled, and addressed, often in a manner unique to each setting. The future presents us with opportunities to improve both technology and practice and, in turn, to help to reduce disease incidence. The magnitude of the food trade, partly in response to the growth in population and in part to changes in the way that people obtain, prepare, and consume food, provides new ways for pathogens to be transported and transmitted. The emergence of new agents, and the economic incentives for some to put others at risk through poor practice or adulteration of food, ensures that the incidence of foodborne diseases will continue to be high.\n\nIn the articles that follow in this issue, the agents, the epidemiology, the clinical aspects and treatment, and the prevention of foodborne illnesses are more thoroughly examined. Although it remains a significant problem even in the United States and in all other developed countries, it also results in a major burden of disease and death in low-income and middle-income countries as well. Although some of the potential approaches to reduce these burdens are common across all environments, many will need to be targeted to the specific conditions that create the risk. For this goal, we need a continued vigorous research agenda, attention to improving practices in the food industry and culturally sensitive customs in different populations, and sensible and affordable regulation in individual nations and in the international setting. There is the need to understand how human health, animal health, and wildlife health are linked together in order to develop policies that address systemic issues that can only be, or are best, tackled together. This approach is termed One Health, and although taken in the abstract, it makes great sense, until the scientific and political leadership truly buy in, and until the general public is engaged, based on knowledge and fact, progress will be slow and avoidable illnesses will, all too often, continue to occur. This issue of Infectious Diseases Clinics of North America is a timely presentation of the most contemporary information, and is of use to all interested in the issue, regardless of their background."}