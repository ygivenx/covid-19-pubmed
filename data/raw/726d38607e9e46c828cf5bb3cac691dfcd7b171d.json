{"title": "Original Article", "body": "The outbreak of the severe acute respiratory syndrome (SARS) epidemic that occurred during 2003 exposed serious deficiencies in Taiwan , s medical care and public healthcare systems, as well as its medical education system. The Department of Health, Executive Yuan of Taiwan, R.O.C., has had no efforts in promoting its \"Project of Reforming Taiwan , s Medical Care and Public Healthcare System\" since the spread of SARS was controlled. The reform of the medical care system aims to provide holistic medical treatment to people. Its strategies and methods include strengthening the improvement of resident education and quality of medical care. A project titled \"Post-graduate General Medical Training Program\" was announced by the Department of Health in August 2003. The evaluation of internal medicine first-year post-graduate (PGY 1 ) residents usually consists of the Objective Structured Clinical Examination (OSCE) because it combines reliability with validity by using multiple testing in a standardized set of appropriate clinical scenarios in a practical and efficient format. 1 The multiple-choice Internal Medicine in Training Examination (IM-ITE \u00d2 ) is a written test that is believed to be an alternative to performance testing such as the test by OSCE. 2, 3 The reliability of the IM-ITE \u00d2 is known to be good, with less testing time required. 2 The IM-ITE \u00d2 , covering knowledge in physical examination, laboratory, technical, and communication skills, is relatively cheap and easier to administer compared with an OSCE. 4, 5 However, a paper-andpencil knowledge test will overemphasize the cognitive aspects of clinical skills if the test does not require a resident to actually demonstrate these skills. Direct observation of procedural skills (DOPS) involves direct observation of a resident performing a variety of technical skills. 6 A combination of the OSCE with the IM-ITE \u00d2 and DOPS could bypass individual undesirable effects of each method and increase the completeness of assessment. 5, 7, 8 High-stakes, large scale-OSCEs are used to assess clinical competence at the performance level of a \"show how\" method based on Miller , s competency pyramid. 9 The format of the OSCE is designed with a circuit of multiple stations in which the candidates accomplish specific tasks within a required time period. 9e11 Replacing some OSCE stations with a written test might save resources and increase overall test reliability. 4 It could offer an adequate compromise between the demands of reliability and feasibility. In post-graduate curriculum, designing a mixed-method assessment is often advised. 12 Additionally, different content, multiple assessors, and a sufficient assessment time seem to be the fundamentals of a reliable assessment in clinical rotations. The 360-degree evaluation (multisource feedback) assesses general aspects of competence, including communication skills, clinical abilities, medical knowledge, technical skills, and teaching abilities of PGY 1 residents. 13 In general, different evaluation tools, including high-stakes, large-scale OSCE, DOPS, IM-ITE \u00d2 , and 360degree evaluations have their own particular roles in the assessment of learning outcomes. Thus, the purpose of our study was to determine the reliability of using a small-scale OSCE combined with other tools (DOPS and IM-ITE \u00d2 ) or a 360-degree evaluation to thoroughly evaluate PGY 1 residents.\n\nBetween 2007 and 2010, 209 PGY 1 residents (trainees) were evaluated by a small-scale OSCE before and after finishing 3 months of PGY 1 internal medicine residency courses of Taipei Veteran General Hospital at Taiwan (Taipei VGH). Taipei VGH is a regional medical center that provides primary and tertiary care to active-duty and retired military members and their dependents. Taipei VGH serves as the primary teaching hospital for its internal medicine residency program. All the raters and senior physicians were recruited from among the clinical faculty and were teachers for the Department of Internal Medicine. The well-trained, non-physician experts for DOPS were independent from the Department of Internal Medicine of Taipei VGH.\n\nThe content of the small-scale OSCE, DOPS, IM-ITE \u00d2 , and 360-degree evaluation were designed by a committee of expert physicians from our system who created the content blueprint and wrote the test questions according to well-established principles of examination construction. The committee members were regularly rotated.\n\nThe small-scale OSCE consisted of six 15-minute stations. The OSCE consisted of six clinical problems that were made up of six core competencies defined by the Accreditation Council for Graduate Medical Education [ACGME (Appendices 1 and 2)]. The content of each clinical problem consisted of physical examination skills, interpersonal skills, technical skills, problem-solving abilities, decision-making abilities, and patient treatment skills. 14 The examination took place simultaneously at two different sites. At each site, there were two sessions, and the raters at each station changed between the two sessions. Thus, for each station, there were a total of four different raters during the test day. The smallscale OSCE had neither written a component nor a technical skills station, but it was entirely performance-based. At some stations, standardized patients were used to mimic the clinical problems of actual patients. A faculty rater graded each PGY 1 resident according to a given set of 10e12 predetermined items presented in the form of a checklist. The score of checklists included items with a dichotomous scoring, yes/no, and an overall trichotomous scoring of pass/borderline/fail. All faculty raters attended serial training sessions that included extensive instruction on how to use the checklist in practice rating sessions. At each OSCE station, the raters acted as passive evaluators and were instructed not to guide or prompt the PGY 1 residents. The summary score of each station was the sum of all the checklist items. The residents' performance score for each OSCE station was obtained by calculating the percentage of checklist items he or she obtained. The OSCE was performed before the training (OSCE before ) and at the end of 3 months of training program (OSCE 3rd month ). Finally, average OSCE scores were calculated by averaging the three monthly scores for further analysis.\n\nAll PGY 1 performed a series of standardized technical skills. For each skill, PGY 1 residents were examined by the direct observation of experts and senior physicians using the technical skill-specific checklist. 15 Four technical skills, including advanced cardiac life support (ACLS), lumbar puncture, central venous catheter insertion, and endotracheal tube insertion, were assessed regularly. Experts and senior physicians were provided with an identical checklist for the four technical skills before the test day and were asked to familiarize themselves with the checklist. In addition, they received a 30-minute orientation session just before examination. The DOPS checklist included items on communication skills, technical performance, and some theoretical questions, including knowledge of the indications, contraindications, potential complications, and different routes for the procedure that related to the task. 16 All of these items were developed from the 11 domains of the DOPS in presented Appendix 3. Finally, the DOPS scores of each PGY 1 resident were the averages of the ratings from the four experts and senior physicians for the four technical skills.\n\nThe 360-degree evaluations were made during the interval between the administration of the small-scale OSCE and DOPS. The 360-degree evaluation assessed general aspects of competence, including communication skills, clinical abilities, medical knowledge, technical skills, and teaching abilities that are shown in Appendices 4 and 5. The Spearman-Brown prophecy formula was used to calculate the number of individuals needed to obtain a reliable rating. 13, 16, 17 Our preliminary study found that the number of raters to achieve a reliability of 0.7 was 4. Five additional raters were needed to achieve a reliability of 0.8. Accordingly, the results of five raters of 360-degree evaluations were included for final analysis.\n\nThe 12-item, one-page 360-degree evaluation forms were made by the faculty members, including one chief resident, one visiting physician, one chief physician, one nurse, and one head nurse of each of the services that residents rotated through monthly. In other words, every PGY 1 resident received five evaluations by the five raters. The monthly 360degree score was the average of scores from the five raters. Finally, the average 360-degree evaluation scores was calculated by averaging the three monthly scores (360-degree evaluation 1st month 360-degree evaluation 2nd month 360-degree evaluation 3rd month ) for further analysis.\n\nThe IM-ITE \u00d2 is designed by the American College of Physicians (ACP) to give residents an opportunity for selfassessment, to give program directors the opportunity to evaluate their programs, and to identify areas in which residents need extra assistance. 2, 18 Our multiple-choice IM-ITE \u00d2 is a modification of the ACP's IM-ITE \u00d2 . Our IM-ITE \u00d2 was developed to test required knowledge that PGY 1 residents most frequently encounter during their in-patient rotation. Initially, our IM-ITE \u00d2 was composed of 80 items. After a first validation of the tool, 50 items were chosen based on experts' and residents' comments and validated again with a group of experts who confirmed the quality of the selected 50 items for our assessment purposes.\n\nAt the end of the course, all PGY 1 residents were instructed to complete the DOPS and IM-ITE \u00d2 as if they were the regular tests, even though the DOPS and IM-ITE \u00d2 scores had no influence on pass/fail decisions of the OSCE. Additionally, the 12-item, 360-degree evaluation was completed for each PGY 1 for each month. Our research used the averaged 360-degree evaluations, DOPS, IM-ITE \u00d2 , and averaged small-scale OSCE scores, which had been collected as part of the routine procedure of the Department of Internal Medicine of Taipei VGH.\n\nFor the trainees who failed the DOPS and small-scale OSCE, special programs were designed according to their defects by senior physicians. Then, these trainees were reevaluated until they passed all these tests. For those who failed the IM-ITE \u00d2 and 360-degree evaluation, special training classes were conducted to re-educate them, program directors monitored their performance in the following 3-year residency (e.g., internal medicine, family medicine, surgery, pediatrics, dermatology, ophthalmology) course.\n\nTo ensure equal weighting of all evaluations formats, which was needed for further analysis, the scores of separate/averaged 360-degree evaluation, DOPS, IM-ITE \u00d2 , and separate/ averaged small-scale OSCE were transformed onto a similar 100% scale. The borderline group method was used to set the standard of \"pass\" for 360-degree evaluation, DOPS, IM-ITE \u00d2 , and small-scale OSCE scores. Each station's \"pass\" score was the mean of the scores of PGY 1 residents whose scores were rated \"borderline.\" 19, 20 To estimate the reliability of the 360-degree evaluation, DOPS, IM-ITE \u00d2 , and the smallscale OSCE separately, Cronback's alpha (a) coefficient were calculated for each evaluation. Kappa statistics were used to check the inter-rater agreement between expert and senior physician for the four procedure stations of DOPS. An a of < 0.05 was accepted as statistically significant.\n\nThe descriptive statistics of the mean scores and standard deviations for each examination tool were analyzed with one sample or two-sample student's t test or analysis of variance when appropriate. Additionally, the correlations between the average OSCE and 360-degree evaluation score, small-scale OSCE \u00fe DOPS-composited score, and average 360-degree evaluation score, small-scale OSCE \u00fe DOPS \u00fe IM-ITE \u00d2 score and average 360-degree evaluation score were analyzed by Pearson's correlation methods (Version 10.1, SPSS Inc., Chicago, Ill., USA). Comparisons between two correlation coefficients from paired measurements were carried out using the formula created by Kleinbaum and colleagues. 21 \n\nBetween 2007 January and 2010 January, 245 PGY 1 residents participated and underwent 24 administrations of the OSCE (every 3 months, two OSCE for each PGY 1 resident), 18 administrations of DOPS (every 2 months), 12 administrations of IM-ITE \u00d2 (every 3 months) and 750 administrations of 360-degree evaluation (every 1 month, three 360-degree evaluations for each PGY 1 resident) in our system. Our study involved 99 specialties and subspecialties in total.\n\nFinally, the complete data of 209 trainees were included for analysis. Thus, the data completeness rate was 85.3%.\n\nOur study included the analysis of the internal reliability of all our evaluation methods. The results showed that the reliability of the different evaluative methods was varied. The before-training OSCE (OSCE before ) had a reliability of 0.73, the after-training OSCE (OSCE 3rd month ) 0.662, DOPS 0.82, IM-ITE \u00d2 0.69, 360-degree evaluation 1st month 0.89, 360-degree evaluation 2nd month 0.9, and 360-degree evaluation 3rd month 0.79 (Table 1) . Additionally, the inter-rater reliabilities between the expert and senior physicians for DOPS were good (ACLS: Kappa 0.71; lumbar puncture: Kappa 0.69, central venous catheter insertion: Kappa 0.75 and endotracheal tube insertion: Kappa 0.78).\n\nBefore further correlation studies, the re-evaluation reliability of the small-scale OSCE and 360-degree evaluation were assessed. As seen in Fig. 1 , OSCE before and OSCE 3rd month scores were closely correlated (r \u00bc 0.64, p < 0.01). Meanwhile, the 360-degree evaluation 1st month , 360-degree evaluation 2nd month and 360-degree evaluation 3rd month scores were well correlated, ranging from a low correlation of 0.54 between 360-degree evaluation 1st month and 360-degree evaluation 2nd month scores and a high correlation of 0.94 between 360-degree evaluation 2nd month and 360-degree evaluation 3rd month scores. Table 2 19 shows that average small-scale OSCE scores was significantly correlated with average 360-degree evaluation scores (r \u00bc 0.37, p < 0.05). Interestingly, the addition of DOPS scores to average small-scale OSCE scores significantly increased its (small scale-OSCE \u00fe DOPS-composited score) correlation with the average 360-degree evaluation scores (r \u00bc 0.72, p < 0.01). Furthermore, a combination of the IM-ITE \u00d2 scores with small-scale OSCE \u00fe DOPS scores (small scale-OSCE \u00fe DOPS \u00fe IM-ITE \u00d2 scores) markedly enhanced their correlation with 360-degree evaluation scores (r \u00bc 0.83, p < 0.01).\n\nNext, we searched for the points that needed to be further improved in the design of the training program. The pass rates and the mean scores were significantly improved after 3 months of internal medicine training course [OSCE before : 36% and OSCE 3rd month : 52%, p < 0.05 ( Fig. 2 and Table 3) ]. The pass rate of the DOPS scores was around 70%. Meanwhile, the pass rate of the 360-degree evaluation scores was also progressively improved among three months of internal medicine training program (360-degree evaluation 1st month : 57% 360-degree evaluation 2nd month : 59% and 360-degree evaluation 3rd month : 69%, p < 0.05). Although the overall pass rates varied between different evaluative methods, the differences did not reach significance level.\n\nThe objective of medical education is to produce excellent medical professionals and performance. To achieve this objective, Taipei VGH introduced and implemented the smallscale OSCE, DOPS, IM-ITE \u00d2 , and 360-degree evaluations. Previous study suggested that the term \"competence\" is often used broadly to incorporate the domains of knowledge, skills, and attitudes. 1 No single assessment method can successfully evaluate the clinical competence of PGY 1 residents in internal medicine. It has been reported that the reliability of medical education performance increases with the addition of each different reliable measure. 22 Thus, educators need to be cognizant of the most appropriate application tool. Our study explored whether a combination of assessment tools provides the best opportunity to evaluate and educate PGY 1 resident in Taiwan.\n\nIt is not clear whether lengthening the written test component (such as IM-ITE \u00d2 ) compensates for the loss of validity due to the use of fewer stations in the OSCE. 4 Nonetheless, the reliability of the OSCE is partly determined by the testing time, and a large-scale OSCE is time-and money-consuming. Accordingly, an expensive large-scale OSCE should still be part of the assessment program.\n\nThe 360-degree evaluation have been widely used in several medical and surgical residency training programs, and their usefulness has been very positive. 13, 16 Our study observed the increase in rating scores with more months of training (Table  3) , which supports the general validity of the 360-degree evaluation in assessing PGY 1 resident competence including knowledge, skills, and attitudes. 12, 16, 23 For formative purposes, the 360-degree evaluation helps a resident understand how other members of their team view his or her knowledge and attitudes. Thus, the 360-degree evaluation scores also help residents develop an action plan and improve their behavior as part of their training. In our study, we used 360-degree evaluation scores as a standard to assess the efficiency of different methods, or a combination of them in evaluating the performance of all PGY 1 residents. Nevertheless, the reliability of 360-degree evaluation in our study was different between the 3 months of the training program. This finding can be explained by the fact that the residents are not working in a stable environment. They change rotation frequently, and there are new raters at the new sites. It is also possible that PGY 1 residents became less homogenous in their abilities during the 3 months of the training program. In fact, the 360-degree evaluation is a method that only provides global rating regarding of the PGY 1 residents , performance; it will not demonstrate the details. In other words, the 360-degree evaluation is a tool for assessing the change of knowledge, skills, and attitude rather than physical examination skills. Actually, a complete evaluation of the PGY 1 performance should include a 360-degree evaluation and an OSCE focusing on physical examination skills.\n\nThe reliabilities of the DOPS, IM-ITE \u00d2 , and 360-degree evaluation were good, indicating a high degree of internal consistency of these assessments. The pass rates of all methods were between 61% and 81% (Fig. 2 ). In comparison with other tools, the reliability of the small-scale OSCE was not acceptable. Meanwhile, the pass rate was not very high for the OSCE of our study. These results indicate that the structure of the small-scale OSCE used in our study should modify to improve the pass rate in the future. Nevertheless, average small scale-OSCE and 360-degree evaluation scores were still significantly well correlated (r \u00bc 0.37, p < 0.05), suggesting a high reliability of the overall program. 1 Further, we combined the small-scale OSCE with other tools to improve its reliability and reflect the real performance of PGY 1 residents as seen in Table 2 . Notably, the correlation between small-scale OSCE \u00fe DOPS-composited scores and 360degree evaluation scores was increased (r \u00bc 0.72, p < 0.01). Finally, a further markedly increase in the correlation between OSCE \u00fe DOPS \u00fe IM-ITE \u00d2 and 360-degree evaluation scores was observed (r \u00bc 0.85, p < 0.01). These results can also be explained by the fact that small scale-OSCE, DOPS, and IM-ITE \u00d2 assess different areas of knowledge and skills. Accordingly, adding all of the three scores showed a high correlation with the 360-degree evaluation because more items were being sampled.\n\nThere are some limitations to our study. First, this was a retrospective study of a single residency program with a relatively small sample size. However, our results are strengthened by the completeness of our data over a 3-year period. The series, small-scale OSCE, DOPS, IM-ITE \u00d2 , and 360-degree evaluations were 3 years apart in time. This is a long period in a learning environment, and many confounding variables can have an impact on the learning of PGY 1 residents. However, there is always \"noise\" in educational measurement, and we can postulate that the impact of these confounding variables may be found to be equally distributed among the observed scores of the four evaluations and could explain the results. Despite the noise and 3-year time interval, we still observed a relatively strong correlation among the variables under study.\n\nSecond, no long-term follow-up, small-scale OSCE, DOPS, and IM-ITE \u00d2 measurements during the 3 years of the residents' training were obtained (to evaluate the validity of these tools), and we did not address the durability of the small-scale OSCE and DOPS. Nevertheless, our study showed a strong correlation between the 360-degree evaluation and small-scale OSCE \u00fe DOPS \u00fe IM-ITE \u00d2 scores. Accordingly, the following of the core competencies of trainees regularly by IM-ITE \u00d2 and 360-degree evaluation in our system may be valid on the program level. In OSCE, it was not possible to blind faculty raters to the PGY 1 resident's identity. Our study was included OSCE before and after 3 months of internal medicine training course. In order to avoid the bias coming from the fact that PGY 1 residents with a weaker OSCE performance might have tended to prepare more diligently for their next post-course OSCE, the raters of small-scale OSCE in our study did not give any feedback to PGY 1 residents before they completed the post-course OSCE. Meanwhile, the trainees knew their OSCE before and OSCE 3month scores only after finishing the entire testing sequence.\n\nThird, only four practical consideration stations were included in the DOPS of our study. Previous study had suggested that if the DOPS were to be used for certification, a greater number of skills stations should be included where the consequences of an erroneous pass/fail judgment were serious. 21 Nonetheless, we arranged two raters (both an expert and a senior physician) to increase the reliability by the multisource evaluation. Notably, the inter-rater agreements were quite good for the four technical skills of DOPS in our study. Use of the experts for the DOPS evaluation can also avoid the \"halo effect\" due to having previous experience with the PGY 1 resident, which could introduce positive or negative bias in scoring.\n\nFinally, previous studies have shown that the reliability of the 360-degree evaluation can be elevated by increasing the number of raters. Our current study only did a rough estimation about the number of raters needed for the reliability of the 360degree evaluation to reach 0.7e0.8. In fact, a detailed analysis of heterogeneity of raters and PGY 1 residents should also be considered, along with analyses by G-theory, in the future.\n\nIn conclusion, the strong correlations between the 360degree evaluation and the small-scale OSCE \u00fe DOPS \u00fe IM-ITE \u00d2 scores suggests that both methods measure the same quality. In the future, a small-scale OSCE associated with DOPS and the IM-ITE \u00d2 could be an important assessment method in evaluating the performance of PGY 1 residents.\n\nThe content of small-scale Objective Structural Clinical Examination stations of PGY 1 residents. "}