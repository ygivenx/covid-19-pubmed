{"title": "Health constraints on the agricultural recycling of wastewater sludges", "body": "The objective of sewage treatment is to remove solids and to reduce its biochemical oxygen demand (BOD) before returning the treated wastewater to the environment. Sewage sludge, increasingly referred to as biosolids, is an inevitable product of wastewater treatment. Conventional wastewater treatment processes comprise separate process streams for the liquid and solid fractions (sludge). The overall aim of treatment (in terms of solids) is: (i) to reduce to the minimum the amount of solids in the treated effluent in order to achieve discharge standards; and (ii) maximize the level of solids in the sludge in order to minimize the volume requiring further treatment and disposal.\n\nSludge is produced at various stages within the wastewater treatment process ( Fig. 17.1) . Usually, these solids are combined and treated as a whole. Dedicated sludge treatment may not be available at all works, particularly smaller plants. In these circumstances it is normal practice to transport the sludge to a larger works for subsequent treatment.\n\nPreliminary treatment consists of screening through bar screens to remove coarse solids and buoyant materials, such as plastics or rags, which may become trapped in pumps or other mechanical plant. The screenings are usually removed from the process stream and disposed of separately by landfilling or incineration. Occasionally, screenings may be shredded (comminuted) to reduce their size and returned to the process stream. This may cause problems with downstream processes and, as a consequence, is rarely practised at works which incorporate secondary (biological) treatment. The other component of preliminary treatment is grit removal, which is accomplished in chambers (or channels) or by centrifugation, taking advantage of the greater settling velocities of these solids. The material is largely inorganic in nature and is usually disposed of to landfill.\n\nPrimary treatment is designed to reduce the load on subsequent biological (secondary) treatment processes. Although the design of primary sedimentation tanks differs, they achieve the removal of settleable solids, oils, fats and other floating material, and a proportion of the organic load. Efficiently designed and operated primary treatment processes should remove 50 -70% of suspended solids and 25 -40% of the BOD (organic load). The separated solids have a high organic content and are usually treated in order to stabilize the material prior to disposal.\n\nBiological processes are used to convert dissolved biodegradable organic substances and colloidal material into inorganics and biological solids (biomass). There are several secondary treatment processes, but these may be divided into fixed film (e.g. trickling filters, rotating biological contactors) and suspended processes (e.g. activated sludge). Solids separation is the final stage of many of these treatment systems, which produces a sludge, the nature of which will depend on the upstream treatment process. Sludges arising from secondary treatment are usually combined with primary sludge and treated as a whole.\n\nTertiary treatment will only be required for treatment works subject to specific discharge conditions. Many of these processes will not produce sludge and those that do are likely to generate only small amounts requiring dedicated treatment. Solids removed by granular media filtration will be passed to the primary sludge treatment process.\n\nThe sludge obtained from the various stages is usually in the form of a liquid containing between 0.5 and 6% dry solids. The typical composition of raw (untreated) and anaerobically digested sludge is shown in Table 17 .1. The nature and extent of any treatment depends on the means of final disposal or beneficial use. The aim of treatment is to reduce the water and organic content of the sludge and render it suitable for disposal or reuse. There are several commonly used methods of sludge treatment, including long-term storage (in lagoons), lime stabilization, digestion (aerobic or anaerobic), air drying, thermal drying, and incineration or gasification (for energy recovery). Detailed descriptions of these processes are outside the scope of this handbook. However, the effect of the various treatments on pathogens is described below.\n\nSewage sludge contains valuable amounts of plant nutrients (nitrogen and phosphorus) and trace elements (Table 17. 2). For this reason sludge has historically been applied to agricultural land as part of an integrated farm management plan. Other options for disposal include energy recovery and land reclamation activities. In Europe, North America and elsewhere, the disposal of sewage sludge is subject to strict controls designed to protect soil quality while encouraging the use of sludge in agriculture. Codes of Practice, such as those published by the UK Department of the Environment (DoE, 1996) and the UK Ministry of Agriculture, Fisheries and Food (MAFF, 1998a,b) , provide advice on practical aspects of utilizing sewage sludge in agriculture.\n\nStrict limits are set on the amounts of potentially toxic elements permitted in sludge which may be used in agriculture. Application rates are controlled to minimize the accumulation in the soil of toxic metals. Due to the low levels of metals in sludge, application rates are governed in practice by maximum nitrogen application rates (250 kg/ha y 21 or 500 kg/ ha y 21 ) and to balance phosphorus addition with crop off-take.\n\nInformation on the amounts of sewage sludge produced and its disposal is collected by a number of countries, principally the USA and the member states of the European Union. Annual sludge production in the USA is in the region of 6.8 million tonnes dry solids (M tds) of which 54% is applied to land (Bastian, 1997) . The figures for the EU are 5.1 M tds and 48% respectively (CEC, 1999) . Within the EU amounts of sludge produced vary considerably, with Germany producing the largest amount of treated sludge followed by the UK and France (Fig. 17 .2). The proportion of treated sludge used in agriculture varies across the European Union, with just over 10% of sludge production in Ireland being applied to land compared with 66% in France ( Fig. 17.3) . Factors affecting the amount of sludge applied to agricultural land include topography, land use, climatic conditions, and the availability of alternative means of disposal. In the UK, sludge production is increasing, principally as a result of the EU Directive on the treatment of urban wastewater (CEC, 1991) . The cessation of sea disposal has resulted in a greater proportion of sludge being used in agriculture (Table 17. 3), a trend which is projected to continue in the medium term ( Fig. 17.4 ).\n\nThe treatment and ultimate disposal of sewage sludge, including domestic septage, derived from the treatment of domestic sewage, is governed by 40 CFR Part 503 rule (US EPA, 1993) . The regulations were developed over In order to attain Class A status sludges must have been treated by 'a process to further reduce pathogens' (PFRP). Such a process is considered capable of reducing the number of pathogens to those normally present in the soil. Provided that the treated sludge complies with end product microbiological standards (Table 17 .4), it may be applied without restriction to a wide range of land types, including that intended for agricultural or horticultural The implicit goal of the requirements for Class A biosolids is to reduce the number of pathogens in sewage sludge to below the level of detection (, 3 MPN salmonella, , 1 PFU enteric viruses, and , 1 viable helminth ova -all per 4-g dry weight). The goal for the production of Class B biosolids is the reduction in the number of pathogens to levels that are unlikely to pose a public health risk (US EPA, 1999).\n\nThe controls on the application of sewage sludge to agricultural land within member states derive from Council Directive 86/278/EEC published in 1986 for implementation within 3 years (CEC, 1986) . The principal rationale of the Directive was to minimize the accumulation in the soil of heavy metals or other potential toxic elements (PTE) with the objective of protecting soil fertility and public health. However, the Directive included measures for controlling transmissible disease by introducing constraints on the use of sludge. Article 7 of the Directive requires Member States to prohibit the use of sludge or the supply of sludge for use on: \u2020 grassland or forage crops if the grassland is to be grazed or the forage crops to be harvested before a certain period has elapsed. This period, which shall be set by the Member States, taking particular account of their geographical and climatic situation, shall under no circumstances be less than 3 weeks: Faecal coliforms/g ds Less than 1000 Less than 2 000 000 p Salmonellae 4/g ds Less than 3 Enteroviruses pfu 4/g ds Less than 1 Parasite ova 4/g ds Less than 1 ds Dry solids. p Geometric mean of seven samples.\n\nSludge treatment processes used in the UK. \u2020 soil in which fruit and vegetables are growing, with the exception of fruit trees \u2020 ground intended for the cultivation of fruit and vegetable crops which are normally in direct contact with the soil and normally eaten raw, for a period of 10 months preceding the harvest of crops and during the harvest itself \u2020 sludge shall be treated before being used in agriculture 1 . Member States may nevertheless authorize, under conditions laid down by them, the use of untreated sludge if it is injected or worked into the soil.\n\nIn the UK, The Sludge (Use in Agriculture) Regulations 1989 directly implement the provisions of the Directive (Anon, 1989) . This was accompanied by a Code of Practice (DoE, 1996) which provided practical guidance on how the requirements of the Directive could be met. It recognizes that pathogens may be present in untreated sludges and that their numbers can be reduced significantly by appropriate treatment. Examples of effective treatment processes are given in the Code (Table 17 .5).\n\nAt the time that the Code was prepared the pathogens of concern were considered to be salmonellae, Taenia saginata (human beef tapeworm), potato cyst nematodes (Globodera pallida and Globodera rostochiensis) and viruses. The guidance was based on the concept of multiple barriers to the prevention of transmission of pathogens when sludge was applied to agricultural land. The barriers are: \u2020 Sludge treatment, which will reduce pathogen content \u2020 Restrictions on which crops may be grown on land to which sludge has been applied \u2020 Minimum intervals before grazing or harvesting.\n\nThe scientific and public health principles which underpin this concept are valid. They recognize that for certain crops the risk of disease transmission is unacceptable, i.e. salad items which have a short growing period and which are to be consumed raw. For other crops the combination of treatment and a suitable period of no harvesting will result in the numbers of pathogenic microorganisms being reduced below a minimum infective dose (MID). The concept of MID is important -it relates to the number of organisms which must be ingested to cause disease. It varies widely depending not only on the particular pathogen but also on the susceptibility of the host (Table 17 .6). For example in the young, elderly, pregnant or those whose immunity is reduced the minimum number of organisms required to initiate disease is much smaller. Despite the current concerns surrounding the risks to food safety, it is important to recognize that there have been no instances documented in which disease transmission to man or animals has occurred where the provisions of the relevant UK Regulations and Codes of Practice were followed.\n\nPathogens are microorganisms that are capable of causing disease in the host species (man, animals or plants). All the major groups of microorganisms contain species which are pathogenic including viruses (e.g. hepatitis virus), bacteria (e.g. salmonellae), fungi (e.g. Aspergillus), protozoa (e.g. Cryptosporidium) and helminths (e.g. Taenia). Although there are several plant pathogens which may potentially be present in sewage sludge (e.g. brown rot, potato root eelworm and beet rhizomania), this review will deal with pathogens affecting man and animals. Many of these are described as zoonotic, i.e. directly transmissible to man from animals. Examples of zoonotic infections include salmonellosis and cryptosporidiosis. This is a particularly important factor when considering the risks to human health arising from the use of sludge in agriculture.\n\nThe type and number of pathogens that are likely to be present in untreated sewage will depend on the inputs to the sewerage system. The spectrum of human pathogens will mirror the incidence of infection in the community. People suffering from diseases of the gastrointestinal tract will excrete large numbers of the pathogen in their faeces. Industrial sources of pathogens include meat processing plants, abattoirs and livestock facilities. The World Health Organization in its review of health risks arising from sewage sludge applied to land described a wide range of pathogens that could be present in sludge (WHO, 1981) . This was subsequently updated and expanded by Strauch (1991) and the United States EPA who collated the data shown in Table 17 .7 (USEPA, 1989) .\n\nThe list is extremely comprehensive and, in reality, the risk from many of these microorganisms is very small. The organisms shown in bold are those identified by the US EPA as posing a significant risk to human health and which were taken into account in the development of the current Part 503 Regulations (US EPA, 1992) . It is interesting to note that at that time they did not consider Escherichia as posing a significant risk to health. It is now known that certain shiga toxin-producing strains, such as E. coli O157 2 , are capable of being transmitted by contaminated foodstuffs (Armstrong et al., 1996; Tauxe, 1997; Mead and Griffin, 1998; Parry and Palmer, 2000) .\n\nIn practice, the list of microorganisms that we need to be concerned with is relatively small. The pathogens of concern will vary from region to region depending on the nature and prevalence of endemic infectious intestinal disease within the indigenous population. For example, data for England and Wales collated by the PHLS Communicable Disease Surveillance Centre reveal that over half of all notified infections are due to Campylobacter (Fig. 17.5) .\n\nIn contrast, intestinal parasites, particularly Ascaris, are a major disease burden in the developing world. These parasites form cysts or ova which are especially robust and resistant to environmental conditions, attributes which contribute to high levels of re-infection. The literature contains numerous reports of surveys and investigations into the occurrence of pathogens and indicator bacteria in wastewater and sludge treatment plants. Because of methodological and geographical differences and, in some instances, lack of details regarding treatment processes, many of these data are not directly comparable. However, the data shown in Table 17 .8 provide an indication of the likely numbers of indicators and pathogens in domestic wastewater and sludges.\n\nIt must be recognized at the outset that treatment is designed to stabilize sewage sludge and reduce its putrescence. Pathogens may be inactivated as a consequence of the particular treatment applied. It has not been normal practice to optimize sludge treatment processes for pathogen reduction. Indeed to do so may reduce the effectiveness of the stabilization process.\n\nA review of the literature by Ward and colleagues (1984) showed that the range of pathogen inactivation reported was large, depending on the extent of the treatment process and variation between operating conditions, even for the same generic treatment process (Table 17 .9).\n\nThere are limited data on the effect of some of these processes on certain pathogens. In practice, research in this area has been restricted to those pathogens with a high prevalence and likely to cause disease (e.g. salmonellae) and those that are more likely to exhibit resistance to the sludge treatment process (e.g. Ascaris). In comparison little is known about the pathogens which have only recently emerged as public health issues, most notably E. coli O157. The recent emergence of this pathogen means that there is little information available on the fate of E. coli O157 (and other shiga toxin-producing E. coli (STEC)) during the treatment of wastewater and sewage sludge. Despite the highly infectious nature of STEC and the presence of multiple virulence factors there is evidence that it is no more resistant to inactivation during sludge treatment than the indigenous populations of E. coli in sewage and sludge (Horan, personal communication).\n\nThe inactivation of indigenous E. coli in fullscale sludge treatment processes was investigated during a 3-month study, which looked at nine different sludge treatment processes at 35 sites in the UK (UKWIR, 1999). All of the processes surveyed reduced the numbers of E. coli. So-called 'enhanced' treatment processes, such as composting, lime addition and thermal drying, were capable of reducing numbers of E. coli to the detection limit of the analytical method. For all of these methods, over 90% of results showed bacterial reductions of 6 log or greater. Lagooning of sludge was capable of significantly reducing numbers of E. coli and, depending on the method of operation, reductions in the order of 5 log were observed. Mesophilic anaerobic digestion (MAD), the process carried out at the majority of sites surveyed, reduced numbers of E. coli by, on average, between 1.4 and 2.3 log depending on the solids content of the product. For sites producing a liquid product (2 -4% ds) 78% of all reductions for were in the range 1 to 2 log. Where digested sludge was subsequently dewatered to produce a cake, 89% of results showed reductions in the range 2 to 4 log. The one vermiculture site in the survey showed results intermediate between MAD and the 'enhanced' treatment processes (Table 17 .10).\n\nCryptosporidium parvum is another pathogen of increasing importance. Wastewater discharges and run off from agricultural land are Faecal coliforms 10 8 10 7 10 6 ,2 1 0 7 10 6 Salmonellae 10 3 10 2 10 ,2 1 0 3 10 2 Shigella 10 3 10 2 1 ,2 1 0 2 3 Listeria 10 4 10 3 Campylobacter 10 5 10 4 10 Enteric virus 5 \u00a3 10 4 1 \u00a3 10 4 10 3 0.002 10 3 10 2 Helminth ova 8 \u00a3 10 2 10 0.08 ,0.08 10 10 Giardia cysts 10 4 5 \u00a3 10 3 2.5 \u00a3 10 3 3 1 0 2 10 a Including coagulation, sedimentation, filtration, disinfection. an important source of Cryptosporidium oocysts found in watersheds. The transmission of cryptosporidiosis is zoonotic and the possibility exists offoodborne infection arising from the use of sewage sludge in agriculture. Stadterman et al. (1995) found that a laboratory activated sludge plant removed 98.6% of seeded Cryptosporidium parvum oocysts. In a comparison of different treatment regimes, activated sludge and anaerobic digestion were found to be the most effective means of removing oocysts, the latter destroying 99.9% in 24 hours. Studies of anaerobic mesophilic digestion under laboratory conditions showed that oocysts added to the contents of a digester operating at 358C rapidly lost viability (as measured by excystation), decreasing to 17% after 3 days from an initial 81% viability (Whitmore and Robertson, 1995) . Losses of viability in distilled water and anaerobic sludge at 358C were similar, amounting to 90% after 18 days, indicating that the principal effect on viability was temperature. Oocysts exposed to mesophilic anaerobic digestion for 3 days and then stored for a further 14 days were completely inactivated. Aerobic digestion or pasteurization, both at 558C, caused 92% loss of viability in 5 minutes. Thermophilic anaerobic digestion at 508C resulted in complete inactivation within the first 24 hours (Whitmore and Robertson, 1995) .\n\nRecently expressed concerns over the use of sewage sludge in agriculture have focused on the risks to human health arising from the production of foods on land to which sludge has been applied (Anon, 1998a) . A number of exposure pathways whereby foodstuffs become contaminated with pathogens can be envisaged. The exposure pathways relevant to sewage sludge in agricultural production are:\n\nThese may be developed into a conceptual model which describes the framework for a microbiological risk assessment (Fig. 17.6 ).\n\nOther routes of exposure, which do not involve the food chain, can be identified. These include direct contact with sludge-treated soil or indirect contact via companion animals. The risk of exposure by this route (direct contact) is probably greatest among children. It can be seen that there are several pathways that are unrelated to the use of sewage sludge, probably the most important of which is the application to land of organic wastes such as animal slurries and manures. The use of such materials in agriculture is less regulated than for sewage sludge and accounts for the majority of organic waste spread to land (Table 17 .11). Despite this, the focus of attention has been on the human health risks via the food chain from the application of sewage sludge to agricultural land (RCEP, 1996; Anon, 1998a) .\n\nAs previously mentioned, at the time that existing controls on the agricultural use of sewage sludge were being formulated, the pathogens of concern were salmonellae and T. saginata. In the intervening period, pathogens such as shiga toxin-producing Escherichia coli and Cryptosporidium have been recognized as important causes of intestinal infectious disease in humans. Any assessment of health risks associated with the beneficial use of sewage sludge in agriculture must consider these two pathogens.\n\nShiga toxin-producing E. coli (STEC) are now recognized as an important group of enteric pathogens. Although there are many serotypes capable of producing shiga toxins, E. coli O157:H7 is the most widely known. This organism was first described in 1982 following an outbreak of haemorrhagic colitis in the USA (Riley et al., 1983) . Outbreaks have been associated with the consumption of foodstuffs, drinking water, and swimming in natural surface waters. Zoonotic infections have also been reported. The majority of cases are believed to be foodborne, with an estimated 85% of cases (n \u00bc 110 220) in the USA suspected to be food-related (Mead et al., 1999) . There is very little information concerning the presence of STEC in sewage and sewage sludge. It is reasonable to assume that domestic sewage will contain STEC, if not continuously then intermittently, reflecting the incidence of infection in the community. The likelihood of STEC being present will be greater for those wastewater treatment works receiving wastes from animal-handling facilities, such as markets, abattoirs and meat processing plants.\n\nSurveillance of E. coli O157 in animals presented for slaughter carried out in northern England revealed that 15.7% of cattle, 2.2% of sheep and 0.4% of pigs were positive for the organism (Chapman, 2000) .\n\nSurvival of E. coli O157 in the environment has been investigated by a number of workers. Maule (1997 Maule ( , 2000 showed that survival of the organism was found to be greatest in soil cores containing rooted grass. Under these conditions viable numbers were shown to decline from approximately 10 8 /g soil to between 10 6 and 10 7 /g soil after 130 days. When the organism was inoculated into cattle faeces it remained detectable at high levels for more than 50 days. In contrast, the organism survived much less readily in cattle slurry and river water where it fell in numbers from more than 10 6 /ml to undetectable levels in 10 and 27 days, respectively. Survival of E. coli O157:H7 in bulk manures may be prolonged, with the organism being detected for more than one year in static piles of ovine manure (Kudva et al., 1998) . Survival was reduced if the manure piles were aerated.\n\nThe fate of E. coli O157 present in animal slurry applied to pasture was investigated by Fenlon and colleagues (2000) . Following application, numbers of both E. coli and E. coli O157 declined steadily with greater than 2 log reduction within 29 days. Relatively few cells (2% of total) were transported away from the soil surface and into the deeper layers of the soil. Run-off following heavy rainfall resulted in a loss from the soil of 7% of the E. coli applied in the slurry. A recent ecological study on predation of E. coli O157 by Acanthamoeba polyphaga has shown that the bacterial cells are capable of surviving and even replicating within this common environmental protozoan (Barker et al., 1999) . This may be important in the dissemination and survival of STEC within the environment.\n\nHuman infection with Cryptosporidium was first reported in 1976 (Fayer et al., 2000) . It is now apparent that Cryptosporidium is a significant cause of infectious intestinal disease (IID), accounting for about 5% of IID in which the causative organism is identified. In England and Wales, during 1999, there were nearly 5000 laboratory confirmed cases of cryptosporidiosis (n \u00bc 4759) (Anon, 2000a) ; in the USA the estimated number of cases is 300 000 of which 10% are believed to be foodborne (Mead et al., 1999) . Oocysts of C. parvum are environmentally hardy and, under certain conditions, may remain viable for many months. Survival studies in microcosms containing untreated river water showed that oocysts are extremely persistent, the times for 10 log reductions in viability being 160 days at 158C and 100 days at 58C (Medema et al., 1997) . Investigations into the survival in soils treated with sewage sludge showed that viability declined by 20 -40% at 208C over 44 days. Temperature was the principal factor affecting oocyst survival (Whitmore and Robertson, 1995) .\n\nLittle is known about the movement of oocysts through the soil. The most relevant data were reported by Mawdsley and colleagues (1996a) who studied the transport of Cryptosporidium oocysts through soil following the application of slurry to a poorly draining silt clay loam soil. Bovine slurry seeded with 5 \u00a3 10 9 C. parvum oocysts was applied to the surface of soil blocks (80 cm \u00a3 56 cm \u00a3 20 cm) removed from a perennial ryegrass ley at an application rate equivalent to 50 m 3 /ha. The blocks were irrigated 24 h following slurry application and periodically thereafter. Samples of run-off (at 4 cm depth) and leachate (at 20 cm depth) were collected and the number of oocysts enumerated. After 70 days the blocks were destructively sampled and examined for the presence of oocysts. Experiments were carried out in triplicate. Numbers of oocysts leaching from the blocks declined from 8.4 \u00a3 10 6 on day 1 to 2.3 \u00a3 10 4 at day 70. Oocysts levels were consistently lower in run-off compared with the leachate from the base of the soil blocks. Numbers fell below the limit of detection after 21 days and 28 days in two blocks, but were detectable in the third block for the duration of the experiment (70 days). These results suggest that oocysts tend not to become associated with soil particles, being either transported away in run-off or moving vertically downwards through the soil column. The majority of oocysts were retained in the top 2 cm of soil (Mawdsley et al., 1996b) .\n\nThe process of microbiological risk assessment is now considered to comprise three phases: problem formulation, analysis, and risk characterization (ILSI, 2000) . The analysis phase consists of two elements: characterization of exposure and characterization of human health effects (Fig. 17.7) .\n\nCharacterization of exposure requires an evaluation of the interaction between the pathogen, the environment and the human population. Factors that need to be considered include the virulence of the pathogen, survival in the environment, route of infection, numbers of pathogen present, effectiveness of control/treatment processes, infectious dose, severity of illness and size of exposed population.\n\nUnlike the field of chemical toxicology, microbiological risk assessment is in its infancy. There exist major gaps in our knowledge about the organisms of concern. This is particularly the case for the emerging pathogens such as STEC and Cryptosporidium. There have been attempts to assess the risks posed by Cryptosporidium in drinking water supplies (Gale, 1996 (Gale, , 1999 Haas, 2000) . However, the foodborne route has not been modelled, probably because of the received view that cryptosporidiosis is primarily waterborne, despite evidence demonstrating the potential for foodborne transmission. Cryptosporidium oocysts have been found on the surface of fresh, raw vegetables obtained from retail markets (Ortega et al., 1991; Monge et al., 1996) . In the UK, an outbreak of cryptosporidiosis affecting 50 school children was linked to the consumption of improperly pasteurized milk (Gelletli et al., 1997) . In the USA, outbreaks have been associated with the drinking fresh-pressed apple juice (non-alcoholic cider) (Millard et al., 1994) . Outbreaks of cryptosporidiosis associated with infected food-handlers demonstrate clearly the potential for significant foodborne transmission of Cryptosporidium (Besser-Wiek et al., 1996; Quiroz et al., 2000) . On the other hand, it is clear that STEC infection is primarily foodborne (Mead et al., 1999) . Data are required on the levels of STEC in treated sludge, their survival following land application and the potential for transfer to food crops before a microbiological risk assessment can be performed. Research is currently being undertaken to address these issues.\n\nAgainst a background of concern over methods of food production in the UK, the water industry, under the auspices of Water UK, and representatives of the food suppliers agreed a set of guidelines matching the level of sewage treatment with the crop under cultivation (Anon, 1998b) .\n\nThe Safe Sludge Matrix (Anon, 2000b, Tables 17.12 and 17.13) forms the basis of the agreement and consists of a table of crop types, together with clear guidance on the minimum acceptable level of treatment for any sewage sludge (biosolids) based product, which may be applied to that crop or rotation. The agreement was driven by the desire to ensure the highest possible standards of food safety and to provide a framework that gives the retailers and food industry confidence that sludge reuse on agricultural land is safe. The matrix enables farmers and Fig. 17 .7 Analysis phase of a microbiological risk assessment for foodborne pathogens (ILSI, 2000) . growers to continue to utilize the beneficial properties in sewage sludge as a valuable and cost effective source of nutrients and organic matter.\n\nThe main impact was the cessation of raw or untreated sewage sludge being used on agricultural land. As from the end of 1999, all untreated sludges have been banned from application to agricultural land used to grow food crops. Treated sludge 3 can only be applied to grazed grassland where it is deep injected into the soil. The regulations require that there will be no grazing or harvesting within 3 weeks of application. Where grassland is reseeded, sludge must be ploughed down or deep injected into the soil.\n\nMore stringent requirements apply where sludge is applied to land growing vegetable crops and, in particular, those crops that may be eaten raw (e.g. salad crops). Treated sludge can be applied to agricultural land which is used to grow vegetables provided that at least 12 months has elapsed between application of the sludge and harvest of the vegetable crop. Where the crop is a salad, which might be eaten raw, the harvest interval must be at least 30 months. Where enhanced treated sludges 4 are used, a 10-month harvest interval applies.\n\nFrom an environmental perspective there is a persuasive argument that, of the disposal options available, recycling nutrients by means of applying sewage sludge to land, with appropriate safeguards, is the Best Practicable Environmental Option (BPEO) (CEC, 1986; RCEP, 1996) . The risks to human, animal and plant health were taken into account when developing the current regulations and codes of practice. The fundamental principle (for reducing disease transmission risk) implicit to these controls on the use of sludge in agriculture is the concept of imposing multiple barriers to the recycling of pathogens from sludge to their hosts. The effectiveness of this approach is borne out in practice as noted by the Royal Commission on Environmental Pollution (RCEP, 1996) which concluded that, 'There are no instances in the UK in which a link has been established between the controlled application of sewage sludge and occurrence of disease in the general population through water or food contamination'. However, it is the case that the current controls predate the emergence of pathogens such as Cryptosporidium and STEC and may not sufficiently reduce the risk associated with these microorganisms.\n\nMore data are required on the numbers and fate of these emergent pathogens before a meaningful microbiological risk assessment can be carried out. Research is being undertaken in the UK, USA and elsewhere to generate these data. The likelihood is that the controls on the use of sewage sludge in agriculture will be strengthened as results of this research and in the light of public perception and expectations about food safety and environmental risks."}