{"title": "Comparing Neural-Network Scoring Functions and the\nState of the Art: Applications to Common Library Screening", "body": "Because of the high\ncost and time requirements associated with\ntraditional high-throughput screens, many researchers now use computational\nmethods to prefilter candidate ligands prior to experimental testing.\nA number of ligand-based computational techniques for identifying\nlikely binders have been utilized. These include 2D screening with\nfingerprints,1\u22123 shape-based screening,4,5 and pharmacophore\nmatching,6 which identify potential actives\nby comparing their atomic connectivities, three-dimensional shapes,\nand three-dimensional pharmacophores to those of known ligands, respectively.\n\nWhen structural information about a macromolecular drug target\nis known (e.g., from X-ray crystallography or NMR), computer docking\nprograms are often used to identify candidate ligands. These programs\nposition three-dimensional models of small molecules into models of\ntarget binding pockets; associated scoring functions subsequently\npredict the binding affinities of these \u201cposed\u201d candidate\nligands. While certainly useful as an enrichment tool, docking has\nnot yet reached its full potential. In part, the inaccuracies inherent\nin this technique stem from factors that are independent of the scoring\nfunction itself. For example, most docking programs do not account\nfor full receptor flexibility, despite the fact that flexibility plays\na critical role in modern theories of small-molecule binding (e.g.,\ninduced-fit7,8 and population-shift9\u221212 models). Indeed, efforts to account\nfor receptor flexibility have proven effective and have led to the\nidentification of a number of experimentally validated ligands.8,13,14 Similarly, most docking programs\ndo not account for binding-pocket water molecules, which can in some\ncases play critical roles in mediating receptor\u2013ligand interactions.15 Even a \u201cperfect\u201d docking program\nwould fail to identify true ligands when presented with sterically\nincompatible binding-pocket conformations and/or pockets devoid of\ncrucial water molecules.\n\nHowever, some of the inaccuracies associated\nwith computer docking\nare intrinsic to the scoring functions themselves. In recent years,\nmuch work has been directed toward improving these functions without\nsacrificing speed.16,17 Some of our own recent efforts\nhave focused on training neural networks to rapidly predict the binding\nenergies of protein\u2013ligand complexes leading to the creation\nof two neural-network-based scoring functions, NNScore 1.018 and NNScore 2.0.19 Neural networks are computer models that mimic, albeit inadequately,\nthe microscopic architecture and organization of the brain. Biological\nneurons and synapses are simulated in silico as \u201cneurodes\u201d\nand \u201cconnections.\u201d Data to be analyzed is encoded on\nan input layer of neurodes, triggering a cascade of signals that propagates\nthrough the network. Both the organization and number of the neurodes\nas well as the weights (i.e., strengths) assigned to each neurode\u2013neurode\nconnection serve to modify the initial input signal during propagation.\nThe cascade eventually reaches an output layer of artificial neurons,\nwhere an analysis of the original input signal is ultimately encoded.\n\nIn the NNScore implementations, the strengths of the connections\nbetween neurodes were varied until the networks could reliably predict\nbinding affinity when given descriptors of a ligand\u2013receptor\ncomplex. For NNScore 1.0, these descriptors included the number of\nprotein\u2013ligand close contacts, categorized by AutoDock atom\ntypes; the electrostatic energy of those close contacts; the number\nof ligand atoms of each atom type; and the number of ligand rotatable\nbonds. For NNScore 2.0, the input additionally included the descriptors\nprovided by the BINANA algorithm20 (counts\nof the number of hydrophobic, \u03c0\u2013\u03c0, hydrogen-bond,\nand salt-bridge interactions), as well as the components of the Vina\nscoring function (steric, hydrophobic, hydrogen-bond, and ligand-rotatable-bond\nterms).21\n\nWhile some efforts have\nbeen made to demonstrate the favorable\nperformance of these neural-network scoring functions, these efforts\nfocused on a limited number of systems, and the neural-network functions\nwere not directly compared to top-tier proprietary docking programs\nlike Schr\u00f6dinger\u2019s Glide.22\u221225 In the current work, we use AutoDock\nVina21 and Glide26,27 to dock the diverse compounds of the NCI diversity set III, a popular\ncompound library available through the National Cancer Institute (NCI),\ninto the 40 protein receptors of the Directory of Useful Decoys (DUD).28 Additionally, Vina- and Glide-docked poses are\nreevaluated using NNScore 1.0 and 2.0. The mean screening performance\nobtained when candidate ligands are docked with Vina and rescored\nwith NNScore 1.0 is not statistically different than the mean performance\nobtained when docking and scoring with Glide. This is particularly\nnoteworthy given that Glide, while state of the art, is expensive\nand has a restrictive token system. In contrast, AutoDock Vina and\nNNScore 1.0 are both free and open source.\n\nAdditionally, we\nnote a correlation between certain chemical properties\nand the associated docking scores, suggesting systematic bias. For\nboth Vina and NNScore, docking scores tended to correlate with small-molecule\nchemical properties like size and polarizability, regardless of the\ntarget receptor. Compensating in part for these potential biases improves\nvirtual-screen performance. Creating composite scoring functions suited\nto a specific receptor can improve performance further still.\n\nThough the mean screening performances of Glide and (Vina + NNScore\n1.0) over all 40 DUD receptors are not statistically different, our\nresults do confirm what has been found by others: the best scoring\nfunction to use for a specific pharmacological target, be it Vina,\nNNScore, or Glide, is highly system dependent.23,29\u221232 Positive controls (known inhibitors), when available, should be\nincluded in virtual screens to ensure that the docking protocol chosen\nis suited to the system at hand. However, when positive controls are\nnot available, we recommend (Vina + NNScore 1.0) as a potential alternative\nto proprietary docking programs like Schr\u00f6dinger\u2019s Glide.\n\nThe 40 protein receptors of the\nDUD28 were downloaded from the DUD website\n(http://dud.docking.org/). All ligands and water molecules\nwere removed. Hydrogen atoms were added to the protein structures\n(neutral pH) and hydrogen bonds were optimized using Schr\u00f6dinger\nMaestro\u2019s Protein Preparation Wizard.61 These processed models were then converted to the AutoDock PDBQT\nformat with MGLTools 1.5.433 for use in\nthe Vina and NNScore screens.\n\nModels of ligands known to bind\nto the 40 DUD receptors were likewise downloaded from the DUD website.\nAdditionally, the compounds of the NCI diversity set III were obtained\nfrom the website of the NCI/NIH Developmental Therapeutics program\n(http://dtp.nci.nih.gov/). All these small molecules were\nprocessed with Schr\u00f6dinger\u2019s LigPrep module to generate\nmodels with appropriate tautomeric, isomeric, and ionization states\nfor pH values ranging from 5.0 to 9.0. These models were ultimately\nconverted to the PDBQT format using MGLTools 1.5.433 for use in the Vina and NNScore screens, and to the SDF\nformat for use in the Glide screens. A few molecular models could\nnot be generated; 1560 NCI models were ultimately used in the virtual\nscreens.\n\nFor Vina docking, the default parameters\nwere used. All ligands were docked into regions (boxes) centered on\nthe respective receptor binding pockets. The centers and dimensions\nof the boxes were taken from the DUD. Edge lengths ranged from 35.9\nto 50.0 \u00c5, and box volumes ranged from 56,365.8 to 88,845.5 \u00c53. Each Vina docking generated multiple poses. For the Vina\nanalysis, the best-scoring pose as judged by the Vina docking score\nwas used. For the NNScore analysis, all Vina poses were reevaluated\nwith NNScore 1.018 and NNScore 2.0,19 and the best-scoring Vina pose as judged by\nNNScore was considered for subsequent analysis. In the case of NNScore\n1.0, the original programmers provided 24 neural networks that were\nparticularly adept at identifying potent ligands when given descriptors\nof crystallographic ligand\u2013receptor complexes. The final NN1\nscores for each docking were derived by averaging the output of these\n24 networks.\n\nFor Glide docking, parameters similar to the defaults\nwere likewise used. The docking grid was defined by two cubes centered\non the DUD-specified active sites. The inner cube, with 14 \u00c5\nedges, imposed restrictions on the location of the ligand center,\nand the outer cube, with 45 \u00c5 edges, imposed restrictions on\nthe location of all ligand atoms. For each system, both the positive\ncontrols and the compounds of the NCI diversity set III were first\ndocked with Glide HTVS. To test more advanced Glide docking protocols,\nthe top 50% of these compounds were subsequently docked with Glide\nSP, and the top 25% of the remaining compounds were docked with Glide\nXP.34 Glide-HTVS- and Glide-XP-docked models\nwere also rescored with NNScore 1.0 and 2.0. The default parameters\nwere again used for both.\n\nIn all screens, where there were multiple\nmodels of a distinct\ncompound due to alternate tautomeric, stereoisomeric, or protonation\nstates, the best-scoring model was selected, and all others were discarded.\n\nMolecular properties\nwere calculated using Schr\u00f6dinger\u2019s Maestro suite. Compounds\nwere processed with LigPrep to optimize geometry and ensure electrical\nneutrality. QikProp was then used to calculate molecular properties.\n\nReceiver\noperating characteristic (ROC35) curves\ncan be generated when performing virtual screens of compound libraries\nthat include known binders. Following docking and scoring, the compounds\nare ordered by their docking scores. A moving cutoff is then employed\nthat sweeps from the best predicted binder to the worst. At each cutoff,\nthe list of compounds is partitioned. The compounds above the cutoff\nare tentatively considered to be binders, and the compounds below\nare considered to be nonbinders. If all compounds not known to be\nbinders are considered decoys, true and false positive rates can then\nbe calculated for each partition. The ROC curve is generated by plotting\nall (false positive rate, true positive rate) points. ROC or partial\nROC curves were calculated for all screens. The trapezoidal rule was\nused to determine the areas under these curves.\n\nROC curves assess\nthe performance of a virtual screen from the best predicted binder\nto the worst. In practice, however, computational chemists are typically\ninterested in the top-ranked compounds. One useful way to assess the\ntop-performing ligands is to determine the true positive rate for\na given fixed false positive rate.36 In\nthe current work, we identify the true positive rate when the false\npositive rate is fixed at 5%. When required to facilitate the optimization\nof this metric, the ROC curve was smoothed using a linear interpolation\nas implemented in NumPy/SciPy.37\u221241\n\nThe ROC curves generated using the multi-tiered (HTVS-SP-XP)\nGlide\nprotocol against dihydrofolate reductase and epidermal growth factor\nreceptor were not complete enough to calculate the early-performance\nmetric. To maintain equal sample sizes for the ANOVA and t-test analyses, all screens against these two receptors were discarded\nwhen appropriate, regardless of the docking scoring protocol used.\nMean and median early-performance metrics over all 40 DUD receptors\nwere calculated using all available screen results.\n\nIn order to compare multiple docking\nprotocols, it is useful to\nperform a series of \u201cmock\u201d virtual screens that draw\nfrom compound libraries containing both known ligands (\u201cactives\u201d)\nand presumed decoy molecules. As the actives are known a priori, screen\nperformance can be assessed by examining the ability of a given docking\nprotocol to accurately separate out actives from decoys. The performance\nof a given protocol is often receptor specific; consequently, it is\nprudent to perform multiple screens into many diverse receptors when\nattempting to assess global utility.\n\nThe Directory of Useful\nDecoys (DUD),28 an excellent resource for\nfacilitating these assessments, contains 40 diverse protein receptors\nand 2950 known actives. For each active, the DUD contains 36 topologically\ndistinct presumed decoys that are by design chemically similar to\nthe known inhibitors, as judged by metrics like molecular weight,\ncLogP, and the number of hydrogen-bonding groups. In the current work,\nwe use the DUD receptors and known active compounds to assess several\ndocking protocols; however, rather than using the DUD decoy molecules,\nwe instead used 1560 models of compounds from the NCI diversity set\nIII (presumed decoys), a set of publically available, diverse, drug-like\nmolecules provided by the National Cancer Institute free of charge.\n\nWithout wishing to in any way disparage the DUD decoy set, which\nis certainly useful in many contexts, it is important to understand\nwhy we opted to use the NCI compounds as decoys instead. Factors that\ninfluence molecular binding can be divided into two general categories:\nthose that are ligand specific (i.e., independent of the receptor)\nand those that are binding specific (i.e., dependent on specific receptor\u2013ligand\ninteractions). The number of ligand rotatable bonds is a good example\nof a ligand-specific factor, as the immobility of highly flexible\nligands that generally occurs upon binding is thought to be entropically\nunfavorable, independent of the receptor. In contrast, receptor\u2013ligand\ncomplementarity of hydrogen-bond donors and acceptors is a good example\nof a binding-specific factor, as it depends specifically on interactions\nbetween the ligand and the receptor. In predicting ligand binding,\nit is prudent to consider both ligand- and binding-specific factors.\n\nThe DUD decoys were specifically selected so as to be chemically\nsimilar to known actives; they consequently may lack the chemical\nheterogeneity that one would see in a set of compounds selected with\ndiversity in mind (e.g., the NCI diversity set). On one hand, it is\ncertainly possible that some scoring functions may be inappropriately\nbiased in their assessment of ligand-specific factors. What if, for\nexample, a scoring function inappropriately assigns better docking\nscores to compounds with larger dipole moments independent of the\nreceptor, and coincidentally, the actives being screened tend to have\nlarger dipole moments than the decoys? The idea of controlling for\nthis inappropriate bias by intentionally selecting decoy molecules\nwith dipole moments similar to those of the actives certainly has\nits appeal.\n\nOn the other hand, insufficient chemical heterogeneity\nin the decoys\nmay unfairly bias the evaluation of scoring functions that rely on\nvalid assessments of ligand-specific factors. What if, for example,\na scoring function correctly considers the number of ligand rotatable\nbonds in assessing the likelihood of binding but the actives and decoys\nall have the same number of rotatable bonds? Such a scoring function\nwould be inappropriately penalized because its ability to utilize\ninformation about ligand rotatable bonds would be underexploited.\nIndeed, these types of concerns have lead others to use modified versions\nof the DUD decoy set.42,43 Of note, Vina includes one ligand-specific\nterm in its scoring function (number of rotatable bonds),21 and the NNScore functions include additional\nligand-specific terms related to the number of ligand atom types.18,19 Consequently, while we believe convincing arguments can be made\nin favor of using the DUD decoys, in the current work, we opted to\nuse the NCI compounds as decoys instead.\n\nA separate issue related\nto decoy selection must also be addressed.\nHigh-throughput screens typically have hit rates that range from 0.1%\nto 1.0%;44\u221250 it is therefore reasonable to assume that for each DUD protein,\nthe NCI set contains between 1 and 16 \u201cdecoys\u201d that\nare in fact actives. A similar assumption underlies the set of DUD\ndecoys, which have likewise not been explicitly tested to rule out\nbinding. Possible inaccuracies in comparison metrics introduced by\nthese kinds of assumptions are at least in part ameliorated by the\nfact that all the docking scoring protocols being compared are subject\nto the same assumption. Furthermore, the NCI set used in the current\nproject may well have fewer true binders than the widely used DUD\nset, given that the DUD decoys were, as mentioned above, carefully\nchosen to be chemically similar to the DUD actives.\n\nHaving selected\nthe receptors, actives, and decoys, we next turn to the question of\nhow best to evaluate virtual-screening performance. Among the many\nmethods that have been considered,36,51 receiver operating\ncharacteristic (ROC) curves are appealing because they are independent\nof the ratio of actives vs inactives and have desirable statistical\nproperties. The area under the ROC curve (ROC-AUC) is thought to correspond\nto the probability that a known binder picked at random will rank\nhigher than a known nonbinder picked at random.\n\nTo compare docking\nscoring protocols using the ROC-AUC metric, we docked NCI decoys and\nDUD actives into the 40 DUD receptors using AutoDock Vina28 and Glide HTVS, a state of the art, fast docking\nalgorithm designed specifically for screening large libraries. The\nVina-docked poses were then rescored with NNScore 1.018 and NNScore 2.0.19\n\nThe\nmore rigorous Glide SP and Glide XP docking protocols were\nnot used at this juncture because, while impressively precise, they\nare not as well suited for use in high-throughput virtual screens.\nGiven that an average of 1634 compounds had to be docked into each\nof the 40 DUD receptors (74 DUD actives and 1560 NCI decoys), 65,350\nindividual dockings were required to test each docking protocol. We\nnote that others have similarly eschewed an exclusive use of Glide\nSP/XP for projects requiring comparable numbers of individual dockings.52\n\nAs has been shown previously,19,23,29\u221232 our results demonstrate that\nthe ideal docking protocol\nfor a given project is highly system dependent. For example, when\nthe screens were assessed by the ROC-AUC metric, Vina\u2013Vina\nperformed better than Vina\u2013NN1, Vina\u2013NN2, and HTVS\u2013HTVS\nfor docking into the progesterone receptor and glycinamide ribonucleotide\ntransformylase. Vina\u2013NN1 performed best for docking into hydroxymethylglutaryl\u2013CoA\nreductase and the glucocorticoid receptor. Vina\u2013NN2 performed\nbest for docking into epidermal growth factor receptor and platelet-derived\ngrowth factor receptor kinase. HTVS\u2013HTVS performed best for\ndocking into adenosine deaminase and AmpC \u03b2-lactamase (Table 1).\n\nGiven that NN2 considers features of molecular binding\nthat NN1\nneglects, it is curious that for many individual receptors Vina\u2013NN1\nperforms substantially better than Vina\u2013NN2 (Table 2). One common criticism of neural networks is that,\nunlike some other machine-learning techniques, they are essentially\n\u201cblack boxes\u201d; it is difficult to impossible to determine\nprecisely how they come to their ultimate conclusions. Though speculative,\nwe suspect two factors explain the favorable performance of Vina\u2013NN1.\nFirst, the additional features of molecular binding that NN2 explicitly\nconsiders may not provide additional information over what NN1 can\ninfer implicitly. For example, in estimating binding affinity, NN2\nexplicitly considers the number \u03c0\u2013\u03c0 stacking interactions;\nhowever, NN1 might be able to implicitly infer \u03c0\u2013\u03c0\nstacking by considering the number of receptor and ligand aromatic\ncarbon atoms that are in close proximity. Second, NN1 and NN2 assess\nligand potency very differently. NN1 is trained to return a binary\nresponse: good binder or poor binder. In contrast, NN2 is trained\nto return a range of scores roughly equivalent to pKi or pIC50 values. It may be that binary classification\nis more effective than continuous classification in this case. Future\nversions of NNScore currently in development will return to the binary-classification\nparadigm.\n\nSetting the specific details of virtual\nscreens against individual\nproteins aside, the best way of assessing the global utility of a\ndocking scoring function is to consider its performance over multiple\ndiverse receptors. When the average area under the ROC curve calculated\nover all 40 DUD receptors was considered, Vina\u2013NN1 and Vina\u2013NN2\noutperformed Vina\u2013Vina and HTVS\u2013HTVS. To determine whether\nor not this difference was statistically significant, we used a technique\ncalled analysis of variance (ANOVA).53 ANOVA\nasserts the null hypothesis that the means of multiple samples are\nequal (i.e., that multiple samples are drawn from populations with\nthe same mean). In this sense, ANOVA is similar to the t-test, which is limited to two samples. When assessing multiple samples,\none might be tempted to simply perform multiple t-tests between all sample pairs; however, each t-test carries with it the risk of incorrectly rejecting the null\nhypothesis (i.e., committing a type I error by rejecting the conclusion\nthat two samples have statistically equal means when in fact they\nare statistically equal). As more and more t-tests\nare performed, the chances of committing this error increase. ANOVA\navoids the problem by considering multiple samples in conjunction\nrather than pairwise.\n\nBoth ANOVA and the t-test\nallow one to assess\nthe degree of statistical significance via a p value.\nThe p value in this case represents the probability\nthat the multiple samples could have means that differ to the degree\nobserved or greater, given that the null hypothesis is true (i.e.,\ngiven that the samples are drawn from populations with equal means).\nIf p < 0.05, the null hypothesis is rejected.\n\nANOVA analysis suggested that the mean screening performances of\nthe Vina\u2013NN1, Vina\u2013NN2, Vina\u2013Vina, and HTVS\u2013HTVS\nprotocols were not statistically different (p = 0.16,\nnot quite the 0.05 required to reject the null hypothesis). Clearly,\nknown inhibitors, when available, should be included in a virtual\nscreen and used to determine which docking scoring protocol is best\nsuited to the specific system at hand. In the absence of any information\nabout known binders, however, we recommend docking with Vina and rescoring\nwith NNScore 1.0, as that protocol did have the highest ROC-AUC mean\nand median performances.\n\nThough the ROC-AUC metric is frequently\nused to evaluate virtual-screening\nperformance, some have criticized its use because it assesses that\nperformance by considering all screened compounds from the best predicted\nbinder to the worst.54 In practice, computational\nchemists are most interested in the top-ranked compounds, the ones\nthat will be subsequently submitted for experimental validation. It\nis therefore the initial portion of the ROC curve, some argue, that\nought to be of primary interest. A number of performance metrics have\nbeen proposed to address this issue (e.g., the BEDROC metric54 derived from a modified ROC curve that weights\ntop-ranked compounds). Additionally, Hawkins et al.36 recently suggested a simple approach using the \u201cmetric\nof early performance based on the ROC curve.\u201d In this scheme,\none analyzes a ROC curve to determine the true positive rate for a\nfixed false positive rate (5% in the current work).\n\nWe used\nthe metric of early performance to compare the Vina-, NNScore-, and\nHTVS-based protocols to a common multi-tiered Glide protocol (HTVS\u2013SP\u2013XP)34 that has been used extensively in the literature\n(see, for example, refs (52), (55), (56), and (57)). The top 50% best ligands\nas judged by the HTVS\u2013HTVS protocol were subsequently redocked\nwith Glide SP. The top 25% of the Glide-SP compounds were then redocked\nwith Glide XP. These XP-docked poses were additionally rescored with\nNNScore 1.018 and NNScore 2.019 to facilitate comparison.\n\nWe note that\nthe multi-tiered HTVS\u2013SP\u2013XP approach\nis best suited for docking large compound libraries. Admittedly, the\nanalysis herein described required that only 1634 compounds be docked\ninto each DUD receptor on average, suggesting that Glide-SP or -XP\ndocking alone might have been feasible. However, as mentioned above,\nbecause we considered all 40 DUD receptors simultaneously, 65,350\ndockings would have ultimately been required had the multi-tiered\napproach been abandoned. Others have similarly turned to multi-tiered\nGlide protocols for projects requiring comparable numbers of individual\ndockings.52\n\nMany compounds were filtered\nout in the initial HTVS and SP steps\nof the HTVS\u2013SP\u2013XP protocol and so were never docked/scored\nusing Glide XP. Consequently, it was not possible to calculate a complete\nROC curve for the HTVS\u2013SP\u2013XP protocol. However, by assuming\nthat compounds filtered out in the preliminary HTVS and SP steps truly\nwere poor binders (the reasoning implicit in the multi-tiered approach),\nit was possible to generate the arguably paramount initial portion\nof the ROC curve.\n\nWhile individual results were system dependent,\nVina\u2013NN1\nand Vina\u2013NN2 again performed better on average than Vina\u2013Vina\nas judged by the average early-performance metric (Table 2). Surprisingly, the mean performances of the HTVS\u2013HTVS,\nHTVS\u2013SP\u2013XP, and Vina\u2013NN1 protocols were not statistically\ndifferent (ANOVA, p = 0.72). In contrast, t-tests comparing the mean performance of the HTVS\u2013SP\u2013XP\nprotocol to that of the Vina\u2013Vina and Vina\u2013NN2 protocols\nled us to reject the null hypothesis of equivalence (t-test, p = 0.002 and 0.049, respectively). HTVS\u2013SP\u2013XP\nperformed better, on average, than Vina\u2013Vina and Vina\u2013NN2.\n\nTo further compare the Glide XP and NNScore scoring functions,\nwe reevaluated the HTVS\u2013SP\u2013XP poses with NNScore 1.0\nand 2.0. As expected, early performance was highly system dependent.\nNevertheless, ANOVA demonstrated that the mean performances of the\nHTVS\u2013SP\u2013XP, HTVS\u2013SP\u2013XP\u2013NN1, and\nHTVS\u2013SP\u2013XP\u2013NN2 protocols were not statistically\ndifferent (p = 0.88). The mean performances of HTVS\u2013SP\u2013XP\u2013NN1\nand Vina\u2013NN1 were likewise not statistically different (t-test, p = 0.70).\n\nIn hopes of further improving virtual-screening\naccuracy, we next sought to specifically characterize scoring-function\nbiases. Schr\u00f6dinger\u2019s QikProp program was used to analyze\nthe screened compounds (known inhibitors and putative decoys). A statistical\ncorrelation between certain chemical properties and the average rank\nof each compound across all 40 DUD receptors was noted. Vina\u2013Vina,\nVina\u2013NN1, and Vina\u2013NN2 scores tended to correlate with\nproperties associated with molecular size (molecular weight, total\nsolvent accessible surface area, volume, number of ring atoms, and\nnumber of heteroatoms) and polarizability (Table 3). It is interesting that both Vina and the neural-network\nscoring functions demonstrated similar trends, even though they evaluate\nligand binding using very different methodologies. Others have identified\nsimilar biases in the FlexX and Gold docking programs.58 The HTVS\u2013HTVS and HTVS\u2013SP\u2013XP\nprotocols did not exhibit these biases to the same extent (Table 3). For the interested reader, we provide a real-world\nexample of how scoring-function biases can affect screening results\nin the Supporting Information.\n\nThese\npotentially artifactual correlations between ligand properties\nand docking scores may result in part from a general neglect of penalty\nterms that ought to be associated with binding (e.g., ligand desolvation,\ntrapping binding-site waters, etc.). We do not mean to imply that\nVina and NNScore neglect penalty terms entirely. The Vina scoring\nfunction, for example, has three steric-interaction terms.21 Additionally, NNScore may be able to implicitly\naccount for some penalty terms as well; information about energetically\nunfavorable buried polar groups, for example, could potentially be\nextracted from the pairwise receptor\u2013ligand atom-type information\nthat NNScore considers. Nevertheless, both Vina and NNScore are likely\n\u201cblind\u201d to many important penalty phenomena. Indeed,\nGlide-based protocols may perform well relative to many other scoring\nfunctions24 because they better account\nfor these penalties, as evidenced by the fact that HTVS and XP scores\nare not strongly correlated with molecular weight (Table 3). Future versions of NNScore currently being developed\nwill consider three-dimensional descriptions of ligand\u2013receptor\nbinding and so may be even more effective than current implementations.\n\nOn the other hand, one must consider the possibility that at least\na portion of these scoring-function \u201cbiases\u201d in fact\nrepresent accurate characterizations of small-molecule binding. To\nthis end, we tested whether or not correlations exist between the\nmolecular properties and experimentally measured binding affinities\nof known ligands independent of receptor. All the binding data deposited\nin the Binding MOAD database as of March 2013 was considered.59 In total, 2081 entries representing 1598 unique\ncompounds had both known structures and precise or approximate Ki measurements (i.e., measurements described\nas \u201c=\u201d or \u201c\u2248\u201d). We were ultimately\nable to calculate molecular properties for 1531 of these compounds.\n\nInterestingly, both molecular weight and polarizabiliy were correlated\nwith the experimentally measured pKi values;\nthese molecular properties are plotted as a function of average pKi (independent of receptor) in Figure 1A and B, respectively. Linear regression suggested\nthat the relationship between molecular weight and pKi was MW = 39.2710 (pKi) +\n122.1835, with a R2 value of 0.23. A t-test on the slope coefficient yielded a p-value of 0.0, leading us to reject the null hypothesis that there\nis no true relationship between pKi and\nmolecular weight (i.e., that the true slope coefficient is 0). Similarly,\nthe regression equation describing the relationship between polarizability\nand pKi was pol = 4.3326 (pKi) + 7.3163, with a R2 value\nof 0.30. A t-test on the slope coefficient of this\nregression similarly produced a p-value of 0.0; the\nhypothesis that polarizability and pKi are not correlated was thus similarly rejected.\n\nSubjectively, the notion that\nligand binding may be in part dependent\non factors that are entirely ligand centric has some appeal. For example,\nwhile it is certainly true that in the absence of ligand\u2013binding-site\ncomplementarity bigger is not necessarily better, larger molecules\nmay well have more interacting moieties on average that serve to enhance\npotency if complementarity is assumed. On the other hand, it may be\nthat the noted correlations between molecular properties and experimentally\nmeasured pKi values are more reflective\nof traditional and perhaps flawed approaches to medicinal chemistry\nthan of actual physical phenomena. For example, during the drug-optimization\nprocess, molecules do tend to increase in size.60 Regardless, our goal ought to be to compensate for true\nbiases while maintaining correlations that reflect actual physical\nphenomena.\n\nTo compensate for potential scoring-function biases,\nwe considered\n15 relevant QikProp properties for each NCI decoy and known DUD inhibitor:\naccptHB (estimated number of hydrogen-bond acceptors), CIQPlogS (predicted\naqueous solubility), dipole (computed ligand dipole moment), donorHB\n(estimated number of hydrogen-bond donors), SASA (total solvent accessible\nsurface area, \u00c52), FISA (hydrophilic component of\nthe SASA, \u00c52), FOSA (hydrophobic component of the\nSASA, \u00c52), mol_MW (molecular weight, daltons), nonHatm\n(number of heavy atoms), PISA (\u03c0 component of the SASA, \u00c52), QPlogPC16 (predicted hexadecane/gas partition coefficient),\nQPlogPo/w (predicted octanol/water partition coefficient), Qppolrz\n(predicted polarizability, \u00c53), rotor (number of rotatable\nbonds), and volume (total solvent-accessible volume, \u00c53). Composite scoring functions were considered of the formwhere NN1 is the Vina\u2013NN1\nscore, Pi is the ith chemical\nproperty (listed\nabove), and Ci is a coefficient associated\nwith that property.\n\nA stepwise selection method was used to\nidentify the composite scoring functions that best improved screen\nperformance. First, training sets were generated for each DUD receptor\nby randomly selecting 75% of the associated known actives and merging\nthem with the set of all NCI decoys. Similar testing sets were generated\nusing the remaining known actives. All 15 coefficients were initially\nset to 0.0 so that the composite and Vina\u2013NN1 scores were identical.\nThe downhill simplex algorithm (SciPy) was then used to adjust the\ncoefficients so as to maximize the average screen performance over\nall 40 training sets. Once training was complete,\nthe resulting composite scoring function was then evaluated by calculating\nthe average early-performance metric over the 40 testing sets, now without adjusting the coefficients.\n\nThe above protocol\nwas repeated; each time, a different coefficient\nwas fixed at 0.0 so that the associated chemical property was essentially\nignored. The single chemical property that when discounted was associated\nwith the smallest drop in the average screen performance over the\n40 testing sets was identified. A new set of 14 chemical properties\nwas generated by excluding this chemical property. In total, this\nelimination step was repeated 15 times until no additional chemical\nproperties remained (Figure 2A).\n\nA general-purpose scoring function was identified by considering\nthe minimum number of chemical properties required to maintain optimal\nearly performance (Figure 2A). Specifically,\nwhen the compounds were ranked by (NN1 + 0.1093 \u00d7 donorHB + 0.0011\n\u00d7 FOSA + 0.0008 \u00d7 PISA), the average early-performance metric\nover all 40 receptors was 0.48, substantially improved over the 0.35\nobtained with Vina\u2013NN1 alone. A t-test in\nfact required that we reject the notion that the mean performances\nof these two screens were statistically equal (p =\n0.04) (Table 2, composite/general).\n\nGiven\nthat we opted to use decoys that were not necessarily chemically\nsimilar to the DUD actives, it was especially important to ensure\nthat the above general-purpose scoring function was not overtrained.\nFor example, consider the hypothetical possibility that the decoys\nand DUD actives all have fewer than three and more than four hydrogen-bond\ndonors, respectively. It would be possible to identify the actives\nusing ligand-specific factors alone, independent of any important\nligand\u2013receptor interactions.\n\nOn the other hand, some\nligand-specific factors (e.g., the number\nof rotatable bonds) may well be legitimately useful for distinguishing\nactives from decoys. It is not unreasonable to expect some genuine\nenrichment when compounds are ranked by a scoring function comprised\nexclusively of ligand-specific terms; nevertheless, one would expect\nthat screen performance would improve further still when additional\nbinding-specific terms (e.g., the NNScore) are included.\n\nTo\nensure that actives were not being identified based on their\nchemical properties alone, we generated a second scoring function\nof the same form (0.1093 \u00d7 donorHB + 0.0603 \u00d7 FOSA + 0.0458\n\u00d7 PISA), comprised exclusively of the ligand-specific terms of\nthe parent general-purpose function. The average early-performance\nmetric over all 40 receptors when the ligands were ranked by this\nligand-specific scoring function was 0.22. A t-test\nrequired that we reject the null hypothesis that the mean performances\nof the ligand-specific scoring function and its parent function were\nstatistically equivalent (p = 0.00005), suggesting\nthat the general-purpose function achieved its enhanced performance\nby considering both ligand-specific and binding-specific factors.\n\nThe implicit assumption behind the creation of any general-purpose\nscoring function is that a single function can perform optimally across\nany number of receptors; however, given the demonstrated system dependence\nof scoring functions in general, this supposition is not likely to\nbe correct. We therefore repeated the above scoring-function optimizations,\nnow generating an independent composite scoring function for each\nreceptor (Figure 2B).\n\nThe average early-performance\nmetric over the 40 receptors was substantially improved when receptor-specific\nscoring functions that included terms for FISA, mol_MW, and volume\nwere employed (0.68). A t-test required us to reject\nthe null hypothesis that the mean early-performance metrics of this\nscoring function and the general-purpose scoring function described\nabove were statistically equivalent (p = 0.00073;\nTable 2, composite/individual). An analogous\nscoring function containing only ligand-specific terms was also generated,\nas above. Again, we rejected the null hypothesis that the mean performances\nof the ligand-specific scoring function and its parent function with\nthe additional NN1 term were statistically equivalent (0.53 vs 0.68, p = 0.01).\n\nIt is not necessarily our purpose to provide\nspecific composite\nfunctions for others to use in their virtual screens; rather, we wish\nto demonstrate the general utility of deriving such functions when\npositive controls (known inhibitors) are available. A composite scoring\nfunction tailored to a specific receptor and designed to optimize\nthe ranking of known inhibitors can potentially enhance virtual-screening\nperformance.\n\nThe performance of a\ndocking scoring protocol is highly dependent\non the specific receptor being studied. When possible, positive controls\n(known binders) should be included in the screen so many different\nprotocols can be tested. However, in the absence of known ligands\nand when a free, open source, general-purpose solution is sought,\ndocking with AutoDock Vina and rescoring with NNScore 1.0 is an excellent\noption. We are hopeful that this work will help guide computational\nchemists in their efforts to best utilize computer-docking programs."}