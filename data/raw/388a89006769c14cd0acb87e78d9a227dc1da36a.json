{"title": "Quantitative risk assessment for food-and waterborne viruses", "body": "One of the fi rst applications of risk assessment was by NASA in the aftermath of the 1967 Apollo fi re. Risk assessment is now widely used in different scientifi c disciplines including nuclear science, fi nance, industrial processing and infectious diseases. Charles Haas used risk assessment in the latter fi eld for the fi rst time to mathematically describe a dose-response relationship for adverse public health events due to virus ingestion (Haas, 1983 ) . The methodology was subsequently extended, with an exposure assessment to estimate the exposure dose and a risk quantifi cation for the presence of viruses in drinking water (Haas et al., 1993 ) . Despite this initial focus on viruses, quantitative microbiological risk assessments for food and water have so far focused predominantly on bacteria. Recently completed assessments include models that describe the entire food production pathway from production of the raw material until the moment of consumption (often described as 'farm-to-fork' models). Examples of this include the assessment of public health risks associated with E. coli O157:H7 in ground beef hamburgers (Cassin et al., 1998 ) with Listeria monocytogenes in ready-to-eat foods (Rocourt et al., 2003 ) and with Campylobacter on broiler meat (Nauta et al., 2009 ) . Such farm-to-fork models are now being developed for viruses in food for the fi rst time as part of the European project 'Integrated monitoring and control of foodborne viruses in European food supply chains' ('VITAL').\n\nPublic health risk assessment comprises several stages: identifying and defi ning the risk, assessing the level of exposure and the exposure-response relationship, and assessing the associated risk for humans. This process can be done either qualitatively (providing a 'yes' or ' no' answer about a risk), semi-quantitatively (weighting or scoring each contributory component of the overall risk and adding them together to calculate the fi nal risk), or quantitatively (using probability distributions to provide mean estimates of the risk and 95% intervals). This chapter will focus on quantitative risk assessments only and will not consider qualitative or semi-quantitative risk assessments.\n\nHealth outcomes are based on information concerning exposure to viruses in foods and water and on dose-response relationships. Several frameworks for assessing microbiological risks have been proposed (ILSI, 1996 ; Haas et al., 1999 ) , that are designed to structure and harmonize the approach to risk assessment. These frameworks use the following steps for the assessment of food-and waterborne viruses:\n\nIdentifi cation of the pathogenic virus(es) that may be present in a particu-\u2022 lar type of food or water sample and that are capable of causing infection, illness or death in humans. In addition, the identifi cation of the exposure route or routes; ingestion or inhalation, for example. Estimation of the virus dose to which individuals are exposed per expo-\u2022 sure route. Quantitative evaluation of the nature and probability of adverse health \u2022 effects associated with a certain intake of the pathogenic agent. Integration of the estimated exposure dose and the probability of the \u2022 adverse health event, given that dose. This aspect thus consists of the joining of the results from the previous two steps.\n\nRisk assessment is preferably an iterative process of systematic and objective evaluation of all available information pertaining to a given hazard -in this particular instance, viruses in foods and water. Some risk profi les, a term which references the fi rst part of quantitative microbial risk assessment (QMRA) as described above, have been reported for foodborne viruses. These include risk profi les of the Norwalk-like virus in raw molluscs in New Zealand (Greening et al., 2003 ) , foodborne norovirus infections (HPA, 2004 ) and hepatitis E virus (HEV) (Bouwknegt et al., 2009 ) . Factors such as the persistence of virus infectivity on foods, the role of irrigation water and food handlers' hands in virus transmission and the effectiveness of existing virus control points in food harvesting, processing and handling have had limited representation in the available data. It was therefore concluded at an international meeting of experts in 2007 that undertaking a full quantitative risk assessment for foodborne virus may be premature (FAO/WHO, 2008a ) . However, increased interest in food-and waterborne viruses and the development of accessible and rapid diagnostic methods in recent years has increased the amount of available data related to viruses in foods and water, with the result that quantitative models could now start to be designed.\n\nStudies employing QMRA to assess adverse public health events associated with viruses in food and water are presented in Table 8 .1 , with aspects of particular relevance to QMRA for viruses highlighted later. The viruses selected in the quantitative risk assessment models described include the relatively larger DNA adenoviruses (Crabtree et al., 1997 ; van Heerden et al., 2005 ) and the small RNA enteroviruses (Regli et al., 1991 ; Mena et al., 2003 ) and rotaviruses (Regli et al., 1991 ; Haas et al., 1993 ; The enteroviruses included coxsackieviruses, (Mena et al., 2003 ) polioviruses type 1 and 3, and echovirus 12 (Regli et al., 1991 ) . Some studies refer to viruses in general Hamilton et al., 2006 ) .\n\nThree of the papers mentioned in Table 8 .1 describe quantitative risk assessment models for fresh produce Hamilton et al., 2006 ) . One is a methodological study to examine the implications of over-dispersion in virus concentration on crops . Hamilton et al . ( 2006 ) provide a simplifi ed model for estimating the number of viruses on an item of produce as a result of spray irrigation. Masago et al . ( 2006 ) estimate the risk of a norovirus infection due to the consumption of drinking water and describe an approach for estimating virus concentrations based on presence/absence data. The remaining studies describe the estimation of risks associated with drinking and/or coming into recreational contact with potentially contaminated waters.\n\nThe choice of statistical distribution used and the assumptions made in quantitative risk assessments affect the estimated risks. Regli et al . ( 1991 ) and Haas et al . ( 1993 ) present a theoretical background for the estimation of infection risks due to consumption of drinking water which can also be useful for estimates related to the consumption of other matrices such as food. Regli et al . ( 1991 ) describe the assumptions which must be made in virological risk assessment and go on to evaluate different dose-response models (exponential and Beta-Poisson). Haas et al . ( 1993 ) provide an approach to including uncertainty and variability in risk assessments. These studies demonstrate the importance of understanding (1) the theoretical backgrounds of the distributions used; (2) the randomness or lack thereof in processes leading to viral contamination; (3) the essence of the dose-response models used (the measured response, the unit of the exposure dose); and (4) that uncertainty and variability are essential to the proper conduct of quantitative viral risk assessments. The three models particularly focused on fresh produce centre on the use of irrigation water for salad crops Hamilton et al., 2006 ) . modelled the clinging of viruses to lettuce crops through sprayed irrigation water. The volume of retained water was taken from a study in which lettuce heads were immersed fully in water (Shuval et al., 1997 ) . The estimated volume amounted to approximately 10 ml per 100 g of lettuce, an estimate which was also later used in relation to lettuce by Hamilton et al . ( 2006 ) . In the absence of better data, this volume was considered to represent a worst-case situation and provided an acceptable starting point for risk assessment. However, full immersion of crops might not accurately represent a worst-case scenario, as the volume of water on shrubs is reported to be larger after a simulated rainfall event than after immersion (Garcia-Estringana et al., 2010 ) . Ideally the volume of water retained after single and multiple irrigation events should therefore be examined experimentally. Such an experiment was conducted by Hamilton et al . ( 2006 ) for broccoli, grand slam cabbage, savoy cabbage and winter head cabbage. Their results gave estimated mean water retentions of 0.02, 0.04, 0.04 and 0.09 ml per g of produce, respectively, after a single irrigation event of 20 min. Probability distributions that may be used in a quantitative risk assessment are provided in the paper.\n\nFor estimation of the exposure dose, data on the amount of food or water consumed are required, with the data preferably representative of the general population. The available papers on food consumption lack specifi c monitoring data, relying instead on such valid approaches as the hypothetical consumption of 100 g of a food product or the amount of food consumed being a function of body weight (Hamilton et al., 2006 ) .\n\nThe outcome of any risk assessment depends on the research aim and this is usually decided in consultation with risk managers. Examples of risk assessment outcomes include identifi cation of exposure doses, risk of infection, likelihood of illness and possible resultant levels of mortality. An important aspect to consider in the translation of an exposure dose to an adverse health event is the dose-response model used, and how this relates to the subsequent modelled response. Depending on the data set underlying the dose-response model, the initial translation from dose to risk represents the risk of the specifi c response used in that particular dose-response model. For instance, the model to estimate norovirus infection risks was based on volunteers showing faecal excretion of virus and seroconversion (Teunis et al., 2008 ) . In the same study, the dose-response model for illness conditional on infection was based on a response including diarrhoea and/or vomiting combined with other symptoms such as abdominal pain, myalgia, fatigue, chills and headache more than 8 h after the infection. Thus, employing either the fi rst or both models estimates the respective response observed in the volunteers.\n\nSimilarly, a dose-response model for hepatitis A virus (HAV) is based on data from Ward et al. ( 1958 ) , whose observed response was the development of jaundice in institutionalized individuals, the majority of whom were children.\n\nHence, the latter dose-response model predicts the probability of developing jaundice for a particular ingested HAV dose in instances where the exposed individual is a child. Due to the lack of more appropriate data, such estimates are often extrapolated to other (sub)populations or to the general population in order to proclaim a risk of adverse health events. The imperfection of this approach should be considered when interpreting the estimated risk and possibly using it for setting health-based targets for public health protection.\n\nThe published public health risk estimates summarized in Table 8 .1 are mainly presented as infection probabilities, with some estimates alternatively translated into probability of disease or mortality. An additional measure used to represent adverse health effects for humans is an estimate of the disability adjusted life years (DALY) (Murray, 1994 ) . This measure is a combination of the years of life lost due to premature mortality and the period of time spent in a suboptimal health status as a consequence of infection. A DALY estimate therefore includes the translation from infection to actual disease. Such an estimate may give greater insight into the adverse health effects of pathogens in the environment than an infection risk, as infection does not necessarily produce illness. The WHO use DALY to measure the global burden of disease (Lopez et al., 2006 ) , and also base their targets for public health protection on DALYs rather than on infection or illness risks. However, the progression from infection to disease can be highly variable between individual humans and depends on multiple host and pathogen characteristics. Depending on the data underlying the dose-response model, such progression cannot always be estimated.\n\nWe conclude that quantitative risk assessment for food-and waterborne viruses may be very useful for the estimation of public health risks and for evaluating risk-reducing interventions. However, several data gaps and needs exist with respect to hazard identifi cation, exposure assessment and dose-response relations which encompass the different steps in risk assessment.\n\nViruses in foods that have been identifi ed as being of highest priority are norovirus (NoV), hepatitis A (HAV) and E (HEV) viruses (Table 8 .2). Other hazardous foodborne viruses include SARS coronavirus, avian infl uenza viruses and tick-borne encephalitis virus (Duizer and Koopmans, 2008 ) . Furthermore, Duizer and Koopmans ( 2008 ) conclude that most emerging viruses cannot easily be excluded from being foodborne.\n\nFood items implicated in human viral infection and disease include shellfi sh products, fresh produce and meat products. Contamination of these food items may occur during production, for example due to fi lter feeding by shellfi sh or irrigation of fresh produce, during processing as a result of contact with contaminated hands and utensils, and at point of sale due to contact with contaminated hands of store personnel and customers ( Table 8 .2 ). With respect to fresh produce, the FAO/WHO expert meeting on the microbiological hazards in fresh fruits and vegetables recommended that leafy green vegetables should be considered the highest global priority in terms of fresh produce safety (FAO/WHO, 2008b ) . With respect to foodborne virus contamination of meat, the presence of HEV in raw pig meat has been demonstrated (Deest et al., 2007 ; Matsubayashi et al., 2008 ) . Meat products that are consumed raw or moderately heated, such as locally produced sausages (Colson et al., 2010) and sashimi (Tei et al., 2003 ) , are of particular concern in this respect.\n\nData on the prevalence of enteric viruses for reservoirs and sources that may relate to potential contamination during food production have been published (Van den Berg et al., 2005 ; Cheong et al., 2009 ; Gentry et al., 2009 ; Boxman et al., 2011 ) . For shellfi sh, numerous data on viruses in harvesting waters are available, whilst for fresh produce, studies are available on foodborne viruses in irrigation water and manure. With regard to food handling environments, a number of foodborne viruses in human and animal faeces have been determined. Nevertheless, such virus presence has largely been determined qualitatively (presence or absence) rather than quantitatively (enumerated). Suffi cient quantitative data (with respect to sampling size and numbers of viral particles) for the performance of quantitative risk assessments is therefore still largely unavailable. Furthermore, the infectivity status of the viruses detected often remains unknown, as molecular methods detect nucleic acids from both infectious and non-infectious viruses, and the non-infectious viruses do not contribute to the risk of a public health event. These data issues therefore enforce an integrated approach at present for quantitative risk assessmentsan approach which includes proper data collection by means of statistically sound sampling plans and proper data analysis using appropriate diagnostic controls and statistical controls for quantifi cation. Another aspect that needs to be considered in quantitative risk assessment is the transfer of viruses to and from foods due to handling and processing. Some data are available for transfer between surfaces and hands (Sattar et al., 2000 ; Bidawid et al., 2004 ) . and these have been used to model, for example, norovirus transfer (Mokhtari and Jaykus, 2009 ). However, robust estimates are not possible given the limited data available. Details of virus transfer between food products, hands and utensils are therefore largely unquantifi ed and uncertain. Viral transfer from carriers (humans, animals) or environmental sources to foods, as well as between foods, are largely based on assumptions rather than on experimental or fi eld data. It is deemed important to perform virus transfer experiments under controlled settings following specifi ed protocols. It was shown that 1% (0.2-10%), 0.05% (0.02-0.3%) and 9% (4-15%) of infectious MNV-1 was transferred from fi ngertips to raspberries, strawberries and lettuce, respectively (Verhaelen et al., in preparation). The transfer fractions of MNV-1 from raspberries and lettuce to fi ngertips was 3% (<0.01-5%) and 4% (2-6%), respectively. Using the estimated mean transfer rate of MNV-1, it was estimated that a single food handler with an initial virus contamination level of 10 4 infectious virus particles on his fi ngertips could contaminate more than 2 kg of raspberries with a minimum of one virus particle per raspberry. This study exemplifi es the substantial role a single food handler can have in the transmission of hNoV, considering hNoV is the most infectious agent described, with an infection probability of about 0.5 per single hNoV particle (Teunis et al., 2008 ) .\n\nAn important requirement for any exposure assessment is data on the amount of a particular food item that has been consumed and the methods used to prepare it. Regarding the amount of food consumed, several national studies have been collected and assembled by the European Food Safety Authority (EFSA) in a database (Comprehensive European Food Consumption Database) (EFSA, 2011a ). This database specifi es consumption data on aggregated levels (for example 'total consumption of vegetables per day, including mushrooms and other fungi'). Specifi c data for a particular product, such as 'lettuce' or, more specifi cally 'butterhead lettuce', are not retrievable from the database. Data for the consumption of drinking water are similarly available from a Dutch national food consumption survey (Teunis et al., 1997 ) , which used as a mean estimated volume of unboiled drinking water 0.28 L day \u22121 (described by a lognormal distribution with \u03bc = \u22121.86 and \u03c3 = 1.07).\n\nSpecifi c information and population frequencies for food and beverage preparation methods used in homes and restaurants prior to consumption are scarce but essential for quantitative risk assessments. Heating or cross-contamination can affect the virus concentration in and on food items, and therefore alter the exposure dose. Consumer habits and preferences regarding the state of a food item at consumption are highly diverse within and across different countries, meaning region-specifi c data is often required. Ignoring any possible effects of preparation on virus presence/infectivity can result in either under-or overestimation of the risk, depending on the effi cacy of the disregarded process.\n\nFor high-risk foods such as soft fruits and salad vegetables, irrigation water may be one of the sources of contamination, making it important that this process is modelled. To estimate virus concentration on fresh produce due to irrigation, it is important to assess the volume of retained water on such products, as a function of the duration of irrigation. Furthermore, the clinging of viruses to food products and their wash-off during prolonged or subsequent irrigation events needs to be determined. The lack of available data currently prevents the possibility of such estimation.\n\nThe persistence of viruses on sources is another important aspect to consider in QMRA. On surfaces, hands and other environments, as well as once transferred onto food products or into water, viruses may persist for prolonged periods of time . Experiments to assess the stability of viruses make use of cell culture systems. By growing in vitro cells that are susceptible to infection with the virus under study, and assessing the infectivity before and after treatment, parameters on virus stability can be estimated and used in risk assessment studies. However, no robust cell culture systems for the detection of infectious human noroviruses, HEV and HAV are available. Information on the persistence of infectious particles of these viruses in the environment is therefore limited.\n\nHowever, several studies have been conducted using a surrogate virus, especially with regard to norovirus. At present, the most promising surrogate is the culturable murine norovirus due to its genetic similarity and environmental stability (Bae and Schwab, 2008 ) . In general, infectivity reduction rates of surrogates were shown to be higher at higher temperatures (>25 \u00b0 C) and room temperature than at 4 \u00b0 C, for matrices such as surfaces of stainless steel, lettuce, berries, deli ham, surface and ground waters (Cannon et al., 2006 ; D'Souza et al., 2006 ; Bae and Schwab, 2008 ; Butot et al., 2008 ) . In addition, the relative humidity is an important determinant for survival in the environment (Stine et al., 2005 ; Cannon et al., 2006 ) . Data obtained for the stability of norovirus-like particles, as well as surrogate viruses, demonstrated stability over a pH range of 3-7 and up to 55 \u00b0 C (Duizer et al., 2004 ; Ausar et al., 2006 ; Cannon et al., 2006 ) .\n\nViruses on foods are challenged by applied or natural production conditions, such as storage temperature, storage humidity and exposure to sunlight. Furthermore, the food matrix itself can induce virus inactivation due to, for example, the effects of pH or the presence of proteases. For most intact fresh produce, no recommended storage temperature is provided by legislation. In general, low temperatures and high relative humidity are applied in the fresh produce industry to prolong shelf-life and maintain produce quality. Unlike for bacteria, these conditions generally promote viral persistence. The usual storage temperature of lettuce, for example, is about 4 \u00b0 C with a relative humidity (RH) of about 80%. The shelf-life of lettuce is strongly dependent on storage conditions: a shelf-life of 21-28 days can be expected at 0 \u00b0 C with >95% RH, whilst at 5 \u00b0 C a shelf-life of 14 days can be expected. At point of sale, whole lettuces are usually stored at ambient temperature. Persistence of feline calicivirus (FCV) on lettuce in commonly applied storage conditions was studied by Mattison et al . ( 2007 ) . After 4 days of storage at 21 \u00b0 C the virus was not detectable, which is equivalent to a reduction of about 2.5 log 10 -units. Infectious FCV was reduced by about 2 log 10 -units after seven days of storage at 4 \u00b0 C. Murine norovirus (MNV-1) was found to be persistent on raspberries and strawberries at 4 and 10 \u00b0 C, meaning that the D-values (fi rst 1 log 10 unit reduction) reached or exceeded the 7-day shelf-life of the berries. However, MNV-1 infectivity dropped by about 1.5 log 10 -units on strawberries after just one day of storage at room temperature, whereas no virus decay was observed on raspberries in this period. A 1 log 10 unit decrease in MNV-1 infectivity only occurred on raspberries after 3 days at room temperature, yet in practice, raspberries are rarely stored over such a long period. Please see Chapter 13 for further information on the natural persistence of food-and waterborne viruses.\n\nFoodborne viruses may be reduced by a diverse range of food treatment processes, each displaying a different estimated effi ciency in infectivity reduction (EFSA, 2011b ). The effectiveness of treatment processes for virus reduction has mostly been assessed by use of indicator viruses such as MNV (murine norovirus), FCV and phages. In order to conduct a quantitative viral risk assessment, it is important to fi rst know which decontamination practices are used in food production, whilst acknowledging that lab experiments may produce different outcomes to standard virus reduction due to current practices in the fi eld. Secondly, process-specifi c infectivity rates for each virus, or an appropriate surrogate thereof, should ideally be included in the study. Unfortunately, the lack of cell culture systems for certain viruses, including NoV and HEV, have hampered such studies in the past. Alternative approaches, such as the use of animal models (Feagins et al., 2008 ) or cell infection experiments (Emerson et al., 2005 ) have been developed for HEV. The results of both experiments indicated that HEV was still viable when heated at 56 \u00b0 C for an hour. In an in vivo animal experiment, one of the livers used as inoculum was stir-fried at 191 \u00b0 C (internal temperature of 71 \u00b0 C) for 5 min and infection was not observed after inoculation. Furthermore, recent progress in HEV culture has been made in the European project 'VITAL', by using a 3D cell culture system. Initial experiments with this system have shown that HEV is inactivated when heated at 100 \u00b0 C for 15 s (Berto et al., 2012 ) . Hence, HEV seems to be heat-intolerant above a certain temperature, but the temperature-dependent inactivation rate is yet to be determined.\n\nLimited information on human dose-response relationships is currently available for quantitative viral risk assessment. The dose-response relationship for norovirus is based on ingestion of an inoculum by volunteers (Teunis et al., 2008 ) , and the exposure dose was quantifi ed using RT-PCR detection. The currently available dose-response model for HAV is based on inoculation of institutionalized individuals with faeces from a patient (Haas et al., 1999 ) , with the original dose quantifi ed as grams of faeces ingested (Ward et al., 1958 ) . In a subsequent study, this dose was adjusted to PCR detectable units using a maximum likelihood estimation to increase usability (the ingested amount of faeces is not frequently the outcome of an exposure assessment) (Bouwknegt et al., in preparation) . The dose-response model currently available for HEV is based on intravenous inoculation of pigs, and corrected to represent oral ingestion . This dose was quantifi ed using RT-PCR.\n\nThese available dose-response models need to be applied with particular care in quantitative microbiological risk assessment. The ratio between infectious and non-infectious viruses can differ by several orders of magnitude between samples . This ratio can therefore differ signifi cantly between the samples used for the dose-response modelling and those used for quantifying viruses at the risk assessment contamination points. The estimated risk can consequently over-or underestimate the actual risk by several orders of magnitude. As indicated before, the availability of an effi cient cell culture system might allow the inclusion of a correction factor in the dose-response model, although this factor is expected to be highly variable.\n\nIn addition, dose-response relations are often established based on foodborne viruses in spiked water, whilst no dose relation for viruses on food is determined. However, very recently, a study was published in which it was demonstrated, via consumption of treated shellfi sh by human volunteers, that high hydrostatic pressure was effective for reduction of Norwalk virus in oysters (Leon et al., 2011 ) .\n\nIn comparison to the general population, vulnerable subpopulations exposed to foodborne viruses may experience higher disease incidence. However, the details of this are at present largely unknown . Exposure to foodborne viruses may result in more severe disease outcomes for vulnerable subpopulations, such as immunodefi cient transplant recipients, those living with HIV/AIDS, children, the elderly and pregnant women. NoV infection, for example, is common in all age groups but the incidence is highest in young children (<5 years). In the case of HAV, severe infections among adults are rare in high endemic areas, due to induced life-long immunity upon exposure as a child, whereas in low endemic areas the disease mainly occurs in adulthood with an increased likelihood of severe symptomatic illness developing. Hepatitis E (at least genotype 1) poses a particular threat to pregnant women, who are at high risk of developing severe hepatitis, resulting in mortality in up to 25% of cases for this category. Similarly, individuals with underlying diseases, such as chronic liver disease, liver cirrhosis or a history of high alcohol consumption, are at a higher risk of developing hepatitis E, whilst immunosuppressed transplant patients are at risk of developing chronic hepatitis E.\n\nOne of the predominant challenges in quantitative viral risk assessment is absolute quantifi cation of infectious viruses. When cell culture systems are available, the number of cytopathologic effects in cell culture, or plaques in a plaque assay, can be interpreted as an indicator of the number of infectious viruses per unit of sample examined. At present, where such systems are not routinely available, as is the case for NoV, HAV and HEV, one is restricted to the use of indirect methods such as genome detection by (RT-) PCR. The methods for correct use of qPCR data in QMRA are lacking, which leads to the currently unavoidable ignorance of two aspects: (1) distinction between genomes originating from infectious (viable) and non-infectious (non-viable) micro-organisms; and (2) uncertainty and bias in quantifi cation of genomes. Distinction between infectious and non-infectious micro-organisms cannot be made with qPCR directly, because the detected genomes can originate from both types of micro-organism. Approaches to distinguish between the two by using, for example, enzymatic pre-treatment are being explored at present (Schielke et al., 2011 ) . Furthermore, results generated by qPCR are, when quantifi ed, translated into point estimates of genome quantities according to a standard curve, based on samples with supposedly known concentrations of targets (i.e., the standards). However, points often neglected in such quantifi cation are the effi ciency of isolation of genomic material from samples, differences in amplifi cation effi ciency between the standard and targeted micro-organisms, uncertainty around the estimated concentration of the standards and target viruses, and measurement error of the apparatus. For results of molecular methods to be useful in quantitative risk assessment, new procedures are needed to provide correct quantifi cation, including uncertainty, of infectious micro-organisms detected by qPCR.\n\nAnother important challenge for QMRA for viruses is to retrieve the required information from published results. The majority of published studies on the contamination of products with enteric viruses, for instance, detail the percentage of detected positive samples (so-called prevalence studies). The effi ciency of such prevalence studies correlates directly to the effi ciency of the method used, thus yielding method-dependent data. Factors that affect the data quality include the limit of the detection assay, the ineffi ciency in amplifi cation, the RNA isolation effi ciency, and the procedure used for quantifi cation.\n\nApproaches have been published to account for several of these factors in the detection protocol (see, e.g., Costafreda et al., 2006 ) . Nevertheless, when proper controls were included in the analyses, results described in published papers were found to be lacking suffi ciently detailed descriptions for the data to be included in risk assessment studies. A solution to this problem can be to share the raw laboratory data with risk analysts. Alternatively, the science community could strive to ensure raw data are included with their publications in journals, and/or submitted to global databases maintained by networks of scientists, such as the COST network for Food and Environmental Virologists or the Food and Water Department of the responsible Ministry, Inspectorate or research institute. When made available, virus concentrations, including details of any uncertainty/variation may be assessed and included in risk estimation.\n\nOnce conceptual mathematical models for estimating virus contamination per contamination source have been developed, validated and made available, such models can be reprogrammed as stand-alone software tools. This allows the access and application of data by a wide range of scientists and specialists, including those who have not necessarily received specialist training in mathematics and statistics (e.g., Schijven et al., 2011 ; Schijven et al., submitted) . Future trends might include the use of such tools for real-time monitoring, for instance when the models are linked to rapid methods for virus detection in potential sources.\n\nQuantitative microbiological risk assessment is a method by which available quantitative data for a microbiological hazard can be utilized to estimate human exposure and resultant levels of morbidity or mortality. The results of applying this method can be more informative for public health risk management than the presence/absence and pathogen prevalence data often presented. Furthermore, by identifying the available and the required data for a risk assessment, data gaps can be identifi ed. This information can subsequently be used to allocate research resources more effectively. The development of statistically sound sampling plans for proper data collection that target these data gaps can extend the effective use of resources still further.\n\nThe continuing development of risk assessment models and their possible distribution in risk assessment tools might increase the use of QMRA to assist public health policy makers. However, it is important that the limitations of risk assessments are considered when interpreting the results, which differ per hazard examined. These, usually data-driven, limitations should be included in the development or analysis of any product based on risk assessment outcomes, as should the assumptions made during assessment."}