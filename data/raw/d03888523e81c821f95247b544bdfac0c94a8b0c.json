{"title": "P1 Infusion of sodium sulfide improves myocardial and endothelial function in a canine model of cardiopulmonary bypass P2 Cytoprotective and anti-inflammatory effects of hydrogen sulfide in macrophages and mice", "body": "Introduction Extracellular release of ATP is an important modulator of immune response. ATP plasma concentration is increased in sepsis [1] . IFN\u03b3 plays a critical role in host defense by promoting Th1 phenotype and bacterial clearance. Low IFN\u03b3 levels are associated with the Th2 phenotype consistent with critical illness anergy [2] . It has been reported that 100 and 300 mM ATP increased LPS/PHA-stimulated IL-10 secretion in human blood [3] . Higher IL-10/IFN\u03b3 ratio shifts the immune phenotype from Th1 to Th2 response. We studied the effect of ATP on LPS-stimulated IL-10 and IFN\u03b3 secretion in a standardized ex-vivo whole human blood culture. Methods Venous blood from 10 healthy volunteers was drawn into tubes containing 10 ng LPS/ml (ILCS\u00d2; EDI GmBH, Reutlingen, Germany) and incubated with or without 100 mM ATP, respectively, at 37\u00b0C for 24 hours. The supernates were separated and frozen at -20\u00b0C. Cytokine levels were analysed on a robotic workstation (epMotion 5075; Eppendorf AG, Hamburg, Germany) in duplicate using the ELISA Cytokine kit (Luminex; Biosource Int., Camarillo, CA, USA). Results Added ATP reduced the mean concentration of IFN\u03b3 in LPSstimulated blood from 1,206 \u00b1 1,667 pg/ml to 140 \u00b1 128 pg/ml; P = 0.006. There was no consistent effect of ATP on IL-10 secretion in our study (21.6 \u00b1 16.9 pg/ml to 17.2 \u00b1 18.8 pg/ml). Interestingly, three subjects of Indian/Indonesian origin had IL-10 levels below the assay detection limit. The mean IL-10/IFN\u03b3 ratio was increased from 0.05 \u00b1 0.04 to 0.16 \u00b1 0.09 in the remaining Caucasian subjects (P = 0.015). See Figure 1 . Conclusions Our results suggest an immunosuppressive effect of extracellular ATP that is evident by the decrease of IFN\u03b3 and therefore the relative shift of the immune response towards Th2 phenotype. Although this may represent a self-protective mechanism, it may contribute to critical illness anergy. \n\nIntroduction Sepsis-associated arterial hypotension may be complicated by inadequate systemic and regional oxygen delivery resulting in lactic acidosis and multiple organ failure. We hypothesized that exogenous administration of adrenomedullin (AM), a vasodilatory peptide hormone with anti-inflammatory properties, may improve the oxygen delivery-demand relationship, thereby limiting the increase in arterial lactate concentrations in ovine endotoxemia.\n\nMethods Fourteen adult ewes were instrumented for chronic hemodynamic monitoring. Following 16 hours of endotoxemia (Salmonella typhosa endotoxin, 10 ng/kg/min) the animals received either a continuous infusion of AM at incremental doses (10, 50, 100 ng/kg/min; each for 30 min) or the vehicle (normal saline; n = 7 each). Results Endotoxin infusion contributed to a hypotensivehyperdynamic circulation characterized by decreases in mean arterial pressure (MAP) and systemic vascular resistance index as well as increases in heart rate (HR), cardiac index (CI) and arterial lactate concentrations. AM infusion at 100 ng/kg/min increased the CI (12.2 \u00b1 0.8 vs 7.8 \u00b1 0.5 l/min) and oxygen delivery index (1,734 \u00b1 121 vs 1,075 \u00b1 63 ml/min/m 2 ), thereby decreasing the arterial lactate concentration (0.7 \u00b1 0.2 vs 1.7 \u00b1 0.3 mg/dl) and mean pulmonary arterial pressure (18 \u00b1 1 vs 24 \u00b1 1 mmHg; each P < 0.001 vs control) noticed in the control group. However, AM infusion at 100 ng/kg/min was linked to a decrease in MAP (64 \u00b1 2 vs 80 \u00b1 4 mmHg, P < 0.001 vs control).\n\nConclusions Despite decreasing MAP, infusion of AM reversed pulmonary hypertension and improved the oxygen supply-demand relationship in a dose-dependent manner, as indicated by a reduced arterial lactate concentration. However, due to the vasodilatory properties of AM, it may be rationale to combine AM with a vasopressor agent.\n\nAngiopoietin-2 correlates with pulmonary capillary permeability and disease severity in critically ill patients Introduction It has previously been shown that angiopoietin-1 (Ang1) protects the adult vasculature against plasma leakage, whereas Ang2 and VEGF destabilize the vascular endothelium resulting in vascular leakage. Consequently they might be involved in the pathophysiology of acute lung injury (ALI) and acute respiratory distress syndrome (ARDS) in sepsis patients. We hypothesized that plasma Ang2 levels are associated with pulmonary capillary protein permeability, the lung injury score (LIS), length of stay on the ICU, the APACHE II score and survival in septic patients with ALI or ARDS. Methods A prospective observational study was performed in an ICU of an university hospital on 112 patients: 38 after elective cardiac surgery, 26 after major vascular surgery, 24 with sepsis and 24 with trauma. Plasma levels of Ang1, Ang2 and VEGF were measured and a mobile probe system was used to measure the pulmonary leak index (PLI) (that is, the transvascular transport rate of gallium-67-radiolabeled transferrin).\n\nResults Plasma levels of Ang2 and the PLI were significantly higher in patients with sepsis compared with other patient groups.\n\nIn the sepsis group, a positive linear correlation was observed between plasma levels of Ang2 and length of stay on the ICU (r s = 0.509, P < 0.05) as index for disease severity. For all patients together, Ang2 had a positive linear correlation with PLI (r s = 0.374, P < 0.01), LIS (r s = 0.489, P < 0.01) and APACHE II score (r s = 0.287, P < 0.01). Furthermore, Ang2 was significantly increased S5 in nonsurvivors. Plasma Ang1 levels did not differ between groups. VEGF levels were undetectable in the plasma of the majority of patients.\n\nConclusions Our results suggest that Ang2 is a mediator of pulmonary capillary permeability and a marker of disease severity in critically ill patients. Furthermore, the plasma levels of Ang2 and the ratio between Ang1 and Ang2 are more important in pulmonary capillary permeability and disease severity than absolute levels of Ang1 and VEGF.\n\nIntroduction Cardiac dysfunction is a feature of sepsis. In order to gain insight into the fundamental mechanisms of this phenotype, gene expression analysis (Affymetrix) was applied to serial cardiac biopsies of sham (n = 2) and E. coli infected pigs (n = 3). Methods Cardiac samples were taken basal and hourly after infection for gene analysis and at the end of the experiment for histopathological examination. Genes were determined to be differentially regulated at a greater than or less than twofold change and P < 0.05. Results Sham pigs had stable heart rate, cardiac output (CO) and core temperature for the 5-hour period; infected pigs demonstrated an early elevation in CO and ventricular shortening and/or ejection (assessed by echocardiography) followed by development of hypodynamics. In infected animals, increasing numbers of genes were upregulated or downregulated (36, 278, 514, 842 and 1,238 at 1, 2, 3, 4 and 5 hours) ( Figure 1 ) whereas sham infection altered fewer (247, 67 and 384 genes at 2, 3 and 4 hours). Comparing sham vs infected animals at the same time, numbers of significantly altered genes increased with time (32 at basal, to 74, 189 and 601 at 2, 3 and 4 hours post infection). In hematoxylin-eosinstained sections, histopathological assessment revealed acute inflammation in pericardium and myocardium in infected pigs.\n\nConclusions These results will provide biomarker and mechanistic insights to pathogenesis of cardiac dysfunction of septic peritonitis and may also help identify some altered novel gene transcription pathways that can serve as new targets for diagnostic tools and therapeutic strategies. All candidate genes will be validated by quantitative PCR. associated with proximal tubule injury. In several in vitro and animal studies alkaline phosphatase (AP) was found to be effective in attenuating the inflammatory response by dephosphorylating LPS and may prevent organ damage. The objective of this study was to investigate the effect of AP on renal iNOS expression and kidney damage in patients with severe sepsis or septic shock. Fifteen patients (nine male/six female, age 55 \u00b1 5 years) with Gram-negative bacterial infection, two out of four SIRS criteria (<24 hours) and acute onset of end-organ dysfunction (<12 hours) were included in a randomized, double-blind, placebo-controlled phase IIa study (2:1 ratio). An intravenous bolus injection of 67.5 U/kg bovine intestinal AP was followed by a maintenance dose of 177.5 U/kg for 24 hours. Arterial blood and urine were collected at different time points and analyzed for stable metabolites of NO. iNOS mRNA was determined by quantitative real-time RT-PCR using RNA isolated from renal cells in urine. The urinary excretion of the cytosolic glutathione S-transferase-A1 (GSTA1-1), a marker for proximal tubule damage, was measured using an ELISA. Data are depicted as the median (25-75% range). NO metabolites in blood were not significantly different between AP-treated (n = 10) and placebo-treated (n = 5) patients. However, the urinary excretion of NO metabolites decreased by 80% (75-85) from 227 (166-531) at baseline to 41 (28-84) \u00b5mol/ 10 mmol creatinine (P < 0.05) after 24 hours of AP administration. After placebo treatment, the amount of urinary NO metabolites increased by 70% (45-570) (from 81 (64-419) to 628 (65-1,479) \u00b5mol/10 mmol creatinine, P < 0.05). Baseline expression levels of iNOS in renal cells were 42-fold induced at baseline (vs healthy subjects), and AP administration reduced this induction by 80 \u00b1 5% ( Figure 1 ). Creatinine clearance improved by 45% (30-180) in patients treated with AP and declined by 25% (15-35) in placebo-treated patients. During the first 24 hours the amount of GSTA1-1 in urine of AP-treated patients decreased by 70% (50-80), compared with an increase of 200% (45-525) in placebo-treated patients, which correlated with urinary NO metabolites, indicating NO-induced proximal tubular damage.\n\nIn conclusion, in septic patients, infusion of AP results in an attenuated upregulation of iNOS and, subsequent, reduced NO production in the kidney, associated with an improvement in renal function.\n\nIntroduction Hypothermia was shown to attenuate ventilatorinduced lung injury (VILI) in high end-inspiratory lung volume models of VILI [1] [2] [3] . Experimental evidence suggests that moderate tidal volumes may, under certain clinical conditions that induce alveolar instability, lead to a lung injury [4] . Recent studies have also suggested that insults like shock [5] or surgery [6] sensitize the lung to injury by priming for an exaggerated response to a second stimulus. The aim of this study was to investigate whether moderate hypothermia attenuates low lung volume injury during low PEEP, high FiO 2 and moderate tidal volume ventilation in animals sensitized to injury by previous anesthesia and surgery. Methods Sixteen male adult Sprague-Dawley rats, instrumented under ether anesthesia with vascular catheters on the previous day, were anesthetized, tracheostomized, connected to a ventilator and randomly allocated to groups of normothermia (37 \u00b1 0.5\u00b0C, group N, n = 8) or hypothermia (33 \u00b1 0.5\u00b0C, group H, n = 8). After 2 hours of mechanical ventilation (FiO 2 1,0, respiratory rate 60/min, tidal volume 10 ml/kg, PEEP 2 cmH 2 O) inspiratory pressures were recorded, rats were sacrificed, the P-V curve of the respiratory system constructed, and bronchoalveolar lavage and aortic blood samples obtained.\n\nResults Group H animals exhibited in comparison with group N animals a lower increase in peak inspiratory pressures (0.7 \u00b1 1.1 vs 2.4 \u00b1 0.5 mmHg, P < 0.001), significant shift of the P-V curve to the left and lower total protein (113 \u00b1 42 vs 201 \u00b1 97 \u00b5g/ml, P = 0.047) and TNF (23.5 \u00b1 8.0 vs 35.2 \u00b1 8.5 pg/ml, P = 0,022) levels in BAL samples. Conclusion Moderate hypothermia attenuated lung injury during low PEEP, high FiO 2 and moderate tidal volume ventilation in animals sensitized to injury by previous anesthesia and surgery.\n\nS8 apparent thickness evaluated using intravital microscopy by comparing 4 and 150 kDa dextran distribution as markers of GLX permeable and impermeable tracers, respectively. Intravital microscopy was used to characterize mesentery functional capillary density. Because glycocalyx is extremely sensitive to free radical, oxidative stress was evaluated by oxidation of dihydrorhodamine (DHR) in microvascular beds and by concentrations of heart malondialdehyde (MDA) and plasma carbonyl proteins (CP).\n\nResults LPS elicited a 4 hours later profound reduction in GLX layer thickness and increase in plasma hyaluronan levels. LPS rats had decreases in capillary continuous flow, and significant increases in intermittent and stopped flow capillaries compared with controls.\n\nThe pressor responses to norepinephrine were greatly reduced, indicative of vascular hyporeactivity. In vivo oxidation of DHR and levels of heart MDA and plasma CP were all increased in LPStreated rats. Interestingly, in LPS rats, APC reduced plasma hyaluronan levels and GLX destruction, which was accompanied with major improvements in vasopressor response and functional capillary density. APC treatment also prevented increases in biochemical and in vivo microvascular oxidative stress markers. Conclusion In our model of septic shock, increased plasma hyluronan levels and reduction in endothelial layer thickness indicated GLX degradation. APC prevented vascular oxidative stress and limited GLX loss. GLX degradation plays a critical role in the septic vasculature and generation of free radicals during septic shock is potentially toxic to GLX function.\n\nIntroduction Mechanical ventilation may induce lung injury in patients with normal lungs. Application of PEEP appears protective. Lung injury is associated with the production and release of inflammatory mediators. Such mediators have been identified in patients' exhaled breath condensate (EBC) in various lung pathologies. In this study we identified EBC inflammatory markers in 27 mechanically ventilated brain-injured subjects with neither acute lung injury (ALI) nor sepsis. Methods Patients were ventilated with 8 ml/kg tidal volume and were put either on PEEP = 0 (ZEEP, n = 12) or 8 cmH 2 O (PEEP, n = 15). EBC was collected using the RTube device (Respiratory Research Inc., Charlottesville, VA, US) on the first, third, and fifth day of mechanical ventilation, and pH, IL-10, IL-1\u03b2, IL-6, IL-8, IL-12p70 and TNF\u03b1 were measured. Applying mixed effects models, we further investigated potential relationships of the above EBC markers with indices of: i, lung injury (LIS score, PaO 2 /FiO 2 , detected pathologies on lung CT); ii, brain injury (ICP, CPP, GCS, serum (s) S100 protein, pentothal and mannitol administration); iii, endothelial injury (sICAM-1, sVCAM-1, von Willebrand factor antigen); iv, systemic inflammation (temperature, leukocyte counts and neutrophil counts in blood, albumin, soluble triggering receptor expressed on myeloid cells (sTREM), CRP, procalcitonin (PCT) and all above-mentioned cytokines in serum or plasma); and v, disease severity (APACHE II score, 24 hour ICU trauma score, presence of SIRS, mean arterial pressure).\n\nResults No significant differences in EBC measurements were observed between the two groups except a time-dependent decrease in IL-10 (P < 0.05, by ANOVA) in the PEEP group. EBC pH and IL-10 showed no significant relationships (mixed effects models) with any parameter measured. All other EBC cytokines were inversely related to sTREM levels. Additional significant relationships were obtained between individual EBC cytokines and sIL-8 (IL-8, IL-12p70, TNF\u03b1), sIL-6 (IL-1\u03b2), PCT (IL-1\u03b2, IL-12p70), the existence of SIRS (IL-6, IL-8), sVCAM-1 (IL-6), and pentothal administration (IL-1\u03b2).\n\nConclusion In our population of mechanically ventilated, braininjured patients with no ALI, ZEEP or applied PEEP did not induce detectable changes in most lung inflammatory mediators in EBC; the latter appear mostly related to markers of systemic inflammation (especially sTREM-1) rather than to indices of brain and endothelial injury.\n\nIntroduction The aim of this study was to access the local inflammatory reactivity by measurement of the cytokine response The onset mechanism of ALI/ARDS and subsequent tissue injury are considered to be associated with neutrophil elastase, and the main causes of ALI/ARDS are considered to be sepsis or aspiration pneumonia. In Japan, sivelestat sodium hydrate (Elaspol), a selective elastase inhibitor, was approved in 2002 for ALI/ARDS accompanied by SIRS, and this medicine has been evaluated in a clinical situation. In this study, we performed a retrospective comparison of the sivelestat sodium administration between two groups of patients: Group Elaspol, consisting of 308 patients(209 males and 99 females, aged 66 \u00b1 15 years) with ALI/ARDS accompanied by SIRS who were treated with sivelestat sodium at a dose of 0.2 mg/kg/hour for 72 hours or more, after approval of this drug; and Group Control, consisting of 41 patients (28 males and 13 females, aged 66 \u00b1 14 years) with ALI/ARDS accompanied by SIRS who were treated in the ICU under similar conditions, but using traditional methods for respiratory control, prior to approval sivelestat sodium. The APACHE II scores of Group Elaspol and Group Control were 23 \u00b1 9 and 23 \u00b1 8, SOFA scores were 8.7 \u00b1 3.8 and 8.9 \u00b1 4.1, and the lung injury scores were 2.1 \u00b1 0.7 and 2.1 \u00b1 0.6, respectively, with no significant differences between the groups. The initial PEEP value of Group Elaspol was 5.9 \u00b1 3.3, which was significantly higher than that of Group Control (3.4 \u00b1 2.7 cmH 2 O). The PaO 2 /FIO 2 ratios under mechanical ventilation management 24, 48 and 72 hours after the beginning of drug administration were 209 \u00b1 87, 222 \u00b1 92, and 222 \u00b1 82 mmHg in Group Elaspol, and were 191 \u00b1 91, 207 \u00b1 91, and 211 \u00b1 100 mmHg in Group Control. The ventilator-free days of Group Elaspol and Group Control were 18 \u00b1 9 and 10 \u00b1 12 days, respectively, and these values showed a significant difference (P < 0.001). Furthermore, the survival rate after 28 days was significantly higher in Group Elaspol than in Group Control (Group Elaspol: 75%, Group Control: 52%; P < 0.001). These results suggest that sivelestat sodium hydrate is a good option as a treatment strategy for neutrophil elastase-associated septic ALI/ARDS accompanied by SIRS.\n\nIntroduction Many patients who experience surgical stress including burn injury become susceptible to severe sepsis and septic organ dysfunction including acute lung injury (ALI), which remains the primary contributor to morbidity and mortality in burn patients. Proinflammatory cytokines including several chemokines are implicated in this process. The pharmacological modulation with steroid inhibiting the process of cytokine synthesis may serve as effective therapy for the prevention of tissue injury and the resultant organ dysfunction including respiratory failure. We developed a murine model of septic ALI after burn insult and examined the effects of prolonged administration of moderate doses of steroid. Methods Male BALB/c mice were divided into three groups. Group I served for sham burns. In groups II and III, a 15% BSA fullthickness burn was made on the dorsum under ether anesthesia, followed by adequate fluid resuscitation. After the burn injury, 3 mg/kg prednisolone (PSL) in group III was administered subcutaneously daily for 10 days. On the 11th day, 10 mg/kg lipopolysaccharide (LPS) was injected intravenously. In the first experiment, we observed the survival within 72 hours after LPS injection in each group (n = 10). In the second experiment, we sacrificed the animals at 12 hours after LPS injection, then obtained plasma and lung tissue to determine the levels of TNF\u03b1 and macrophage inflammatory protein-2 (MIP-2, a functional homologue of human IL-8 in mice) in these samples (n = 8, sandwich ELISA). We also determined gene expression (n = 4, MIP-2/GAPDH mRNA ratio by RT-PCR), myeloperoxidase activities (MPO, n = 8) and histopathological findings in the lung tissue.\n\nThe survival and production of cytokines are shown in Table  1 . Histopathological findings in group III were obviously attenuated. Introduction Lethal sepsis occurs if an excessive inflammatory response evolves that cannot be controlled by physiological anti-inflammatory mechanisms. Vagus nerve stimulation showed improved survival in sepsis; however, this seems not to be feasible in septic patients. We therefore investigated the effect of activation of the cholinergic anti-inflammatory pathway by pharmacologic cholinesterase inhibition on survival and inflammation in a septic mouse model. Methods To investigate the therapeutic effect of nicotine and physostigmine we performed cecal ligation and puncture (CLP) in female C57/B6 mice (each group n = 21). Substances were administered by intraperitoneal injection. Control groups received the same volume (50-180 \u00b5l) of LPS-free 0.9% NaCl (solvent). CLP was performed blinded to the identity of the treatment group.\n\nIn addition to survival experiments we performed measurements of cytokines in plasma and the electrophoretic mobility shift assay (EMSA) for NF-\u03baB in peritoneal skin, liver and kidneys. Results (1) Animals treated with nicotine (400 \u00b5g/kg) or physostigmine (80 \u00b5g/kg) survived significantly better than control mice (P < 0.05). There was no difference between the treatment groups.\n\n(2) Dose escalation of physostigmine was not superior to the normal dose. Survival in the high-dose group, however, was still significantly better than in the control group. (3) Proinflammatory cytokine levels of TNF\u03b1, IL-6 and IL-1\u03b2 were significantly reduced in animals treated with physostigmine (P < 0.01). (4) Cholinesterase inhibition with physostigmine in CLP reduced NF-\u03baB activation in the peritoneum, kidney and liver compared with the control and sham-operated group (P < 0.01).\n\nConclusion We show that pharmacological cholinesterase inhibition with physostigmine improves survival in experimental sepsis, most probably by activation of the cholinergic anti-inflammatory pathway. One possible mechanism is modulation of the NF-\u03baB pathway. Therefore, cholinesterase inhibition may have important implications for treatment of sepsis.\n\nIntroduction High-mobility-group box protein 1 (HMGB1) is a highly conserved, ubiquitous protein present in the nuclei and cytoplasm of nearly all cell types and, secreted into the extracellular milieu, acts as a proinflammatory cytokine. The function of HMGB1 has been widely studied for sepsis and inflammation. HMGB1 was reported as a late mediator in endotoxic shock and was known as an abundant protein present in nuclei and cytoplasm and involved in maintaining nucleosome structure and regulation of gene transcription. Moreover, elevated, circulating levels of HMGB1 also have been described in a case of human hemorrhagic shock due to abdominal aortic aneurysm without evidence of infection. However, the relationship between HMGB1 and trauma has not been studied except for the report of a rat model of burn.\n\nThe study cases consisted of 20 trauma patients who were admitted to the emergency room by ambulance.\n\nAs soon as they arrived in the emergency room, their blood sample were collected, centrifuged, and stored at -80\u00b0C. The serum HMGB1 concentration was measured by ELISA. We compared the injury severity score ( Lung heat shock protein (HSP70) expression in glucosamine vs saline following cecal ligation and puncture.\n\nIntroduction While early aggressive fluid administration has been associated with improved outcome in sepsis [1] , this approach may increase the risk of lung edema and abdominal compartment syndrome when capillary permeability is increased. The aim of this study was to test two different approaches of volume resuscitation in septic animals. Methods Thirty pigs were anaesthetized and invasively monitored (systemic and regional flows and pressures). They were randomized to control, moderate volume (C; n = 7), control, high volume (CH; n = 8), peritonitis, moderate volume (P; n = 8) and peritonitis, high volume (PH; n = 7). Peritonitis was induced by instillation of 1 g/kg autologous faeces dissolved in glucose solution. Ventilation was adjusted to maintain an arterial pO 2 >100 mmHg. Groups CH and PH received 15 ml/kg/hour Ringer's solution plus 5 ml/kg/hour HES 6%, whereas groups C and P received 10 ml/kg/hour Ringer's solution. If clinical signs of hypovolaemia were present, additional boluses of HES 6% (maximally 100 ml/hour) were given. The animals were treated and observed for 24 hours or until death. Results Cardiac output was higher in group PH as compared with the other groups (P < 0.05), while mean arterial pressure was Available online http://ccforum.com/supplements/11/S2\n\nOxygenation index.\n\nSurvival proportion. S12 similar in all groups. While the oxygenation index (paO 2 /FiO 2 ) decreased in all groups, group PH had the lowest values after 6 hours and throughout the rest of the experiments (P < 0.05) ( Figure 1 ). Survival was lowest in group PH, followed by group P, while all animals in the control groups survived until 24 hours ( Figure 2 ). Conclusion High-volume administration decreased oxygenation and survival in peritonitis but not in control animals. A high-volume approach may not be generally beneficial in abdominal sepsis. \n\nIntroduction Fluid resuscitation is necessary in sepsis, but positive fluid balance may increase the risk of mortality. We tested the hypothesis that a volume resuscitation strategy may modify liver mitochondrial function and outcome. Methods Twenty-nine anesthetized pigs received for 24 hours either endotoxin or placebo, and either Ringer's lactate 10 ml/kg/hour or 15 ml/kg/hour + 5 ml/hour HES. Systemic and regional hemodynamics were measured. Liver mitochondrial state 3 and state 4 oxygen consumption were determined. Results Hepatosplanchnic oxygen delivery was similar in endotoxic pigs with high (2.97 \u00b1 1.58 ml/min/kg) vs moderate volume administration (3.06 \u00b1 0.6 ml/min/kg), but hepatosplanchnic VO 2 was lower in animals with high (1.32 \u00b1 0.4 ml/min/kg) vs moderate volume administration (1.75 \u00b1 0.3 ml/min/kg, P = 0.019). Endotoxin high-volume pigs exhibited a decrease in state 3 respiration for complex I and complex II (not significant) in comparison with control high-volume and with endotoxin low-volume pigs ( Figure 1 ). They also had an increased mortality rate during the 24-hour study period (60% vs 0% in controls). Conclusion A prolonged high-volume resuscitation approach during endotoxemia may be associated with impaired hepatosplanchnic oxygen consumption, liver mitochondrial dysfunction and high mortality. The impact of aggressive and prolonged volume administration on hepatosplanchnic oxygenation and mitochondrial function in human sepsis should be determined. Multiple studies have stressed the importance of the contribution of activated complement to the pathology of reperfusion injury after tissue ischemia. Using intravital microscopy, this study explores functional consequences of the inhibition of the classical pathway of complement activation with C1-esterase inhibitor (C1-INH) in the context of superior mesenteric artery occlusion (SMAO)/ reperfusion. Thirty anesthetized, spontaneously breathing, male Sprague-Dawley rats underwent SMAO for 60 minutes followed by reperfusion (4 hours). C1-esterase inhibitor (100 IU/kg, 200 IU/kg body weight) or saline (0.9%) was given as a single bolus before reperfusion. Sham-operated animals (n = 10) without SMAO served as controls. Systemic hemodynamics were monitored continuously, arterial blood gases analyzed intermittently, and leukocyte/ endothelial interactions in the mesenteric microcirculation quantified at intervals using intravital microscopy. Ileal lipid-binding protein (I-LBP) levels were measured from serum samples with an ELISA at the end of the experiments. C1-INH restored microcirculatory perfusion of postcapillary venules to baseline levels in a dose-dependent manner and reduced leukocyte adhesion following SMAO/reperfusion to similar levels in both C1-INH-treated groups during reperfusion. Furthermore, C1-INH treatment efficiently prevented metabolic acidosis, and reduced the need for intravenous fluids to support blood pressure. Furthermore, I-LBP levels decreased in a dose-dependent manner, and were comparable with the levels of sham-operated animals at the end of the experiments. Survival rates were 100% in controls and after 200 IU/kg C1-INH, 90% after 100 IU/kg C1-INH, and 30% in saline-treated animals.\n\nIn the setting of mesenteric ischemia, C1-INH given as a bolus infusion shortly before reperfusion efficiently restored microcirculatory perfusion in a dose-dependent manner, reduced local and systemic inflammatory response, and improved outcome. I-LBP levels correlated well with the functional consequences of mesenteric ischemia/reperfusion and treatment at the end of the experiments.\n\nIntroduction Terlipressin is increasingly used in the treatment of sepsis-associated hypotension. However, terlipressin may reduce cardiac output and global oxygen supply. Methods We performed a prospective, randomized, controlled clinical study to determine whether dobutamine may counterbalance the depressions in cardiac index and mixed-venous oxygen saturation resulting from sole terlipressin infusion. We enrolled 60 septic shock patients requiring high doses of norepinephrine (0.9 \u00b5g/kg/min) to maintain mean arterial pressure at 70 \u00b1 5 mmHg.\n\nPatients were randomly allocated to be treated either with (a) 1 mg terlipressin, (b) 1 mg terlipressin followed by incremental dobutamine doses to reverse the anticipated reductions in mixed-venous oxygen saturation, or (c) sole norepinephrine infusion (control; each n = 20). Results Data from right heart catheterization, thermo-dye dilution catheter, gastric tonometry, as well as organ function and coagulation were obtained at baseline and after 2 and 4 hours. Terlipressin (with and without dobutamine) infusion preserved the mean arterial pressure at threshold values of 70 \u00b1 5 mmHg, while allowing one to reduce norepinephrine doses to 0.18 \u00b1 0.04 and 0.2 \u00b1 0.05 \u00b5g/kg/min, respectively (vs 1.4 \u00b1 0.07 \u00b5g/kg/min in controls at 4 hours; each P < 0.01). The terlipressin-linked decrease in mixed-venous oxygen saturation was reversed by dobutamine (at 4 hours: 59 \u00b1 2 vs 69 \u00b1 3%, P = 0.023). No statistically significant differences were found intra-group and between groups in terms of differences between gastric mucosal and arterial carbon dioxide partial pressure, blood clearance of indocyanine green, as well as the plasma disappearance rate of indocyanine green Conclusions In catecholamine-dependent human septic shock, terlipressin (with and without concomitant dobutamine) stabilizes hemodynamics and reduces norepinephrine requirements. Dobutamine is a useful inotropic agent to reverse the depression in global oxygen transport resulting from sole terlipressin infusion without obvious side effects.\n\nThe thenar muscle StO 2 was measured by near-infrared spectroscopy (InSpectra; Hutchinson Technology, Hutchinson, MN, USA). Oral mucosal tissue oxygen saturation, microcirculatory blood flow and blood flow velocity were measured in depths of 1 and 4 mm with a laser Doppler flowmetry and remission spectroscopy system (O 2 C).\n\nResults See Table 1 . Vasopressin infusion led to a significant decrease of oral mucosal oxygen saturation and blood flow, and a significant decrease of flow velocity in a depth of 1 mm. Changes in thenar tissue perfusion were not detectable. Conclusion Vasopressin causes a deterioration of oral mucosal blood flow but not in thenar tissue perfusion.\n\nIntroduction Sepsis alters vascular reactivity. We studied the impact of peritonitis and endotoxemia on hepatic and superior mesenteric arterial contractility. Materials and methods We studied fecal peritonitis (P, n = 7), endotoxin-infusion (E, n = 8) and control (C, n = 6) for 24 hours after abdominal surgery and eight control pigs without surgery (SPA). Systemic and regional hemodynamics and ex-vivo splanchnic vascular reactivity to norepinephrine (NE; tissue bath) were measured and cumulative dose-response curves to NE were constructed. Tension was expressed in grams. Results CO increased (P < 0.05) in P and E. SMA flow (median (range)) decreased in C from 24 (15-30) to 15 (11-21) ml/kg/min (P = 0.022) ( Table 1) . 10 (8-16)** , \u2020 8 (7-10)** , \u2020 Data presented as median (range). *P = 0.002 E vs P and C; **P < 0.01 vs C, P, and E; \u2020 P < 0.001 vs C, E and P; \u2021 P = 0.008 vs C.\n\nThe splanchnic vascular response to NE is heterogenous in sepsis, and SMA is most affected. This may modify blood flow distribution if high NE doses are used. \n\nMean arterial pressure (MAP) during continuous and intermittent bolus infusion of terlipressin in endotexemic ewes.\n\nIntroduction Tissue Doppler imaging (TDI) is a novel technique that measures myocardial velocity. The peak early diastolic mitral annulus velocity (E\u2032) offers a relatively preload-insensitive measure of LV relaxation. There are scant data regarding its use in sepsis or endotoxemia. This study sought to determine the effect of endotoxemia upon TDI variables. Methods With ethics committee approval, 10 male Sprague-Dawley rats were studied. Anesthesia was induced with alfaxalone and maintained with isofluorane. Mechanical ventilation was performed via tracheostomy. All rats received 0.9% NaCl 3 ml/hour via a carotid line. Immediately after baseline assessment (T = 0), rats received 1 ml/kg i.v. infusion over 30 minutes (study group (n = 5), endotoxin 10 mg/ml (Escherichia coli O55:B5; Sigma, MO, USA); control group, 0.9% NaCl). Echocardiography was performed (15 MHz transducer, Vivid5; GE Healthcare) at T = 0, 60 minutes (T = 60) and 2.5 hours (T = 150). Measurements included the heart rate, mean arterial pressure (MAP), femoral venous pressure, LV outflow tract diameter and flow (peak velocity (V peak ), cardiac output (CO)), peak early diastolic mitral inflow (E), peak systolic mitral annulus velocity (S\u2032) and E\u2032. Results There was no significant difference in mean \u00b1 SD weight (study 539 \u00b1 88 g, control 504 \u00b1 108 g, P = 0.6) or hemodynamic variables at T = 0. At T = 60, only V peak was higher in the study group compared with controls (1.29 \u00b1 0.24 vs 0.86 \u00b1 0.21 m/s, P = 0.03). The study group demonstrated lower MAP, E and E\u2032 at T = 150 (Table 1) . Introduction The aim of this retrospective study is to evaluate hemodynamic and neurohormonal effects of levosimendan in cardiac patients with sepsis-induced cardiac dysfunction. Septic shock is characterized by profound cardiovascular alterations including myocardial depression. Levosimendan has recently been shown to improve cardiac function in septic shock. Methods Fifteen patients with myocardial depression related to septic shock were enrolled. All patients had SIRS criteria, culture isolation of one or more pathogens, positive procalcitonin, SBP < 90 mmHg unresponsive to load challenge. We defined myocardial depression as a reduced SvO 2 in the presence of increased brain natriuretic peptide secretion and Troponin I release, and systolic and/or diastolic dysfunction by transoesophageal echo evaluation of ejection fraction and mitral annulus tissue Doppler imaging velocities. All patients received levosimendan infusion for 24 hours at 0.1 \u00b5g/kg/min combined with norepinephrine. Results Data were obtained by evaluating the average of the percentage variation between T 0 (starting infusion) and T 1 (24 hours after infusion), T 2 (48 hours), T 3 (72 hours), T 4 (96 hours), T 5 (120 hours) and T 6 (144 hours). Levosimendan significantly increased SvO 2 and ejection fraction, and decreased S16 Troponin I and brain natriuretic peptide. Levosimendan improved diastolic function by increasing the E\u2032 velocity at tissue Doppler imaging at 48 hours. All data were analysed by the Fisher F test. Introduction Sickeuthroid syndrome is very frequent in critically ill patients. Cytokines may have a role in this syndrome. IL-12 is involved in the central regulation of the hypothalamic-pituitarythyroid (HPT) axis during illness. The aim of this study is to evaluate the relationship of IL-12 and thyroid functions in septic patients. Materials and methods Twenty-four septic patients and 18 healthy controls were enrolled into the study with the mean ages of 49.9 \u00b1 20.6 and 45.8 \u00b1 22.3 years, respectively. Hyperthyroid and hypothyroid patients were excluded. Free triiodothyronine (fT3), free thyroxine (fT4) and TSH were measured simultaneously with IL-12.\n\nThe mean IL-12, fT3, fT4 and TSH values of septic patients and the control group are presented in Table 1 . IL-12 was significantly higher in septic patients (19.05 \u00b1 10.7 pg/ml vs 4.8 \u00b1 2.0 pg/ml, P < 0.005). fT3 and TSH values were significantly low in septic patients. There was a significantly strong correlation between IL-12 and fT4 in septic patients but not fT3 and TSH (r = 0.88, P = 000). There was no correlation between IL-12 and other thyroid indices in the control group. S17 center directly after admission into the network hospitals. The final step was the presentation of the project in the different hospitals. Results We treated 10 children with Waterhouse-Friderichsen syndrome in the network. Three of them were attended on site and seven were transferred in the tertiary center. The announcement time in eight cases was 15 minutes-1 hour. Primarily, a consultation was accomplishing routinely. The transportation team of the tertiary center continued the treatment on site and afterwards in the center. All patients showed typical signs of Waterhouse-Friderichsen syndrome with purpura fulminans and severe multiorgan failure. No patient died and only one patient had necrosis of the skin, which existed already at admission. The others had a restitution ad integrum. No adverse effects were observed with the PC concentrate administration. Conclusions The network system and the standard treatment with PC worked without severe problems. The survival rate and the outcome in our small study group were excellent. Our experience allows us to enlarge the system on other diseases.\n\nIntroduction Neonates and infants in the ICU are at high risk of severe infections and sepsis. Often it is not easy to diagnose sepsis based only on clinical findings; reliable biomarkers are needed to prove the diagnosis.\n\nObjective To study the value of procalcitonin (PCT) as a marker, verifying the diagnosis, which enables the start of de-escalating ABT in patients with clinical signs of sepsis.\n\nMethods Three hundred and seventy-four patients on artificial lung ventilation from two pediatric ICUs of two Russian hospitals were enrolled. Blood samples for PCT testing (PCT LIA; BRAHMS AG, Germany) were taken under suspicion of sepsis or exacerbation of bacterial infection. In the first stage (January-December 2005), 50 neonates (age 6 (4-12) days) with various perinatal pathologies were studied (Group A), and routine ABT was prescribed, with blood samples taken and stored for further PCT assessment. In the second stage (January-November 2006), 324 infants (age 6 (1.5-9.4) months) after cardiac surgery were enrolled (Group B), and ABT was adjusted based on PCT-testing results. PCT > 2 ng/ml indicative of systemic bacterial inflammation in addition to clinical signs of sepsis was an indication for ABT with carbapenems. Data are shown as the median and interquartile range.\n\nResults Group A. Sepsis was diagnosed in 16/50 (32%) patients. PCT > 2 ng/ml was observed in 23/50 (46%) cases, including 15/16 (94%) patients with clinically diagnosed sepsis. In patients with PCT > 2 ng/ml the mortality rate was 7.7% if carbapenems (meropenem or imipenem/cilastatin) were administered (n = 13), compared with 20% with different ABT (n = 10) -although in patients with PCT < 2 ng/ml (n = 27), ABT with carbapenems (n = 12) resulted in paradoxically higher mortality compared with other ABT schemes (n = 15): 17% vs 6.6%. Group B. Sepsis was defined in 24/324 (7.4%) patients. PCT > 2 ng/ml was in 53/324 (16%) cases, including all patients with clinically diagnosed sepsis. Early ABT with meropenem, combined with vancomycin or linezolid, allowed one to decrease sepsis-related mortality in these patients to 29%, which used to be as high as 74% before the introduction of this algorithm (P = 0.0028). Conclusion Early verification of sepsis using PCT combined with carbapenems-based ABT enables decreasing sepsis-related mortality in critically ill infants and newborns staying in the ICU.\n\nAvailable online http://ccforum.com/supplements/11/S2 Introduction The aim of this study was to investigate the prevalence of endotoxemia early after elective surgical procedures in patients admitted to an ICU of a university hospital. Methods One hundred and four nonselect patients were recruited. Patients were excluded if they were admitted during the weekend or from another ICU and if they were on chronic dialysis. Within 4 hours of admission functional data were collected and severity scores (APACHE, SOFA) calculated. Arterial blood samples were also taken and processed according to Spectral Diagnostics' endotoxin activity (EA) assay [1] . The method allows one to express EA as a function of each patient's neutrophil chemiluminescence activity (on a scale from 0 to 1). An EA level of 0.4 is approximately equivalent to an endotoxin concentration of 25-50 pg/ml, and a level of 0.6 is approximately equivalent to a LPS concentration of 100-200 pg/ml. Data were analysed according to EA ranges: low (EA < 0.4), intermediate (0.4 \u2264 EA < 0.6), and high (EA \u2265 0.6). Differences between ranges of EA were assessed by analysis of variance (Sigma Stat, SPSS), accepting P < 0.05 as significant. Data are expressed as the mean \u00b1 SD. Results In our case mix, patients were 68 (65%) in the low group, 17 (17%) in the intermediate group and 19 (18%) in the high group. Age (61 \u00b1 17 years) was not significantly different in the three groups (P = 0.493). Functional and severity scores were not significantly different between groups. Average values were as follows: WBC 11,093 \u00b1 4605 n/mm 3 (P = 0.385), HR 76 \u00b1 16 bpm (P = 0.898), MAP 88.8 \u00b1 13.6 mmHg (P = 0.576), lactate 1.18 \u00b1 0.77 mmol/l (P = 0.370), PaO 2 /FiO 2 383 \u00b1 109 mmHg (P = 0.474), APACHE II score 8.3 \u00b1 3.7 (P = 0.542) and SOFA score 1.5 \u00b1 1.4 (P = 0.245). Interestingly, those patients with higher levels of EA were characterized by longer length of stay in the ICU. The ICU length of stay was 1.9 \u00b1 3.1 days in the low group, 8.7 \u00b1 6.7 days in the intermediate group and 4.7 \u00b1 7.7 days in the high group (P = 0.038).\n\nConclusions A rather high number of patients admitted to the ICU following elective surgery are characterized by intermediate-high levels of endotoxemia, as assessed by the EA assay, despite their relative low level of complexity on admission. High levels of EA were associated with a longer length of stay. Objectives To evaluate in septic patients the plasma levels of sTREM-1, a soluble form of TREM-1, which seems to play an important role in inflammatory diseases, and to determine whether plasma sTREM-1 could be used as a diagnostic and prognostic marker in sepsis in the surgical ICU. Design An observational clinical study. Setting The surgical ICU of the University Hospital of Heidelberg, Germany.\n\nPatients Patients admitted to the ICU over a 6-month period with clinical evidence of severe sepsis or septic shock. Interventions None.\n\nSixty-six intensive care patients were enrolled in the study within the first 24 hours after the onset of severe sepsis or septic shock. Twenty-one healthy individuals served as controls. At day 0, day 1 and day 3 after diagnosis of severe sepsis or septic shock, plasma sTREM-1 was measured by ELISA. Plasma sTREM-1 concentrations of healthy controls did not differ from patients with severe sepsis or septic shock at day 0 (42.8 \u00b1 44.9 pg/ml vs 40.8 \u00b1 45.5 pg/ml, not significant), day 1 (42.8 \u00b1 44.9 pg/ml vs 48.6 \u00b1 57.2 pg/ml, not significant) nor at day 3 (42.8 \u00b1 44.9 pg/ml vs 51.9 \u00b1 52.8 pg/ml, not significant). Survivors were defined as patients surviving to at least day 28. There were no differences of plasma sTREM-1 between survivors and nonsurvivors at day 0, day 1 and day 3 (34.8 \u00b1 44 52.4 pg/ml S19 vs 49.5 \u00b1 35.9 pg/ml, 42.6 \u00b1 61.1 pg/ml vs 59.6 \u00b1 47.1 pg/ml, and 47.9 \u00b1 60.2 pg/ml vs 58.2 \u00b1 37.1 pg/ml, not significant). Conclusion In this study including surgical patients with severe sepsis or septic shock, plasma sTREM-1 is not elevated compared with healthy controls. Furthermore, the measurement of plasma sTREM-1 did not allow one to differ between survivors and nonsurvivors. The suggested role of sTREM-1 as a diagnostic and prognostic marker in sepsis was not confirmed in this study.\n\nCan plasma-free DNA concentration be a diagnostic tool in critically ill septic patients? Recent evidence suggests that the plasma-free DNA concentration has potential use as a prognostic marker in many clinical situations including sepsis, trauma, and acute stroke [1] . However, its predictive value is arguable. We hypothesized that plasma DNA is increased in septic patients admitted to the ICU compared with nonseptic ICU patients, and it is correlated with disease severity and clinical outcome. Forty-two consecutive patients (11 septic, 31 nonseptic) admitted to a mixed ICU and mechanically ventilated were recruited. Plasmafree DNA concentration was measured by real-time PCR assay for the \u03b2-globin gene, and the APACHE II score, SOFA score, serum C-reactive protein (CRP) concentrations, procalcitonin (PCT) concentrations, serum lipid concentrations, and clinical outcome (ICU/hospital days and mortality) were assessed on admission to the ICU. Assessments and samplings were repeated as the diagnosis of the patients changed (sepsis, severe sepsis and septic shock). Finally, 86 plasma samples were collected. Descriptive statistics, Mann-Whitney U, Kruskall-Wallis and Spearman's tests, and receiver operating characteristic analysis were used when appropriate. Demographic data were similar. ICU and hospital mortalities were 26.2% and 33.3%, respectively. The mean DNA concentrations on admission were significantly higher in ICU patients compared with healthy subjects (n = 11) (13,405 GE/ml versus 390 GE/ml, P < 0.05) and septic patients compared with nonseptic patients (33,170 GE/ml versus 1,171 GE/ml, P < 0.001). Furthermore, during the overall ICU stay, increased DNA concentration associated with the increase of severity of illness was noted; however, this increase was statistically significant only between septic and septic shock samples (26,624 GE/ml versus 42,861 GE/ml, P < 0.05). The area under the curve obtained for the plasma-free DNA concentration in distinguishing between septic and nonseptic patients on admission was 0.9 (sensitivity 84%, specificity, 95%; cutoff 4,083 GE/ml). Also, the plasma-free DNA concentration was found to be higher in patients who died in the ICU compared with patients who survived, although not statistically significant. The DNA concentration demonstrated a significant correlation with CRP (P = 0.037, r = 0.365), PCT (P = 0.007, r = 0.457) and highdensity lipoprotein (P = 0.015, r = -0.415) concentrations.\n\nIn conclusion, plasma DNA may be a potentially valuable tool to confirm the diagnosis of sepsis on admission to the ICU and to monitor disease severity. Introduction Inadequate tissue perfusion and an uncontrolled systemic inflammatory response are associated with poor outcome in critically ill surgical patients. An increased concentration of unmeasured anions, reflecting hypoperfusion, and the magnitude of the early inflammatory response both correlate strongly with mortality. Our aim was to assess the relationship between these factors, and their ability in combination to predict outcome.\n\nMethods In a prospective study we evaluated 108 consecutive patients admitted to a surgical high dependency unit. Regional Ethics Committee approval was obtained. Serum electrolytes, albumin, phosphate, lactate and C-reactive protein (CRP) were measured on admission and on day 1. We derived the calculated ion gap (CIG) using a simplified modification of the Stewart-Figge equations.\n\nResults Based on previous work, thresholds of 10 mmol/l for CIG and 100 mg/l for CRP were used to categorise patients. Of the patients with a CRP < 100 mg/l, 15.4% had an elevated CIG. Of the patients with a CRP > 100 mg/l, 36.7% had an elevated CIG (P = 0.016, chi-square test). Patients (n = 63) with a CIG < 10 mmol/l and CRP < 100 mg/l had a 1.5% mortality, whereas those (n = 11) with a CIG > 10 mmol/l and CRP > 100 mg/l had a 54.5% mortality (P < 0.0001, chi-square test) ( Table 1) . CRP < 100 mg/l, CRP > 100 mg/l, CIG < 10 mmol/l CIG > 10 mmol/l Inhospital mortality 1.5% (n = 63) 54.5% (n = 11)\n\nConclusion Inflammation is a potent cause of oxidative stress, which in turn results in endothelial damage and increased concentrations of unmeasured anions. The combination of CRP and the CIG, as markers of inflammation and inadequate tissue perfusion, respectively, is a powerful predictor of mortality in the critically ill surgical patient.\n\nC-reactive protein predicts mortality on admission to a surgical high-dependency unit S20 was measured on admission, day 1 and day 2 and was evaluated with respect to inhospital mortality. Results CRP on admission to HDU discriminated survivors from nonsurvivors (P < 0.0001, analysis of variance). A CRP greater than 100 mg/l correlated very strongly with mortality. The mortality in patients with a CRP less than 100 mg/l (n = 93) was 2.2%. The mortality in patients with a CRP greater than 100 mg/l (n = 39) was 25.6% (P < 0.0001, chi-square test), (Table 1) . However, there were no significant differences in CRP with respect to mortality on day 1 or day 2 (P = 0.136 and 0.236, respectively). Conclusion CRP on admission to the surgical HDU is a powerful predictor of mortality (P < 0.0001), but this correlation does not persist after the initial measurement. Our data suggest that early CRP measurement should be undertaken in all critically ill surgical patients in order to quantify the ultimate magnitude of the inflammatory response and the associated mortality. \n\nWe tested the ability of the biphasic aPTT waveform to diagnosis sepsis in patients presenting to the Emergency Department (ED) with the systemic inflammatory response syndrome (SIRS). The biphasic aPTT waveform (BPW), which results from rapid complexing of VLDL and C-reactive protein during aPTT testing, has demonstrated promise as an early diagnostic test for sepsis. Methods A prospective, observational study was designed in which all patients presenting to the ED of an urban university hospital were screened for SIRS. Patients with SIRS unrelated to trauma or myocardial infarction were eligible. Plasma for BPW testing was obtained at the time of enrollment and daily for 7 days in admitted subjects. The primary outcome was a diagnosis of sepsis related to the presence of a BPW at enrollment. Secondary measures were mortality related to the BPW, correlation of any positive BPW with sepsis, and of the BPW with statin therapy. Two criteria for a positive test, light transmittance at 18 seconds (TL18) and the initial slope of the waveform (slope) are used. Two independent experts made the final diagnosis. Results We screened 5,400 consecutive admissions to the ED, identified 207 eligible subjects and enrolled 105 participants. The BPW was present at enrollment in 12 subjects by TL18 and in 28 subjects by slope. Forty-six out of 105 subjects eventually developed a BPW, 54 were diagnosed with sepsis. The sensitivity and specificity for sepsis were 17% (95% CI, 7-27.6) and 93.8% (95% CI, 87-100) by TL18 and 26.9% (95% CI, 14.9-38.9) and 71.4% (95% CI, 58.7-84.1) by slope. The positive predictive value of the test was 75 by TL18 and 50 by slope criteria. The AUC for ROC analysis of the BPW for diagnosis of sepsis is 0.469 by TL18 and 0.560 by slope. The odds ratio for developing sepsis related to any positive BPW was 2.977. The odds ratio for development of a BPW in patients on a statin at the time of presentation was 0.597. Five subjects died by 28 days, 4/5 having a BPW.\n\nThe BPW has no utility in the ED to predict the development of sepsis in at-risk patients. The development of a BPW at any time during the hospital stay correlates with an increased risk of sepsis and mortality. Baseline statin therapy may reduce the chance of developing a BPW. Objective To know whether functional protein C (FPC) levels in critical septic patients could be intended as an evolution marker correlated with prognosis and mortality. Materials and methods A prospective study with determination of FPC levels in all septic patients admitted to the ICU. We used the IL test\u2122 PC kit (Instrumentation Laboratory; synthetic chromogenic substrate). We considered an abnormal low FPC when levels were below 40%, normal FPC when levels were above 80% and low FPC when levels where between 40% and 80%. Data included patient age, diagnosis, SAPS II, SOFA score, OSF and mortality. The analytical data included serum lactate and FPC. Patients were divided into three groups: group I (FPC below 40%), group II (FPC 40-80%) and group III (FPC above 80%). The statistical study was performed with the Analyse-it \u00ae program. The severity was defined by the usual criteria of SAPS II score and lactate levels and then compared with the different FPC groups. Mortality was considered. Results We included 65 patients. The total mortality rate was 16.9% (11 patients). Conclusions We found a direct and a progressive relation with statistical significance between the higher mortality rate and the lower protein C values. The results could mean that the level of protein C can be used as an evolution marker in septic patients. In severe sepsis, microcirculatory dysfunction caused by inflammation, endothelial activation and procoagulant response leads to mithocondrial dysfunction (termed microcirculatory and mitochondrial distress syndrome). If undetected, this condition can lead to parenchymal cellular distress and so to organ failure. As regional and microcirculatory distress are independent of systemic hemodynamic-derived and oxygen-derived variables, we recorded the course of microvascular parameters with a Microscan Video Microscope (Microvision, The Netherlands) in four patients with severe sepsis. We studied the sublingual region because of its embryologic correlation to splanchnic circulation, its thin mucosa. The instrument used a new improved imaging modality for observation of the microcirculation called sidestream dark-field imaging. We consider here four patients with severe sepsis related to esophagectomy, severe polytrauma with splanchnic organ damage and mediastinitis treated with drotrecogin alpha (activated) (DA) at 24 \u00b5g/kg/hour for 96 hours. The patients were admitted to the ICU, ventilated mechanically, monitored hemodynamically via a PICCO system and supported with dobutamine. Videomicroscopy was made before administration of DA and was repeated every 24 hours during the treatment with DA and at 24 hours after its suspension. We recorded values of blood pressure, cardiac function, lactate levels, acid-base balance, temperature and dobutamine dosage. At admission the sublingual microcirculation showed a low capillary density, vessel heterogeneity with a qualitative low flow and flow-no flow. After the first 24 hours from the beginning of DA infusion, sublingual flow showed an increase of vessel density, particularly of the number of small vessels, and the number of continuously perfused vessels increased during and post therapy with DA. We analyzed the microvascular flow with a simple semiquantitative method dividing the images into four equal quadrants and quantificating flow (hyperdynamic, continuous, sluggish, flow-no flow, no flow) for each cohort of vessel diameter (small, medium, large). We analyzed the mean value of results of three images for each patient pre and post DA therapy. Data are presented as the median. Before starting therapy with DA, the microvascular flow index (MFI) was 2.06 for small vessels, 2.09 for medium vessels, and 2.37 for large vessels. After DA infusion, the MFI was 3, 3, and 3, respectively, for small, medium and large Available online http://ccforum.com/supplements/11/S2 vessels. Differences between groups were assessed using the Mann-Whitney U test. We showed a statistically significant difference with P < 0.0001 between MFI before and post DA therapy. We demonstrated a quantitative and qualitative improvement of sublingual microcirculation with an increase of capillary density distribution (area-width) and average velocity versus vessel width. The course of microvascular blood flow may play an important role in sepsis and septic shock because of its relation to the development of multiple organ failure and death. Several studies have demonstrated that changes in microvascular perfusion are an independent predictor of outcome. The improvement of the microcirculation and vascular tone in septic shock by DA is probably related to its anticoagulant/antithrombotic and antiinflammatory action, to the decrease of TNF\u03b1 production and inhibition of iNOS induction, and to improvement of endothelial barrier function and inhibition of chemotaxis, but further investigations are required to elucidate the exact mechanisms. These observations could suggest that DA could have a particular interest in the early management of severe sepsis. Discussion During 4 years of treatment of severe sepsis in the ICU with APC, important changes were observed: faster recognition and diagnosis, transfer to the reference hospital, and introduction of adequate therapy. The decrease in the surviving ratio in 2006 is probably due to a more serious state of the admitted patients -more initial infection located in the abdomen after surgery. Conclusion The education program is essential in increasing the number of fast recognitions, which influences the surviving ratio.\n\nIntroduction As one of the few treatment interventions to demonstrate mortality efficacy at a randomized controlled trial level [1] , the prescription of drotrecogin alfa-activated (DrotAA) (Xigris\u2122), where appropriate, plays an important role in the management of severe sepsis. However, concerns regarding the potential for serious bleeding events have helped sustain a degree of scepticism regarding the use of DrotAA [2] . As early adopters of evidence-based medicine, Cardiff Critical Care Unit has prescribed DrotAA since late 2002 and has considerable experience with S23 respect to its use. The aim of this study was to demonstrate the safety profile and efficacy of DrotAA treatment within a large, 29bed university hospital critical care unit. Methods Demographic data were obtained from the unit's daily updated Riyadh ICU programme database and clinical data were collected from patients' medical notes and observation charts. All data were prospectively entered into our DrotAA registry, the results of which are shown below. Results Between October 2002 and November 2005, 133 patients with severe sepsis were treated with DrotAA. The mean age was 61 years (range: 20-87 years) and 54% were male. The mean admission APACHE II score was 22 (range: 11-48), and on day 1 of DrotAA infusion the median number of organs that failed was 2.0 (range: 0-4), 129/133 (97%) were mechanically ventilated and 131/133 (98.5%) were on vasopressors. The median time to start DrotAA after documented diagnosis of severe sepsis was 12.6 hours (range: 0-41 hours) and the median duration of DrotAA infusion was 89.5 hours (range: 10-105 hours). The incidence of serious (life-threatening) bleeding events was 2.3% (n = 3): gastrointestinal (n = 1), intraabdominal (n = 1) and intrathoracic (n = 1); all were nonfatal and there were no intracranial bleeds. The 28-day mortality was 31.6%, the ICU mortality was 33.1%, the hospital mortality was 36.8% and the 1-year mortality was 47%.\n\nConclusions This is one of the largest UK registries of DrotAA usage published to date. Our results demonstrate a very low incidence of serious bleeding events associated with DrotAA treatment (2.3% vs 3.5% in PROWESS); it is interesting to note that all three adverse events occurred prior to 2004. This detail, combined with our low median time to start DrotAA infusion (which has steadily decreased over the past 4 years), would suggest the presence of a learning curve for DrotAA usage on ICUs. It is also encouraging to note that our overall hospital mortality was lower than the predicted APACHE II hospital mortality for these patients (36.8% vs 42.4%). Finally, this is one of the first UK studies to describe long-term mortality outcome in patients receiving DrotAA therapy. Further studies are required to more formally assess the impact of DrotAA treatment on long-term survival from severe sepsis. \n\nAudit of adherence to National Institute of Clinical Excellence guidelines for the use of drotrecogin alfa (activated)\n\nIntroduction Activated protein C (APC) is an endogenous protein, which has fibrinolytic and anti-inflammatory properties. This is available as human recombinant APC and is used in the treatment of patients with severe sepsis [1] . The National Institute of Clinical Excellence (NICE) suggested guidelines for the use of APC [2] . We retrospectively audited the records of patients who received APC during their admission to our ICU between January 2003 and August 2006. We audited our practice against three parameters: compliance with the NICE guidelines, accuracy of data forms, and outcomes of treatment.\n\nFrom January 2003 to August 2006 we used APC to treat 44 severely septic patients in our ICU. We obtained complete data for 37 patients. We collected data from the case notes, ICU charts and drotrecogin alfa (activated) data forms and recorded relevant data on an Excel spreadsheet proforma.\n\nResults NICE guidelines. We were 100% compliant with patient selection criteria for APC administration, which included a known or suspected site of infection, SIRS criteria and organ dysfunction criteria. All prescriptions were made by intensive care consultants. We were not fully compliant in excluding patients who met exclusion criteria (2/37 patients), although these cases were justified clinically by the consultants prior to administration. Data entry. In 90% of cases the patient selection fields were completed, but only 30% of the exclusion and outcome fields were completed. In 30% of patients where a lactate \u22651.5 times normal was listed as one of the inclusion criteria, it was not associated with a pH \u2264 7.30 or a base deficit \u22655.0; however, all these patients had \u22653 organ-dysfunction criteria and hence still met the inclusion criteria.\n\nOutcomes. Seven patients (15.9%) died during or within 28 days of APC administration. The standardised mortality ratio (SMR) was lower in patients receiving APC when compared with the rest of patients admitted over the same period (SMR ~0.5 vs ~1.0). Twenty-eight patients had an APACHE II score <25 and the effective cost per survivor was ~\u20ac16,800. Patients with APACHE II scores \u226525 had an effective cost per survivor of ~\u20ac22,400. Nine patients (20.5%) had their drotrecogin alfa (activated) infusions interrupted or discontinued for various reasons (including seven patients who had hemorrhagic complications, three of which were serious).\n\nWe use APC in compliance with the NICE guidelines. APC is cost-effective in patients with an S24 59.4 years. Primary sources of infection were: intra-abdominal 36.2%, respiratory 27.6%, genitourinary 8.6%, and 27.6% from other sources. GRH ICU mortality was 44.8% and hospital mortality was 51.7%. Analysis by age revealed overall survival rates of 78.6% for patients \u226450 years, 54.5% for 51-60 years, 52.9% for 61-70 years, 20% for 71-80 years, and 0% for patients >80 years of age. Hemorrhagic complication rates were higher than in published reports. Of 58 treatments, we recorded a total of nine hemorrhages (15.5%). The mortality rate in this cohort was 33.3%. Conclusions These data suggest that 'field performance' of DAA may not be replicating the favorable clinical endpoints as reported in PROWESS. The Ontario Ministry of Health should consider implementing a provincial registry system for patients with severe sepsis and septic shock, empowering ICUs to track relevant demographic, acuity, and outcome data with a view to optimizing DAA use through patient selection and risk stratification.\n\nIntroduction and objective To establish whether activated protein C (APC) is safe in surgical patients with intra-abdominal sepsis (IAS). APC has been used in the treatment of IAS in our hospital since 2003. Fears persist regarding the potential for clinically significant bleeding in this surgical subgroup of patients. Methods Forty-four patients with IAS received APC as a standardized regime between March 2003 and August 2006. A retrospective medical and ICU chart review was undertaken. Data collected included clinically significant bleeding episodes and mortality. Descriptive subgroup analysis of unexpected nonsurvivors(died in the ICU with APACHE II (APII) predicted mortality < 50%) and unexpected survivors (survived to ICU discharge with APII predicted mortality > 50%) was performed as statistical analysis of such small patient numbers was inappropriate. Results There was one episode of clinically significant bleeding (from a mucous fistula: self-limiting). There were no intracranial haemorrhagic events. ICU mortality was 38.6% with mean APII predicted mortality of 37.16% and inhospital mortality of 47.7%. These exceeded rates for APC-treated surgical cohorts in the literature [1] . Unexpected survivors (5/44) were more likely to have been admitted from theatre. They had a shorter mean time from hospital-ICU admission (10.5 vs 5.6 days), duration on a ventilator (10.8 vs 17.5 days), vasopressor (9 vs 17.7 days) and renal replacement therapy (10.5 vs 23.5 days) dependence. All unexpected nonsurvivors (11/44) had a diagnosis of fistula or perforation. They were more likely to have been transferred to the ICU from another hospital or ward than from theatre. Comorbidities were more severe. Conclusion 1. APC was very safe to use in this group of critically ill surgical patients. 2. Although patients may fulfil standard criteria for APC use, if there is no definitive surgical cure for the IAS, then APC is inappropriate. 3 . Delay in commencement of APC in surgical patients due to bleeding concerns may be contributing to the high mortality. Earlier perioperative use of APC in selected cases may offer improved mortality benefit, and we are undertaking a prospective audit to investigate this. Results A total of 444 patients were admitted to the ICU with the diagnosis of sepsis or severe sepsis. One hundred and forty-nine severe septic patients were assessed for APC: 85 patients received APC, and 64 patients could not receive APC due to financial problems or due to bleeding, coagulation derangement or very recent surgeries. In the total 444 septic patients admitted to the ICU, 152 patients expired (mortality 34.2%) and 141 had positive blood culture; 233 patients received inotrops. The total average APACHE II score was 28.9 and for expired patients was 35.1.\n\nOut of the total 444 septic patients 149 were assessed for APC; in the 85 patients fulfilling criteria for and receiving APC the mortality was 43.5%, and for the 64 patients not receiving APC the mortality was 64%. All suspected septic patients admitted to the ICU received appropriate antibiotic therapy within 4 hours of ICU admission and were upgraded/changed according to culture/ sensitivity reports if necessary. In the nonreceiving group (i.e. 64 patients) 12 patients could not receive APC due to financial restriction because initially foreigners were not entitled to this drug in Bahrain, but later this restriction was removed, and the remaining 52 patients could not receive either due to bleeding or very recent surgeries. Some patients could not receive complete treatment either due to bleeding complications or because they died. The mortality was measured at 28 days. Furthermore, as per our experience, if APC started in the early stage of sepsis and the course is completed the outcome is better -out of 85 patients who received APC, 45 patients received in the early stage and completed the dose and 32 of these patients survived at 28-day mortality. An average three (ventilator-free) organs failed in the survival group and two (ventilator-free) organs failed in the expired group. Seventeen patients started treatment in the early stage and could not complete the course due to bleeding or other complications, 11 patients expired; 13 patients started in the late stage and completed the course, five patients expiring; and 10 patients started in the late stage and eight of these patients expired.\n\nConclusion On the basis of our experience and the results of multiple trials, we recommend APC should be given to the patients who meet all the inclusion criteria. \n\nsevere sepsis was 54.1%. Hospital mortality and 28-day mortality of severe sepsis were 59.3% and 57.6%, respectively. The standardized mortality ratio of severe sepsis patients was 1.40. The median duration of stay in ITUs of the severe sepsis cohort who survived was 6 days (IQR 3-12). The number of episodes where infection was the primary reason for admission to the ITU was 89.8% and the rest of episodes were ITU acquired. See Figure 1 for infection characteristics.\n\nConclusion Sepsis was common in Indian ITUs and had predominant medical populations. ITU mortality was higher compared with western literature. Gram-positive infections were less common although the incidence of parasitic and viral infections were higher than in the West. S27 unit (NCCU) is at the moment unknown. It is known that being a patient in the intensive care environment is in itself a risk factor for the development of bacteraemia (3.2-4.1 per 100 admissions in several papers). The higher amount of invasive procedures and the severity of illness in this group of patients have been blamed. The aims of our study are: (1) to identify the incidence of bacteraemia in the NCCU, (2) to recognise the incidence of bloodstream infection (SIRS with bacteraemia), (3) to identify the most common pathogens associated with bacteraemia, and (4) to promote the continuous collection of data aiming to follow the behaviour of this problem in time.\n\nMethods This is a prospective observational study looking at the presence of positive blood cultures in all the patients admitted to the NCCU during the period from 1 June to 31 August 2006. Blood cultures were taken from a peripheral site under aseptic conditions as per the NCCU guidelines. We tried to identify how many of the patients with positive blood cultures had evidence of concomitant SIRS/sepsis, as described by the modified Bone criteria, and the severity of this. An attempt was made to identify the most frequent microorganisms involved in this problem as well as their antibiotic susceptibility. As a secondary aim of our study we described the number of fatalities in the patients with bacteraemia. We tried to focus our approach to the fact that we serve a large neurological/surgical population as well as general patients and to see whether we could pinpoint differences in these two groups. Results There were 201 patients admitted to the unit during the period of our study; 140 of these were neurosciences (NS) patients and the rest (61) were general (G) (either medical or surgical). Most of the patients were men and had a mean APACHE II score of 39 (NS group 33, G group 45).There were in total 64 episodes of positive blood cultures (BC); 39 of these episodes were accompanied by inflammatory signs (incidence of bloodstream infections of 19.4% of total admissions). Twenty-five of the episodes were not associated to clinical signs of infection. There were more patients with at least one episode of positive BC in the NS group (29 (20.7%)) than in the G group (10 (16.39%)). Out of 49 episodes in the NS group, 59.18% (29) were associated to some degree of inflammatory response (SIRS, severe sepsis, and MODS). Out of 15 episodes in the G group, 66.6% (10) developed inflammatory response. In 59% (25) of the positive BCs, the organism isolated was coagulase-negative staphylococcus (CNS). In the G group, 47% (7) Due to the specialist origin of our unit, we had more cases in the neurosciences group than in the general group. However, the incidence of sepsis and MOF in these patients was almost the same for both groups. We noted, as well, a larger number of deaths in the patients with sepsis and MOF. There needs to be more studies aiming to establish a casual relationship to explain this. CNS was the most frequently isolated organism and there was no difference among the groups. There is a potential for increased mortality in the patients that develop bloodstream infections in our unit, and we need to implement urgent measures to decrease them while further research is done in this area.\n\nThe early recognition and rapid start of goal-directed treatment (EGDT) are important elements for better outcome in severe sepsis. These actions should take place in the emergency department (ED) before admission to the ICU. The aim of our study was to determine how the EGDT was performed and to evaluate the impact of EGDT principles on mortality in septic shock in Finland. Our study was conducted before national guidelines for severe sepsis were published. Intellivue patient monitoring. Data were collected to compare the use of bedside monitor displays with and without horizon screen trends in the care of patients with sepsis. Group 1 (n = 37) completed the sepsis scenario using a standard screen display, and group 2 (n = 38) had the addition of horizon trends on the display.\n\nThe point that marked the onset of sepsis was when each of the physiologic parameters met the current evidence-based screening criteria (HR > 90, RR > 20, MAP < 65, temperature >38\u00b0C). Results of this study found statistically significant differences between the standard screen and horizons screen participant groups in the speed in which clinicians were able to reach each measured outcome. This was true in each of the five outcome measurements: onset of sepsis (P < 0.001), initiation of fluid bolus (P < 0.001), initiation of vasopressor (P < 0.001), blood culture order (P = 0.012), and antibiotic administration (P = 0.020 Background Direct microbiological input to critical care is essential for the management of the septic patient. Early broadspectrum antimicrobial therapy with appropriate diagnostic studies to ascertain causative organisms is well established; there should be reassessment with the aim of using narrow-spectrum antibiotics to prevent the development of antimicrobial resistance, to reduce toxicity and to reduce costs [1] . In systematic analysis of ward rounds in ICUs the information most commonly missing from a patient's file concerned microbiology findings [2] . . When asked to rate the value of this ward round, the mean score was 8.6 out of a possible 10 (range 10-5, mode 9). In those units without a microbiology ward round the desirability of such a service was scored on average at 8.5 out of 10 (range 10-3, mode 9). Conclusion Direct microbiological advice at the bedside is highly valued by ICU consultants. Antibiotic prescribing is generally well controlled, with two-thirds of units having an agreed antibiotic policy in place. Work will continue to determine whether these results reflect the national picture in the United Kingdom. \n\nImpact of a selective digestive decontamination and nasal mupirocin on the incidence of ventilatory-associated pneumonia and the emergence of bacterial resistance Introduction Selective digestive decontamination (SDD) can reduce the incidence of ventilatory-associated pneumonia (VAP). Some concerns have been raised about the risk of selection of resistant bacteria. We evaluated the impact of a SDD regimen on the incidence of VAP and the development of resistant pathogens. Methods In a polyvalent eight-bed ICU, a retrospective analysis was performed of two periods of 8 months before (no-SDD, 178 patients, mean SAPS II 44.8) and after (SDD, 110 patients, mean SAPS II 48.9) the use of SDD with amphotericin, tobramycin and colistin for oropharyngeal and gastric decontamination and mupirocin for nasal decontamination. The results were analyzed with the chi-square test. Results The incidence of VAP was reduced in the SDD group, even though it was not statistically significant (26.9% vs 16.3%, P = 0.138). The mortality of VAP and septic shock was reduced respectively from 39.6% to 16.7% (P = 0.312) and from 60% to 37.5% (P = 0.835). During the SDD period, Gram-positive infections increased while Gram-negative infections and Candida infections showed a reduction. The percentage of resistant species showed a reduction from 49.1% to 30.5% in all the categories of pathogens (Table 1) . Results Ninety-three CT critical care patients received a tunnelled subclavian CVC. The indications were inotropes (n = 40 (43%)), antibiotic administration (n = 27 (29%)), RRT (n = 14 (15.1%)) and unknown (n = 10 (10.8%)). The mean duration of the catheter remaining in situ was 36 days (SD 44.0, range 1-164). Culture results are presented in Table 1 . Twelve patients had an established CVC-related BSI. The mean infection rate/1,000 catheter-days was 3.6. Results Of 120 catheters inserted, 100 could be evaluated for colonization and CR-BSI. Forty-nine in the uncoated group and 51 in the coated group. Clinical characteristics of patients and risk for infection were similar in the two groups, use of propofol was more frequent in the uncoated group and the presence of a vascular device, other than the study catheter, was more frequent in the antibiotic-coated group. Three RM-coated catheters (5.9%) were colonized compared with nine (18.4%) control catheters (relative risk, 0,28; 95% confidence interval, 0.07-1.096; P = 0.05). Three cases of CR-BSI (5.9%) occurred in patients who received RM catheters compared with five in the control group (10.2%). There was no significant differences in the incidence of CR-BSI between RM-coated and uncoated catheters. Uncoated catheters were more frequently colonized but this difference just failed to show statistical significance. When the duration of catheter placement were taken into consideration, Kaplan-Meier analysis showed no significant differences in the risk of colonization or CR-BSI between RM-coated and uncoated catheters. Rates of CR-BSI were seven per 1,000 catheter-days in the RM-coated group compared with 11.4 per 1,000 catheter-days in the uncoated group (P = 0.7). Gram-positive and Gram-negative organisms were similarly responsible for colonizing catheters in our study; there was no difference in rates of colonization by Candida species. Conclusion In this pilot study, we showed a trend toward lower rates of colonization in RM-coated catheters when compared with uncoated control catheters. The incidence and rates of CR-BSI were similar in the two groups, probably because of a small number of catheters studied. Development of a prospective randomized trial with a larger number of patients is underway to confirm or refute these results. \n\nThere are significant differences in antibiotic prescribing practices when public and private sectors are compared. Appropriate early antibiotic prescriptions reduce mortality. Attention to education and systems that address prescribing practices is indicated.\n\nIntroduction This study aims to assess the association between the timing of admission and outcome in patients admitted with pneumonia to ICUs in the United Kingdom. Methods All patients admitted to an ICU with a primary reason for admission of pneumonia were extracted from the Case Mix Programme Database. 'Early' admissions, admitted to the ICU on the day of admission to hospital (12,475), were compared with 'late' admissions, admitted to the ICU on a later date (21,948). The ICU and hospital mortality, number of organs failed, renal dysfunction, and length of stay in hospital were compared between the two groups. An association was sought between timing of admission and mortality. Patients were stratified by CURB 65 score on admission to the ICU. Mortality was compared between the two groups. Odds ratios were used to analyse data. P < 0.05 was considered significant.\n\nResults There were small but statistically significant differences between the two groups in mean age, APACHE II score, CURB 65 score and number of organ failures, and the presence of Introduction Patients who develop ventilator-associated pneumonia (VAP) caused by either multidrug-resistant organisms (MDRO) or Pseudomonas may have poor clinical outcomes. We sought to further clarify this potential relationship using a database from a large multicenter trial of diagnostic and therapeutic strategies in patients who had suspected VAP. Methods Patients receiving mechanical ventilation (MV) for \u226596 hours and who developed suspected VAP (new or worsening pulmonary opacities on CXR, and at least two of fever, leukocytosis, change in sputum purulence, increased O 2 needs, or isolation of potentially pathogenic bacteria from sputum) were eligible. At enrolment, all patients had cultures obtained from either BAL or endotracheal aspirates. MDRO were defined as those resistant to \u22652 classes of antibiotics. Patients were followed until 28 days after enrolment, death, or hospital discharge. Results Seven hundred and thirty-nine patients from 28 ICUs in Canada and USA were enrolled. At enrolment, cultures from 10.0% (95% CI 7.9-12.4%) of the patients grew MDRO or Pseudomonas. The prevalence of MDRO at enrolment was 5.2% (3.6-6.8%). There were no differences in APACHE II, MODS, or PaO 2 /FiO 2 at baseline between those whose specimens grew MDRO or Pseudomonas and those whose specimens did not. Patients with MDRO or Pseudomonas had higher 28-day mortality (RR 1.59, 95% CI 1.07-2.37, P = 0.04) and inhospital mortality (RR 1.48, 95% CI 1.05-2.07, P = 0.05) and a trend towards higher ICU mortality (RR 1.42, 95% CI 0.90-2.23, P = 0.14) than those whose specimens did not grow these organisms. Median duration of MV (12.6 vs 8.7 days), ICU length of stay (16.2 vs 12.0 days) and hospital length of stay (55.0 vs 41.8 days) was greater in patients with MDRO or Pseudomonas than in those whose specimens did not grow these pathogens (P = 0.05). Adequacy of initial empiric therapy was 68.5% in patients whose specimens grew MDRO or Pseudomonas compared with 93.9% in those without these organisms (P < 0.001). Conclusion The isolation of MDRO or Pseudomonas from respiratory tract specimens of patients with suspected VAP is associated with prolonged MV, increased ICU and hospital stay, and increased risk of death. Inadequate initial empiric antibiotic treatment may be a contributing factor.\n\nIntroduction Cytokines play an important role in pulmonary host defense. However, nonuniform findings have been reported about the correlation between bronchoalveolar bacterial burden and the lung inflammatory response. Objective The aim of the present study was to evaluate the relationship between bronchoalveolar cytokine expression and bacterial burden in mechanically ventilated patients with suspected pneumonia. Methods Mechanically ventilated patients with suspected pneumonia admitted to the ICU from November 2004 to January 2006 were prospectively enrolled. Fiberoptic bronchoalveolar lavage (BAL) was performed with 150 ml sterile isotonic saline in three aliquots of 50 ml; local anesthetic was not used. BAL samples for microbiologic quantitative cultures and BAL cytokines -IL-6, IL 8, TNF\u03b1, granulocyte colony-stimulating factor (G-CSF) and granulocyte-monocyte colony-stimulating factor (GM-CSF)were measured. Results Fifty-nine patients were included, and most of the patients (79.7%) had prior antibiotic therapy. Twenty-two patients (37.2%) had a positive bacterial culture defined as a diagnostic threshold >10,000 colony-forming units/ml. Only the concentration of TNF\u03b1 was significantly higher in the group of patients with positive BAL (Table 1) . Conclusions (1) There is a significant correlation between TNF\u03b1 in BAL fluid and the lung bacterial burden. (2) BAL TNF\u03b1 is an early marker of pneumonia in mechanical ventilated patients despite prior antibiotic therapy. Clinical implication Cytokine measurements in BAL may be a diagnostic tool to support the diagnosis of the initial phase of pneumonia. Excluding fungi and unspecified isolates, we had 8/45 multisensitive isolates and 32/45 isolates sensitive to colistin (32), meropenem (26) and gentamicin (21). According to these data in early and late VAP the most adequate therapeutic combination to cover possible pathogens is meropenem + colistin. Using this combination we cover all possible pathogens and then de-escalate according to susceptibility results. Following the ATS/IDSA guidelines we would cover only 8/45 isolates. Conclusions ATS/IDSA [1] guidelines may not be applicable in all institutions or countries and thus clinicians should incorporate local microbiologic data into institution-specific guidelines [2] .\n\nThe goal of the study was to evaluate the clinical efficacy of meropenem by continuous infusion administration (CIA) or by bolus intermittent infusion (BII) for the treatment of VAP caused by Pseudomonas aeruginosa. An historic control group with VAP caused by P. aeruginosa who received initial empiric antibiotic therapy with meropenem by BII (n = 32) was compared with a prospective cohort treated with meropenem by CIA (n = 20) in a 12-bed surgical ICU, at a 400-bed surgical complex of a district hospital. We looked for demography, APACHE II score, mortality, attributable mortality for VAP, days on mechanical ventilation (MV), and ICU length of stay. VAP was treated during 14 days with meropenem (1 g/6 hours intravenously). The antibiotic clinical effect was categorized as cure or failure. Difference between groups were tested by means of Student's t test end exact chi-square test, using the MedCalc program. We consider values of P < 0.05 as a significant difference. Results Significant differences were not found between both groups of patients in sex, age, APACHE II score, and diagnosis. The CIA group showed significantly greater clinical cure than the BII group (CIA 18/20 (90%) vs BII 21/32 965.6%), P = 0.041) and smaller but not significant attributable mortality to VAP (2 of 20 (10%) vs 10 of 32 (31.3%), P = 0.288). Conclusion Our results suggest that administration of meropenem by CIA may have more clinical efficacy than administration by BII for the treatment of VAP, but more studies are required to confirm this.\n\nA randomized trial of combination therapy versus monotherapy for the empiric treatment of suspected ventilator-associated pneumonia Introduction Delays in adequate antibiotic therapy for ventilatorassociated pneumonia (VAP) are associated with poor outcomes, and early use of broad-spectrum antibiotics may improve clinical outcomes. However, indiscriminant use of broad-spectrum antibiotics is associated with the emergence of antibiotic-resistant bacteria, fungal infections, and increased healthcare costs. The purpose of this study was to determine optimal empiric treatment of VAP by comparing a strategy of combination therapy to monotherapy with broad-spectrum antibiotics. Methods In a multicenter trial, we randomized mechanically ventilated adult patients with suspected VAP that developed after 96 hours in the ICU to receive either meropenem and ciprofloxacin or meropenem alone, as initial therapy. In addition, before starting antibiotics, diagnostic specimens were obtained using either bronchoalveolar lavage with quantitative cultures or standard endotracheal aspirates. Results We randomized 740 patients in 28 ICUs in Canada and the United States. The baseline characteristics and etiologies of VAP were similar between groups. There was no difference in 28day mortality between the combination and monotherapy groups (RR = 1.05, 95% confidence interval 0.78-1.42; P = 0.74). The duration of ICU and hospital stay, clinical and microbiological response to treatment, emergence of antibiotic-resistant bacteria, isolation of Clostridium difficile, and fungal colonization were similar between groups. Combination therapy resulted in a higher rate of adequate empiric therapy compared with monotherapy (93.1% vs 85.3%, P = 0.01). In a subgroup of patients with infection due to pseudomonas species, acinetobacter species and multidrug-resistant Gram-negative bacilli at enrollment (n = 56), the adequacy of initial antibiotics was 82.4% in the combination group versus 18.8% in the monotherapy group (P < 0.001); this difference was associated with an increase in the microbiological eradication of the infecting organisms (64.1% vs 29.4%, P = 0.05) but no differences in clinical outcomes. Conclusion In patients who have suspected VAP, empiric treatment with combination therapy, as compared with monotherapy, is safe and is associated with a higher rate of adequate antimicrobial coverage but has no effect on clinical outcomes.\n\nIntroduction This prospective pilot study set out to develop an animal model of Pseudomonas aeruginosa that would be suitable for the application of molecular techniques to evaluate virulence in which instillation of a reference strain of P. aeruginosa results in a monoculture ventilator-associated pneumonia. For this purpose, male adult greyhounds were used in an animal research laboratory. Methods The animals were anaesthetised, orally intubated and mechanically ventilated. An inoculum of P. aeruginosa (strain PA01) was instilled into the oropharynx at 1 hour and 8 hours postintubation. The animals were terminated at 78 hours. Results Pneumonia was evaluated based on macroscopic grading and microbiological (bacterial count) findings. We were able to maintain anaesthetic, haemodynamic and respiratory support for the study duration of 78 hours. A monobacterial pulmonary infection was established in four out of five animals. Administration of ceftriaxone 1 g daily effectively suppressed all other bacteria. This allowed proliferation of the single strain P. aeruginosa (PA01) we had inoculated with no culture of other organisms. Conclusions Over a short period of time we were able to reproduce a monoculture ventilator-associated pneumonia in a significant percentage of animals. We successfully developed an animal ICU model that we were able to sustain for 78 hours. This canine model of P. aeruginosa (PA01) ventilator-associated pneumonia is suitable for the application of molecular techniques such as signature-tagged mutagenesis, differential fluorescence induction, and in vivo expression technology.\n\nDecrease in intravenous antibiotic use with adjunctive aerosolized amikacin treatment in intubated mechanically ventilated patients with Gram-negative pneumonia S39 safety and i.v. antibiotic use with inhaled amikacin (AMK) during adjunctive treatment of intubated patients with Gram-negative pneumonia. Methods A double-blind, placebo-controlled, study of aerosol AMK delivered via the Pulmonary Drug Delivery System (PDDS \u00ae ; Nektar Therapeutics) in ventilated patients with Gram-negative pneumonia as an adjunctive to i.v. therapy per ATS guidelines. Patients were randomized to receive aerosol containing 400 mg AMK daily with placebo (normal saline) 12 hours later, 400 mg AMK twice daily or placebo twice daily. The i.v. antibiotics (agent and duration) were determined by the attending physician. The AMK peak serum concentration, trough concentrations and tracheal aspirates were drawn. Results The mean number of i.v. antibiotics at the end of the study (mean 7 days) were two times greater with placebo than with twice-daily AMK (P < 0.02) ( Figure 1 ). For daily and twice-daily AMK, the serum C max were 1.3 and 1.8 \u00b5g/ml (respectively) on day 1, and 2.3 and 3.2 \u00b5g/ml on day 3. Mean trough levels were 0.87 and 1.49 \u00b5g/ml. Tracheal aspirate levels (mean) on day 3 were 6.9 mg/ml (daily) and 16.2 mg/ml (twice daily). Aerosol AMK was well tolerated with no difference in adverse events across treatment groups. Conclusion Repeated doses of adjunctive inhaled AMK to mechanically ventilated patients with Gram-negative pneumonia was safe, well tolerated, and associated with less i.v. antibiotic use than placebo. Despite isolation, MRAB spread over and infected eight more patients in separate rooms and different sections of the ICU 32 days later. Further transmission occurred within a few days: three male patients with multiple trauma (42, 20, and 62 years old; patients 2, 3, and 4), cardia carcinoma (female, 66 years old; patient 5), necrotizing pancreatitis (female, 78 years old; patient 6), splenomegaly owing to polycythaemia vera (male, 74 years old; patient 7 -MRAB diagnosis postmortem), rectal carcinoma (female, 76 years old; patient 8 -isolation because of MRSA infection even before) and respiratory failure after gastric banding (female, 41 years; patient 9). All patients suffered from septic shock with high fever, needed high volume replacement and catecholamines several times and prolonged mechanical ventilation. MRAB was isolated in the tracheal secretion or BAL in all patients, in abdominal drainage (patient 6), and in central venous catheter (patient 5). Environmental investigations showed no problematic circumstances. Colistin i.v. is not available in Germany so it had to be procured from the USA, which caused a delay of treatment for a few days. Another delay occurred because of the rapid growing number of patients who needed Colistin. Patients were treated with an adjusted dosage for 16 days. All patients of the ICU were isolated to avoid new infections as a precaution. After convalescence of two patients, all MRAB patients were moved to the IMC, which was converted to an ICU for this period, to isolate infected patients from uninfected. Three out of nine patients died. All these laborious measures with a great expenditure of logistics worked well; no further transmissions were observed. Objectives Gram-negative bacilli including multidrug-resistant Acinetobacter baumannii (MDR-AB) are responsible for severe ICU-acquired infections, mainly pneumonia and bacteraemia. The aim of this study was to determine the incidence and mortality of this multiresistant strain of Acinetobacter in patients undergoing cardiac surgery, to elucidate the effectiveness of treatment with colistin and to identify whether additional measures were able to prevent and control the dissemination of MDR-AB isolates in our institution. Methods A total of 1,451 patients attended the surgical ICU (SICU) after cardiovascular surgery from 1 September 2005 to 31 August 2006. We reviewed the prophylactic measures of the SICU and tried to identify epidemiological links between MDR-ABinfected patients. We implemented a two-scale multiple program. Scale 1 included classical infection control measures (that is, strict contact and droplet isolation, surveillance of throat, nasal and anal flora for MDR pathogens on all patients transferred from other hospitals, separate nursing staff for each infected or colonized case and strict antibiotic policy), while Scale 2 referred to geographic isolation of MDR-AB cases with exclusive medical and nursing personnel, use of separate supplies and facilities and intense environmental surveillance. Results Fifteen patients were infected by MDR-AB, of which 13 presented respiratory tract infection, one suffered deep surgical site infection and bacteraemia and one from catheter-related infection. They were all treated with intravenous and aerolized colistin in combination with rifampicin or ampicillin and sulbactam.\n\nAvailable online http://ccforum.com/supplements/11/S2\n\nS40 Despite significant 'in vitro' activity of colistin against this virulent organism and its acceptable safety profile, results were discouraging as only 13% survived. In fact, cure or clinical improvement was observed only in four patients (27%) while 11 patients (73%) developed sepsis and multiple organ failure. Scale 1 measures were implemented for the whole 12-month period while Scale 2 for two separate 3-week periods. Following this infection control strategy we achieved intermittent eradication of the pathogen during a 12-month period with continuous function of the SICU. Conclusions Increasing prevalence of MDR-AB in ICU patients demands installation of strict screening and contact precautions. Due to significant mortality of MDR-AB-infected patients, additional measurements like geographic isolation of all positive cases, exclusive medical and nursing personnel, use of separate supplies and facilities and intense environmental surveillance is highly recommended.\n\nIntroduction Tigecycline (Wyeth) is a new glycylcycline antimicrobial that has been used in the treatment of deep-seated multidrug-resistant Acinetobacter (MDRA) infections. Unexpected changes in routine hematology or serum chemistry have not been reported. Methods All patients were managed within the liver ICU and received standard care. Laboratory data were collected daily and entered onto a specialist database. MDRA-positive cultures from blood, bronchoalveolar lavage, drain fluid or samples taken at laparotomy in the context of systemic inflammatory response syndrome resulted in the initiation of tigecycline 100 mg i.v. followed by 50 mg i.v. 12 hourly. Results Eleven patients received tigecycline treatment for MDRA infections (seven male). Ten patients had a single course whilst one patient had three courses. Underlying disease states were necrotising pancreatitis (one), polytrauma (one), post hepatectomy (one), acute and acute on chronic liver failure (four), and postorthotopic liver transplant (four). The median duration of treatment was 9 days (range 4-23 days); courses <7 days were because of patient death (2/11). The mean APACHE II score at initiation of therapy was 18 (range 13-26). Four out of 11 survived to ICU discharge and 3/11 to hospital discharge. Tigecycline was well tolerated but increases in corrected calcium were observed in 9/11 patients. The patient that received three courses of treatment had elevations in corrected calcium after each course. For the 11 patients, the mean corrected calcium before treatment with tigecycline was 2.41 mmol/l. The mean corrected calcium on finishing the course increased to 2.59 mmol/l (P = 0.012). There was no correlation between duration of treatment with tigecycline and degree of change in the corrected calcium level (r = 0.08). Hypercalcaemia resolved on discontinuation of the drug; 7/11 survived >7 days after treatment and had a mean corrected calcium of 2.46 mmol/l, which was not significantly different from pretreatment levels (P = 0.94). Conclusion Tigecycline is well tolerated but appears to be associated with an elevated corrected calcium in critically ill patients. This returns to baseline values on discontinuation of the drug. Introduction The objective of this study was to quantify the impact of continuous venovenous haemodiafiltration (CVVHDF) on aminoglycoside pharmacokinetics and to suggest dosing strategies to improve therapeutic outcomes for these drugs in critically ill patients treated with CVVHDF. There has been limited published data on aminoglycoside pharmacokinetics during CRRT. This data deficit had led to subtherapeutic dosing, identified by a retrospective evaluation of amikacin and gentamicin serum concentrations, in patients treated with CVVHDF, undertaken as part of this research. Methods A prospective pharmacokinetic evaluation of aminoglycoside pharmacokinetics during CVVHDF was undertaken. Pharmacokinetic profiles of once-daily doses of intravenous amikacin and gentamicin were obtained from blood and dialysate/ ultrafiltrate samples for 12 critically ill patients treated with CVVHDF using varying flow rates (1 l/hour dialysate plus 2 l/hour filtration fluid or 2 l/hour dialysate plus 2 l/hour filtration fluid, extracorporeal blood flow 200 ml/min). Drug concentrations were measured using an immunoassay. Results The mean clearance of gentamicin due to CVVHDF was 2.3 \u00b1 0.3 l/hour (82.1 \u00b1 11.3% of total body clearance (TBC)). The sieving coefficient (SC) was 0.85 \u00b1 0.05. The CVVHDF clearance of amikacin was 2.8 \u00b1 0.5 l/hour (93.0 \u00b1 7.8% TBC). The SC for amikacin was 0.88 \u00b1 0.06. The difference in gentamicin clearance versus amikacin clearance reflects differences in CVVHDF conditions. The mean effluent flow rate among the patient sample treated with gentamicin was 2.7 l/hour compared with 3.5 l/hour for amikacin. There was a strong correlation between creatinine clearance by the filter and measured drug clearance (P < 0.001). Individual patient estimates of aminoglycoside pharmacokinetic parameters (k, Vd) obtained during CVVHDF were used to allow appropriate dosage adjustment. Individualized pharmacokinetic-pharmacodynamic goals (e.g. Cpmax/MIC ratio) were used as indicators of adequate aminoglycoside dosing. The mean gentamicin and amikacin halflives (approximately 8 hours) during CVVHDF therapy were far shorter than those previously reported in the literature for less efficient forms of renal replacement therapy. Failure to adjust for increased aminoglycoside clearance capacity due to CVVHDF carries a risk of subtherapeutic dosing and therapy failure. Conclusion Dosing strategies on the basis of pharmacokinetic analysis of serum drug concentrations, effluent fluid drug concentrations and CVVHDF conditions improved therapeutic outcomes for aminoglycoside drug therapy.\n\ninfluenza A/H5N1 develop acute renal failure. A proportion will require haemofiltration. There are no data to determine the elimination of oseltamivir carboxylate (the active metabolite) by haemofiltration. An in vitro study to determine elimination by measuring the adsorption and sieving coefficient of oseltamivir carboxylate using two haemofilter types was undertaken. Methods An in vitro one-compartment model of continuous venovenous haemofiltration was used. In phase 1 oseltamivir carboxylate adsorption to the haemofilter and circuit was studied by circulating a blood-crystalloid mixture containing clinically relevant concentrations of oseltamivir carboxylate through a haemofilter circuit and returning the ultrafiltrate to the mixing chamber. In phase 2 the ultrafiltrate was removed and replaced with a bicarbonate-based fluid to enable calculation of the sieving coefficient. The study was repeated 10 times with two haemofilter types: polyamide and polyacrylonitrile (PAN). Finally, oseltamivir carboxylate was added to the blood-crystalloid mixture without circulation through the circuit to determine its stability in solution. Blood samples collected were assayed by HPLC-MS/MS. Results Oseltamivir carboxylate remained stable in solution (mean percentage change from baseline at 30 min: +3.97%, at 60 min: +1.91%, at 90 min: +2.36%). The mean \u00b1 SD initial oseltamivir carboxylate concentrations for the PAN (346 \u00b1 85 \u00b5g/l) and polyamide (453 \u00b1 185 \u00b5g/l) showed no significant difference. The mean \u00b1 SD adsorption at 90 min was 58.18 \u00b1 17.84 \u00b5g for PAN and 75.22 \u00b1 36.88 \u00b5g for polyamide haemofilters. There was no statistical difference in adsorption between the haemofilters. The initial drug concentration was a significant predictor of adsorption (r 2 = 0.734). The mean \u00b1 SD sieving coefficient of oseltamivir carboxylate for PAN (1.06 \u00b1 0.04) and polyamide (1.03 \u00b1 0.06) haemofilters showed no statistical difference between the haemofilters. Conclusions Total adsorption is low and unlikely to be of clinical significance. Adsorption and the sieving coefficient are independent of the type of haemofilter membrane. The sieving coefficient of oseltamivir carboxylate is 1, therefore clearance during haemofiltration can be estimated from the ultrafiltration rate. \n\nThis was an open-label, multicentre, observational study in patients receiving teicoplanin for suspected or diagnosed Grampositive infection. Data collection included demographics, method of administration, loading and maintenance doses, creatinine and adverse events. Trough and peak concentrations were determined 15 minutes prior to drug administration and 60 minutes after. Serum was separated and stored at -20\u00b0C until analysis. Levels were determined with an Abbott TDx \u00ae /FLx \u00ae analyzer and Seradyn Teicoplanin Innofluor assay kits. Seradyn internal teicoplanin controls were run within and between each batch. Mean trough and peak plasma levels were calculated for 4 days of therapy. Results Seventy-four patients with complete records were analyzed and whilst all patients received an 800 mg loading dose on day 1, 40 received 400 mg twice daily thereafter (BD group) and 34 once daily (OD group), for nosocomial pneumonia (n = 14), skin and soft tissue infection (burn and nonburn including diabetic foot) (n = 13), bacteraemia (n = 10), intra-abdominal infection (n = 8), bone and joint infection (n = 6) and as pre-emptive therapy for severe trauma (n = 13). In the OD group, mean trough levels remained at 9.64 \u00b5g/ml from days 2 to 4 and peak levels remained at a mean of 24.84 \u00b5g/ml. In the BD group, mean trough levels increased by 5.65 \u00b5g/ml/24 hours to 21.8 \u00b5g/ml by day 4; the mean peak level increased by 5.06 \u00b5g/ml/24 hours to 43.89 \u00b5g/ml by day 4. Conclusion Higher trough levels of glycopeptides (15-20 \u00b5g/ml) are targeted to improve efficacy and reduce resistance development. In the OD group the conventional target of 10 \u00b5g/ml was achieved, whilst in the BD arm 20 \u00b5g/ml was exceeded for 60% of the time by day 2 and 100% by day 4. BD dosing is recommended for most patients with severe infections, particularly those that are critically ill. No premature discontinuations or adverse events were reported during the study.\n\nIntroduction The incidence of HIV-infected patients with complicated skin and soft tissue infections has risen. Because of advanced immune suppression, slower responses to antibacterial treatment, and increased risk of bacteraemia relative to noninfected patients, the choice of initial appropriate empiric antibacterial therapy is an important aspect of care for HIV-infected patients. However, in recent years a dramatic increase of the resistance among Staphylococci to all classes of antimicrobial agents, including glycopeptides, has been reported. Patients and methods We studied 146 patients with skin and soft tissue infections co-infected by HIV and 72 noninfected patients with soft tissue infections aged 18-45 years. All of the patients underwent operations aimed at surgical removal of the dead tissues and pus and received different combinations of antibacterial agents. Twenty-three patients after adequate surgery received Linezolid in doses of 600 mg twice a day intravenously during 3-4 days with oral follow-up of 600 mg twice a day. Results The most frequent pathogens are Staphylococci in both groups of patients with soft tissue infections: 56% was noted among the noninfected patients and 61% among the HIV-infected patients. MRSA was identified in 30% of Staphylococci in HIVinfected patients. Among the patients receiving Linezolid, MRSA was identified in nine cases; in two cases vancomycin-intermediate S. aureus strains, and in one case vancomycin-resistant S. aureus strain. In three cases we revealed Staphylococcus bacteraemia, in one case MRSA bacteraemia in patient with retroperitoneal phlegmon. A statistical difference was identified in duration of high temperature, purulence and wound healing in comparison with patients receiving different combinations of antibacterial agents. All patients receiving Linezolid were discharged from the hospital. The length of stay was 17 \u00b1 1.67 days in comparison with patients receiving other antibacterial agents (from 19.52 \u00b1 1.37 to 20.3 \u00b1 1.46 days). The length of stay in hospital among the noninfected patients with soft tissue infection was 9.5 days. Modification of antibacterial treatment was not required in the group of patients Available online http://ccforum.com/supplements/11/S2 S42 receiving Linezolid. No significant laboratory abnormalities and side effects were noted. We did not reveal statistical differences in the platelet count in group of patients receiving Linezolid (5 days after operation 213 \u00b1 26.0/mm 3 ) in comparison with the group receiving other antibacterial agents (256 \u00b1 32/mm 3 ). Thrompocytopenia is characterized to HIV-infected patients, but did not deteriorate in patients receiving Linezolid Conclusion Linezolid in the complex treatment of HIV-infected patients with complicated skin and soft tissue infections may improve the results of therapy and may be used for initial empirical intravenous-to-oral antibacterial therapy.\n\nA clinico-microbiological study of extended spectrum \u03b2 \u03b2-lactamases in the intensive care unit Introduction Extended spectrum \u03b2-lactamase (ESBL) producing organisms are emerging as common nosocomial pathogens in the ICU worldwide. Early detection and prevention of spread is the primary measure to overcome the challenge posed by these difficult to treat ESBL infections. The aim of this study was to find the incidence, risk factors and microbiological and clinical outcome of patients infected with ESBL producing Escherichia coli and Klebsiellae in the ICU of a tertiary care cardiac center in India. Methods A prospective, observational, case-control study of 150 patients was conducted from August 2004 to July 2005. ESBL testing was performed by the phenotypic confirmatory disc diffusion method. Clinical data and risk factors for ESBL acquisition were analysed as well as the antimicrobial therapy, and clinical and microbiological outcomes were studied. Results A high incidence of ESBL producing E. coli and Klebsiellae was observed (85.8%). Meropenem (9.3%) and imipenem (2.8%) resistance in the ESBL producers was seen. On multivariate analysis with logistic regression, a central venous catheter was an independent risk factor for ESBL acquisition (P = 0.01, OR 3.55, 95% CI 1.4-9.02). The median ICU length of stay was 3.5 days and 3 days in the ESBL and non-ESBL groups, respectively. The overall mortality was 13.28% and 13.6% in the two groups, respectively. Microbiological outcomes were similar to clinical outcome, with 83.6% microbiologic success rate among ESBL producers. Conclusion ESBL producing E. coli and Klebsiellae are problematic pathogens in our ICUs. Emergence of carbapenem resistance is of serious concern. Stringent infection control practices such as aseptic insertion and proper handling of central lines within the ICU should be followed by all. Introduction Antibiotic resistance patterns are continually changing; a new problem has been the emergence of Gram-negative bacteria, primarily Escherichia coli and Klebsiellae pneumoniae, producing extended spectrum \u03b2-lactamase enzymes (ESBL). Antibiotic use measures are presumably the most important intervention in preventing their clonal outbreak, and the risk factors for ESBL include intensive antibiotic exposure (especially third-generation cephalosporin monotherapy). The present study was performed to determine the impact of using piperacillin/tazobactum in reducing the acquisition rate of ESBL producing Gram-negative bacteria in the ICU. Methods This open-label, prospective study was carried out in 140 adult patients admitted to the ICU over a period of 9 months, and was divided into two phases. Phase I (pre-intervention phase, 0-3 months): upon admission to the ICU, besides standard investigations, additional rectal swab cultures were taken for detection of ESBL within and after 48 hours of admission, and were repeated every 7 days of the stay in the ICU. Routinely prescribed antibiotics were allowed. Phase II (intervention phase, 4-9 months): this was subdivided into (a) first 3 months (4-6 months): piperacillin/tazobactum was the primary antibiotic used (more than 50% replacement of cephalosporins), and (b) last 3 months (7-9 months): here again, rectal swab cultures were taken and piperacillin/tazobactum was the primary antibiotic used. McNemar's test and Fisher's exact test were used for statistical analysis.\n\nResults Eighty-five patients in phase I and 55 patients in phase II were enrolled. Third-generation cephalosporins were the primary antibiotic in 75.2% of cases in phase I and in 1.8% of cases in phase II (P < 0.001). The incidence of ESBL was 62.3% in phase I and it came down to 34.5% in intervention phase II (P < 0.01). Conclusion Data from this intervention study support the concept that third-generation cephalosporins are of substantial importance in the emergence of ESBL; by decreasing the level of thirdgeneration cephalosporin use and increasing the piperacillin/ tazobactum use, their was a notable reduction in the acquisition rate of ESBL producing E. coli.\n\nThe most important way to prevent infections in the ICU is to respect asepsis during the numerous invasive procedures to which patients are exposed (central venous catheter, urinary catheter, orotracheal tube (OTT), fibrobronchoscopy (FOB), surgical drainages, patients nursing, surgical medications). Objective To determine the incidence of colonization and infection by MRSA in critically ill patients. Methods A prospective study during 30 months of the patients admitted to the ICU for 24 hours or more. Throat swab, tracheal aspirate and urine were taken on admission and twice weekly. The colonization and infection by MRSA were registered. The infections were diagnosed according to CDC criteria. The infections were classified based on throat flora as: primary endogenous (PE) when they were caused by germs that were already colonizing the throat on the ICU admission; secondary endogenous (SE) when they were caused by germs that were not colonizing the throat on the ICU admission but were acquired during the stay in ICU; or exogenous (EX) when they were caused by germs that were not colonizing the throat. The infections were classified based on the onset moment: early onset (EO) were those developed during the first 4 days of the ICU stay; and late onset (LO) were those developed 5 days after ICU admission. Introduction An emergency department (ED) is a major hospital entrance and its case mix consists of patients at high risk of both introducing and acquiring infections. Alerted by the rise of hospitalacquired MRSA infections, the ED of a teaching hospital set up an ED infection control (IC) programme. The programme and its impact are discussed. Methods The campaign consisted of the appliance of a proactive MRSA admission screening protocol, selective contact isolation (quarantine) and improving hand hygiene (HH). The MRSA admission screening strategy took into account past medical history or actual suspicion of MRSA carriage, transfers from other hospitals and long-term care facilities and admission of hospitalised patients to the ED for upgrading of care. According to their critical illness status, some patients were subject to quarantine. Improving HH was achieved by promoting alcoholbased hand disinfection, refraining all health care workers (HCW) from wearing hand jewellery or artificial fingernails, supplying HCW with clip watches and by developing promotional material. Education of HCW regarding principles and techniques of HH was provided by the IC department, supervised by link persons selected among medical, nursing and domestic staff. The number of new hospital-acquired MRSA infections per 1,000 admissions was recorded. Compliance to HH was measured by observation, microbiological analysis of total counts of colony-forming units on fingerprints, and by monitoring the consumption of hand-rub solutions (HH moments per patient-care day). Results A selective MRSA admission screening policy increased the carrier detection rate up to 15%, compared with 1-2% in our preoperative outpatient clinic. The observed compliance to HH increased from 49% to 79% and consumption of hand-rub solution from 6 to 33 l per 1,000 patient-days. The number of HH moments increased from 19 to 47. Total counts of colony-forming units less than 50 improved from 39% of the analyses to 55%. Concomitantly, a decrease in MRSA attack rate from six to one new case per 1,000 patient-days was seen. Conclusion An ED tailored selective MRSA screening and contact isolation protocol and a change in HH behaviour in the ED have mainly contributed to a decrease of the MRSA attack rates in our hospital far below the national rate.\n\nIntroduction Candida airway colonization is common in mechanically ventilated ICU patients. Its significance and impact on outcomes are not well defined. We aimed to describe Candida airway colonization and assess clinical outcomes of patients with a clinical suspicion of ventilator-associated pneumonia (VAP) colonized with Candida. Methods A retrospective post-hoc analysis of the prospective, multicentre VAP study, which enrolled patients with a clinical suspicion of VAP, admitted to an ICU for > 96 hours and on mechanical ventilation (MV) for > 48 hours. Airway cultures were done on randomization. Patients with positive Candida cultures from other sites were excluded. The remaining patients were divided into two groups according to their Candida airway culture status. Demographics, admission diagnosis, comorbidities, PaO 2 / FiO 2 ratio and APACHE II score were recorded at randomization. The length of MV, ICU and hospital stay were compared, as well as hospital, ICU and 28-day mortality. Appropriate parametric statistical tests were applied according to data. Results Of the 739 patients enrolled in the VAP study, 639 were included for analysis: 114 had Candida airway colonization (C) and 525 did not (NC). No significant differences were noted in demographics and APACHE II score (20 \u00b1 6 vs 20 \u00b1 6, P = 0.37) except more frequent admission for sepsis (7.0% vs 2.1%, P = 0.005) and respiratory conditions (21.9% vs 14.3%, P = 0.04) in group C. More colonized patients were on antibiotics at randomization (81.6% vs 56.7%, P < 0.001). A trend for increased S44 ICU (21.1% vs 13.9%, P = 0.06) and 28-day mortality (23.7% vs 16.4%, P = 0.08) and a significant difference in hospital mortality (34.2% vs 21.1%, P = 0.003) was observed in group C. A trend was found for increased median length of ICU stay (14.1 vs 11.6 days, P = 0.07) and duration of MV (10.9 vs 8.1, P = 0.06). Hospital stay was significantly longer (59.9 vs 38.6 days, P = 0.006) in group C. Conclusion Respiratory tract Candida colonization in patients with clinical suspicion of VAP is associated with an increased burden of illness. Whether Candida colonization is responsible for worse outcomes remains to be established. Introduction Candida spp. is the third most common reason for sepsis in the ICU, not differentiating our results from the classic pattern of ICU-acquired infection. Prevention of sepsis development and identification of potentially modifiable risk factors are important goals in intensive care patents. Preemptive treatment of Candida sepsis accepted by some authors is defined as an early antifungal treatment given to patients with evidence of substantial colonization in the presence of multiple risk factors for Candida infection prior to establishing the diagnosis by cultures. Our aim was to form a focused group of patients with significant risk for Candida sepsis; to prove the feasibility and efficacy of our preemptive scheme for antimycotic treatment in order to reduce the risk of development of proved Candida sepsis. Methods During a 2-year period (2005-2006), a study was performed in a 17-bed general ICU, divided into two phases: a case-control retrospective study in which controls comprising a representative subpopulation with severe bacterial sepsis were compared with cases (patients with Candida sepsis) with respect to multiple demographic and clinical factors in a univariate analysis; and a prospective phase creating a preemptive scheme based on results from the retrospective part followed by progressively implementing it among targeted patients. Results Identified were 28 cases with Candida sepsis and 50 controls with severe bacterial sepsis with an all-cause mortality rate of 40.2%. The mortality rate for Candida sepsis was 46.4% with an attributable risk of 10/100 and was associated with a worse score of systemic injury (SAPS II = 51.7 \u00b1 15.0), comparing with a mortality rate of 35.7% and SAPS II = 38.8 \u00b1 13.3 for bacterial sepsis. Candida sepsis was always accompanied by concurrent bacterial sepsis (2.8 \u00b1 1.1 microorganisms/patient isolated from blood cultures). Identified were risk factors with great significance in addition to already known ones: Candida colonization (OR = 3.4), diabetes (OR = 3.2), number of antibiotics used (OR = 2.9), a nothing per os regimen (OR = 2.63), ICU length of stay (OR = 1.97), length of antibiotic use (OR = 1.74), pancreatitis (OR = 1.7), shock at admission (OR = 1.54), ventilator days/ICU stay ratio (days)(OR = 1.4), multiple resistant bacterial strains (OR = 1.5). Patients with gastrointestinal surgery were at risk for development of early fungal sepsis -by the 10th day of admissioncompared with the other clinical cases -by the 21st day of admission. The incidence rate of positive blood cultures for Candida in the group exposed to our scheme was calculated as 6.7% vs 18.5% in the control group.\n\nConclusions Based on our results, we accepted an algorithm for performing a preemptive therapy for which we observed clinical efficacy and which we considered indicated the following target groups of patients: with presence of clinical features of unresolving sepsis plus three defined risk factors (PPV > 70%) in a patient with length of ICU stay >20 days; lack of clinical improvement with combined antibiotic treatment against established bacterial strains; evidence of sepsis accompanied with multifocal Candida colonization of sterile body spaces. Candida colonization without risk factors requires continuous monitoring. The most important presumption to accept the preemptive strategy for a certain patient is to have a serious clinical conviction that there is an invasive fungal infection but it is still pending to be proved.\n\nThe aim of the study was to evaluate the incidence of Candida colonization in a cardiac surgical ICU, the predisposing risk factors and the impact of candidemia on outcome. Methods In an effort to answer this question a prospective study was conducted among patients admitted to our 16-bed cardiac surgical intensive care unit ICU during 1 December 2004-30 October 2005. Candida colonization and candidemia were identified. Fungal colonization was defined as colonization index exceeding 0.20 (3 g, at least two samples of seven growing Candida spp.). Candidemia was defined as the isolation Candida spp. in at least one blood culture in a patient with temporally related clinical signs. The demographic characteristics of patients who developed candidemia, as well as the underlying disease and comorbidities, were recorded. Results Over a 22-month period, 2,509 critically ill patients were evaluated. Candida spp. was isolated from any site in 141 patients (5.6%), while 10 patients (0.4%) presented ICU-acquired candidemia. They were all hospitalized for more than 7 days (range 7-34 days) in the ICU and had been exposed to broad-spectrum antibiotics (>3 agents). The mean age was 68 years (range 50-82 years) and the mean ICU stay 28 days. Candidemia appeared at a mean of 15.8 days after ICU admission. Candida albicans was the most common isolated pathogen. Candiduria in any count was detected in 12 patients but none of them experienced candidemia, while in seven patients Candida was isolated from urine and the respiratory tract. Six patients had major postoperative complications. Mortality due to candidemia was 60%. All patients received appropriate antifungal treatment. Prophylactic antifungal treatment was used in patients with multifocality colonization and in patients spending more than 7 days in the ICU after cardiac surgery. Conclusion C. albicans is the most common fungal pathogen in our ICU. Seven percent of colonized patients developed candidemia. Major postoperative complications, excessive antibiotic exposure and acute renal failure seem to predispose to the development of candidemia. Patients with candidemia have high inhospital mortality, perhaps as a reflection of illness severity. Objective To evaluate utilization patterns and outcomes associated with i.v. fluconazole therapy within ICUs in Spain and Germany. Methods A prospective longitudinal observational study was conducted within 14 hospital ICUs in Spain and five in Germany. Patients on i.v. fluconazole therapy were included and were followed over one hospitalization period (admission until discharge). Data were collected during 2004, using electronic case report forms. Data included patient disease characteristics, patient risk status (APACHE scores), type of fluconazole therapy, drug-related adverse events, length of fluconazole therapy, and length of hospital stay. Switches in fluconazole therapy, dosing changes, additional concomitant antifungal therapy, overall mortality, and clinical outcomes were also evaluated. Logistic regression models determined univariate and multivariate associations with mortality. Results A total of 303 patients were enrolled. Fluconazole was used initially as prophylaxis in 29 (9.6%) patients, preemptive therapy in 85 (28.1%) patients, empiric therapy in 140 (46.2%) patients and as definitive therapy in 49 (16.2%) patients. Thirty-six patients switched from fluconazole to a broader spectrum antifungal agent, and seven received a second concomitant antifungal drug. Reasons for switching therapies included lack of response due to suspected resistance, documented resistance or clinical reasons other than resistance. Thirty-two patients (10.6%) experienced fluconazole-related adverse events. The overall study mortality rate was 41.9% (127/303 patients). Mortality was significantly associated with switching i.v. treatment (odds ratio 5.0; 95% CI 2.3-11.1) and the presence of adverse events (odds ratio 4.1; 95% CI 1.8-9.2). Conclusion The observational nature of this study precludes the establishment of any causality. This research merely documents the experiences of ICU patients who have been prescribed i.v. fluconazole therapy. Our results showed high mortality rates in the enrolled ICU patients. Patients developing adverse events and complications requiring a switch in fluconazole experienced worse outcomes Introduction Toxoplasma gondii, a worldwide-distributed parasite, could cause opportunistic infection with high mortality in immunosuppressive individuals. Its association with severe manifestations of immunosuppression has been known for several decades, and the occurrence of encephalitis and disseminated disease has since been observed in different clinical conditions such as lympho-reticular neoplasias, solid organ transplants, and mainly in patients with AIDS [1] . To our knowledge, the toxoplasmosis seropositivity rate in ICU patients who have critical illness induced immunosuppression is not yet investigated. We studied the seropositivity incidence of T. gondii in ICU patients by assessing IgG and IgM antibodies. Materials and methods One hundred and three ICU patients with the mean age of 53.9 \u00b1 13.9 years (51 men, 52 women) and 78 healthy volunteers with the mean age of 51.4 \u00b1 9.2 years (39 men, 29 women) as a control group were included in the study. Anti-T. gondii IgG and IgM antibodies were determined by ELISA. Statistical analyses were done with the chi-square test and Kolmogorov-Smirnov one-sample test. P < 0.05 was considered as statistical significance. Results T. gondii IgG antibodies were positive in 56.3% of ICU patients (n = 58) and in 24.3% of healthy volunteers (n = 19) (P < 0.031). IgM antibodies were positive in 13.8% of ICU patients (n = 15) and in 6.4% of healthy volunteers (n = 5); however, this difference could not reach statistical significance. increased mortality in CMV+ patients, although CMV disease did not occur. There was a striking difference between the groups in respect to the period on ventilator: 21.5 days vs 8.0 days (median) in CMV+ and CMV-patients, respectively (P < 0.005). Similarly, CMV+ patients had a longer median LOS after enrollment either in the ICU (29.5 days vs 10 days, P < 0.001) and in the hospital (49 days vs 23 days, P < 0.001). This difference was assured when the analysis was restricted to survivors. Conclusion Our data suggest that CMV reactivation leads to increased morbidity and treatment expenditure independently from CMV disease. Further analysis points at a crucial role of lung pathology due to CMV reactivation.\n\nIntroduction A polymyxin B immobilized fiber column (PMX; Toray Industries Inc., Tokyo, Japan) was developed in Japan in 1994 and it has been used for treatment of endotoxemia or septic shock patients. Materials and methods All patients received an urgent operation due to intra-abdominal infection. In 88 cases treated with a polymyxin B immobilized column through direct hemoperfusion (PMX-DHP), changes in hemodynamics, pulmonary oxygenation (PaO 2 / FIO 2 ) and various mediators (IL-6, IL-8, IL-ra, plasminogen activator inhibitor 1 (PAI-1)) were examined before and after PMX-DHP, stratifying with the outcome (64 survivors and 24 who died). PMX-DHP was performed through a double lumen catheter (11.5 Fr), placed in the femoral vein or internal jugular vein, at a blood flow rate of 80 ml/min using nafamostat mesilate as an anticoagulant for 2 hours. Results PMX-DHP significantly increased systemic arterial pressure and mean arterial pressure, with a greater increase in the survival group. Also, there appeared to be a trend for PaO 2 /FIO 2 improvement as blood pressure increased. As the mechanism for improvement of pulmonary oxygenation by PMX-DHP has not been shown clearly, it remained to be examined further. PAI-1 values significantly decreased in the survivor group (from 436 \u00b1 549 to 251 \u00b1 283 ng/ml) immediately after PMX-DHP; also intracellular adhesion molecule-1 and endothelial leukocyte adhesion molecule-1 tended to decrease in both groups. Discussion PAI-1 is elevated by endotoxin, thrombin and cytokines, and is an indicator of vascular endothelial cell activation.\n\nIn septic dissminated intravascular coagulation from Gram-negative bacilli, a massive amount of PAI-1 is produced on vascular endothelial cells along with elevation of cytokine production and coagulation activity. In addition, PAI-1, one of the fibrinolysis inhibitory factors, plays an important role in regulating fibrinolysis by inhibiting tissue plasminogen activator, which converts plasminogen to active plasmin on fibrin, to block unnecessary fibrinolysis.\n\nConclusion The determination of PAI-1 may be a useful clinical parameter for predicting PMX-DHP efficacy.\n\nMechanism and effectiveness of polymyxin B-immobilized fiber columns for removing mediators (HMBG-1, 2-arachidonoyl glycerol, anandamide, PAI-1, protein C and IL-6) in septic shock patients Introduction Septic shock remains a major cause of multiple organ failure with a high mortality rate. To remove an endotoxin in patient plasma, direct hemoperfusion (DHP) using a polymyxin Bimmobilized fiber column (PMX; Toray Industries Inc., Tokyo Japan) was developed in Japan in 1994 and has since been used for the treatment of septic shock. The precise role of PMX is not clear.\n\nWe treated 27 septic shock patients using DHP-PMX. The patients were separated into two groups for analysis: those whose systolic blood pressure (SBP) increased by more than 30 mmHg immediately after DHP-PMX (15 cases), and those whose SBP did not increase by more than 30 mmHg after DHP-PMX (12 cases). Furthermore, the patients were separated into two other groups for analysis: those whose P/F ratio increased by more than 20% immediately after DHP-PMX (15 cases), and those whose P/F ratio did not increase by more 20% after DHP-PMX (12 cases). Mediators were measured at four points: before and after DHP-PMX, and 1 day and 3 days afterward.\n\nThe patient group consisted of 17 males and 10 females, 59.6 \u00b1 12.7 years old. The average APACHE II score was 27.2 \u00b1 9.1, and the average SOFA score was 11.7 \u00b1 5.2 before DHP-PMX. Nineteen patients survived and eight died. When the changes in PAI-1, protein C, ATIII, IL-6 and high mobility group box protein 1 (HMGB-1) were compared between the groups, only the HMGB-1 levels had improved significantly in the SBP increased group (P = 0.0125). The SBP increased significantly after DHP-PMX in the HMGB-1-improved group (P < 0.0001). An improvement in the P/F ratio and a reduction in 2-arachidonoyl glycerol during DHP-PMX were significantly correlated (P = 0.0184). Conclusion We showed that the circulation dynamics of septic shock patients can be improved by reducing HMGB-1 levels and that respiratory function can be improved by reducing 2arachidonoyl glycerol levels using DHP-PMX.\n\nCharacterization of the coupled plasma filtration-adsorption resin cartridge adsorptive capacity for cytokines and inflammatory mediators: adsorption profiles from septic patient plasma and in vitro endotoxinstimulated whole blood\n\nIn vitro experiments included static and dynamic evaluations of resin binding. Whole human blood was stimulated with endotoxin for 4 hours at 37\u00b0C; or with the addition of added cytokines or toxins for evaluation. For static conditions, 4 ml plasma with 1 ml resin were incubated for up to 10 hours. Aliquots were withdrawn between 0 and 10 hours and cytokine inflammatory mediator and toxin adsorption were determined with standard ELISAs, multianalyte protein arrays, HPLC and diode array adsorption spectroscopy. Dynamic conditions involved defining the optimal linear velocity and evaluating the adsorption capacity under flow conditions. These experiments used a closed circuit consisting of a plasma filter and resin cartridge. Samples were taken from a blood port and immediately before and after the plasma cartridge. In addition, serum, pre-cartridge and postcartridge plasma samples were also taken from septic patients undergoing CPFA.\n\nResults Endotoxin-stimulated blood or samples from septic patients had high levels of cytokines and inflammatory mediators. The resin used in the CPFA adsorptive cartridge showed higher than 80% adsorption under both static and dynamic conditions for: IL-1\u03b1, IL-6, IL-8, MIP-1\u03b1 and MIP-1\u03b2, TNF\u03b1, MCP, myoglobin. IL-6 appeared to be particularly adsorbed by the cartridge. Severe septic patients had great variability and often very high levels of IL-6 ranging from normal levels (50 pg/ml) up to 12,300 pg/ml. The mean of 10 patients treated before CPFA was 1,775 \u00b1 3757 pg/ml, while postsession IL-6 was 995 \u00b1 2178 pg/ml. The plasma levels before the cartridge ranged from 12 pg/ml to 1,750 pg/ml, while postcartridge levels were below the level of detection.\n\nConclusions The resin in CPFA has a high adsorption capacity for several cytokines and mediators involved in severe sepsis and septic shock. Studies are currently ongoing to correlate cytokine reduction with clinically relevant improvements in these patients. Methods Twenty-four patients with septic shock induced by peritonitis underwent laparotomy for drainage. Endocannabinoid absorption with PMX-DHP was examined in two groups of patients: patients in whom systolic arterial BP had increased more than 20 mmHg (BP elevation group; n = 12) and patients in whom BP did not increase or had increased no more than 20 mmHg (BP constant group; n = 12). Results Levels of AEA did not change after PMX-DHP in the either the BP constant group or the BP elevation group, whereas levels of 2-AG decreased significantly after PMX-DHP in the BP elevation group but not in the BP constant group (Figure 1 ). F2-isoprostane gradually increased after PMX-DHP. On the other hand, levels of F2isoprostane remained constant in the BP elevation group (Figure 2 ).\n\nPatients with septic shock are under considerable oxidative stress, and 2-AG plays an important role in the cardiovascular status of these patients. The removal of 2-AG by PMX-DHP benefits patients with septic shock by stabilizing cardiovascular status and decreasing long-term oxidative stress.\n\nWe examined the relationship between proinflammatory cytokines, adipocyte-derived adiponectin and hyperglycemia. Patients requiring long periods in the ICU have a relatively high mortality. Tight glucose control with insulin infusions has been shown to improve survival and prevent complications.\n\nMethods A prospective, observational study at an academic ICU. A sequential sample was taken over a 2-month period. Ethics approval was obtained from the University Ethics Committee.\n\nAvailable online http://ccforum.com/supplements/11/S2 Baseline bloods for TNF\u03b1, IL-6, adiponectin (Adipo), total cholesterol (TC), triglycerides (TG), insulin, C-peptide (CPep) and cortisol (Cort) were collected on admission (D0). These were repeated on day 3 (D3), day 7 (D7) and discharge (D/C). Routine bloods ordered were also used. Data on the ICU charts were also used. No changes to ICU protocols were required. Of note, insulin infusions were started for blood glucose concentrations greater than 6 mmol/l. Exclusion criteria included all patients with diabetes mellitus, chronic renal failure and liver failure or cirrhosis. Results Forty patients admitted to the ICU were enrolled and followed up to discharge. The median age was 35.5 years (minimum 18, maximum 66). The median APACHE II score was 10.5 (minimum 2, maximum 28) and the median duration was 6 days (minimum 1, maximum 43). D0, D3, D7 and D/C glucose concentrations did not differ (Kruskal-Wallis ANOVA, P = 0.98). TNF\u03b1 peaked at D3 (4.9 pg/ml) and then started decreasing. Administered insulin (InsAd) accompanied the TNF\u03b1 peak at D3 (32U) and then decreased. Adipo peaked at D7 (10,774 pg/ml) after the TNF\u03b1 peak, which coincided with the TNF\u03b1 decrease at D7 to 4.76 pg/ml. Endogenous insulin indicated by CPep peaked with Adipo at D7 (2.8 \u00b5g/l). TG levels increased in parallel with increasing TNF\u03b1 from 0.7 mmol/l at D0 to 1.1 mmol/l at D3 and then declined. TC was lowest at D0 and increased up to D/C but remained relatively low. Table 1 shows several variables and their change over time from admission to discharge. Table 2 shows the correlations between these variables. Survivors had a lower median TNF than nonsurvivors (Mann-Whitney U test, P = 0.066).\n\nConclusion TNF contributes to increased insulin needs. TNF is known to cause insulin resistance. We have shown that TNF correlates inversely with Adipo. As Adipo increases, insulin needs are decreased (inverse correlation with InsAd). Also, TNF contributes to increase TG indicating increased free fatty acids (FFA) by lipolysis, which impairs glucose clearance. Adipo, an insulin sensitizing protein, is known to negatively regulate TNF levels as was indicated by our study. Adipo contributed to a decrease in TG indicating lower FFA and better glucose clearance. IL-6 at D/C also contributed to a higher glucose concentration at D/C. Increasing age contributed to lower Adipo levels at D/C, indicating lower insulin sensitivity. A higher BMI contributed to a higher glucose level at D0 and increased insulin needs at D0. Finally, a higher TNF level appears to be related to increased mortality Objective The aim of the study was to investigate the effectiveness of octreotide at different doses in reducing the hypoglycemic attacks and the need for dextrose in treatment of refractory and recurrence hypoglycemia related to the toxicity of sulfonylurea. Methods In the study, 40 New Zealand type of rabbits were used weighing between 2,500 and 3,000 g. The rabbits were randomly divided into four groups consisting of 10 animals. All the animals were given gliclazide 100 mg orally. For the treatment of hypoglycemic attacks in Group I, only 15 cm 3 50% dextrose (7.5 g) intravenously (i.v.) was used; in Group II, Group III and Group IV octreotide 25 \u00b5g, 50 \u00b5g and 100 \u00b5g single doses were used subcutaneously, respectively. Octreotide was given in Groups II and III and Group IV at the eighth hour (when hypoglycemic attacks onset). Groups II, III and IV were given an additional 15 cm 3 50% dextrose (7.5 g) i.v. infusion for each hypoglycemic attack developed. Following the toxic dose, animals were given the amount of dextrose used before and after octreotide administration and the number of hypoglycemic attacks were recorded. Results There was a significant difference between Groups I, II, and IV in the number of hypoglycemic attacks and the number of dextrose requirement between 9 and 24 hours (P = 0.001).\n\nGroups receiving octreotide showed less hypoglycemic attacks and dextrose requirements than controls. Conclusion Our experience suggests that octreotide may be used to reduce the number of refractory and recurrence hypoglycemic attacks developing due to overdose of sulfonylurea; large prospective studies would be needed to validate these findings. Introduction Acute hyperglycemia and insulin resistance are characteristics of metabolic and endocrine imbalances of critically ill patients and are subject to a substantial inflammatory response that is partly mediated by cytokines produced by peripheral blood mononuclear cells (PBMC). Treatment with intensive insulin therapy to keep patients normoglycemic has been shown to reduce inflammatory responses. It is unclear whether hyperglycemia, insulin or osmolarity changes exert direct effects on proinflammatory cytokines. We investigated the direct effects of these substances on cytokine production of PBMC in vitro.\n\nMethods PBMC were isolated from peripheral blood of 10 healthy volunteers via Ficoll gradient. Cells were incubated for 3 hours at 37\u00b0C with/without low/high concentrations of glucose, mannitol, urea, insulin and stimulated with 0.5 ng/ml LPS. After 24 hours, concentrations of IL-6 and IL-1\u03b2 were measured with an ELISA method.\n\nResults Increasing concentrations of glucose, mannitol and urea resulted in a significant increase of IL-6 and IL-1\u03b2 cytokine production. Insulin had no effect (Table 1) . Conclusion High concentrations of glucose, mannitol and urea lead to a significant increase in IL-6 and IL-1\u03b2 cytokine production by PBMC in vitro. The most profound effect was seen with hyperglycemia. Besides hyperglycemia, also uremia and high osmolarity seem to augment inflammation. Insulin could not reverse the increase in inflammation. These findings may be relevant in explaining the beneficial effects of normoglycemia on the inflammatory response in critically ill patients.\n\nInsulin therapy inhibits poly(ADP-ribose)polymerase activation in endotoxin shock The nuclear enzyme poly(ADP-ribose) polymerase (PARP) is activated in various forms of circulatory shock. By triggering a cellular energetic dysfunction, and by promoting proinflammatory gene expression, PARP activation significantly contributes to the pathogenesis of shock. The activation of PARP is usually triggered by DNA strand breakage, which is typically the result of the overproduction of reactive oxidant species. In the present study we tested whether endotoxin-induced PARP activation and pro-inflammatory mediator production can be modified by insulin therapy. Rats subjected to bacterial lipopolysaccharide (LPS) with or without insulin pretreatment were studied. LPS-induced PARP activation in circulating leukocytes was measured by flow cytometry, and production of TNF\u03b1 was measured by ELISA. LPS induced a significant hyperglycemic response, activated PARP in circulating leukocytes and induced the production of TNF\u03b1. Insulin treatment prevented the LPS-induced hyperglycemic response, blocked the activation of PARP and blunted the LPS-induced TNF\u03b1 response. As hyperglycemia is known to induce the cellular formation of reactive species, we propose that PARP activation in endotoxin shock occurs as a result of hyperglycemia-induced reactive oxidant and free radical generation. The current findings may have significant implications in the context of the emerging concept of tight glycemic control for critically ill patients.\n\nInfluence of diabetes and HbA1c on the course and outcome of sepsis in the intensive care unit and HDL (not significant) were different between groups. The mortality rate was the same but LDL and HDL had negative correlation with organ function among nonsurvivors.\n\nConclusions Targeting blood glucose levels to below 110 mg/dl with insulin therapy prevented morbidity, probably due to a better control of lipid metabolism, expressed as a more rapid serum LDL normalization and avoiding a greater decrease in serum HDL levels in the first 72 hours of septic shock.\n\nEffect of intensive insulin therapy on coagulation and fibrinolysis of respiratory critically ill patients Most intensive care deaths beyond the first few days of critical illness are attributable to nonresolving multiple organ failure (MOF), either due to or coinciding with sepsis. One of the mechanisms that is thought to contribute to the pathogenesis of MOF is microvascular thrombosis. Recently, we reported improved survival and prevention of MOF of critically ill patients with the use of intensive insulin therapy to maintain normoglycemia for at least several days [1, 2] . We hypothesize that intensive insulin therapy also prevents severe coagulation abnormalities, thereby contributing to less organ failure and better survival. We studied a subgroup of long-stay critically ill patients with a respiratory disease upon ICU admission, who had been enrolled in a randomized controlled trial evaluating the impact of intensive insulin therapy in medical ICU patients [2] . Plasma samples were analyzed for a panel of coagulation markers (prothrombin time, activated partial thromboplastin time, fibrinogen and D-dimer levels) that were used to assign points towards the International Society of Thrombosis and Haemostasis overt disseminate intravascular coagulation (DIC) score. Circulating plasma thrombin-antithrombin complexes and plasminogen inhibitor type 1 levels were also determined. As markers of inflammation, we measured circulating serum levels of several cytokines and CRP. \n\nMortality of intensive insulin-treated patients was lower than of conventionally treated patients for all classes of upon-admission DIC score, except for those patients with a DIC score of 6 or higher. There was no effect of insulin therapy on any of the fibrinolytic, coagulation or inflammatory parameters tested. The accuracy of the DIC score to predict mortality in this patient sample was only moderate and comparable with that of CRP and the SOFA score. Also, circulating plasminogen inhibitor type 1 or thrombin-antithrombin complexes levels did not correlate well with mortality or DIC score. These findings indicate that the coagulation system did not play a key role in mediating the survival benefit of intensive insulin therapy. Introduction Intensive insulin therapy reduces morbidity and mortality in postoperative critical care; however, this treatment also increases the risk of hypoglycemia. A possible cause for unstable blood glucose (BG) levels could be a variable adsorption of insulin at plastic material of infusion tubings. We evaluated in vitro and in vivo the adsorption of insulin at standard tubing materials (polyethylene (PE) and polyurethane (PU)) and the effects of this adsorption process on blood glucose levels. Methods (1) In vitro, a standard perfusor syringe (Perfusor \u00ae ; BBraun, Germany) was filled with 50 IE normal insulin (Actrapid \u00ae ; NovoNordisk, Germany) dissolved in 50 ml saline 0.9%. The syringe was connected to PE or PU tubings (BBraun) and, at an infusion rate of 1 ml/hour, the insulin concentration in the syringe and at the end of the tubings was measured at hourly intervals for 5 hours and again after 24 hours by the Bradford protein assay. Insulin concentrations were compared using the Student t test.\n\n(2) In a prospective, double-blinded, cross-over study, approved by the ethics committee, 10 patients on the surgical ICU received insulin via PE or PU tubing each for 24 hours in random sequence. All blood BG values, total infused insulin solution volume, and critical care scores were documented and statistically analysed by the Wilcoxon test. Results (1) The insulin concentration in all syringes was always >97% of the estimated value. The initial concentrations of insulin at the end of PE and PU tubings were lower than expected (23 \u00b1 4% of anticipated concentration in the first 6 min). In PE, the concentration rose to 37 \u00b1 2% and in PU to 78 \u00b1 4% after 24 hours (P < 0.0001). (2) In vivo the mean BG values did not differ between PE and PU (PE 141 \u00b1 17 mg/dl; PU 132 \u00b1 23 mg/dl (not significant)). Severity of illness was not different between the groups: TISS 37 \u00b1 5 (PE) vs 39 \u00b1 5 (PU), SAPS 43 \u00b1 13 (PE) vs 41 \u00b1 15 (PU) on both days; neither were catecholamine doses and 24-hour fluid balance. However, significantly more insulin solution was infused in PE (66 \u00b1 18 ml/ 24 hours) compared with PU (44 \u00b1 15 ml/24 hours) (P = 0.0015). Conclusion Infusion of insulin using PE and PU tubings leads to a relevant adsorption of the drug in both materials. Adsorption to PE is significantly higher compared with PU. Thus, a large variation of insulin application to the patient is possible if different tubing materials are used. Furthermore, variability of adsorption, a competitive adsorption with other drugs if insulin is not infused via a single line as well as changes of effective insulin application following routine change of tubings, may be one cause of unexpected hypoglycemia that can be deleterious to the patient. Introduction Hyperglycemia in critically ill patients is frequently related to a hypermetabolic stress-response and has been associated with increased morbidity and mortality. The aim of this study was to assess the relationship between blood glucose levels and clinical outcome in a mixed cohort of critically ill patients with a nosocomial bloodstream infection (BSI). Methods A retrospective observational cohort study was conducted including 130 adult patients with a microbiologically documented BSI admitted over a 2-year period (2003) (2004) to the ICU. Blood glucose levels were evaluated from 1 day prior to onset of BSI (d-1) until 5 days after onset of BSI (d+5). The contribution of hyperglycemia, divided into three subgroups (\u2265150 mg/dl, \u2265175 mg/dl, and \u2265200 mg/dl, respectively), to inhospital mortality was estimated by logistic regression.\n\nThe mean age of the study population was 54.7 \u00b1 19.0 years. Inhospital mortality was 36.2%. Hyperglycemia (\u2265175 mg/dl and \u2265200 mg/dl) was observed more often among the nonsurvivors. Over the seven study days, no differences were found in daily morning blood glucose levels between survivors (n = 83) and nonsurvivors (n = 47) (all P > 0.05). Although in the nonsurvivors the evolution of glycemia tended to be higher, this trend was not statistically significant compared with the survivors. Multivariate logistic regression revealed that age (P = 0.022), APACHE II score (P = 0.003), antibiotic resistance (P = 0.001), and hyperglycemia (\u2265200 mg/dl) upon onset of BSI (P = 0.001) were independently associated with inhospital mortality, whereas appropriate antimicrobial therapy \u226424 hours (P = 0.016) and previous history of diabetes (P = 0.022) were associated with better outcome. Conclusion Trends of blood glucose levels were higher among nonsurvivors. Hyperglycemia (\u2265200 mg/dl) upon onset of nosocomial BSI adversely affects outcome in a heterogeneous ICU population.\n\nSystem for automated discontinuous venous blood withdrawal for glucose determination of patients in the intensive care unit S52 high workload that has to be performed by the staff. Hence the usage of an automated discontinuous venous blood sampling system might be an alternative to improve the adjustment of the insulin therapy. The primary aim of the study was to investigate whether the glucose concentration in manually withdrawn blood samples correlates with automated withdrawn blood samples. Methods In a 12-hour trial, six volunteers were investigated (male/ female 5/1; age 28.2 \u00b1 2.2 years, BMI 22.5 \u00b1 1.3, nondiabetics). A 75 g OGTT was performed to enable a better dynamic range of the glucose values. Two venous cannulae were inserted into the dorsal hands for reference measurement and for connection to the automated blood sampling system. To reduce the volunteer's health risk, pressure, air bubble sensor and flushing fluid monitoring were integrated into the system. Blood samples were obtained frequently in 15/30-minute intervals. Roche Microsamplers and the OMNI S6 glucose analyser were used for determination of the blood readings.\n\nThe automated blood sampling system performed its operation in all volunteers over the whole trial period. The median Pearson coefficient of correlation between manual and automated withdrawn blood was 0.983 (0.862-0.995). Furthermore, the results (173 data pairs) were analysed via the recently published 'Insulin Titration Error Grid Analysis' and 99.4% were suggesting an acceptable treatment. The results of the traditional 'Error Grid Analysis' showed that 96% of the data were in zone A and 4% in zone B. Conclusion The automated discontinuous blood withdrawing system provides reproducible blood samples from peripheral venous blood. In combination with a glucose sensor and an algorithm it might be used in future as a closed loop system for insulin and glucose infusion at the ICU. Introduction It has been proposed that intensive insulin therapy (IIT) aiming for a blood glucose (BG) of 4.4-6.1 mmol/l reduces mortality in critically ill patients when compared with conventional insulin therapy (CIT) targeting BG at 10.0-11.1 mmol/l. Difficulties with IIT include inadvertent hypoglycaemia and low efficacy at achieving the target BG. We proposed that computerised decision support may mitigate these problems. Objective To comprehensively describe BG and outcome from decision-supported IIT. Methods A clinical information system at each bedspace guided staff through the IIT algorithm. The time spent within glucose ranges was calculated assuming a linear trend between successive measurements. Results Patient characteristics are shown in Table 1 . The IIT group had more frequent BG evaluation (7,007 over 8,944 patient-hours, 0.78 tests/hour) than the CIT group (3,609 over 8,617 hours, 0.42 tests/hour). The median (interquartile range (IQR)) proportion of time spent in the target range 4.4-6.1 mmol/l was similar in the IIT and CIT groups (23.21% (15.4-29.8) vs 17.9% (9.8-29.3), respectively; P = 0.17). Similarly, time spent with a BG between 6.2 and 7.99 mmol/l was no different for the two groups (48.5% (IQR 36.9-59.3) for IIT and 43.9% (IQR 34.7-60.9), P = 0.72). In the IIT and CIT groups, five and six patients experienced a BG below 2.2 mmol/l, respectively. Discussion Computerised decision-support and more intensive monitoring did not improve BG control or reduce the incidence of hypoglycaemia. Introduction The objective of this study was to determine the efficacy and safety of subcutaneous (s.c.) once-daily (OD) glargine insulin, a long-acting insulin, in comparison with a s.c. regular insulin, based on a protocolized sliding scale regimen for achieving glycemic control in patients admitted to the ICU. Methods One hundred patients admitted to the ICU with an admission capillary blood glucose (CBG) >150 mg/dl (8.3 mmol/l) were involved in this prospective, randomized study. Patients with age <18 years, pregnancy, shock, requiring continuous intravenous insulin infusion, renal failure were excluded. Patients were randomly assigned to receive either s.c. glargine insulin 10 U (CBG \u2264 9.9 mmol/l) or 18 U (CBG \u2265 10.1 mmol/l) s.c. OD (Group G, n = 50), or s.c. regular insulin based on a 6-hourly sliding scale (Group R, n = 50). CBGs were recorded at 6-hour intervals up to 96 hours or until ICU discharge, whichever was earlier. The target CBG in both groups was <150 mg/dl (8.3 mmol/l). Patients in group G received rescue doses of regular insulin, as required. Demographic characteristics, mean and median CBG, and episodes of hypoglycemia were studied. Results Demographic profiles were comparable between the two groups. There was no significant difference in mean CBG in both groups (Group G 152.1 mg/dl (8.4 mmol/l), Group R 149.9 mg/dl (8.3 mmol/l), P = 0.66). Median CBGs were comparable at 6hourly time points in both the groups except at 0 and 6 hours in the glargine arm (CBG at 0 and 6 hours, Group G 10.0 mmol/l and Median capillary blood glucose (CBG) at different time points. S53 9.9 mmol/l, Group R 9.4 mmol/l and 8.3 mmol/l, P = 0.04 and 0.02, respectively) ( Figure 1 ). There were three episodes of hypoglycemia in Group G and one in Group R, which were corrected. Conclusion OD s.c. glargine insulin is a safe and effective alternative to regular insulin for glycemic control in critically ill patients.\n\nInvestigation of insulin clearance in severely acutely ill patients with glucose intolerance evaluated by means of bedside-type artificial pancreas Background and purpose Glucose intolerance in acutely ill patients is one of the risk factors of their morbidity and mortality, and glucose control with insulin therapy improves the outcome. We investigated relationships among insulin clearance (IC), which is considered to be one of the factors related to effectiveness of insulin therapy, and glucose tolerance, glucose intolerance, and severity of the diseases, in order to clarify the significance of IC on the severity of the diseases including glucose intolerance and on the therapies. Materials and methods Twenty-three ICU patients with glucose intolerance in whom strict blood glucose control was performed by means of a bedside-type artificial pancreas (NIKKISO Corp., Japan) were investigated. The diabetics were excluded. The items investigated were IC (ml/kg/min) measured by the glucose clamp method, daily mean blood glucose level as a parameter of glucose intolerance (BGm, mg/dl), proportion of septic patients (%), SOFA score and mortality (%) as indicators of the severity of the diseases, and blood concentration of free fatty acid (FFA) and stress hormones (glucagon, growth hormone, cortisol, adrenalin, noradrenalin) as factors that might affect glucose intolerance. The method of investigation involved patients being classified into four groups according to IC, and those groups were compared with each other; low IC group (group L: IC < 9, n = 2), normal IC group (group N: 9 < IC < 15, n = 13), high IC group (group H: 15 < IC, n = 8), and severely high IC group (group S: 19 < IC, n = 5) (group S was a subgroup of group H). Results (1) FFA values were low or normal in all groups. (2) There were no significant differences in stress hormones among group N, group H, and group S. Those hormones in group L were significantly higher than, or had a tendency to be higher than, those in group N, group H, and group S. (3) The mean values of BGm in the groups had a tendency to be higher in the order of group S (179 \u00b1 30), group H (172 \u00b1 25), group N (162 \u00b1 26), and group L (153 \u00b1 8). (4) The severities of the diseases (sepsis (%)/SOFA score/mortality (%)) in the groups were significantly higher in the order group L (100%/20.0 \u00b1 1.4/100%), group S (100%/ 9.6 \u00b1 7.0/40%), group H (88%/7.0 \u00b1 6.5/25%), and group N (54%/5.8 \u00b1 5.2/15%).\n\nThe increase of IC was related to glucose intolerance. IC increased and glucose intolerance became severe as the severity of the diseases progressed. In the most severe state, or in a near-terminal state, however, IC decreased and glucose intolerance improved, although stress hormones increased significantly. Therapies focused on the improvement of IC were considered important in acutely ill severe patients with glucose intolerance as well as blood glucose control by insulin therapy. Results We compared the 6:00 a.m. glucose value with those collected at all other times in 149 consecutive patients. The 6:00 a.m. values were lower than the remaining values (mean \u00b1 SD: 112 \u00b1 30 mg/dl (n = 477) vs 119 \u00b1 35 mg/dl (n = 10,364); P < 0.0001) and as hypothesized had a smaller variance by F test (P < 0.0001). Inspection of the time-averaged data (\u00b1SE) revealed a diurnal variation in the blood glucose with peaks occurring at 11:00 a.m. and 10:00 p.m. (Figure 1 ). This diurnal pattern may account for some of the observed variation in insulin requirements and contribute to episodes of hypoglycemia in the critically ill. Conclusion Glucose variance is increased if all time values are considered rather than a single time point and there is a diurnal pattern to glucose in ICU patients receiving insulin. Consideration of this diurnal variation when treating hyperglycemia in the ICU may avoid hypoglycemia and so facilitate better glucose control with insulin infusions.\n\nAvailable online http://ccforum.com/supplements/11/S2\n\nIntroduction Tight glycemic control reduces mortality in surgical intensive care patients and in long-term medical intensive care patients. The incidence of severe hypoglycaemia (glucose \u22642.2 mmol/l) in the intensive treatment group has been 3.1-5.1%.\n\nRecently, a large study on intensive insulin therapy was prematurely discontinued due to safety issues. The incidence of hypoglycaemia was 9.8% in intensive treatment group and the mortality among patients experiencing hypoglycaemia was 18.6%. As the safety of intensive insulin therapy has been questioned, we screened all patients during a 17-month period to see the incidence of hypoglycaemia and its effects on the prognosis of the patients.\n\nMethods A retrospective study was performed in two ICUs, one eight-bed general ICU and one 10-bed surgical ICU. All patients treated between 7 February 2005 and 30 June 2006 were included in the study. A nurse-driven intensive insulin protocol with a target blood glucose level of 4-6.1 mmol/l had been introduced in 2004. All blood glucose measurements performed during the ICU treatment were analysed. The patients were divided into two groups according to the lowest detected blood glucose value (\u22642.2 or \u22652.3 mmol/l). Results A total of 1,024 patients (1,124 treatment periods) were included in the study. Thirty patients were excluded due to incompleteness of the data. During the study period 61,203 blood glucose measurements were performed, 1,578 (2.6%) of which were below the target value of 4 mmol/l. Severe hypoglycaemia (\u22642.2 mmol/l) occurred in 25 patients (36 measurements). The incidence was 0.059% of the measurements and 2.3% of the patients. The median age, sex, APACHE II score, SAPS II or diagnosis category did not differ between the groups. The median (IQR) ICU and hospital length of stay was 4.3 (1.8-10.6) and 18 (8.5-39.5) days in patients with lowest blood glucose \u22642.2, and 2.7 (1.2-5.7) and 13 (7-23) days in patients with lowest blood glucose \u22652.3 (P = 0.058 and P = 0.077, respectively). The hospital mortalities were 25% and 15%, respectively; the difference was not statistically significant. Introduction Insulin resistance and hyperglycemia are common in critically ill patients, and are associated with higher morbidity and mortality in these patients if not controlled. Intensive insulin therapy has been shown to reduce morbidity and mortality. It is not clear, however, whether the patients' indication for admission into the ICU is related to the time to achieve glycaemic control or the total dose of insulin required. This study was designed to audit the efficacy of an intensive insulin therapy protocol in achieving glycaemic control in patients presenting with different conditions. Methods A prospective observational study was performed over 8 weeks on patients admitted to an adult ICU who received nutrition support for up to 48 hours. Intensive insulin therapy was administered to those patients who developed hyperglycemia. The demographics, blood glucose and insulin doses were documented. Haemoglobin, white cell count, neutrophil count, antioxidants, CRP and prealbumin were measured. Outcome measures were the mean and total insulin dose and the time to achieve glycaemic control.\n\nResults Forty patients, 22 (55%) males and 18 (45%) females, who received nutritional support for 48 hours or more were studied. The mean (SD) age was 59.4 (14.7) years. Enteral feeding was given in 32 (80%) and parenteral feeding in 14 (35%) patients, while six (15%) patients received both enteral and parenteral feeding. The mean (SD) energy in 48 hours was 3,307.4 (527.0) kcal, mean (SD) insulin was 1.37 (1.23) IU, mean (SD) blood glucose was 7.76 (0.9) mmol/dl and total insulin to achieve glycaemic control was 65.51 (58.6) IU. The time taken (SD) to achieve glycaemic control was 15.16 (12.65) hours. As expected, there was a relationship between the total insulin dose and the time to achieve three consecutive glycaemic controls (r = -0.43, P = 0.023). Also, between the total insulin dose and mean blood glucose r = 0.508, P = 0.001. There was no significant relationship between the total insulin dose and indication for ICU admission, and the total insulin dose and body mass index. Conclusion Findings from this study showed that the indication for admission did not affect either the total dose of insulin required to achieve glycaemic control or the time it takes to achieve three consecutive glycaemic controls. Introduction Glycaemic control is another example of protocoldriven therapy in intensive care medicine to improve outcome in critically ill patients. While the advantage of this approach seems to be obvious, little is known about the problems of implementing such a protocol. The intention of this study was to evaluate problems of implementation and to develop strategies to overcome them. Setting A 16-bed surgical ICU of a university teaching hospital with 50 critical care nurses, about 30% in part-time employment.\n\nMethod Over a 7-month period all patients staying longer than 48 hours in the ICU with hyperglycaemia (>150 mg%) on three consecutive measurements were included in the study. These patients were treated according to a protocol at the discretion of the attending nurse. On daily rounds and every 4-5 weeks supervision was performed, and the protocol was modified three times during this period according to staff comments. Further on, medical as well as nonmedical problems of implementation were analysed and discussed. Attitudes and perceived impeding aspects of the implementation process were recorded by means of a questionnaire.\n\nSince insulin sensitivity showed enormous variability, glycaemic control required a high nursing effort. Impeding aspects to titrate blood glucose into the target range were the absence of a nutritional protocol (high carbohydrate intake, despite inflammation/infection leading to hyperglycaemia that was difficult to S55 control) and fear of hypoglycaemia (<60 mg%) leading to low-dose insulin with consecutive hyperglycaemia. Lack of communication (and therefore a loss of information) between critical care nurses and the intensivists and poor acceptance from physicians to leave this field of intensive care medicine to the nurses were additional factors that slowed the implementation process. Conclusion Implementation of protocol-driven medicine requires a high quality of information flow. The lack of linearity between blood glucose and insulin dose (variability of insulin sensitivity) required a sometimes intuitive (experienced) decision to titrate the insulin dose. The conflict of physicians with this new role of critical care nurses might be due to the lack of understanding of the evolution of the nursing profession. Introduction Tight blood glucose (BG) control has been shown to decrease morbidity and mortality in patients in the surgical ICU [1] but is difficult to achieve using standard insulin infusion protocols. We previously evaluated a software model predictive control (MPC) insulin administration algorithm in postcardiac surgery patients [2] . This study investigated the use of an enhanced MPC algorithm (eMPC) in more severely ill patients over 72 hours. Methods Fourteen (seven male) critically ill ventilated medical and surgical patients, mean age 65 years, with an arterial BG > 6.7 mmol/l within 24 hours of ICU admission (RBH) or already receiving insulin infusion, and expected to require mechanical ventilation for more than 72 hours, were treated either with BG control by the standard ICU insulin intravenous infusion protocol [2] or eMPC-advised insulin infusion (n = 6) for 72 hours. The eMPC algorithm, installed on a bedside computer, requires input of current insulin requirements, bodyweight, carbohydrate intake and BG concentration. The algorithm advises the time to next BG sample (up to 4 hours) and the insulin infusion rate, targeted to maintain BG at 4.4-6.1 mmol/l. Patients in the eMPC group had BG measured hourly (for safety) but values were only entered if requested by the algorithm.\n\nThe mean (SD) glucose concentration was significantly lower in the eMPC group (6.0 (0.34) vs 7.1 (0.54) mmol/l, P < 0.001). The mean insulin infusion rate was not significantly different (4.1 (2.7) vs 3.1 (1.8) IU/hour, eMPC vs standard care). BG sampling occurred more frequently in the eMPC group, with a mean of every 1.1 vs 1.9 hours (P < 0.05). No patients in either group had any BG measurements <2.2 mmol/l. Conclusion The eMPC algorithm was effective in maintaining tight BG control in this more severely ill patient group without any episodes of hypoglycaemia (BG < 2.2 mmol/l), but required more frequent BG measurement. Acknowledgements This study is part of the CLINICIP project funded by the EC (6th Framework). The University of Cambridge also received support from EPSRC (GR/S14344/01 Table 1 ). A single BG measurement < 2.2 mM was detected in the MPC group vs 0 in the control group. The sampling frequency was significantly higher in the MPC group. Introduction Evidence is accumulating that tight glucose control improves outcome in critically ill patients. This study was performed to evaluate the effect of lower blood glucose levels in critically ill patients on outcome.\n\nThe unit is a 10-bed closed-format medical-surgical ICU in a general hospital. Starting in 2003 insulin was prescribed to ICU patients using several nurse-driven computerised protocols, each subsequent protocol aiming for lower glucose levels. From February 2004 until May 2005 protocol 1 was used, aiming for glucose between 5.0 and 9.0 mmol/l; from July 2005 until December 2005 protocol 2 was used, aiming for glucose between 4.5 and 7.5 mmol/l. Serum glucose was measured at 6:00 a.m. in all patients from blood derived from arterial lines or venous puncture. The rest of the day blood glucose was measured either using the Glucotouch (protocol 1) or the AccuCheck (protocol 2) devices. To eliminate differences due to these different methods of measurement, only the 6:00 a.m. glucose measurements done by the central laboratory were studied here. Data were derived from ICU and laboratory databases.\n\nResults See Table 1 . The median morning glucose was reduced from 7.5 mmol/l with protocol 1 to 6.8 mmol/l with protocol 2, resulting in small but nonsignificant improvement in outcome. Subgroup analysis focusing on medical or surgical patients or on patients with specific length of stay in the ICU also revealed nonsignificant differences in outcome. Conclusions A small but significant decrease in serum glucose probably results in a small but statistically nonsignificant decrease in mortality and length of stay.\n\nHospital Israelita Albert Einstein, S\u00e3o Paulo, Brazil Critical Care 2007, 11(Suppl 2):P140 (doi: 10.1186/cc5300) Introduction Increased risk of hypoglycemia is the major drawback of strict glycemic control, which has been extensively used in critically ill patients. Fast and precise glucose measurements are therefore mandatory. Our aim was to evaluate the accuracy of two methods of bedside point-of-care testing for glucose measurements using arterial, capillary and venous blood samples in ICU patients. Methods A cross-sectional study with prospective data collection included 86 patients admitted to a 40-bed clinical-surgical ICU of a tertiary care hospital. Results from two different methods of glucose measurement were compared with central laboratory arterial blood measurements: Accu-chek Advantage \u00ae (Roche) arterial, venous and capillary samples; and Precision PCx \u00ae (Abbott) arterial samples. All samples were collected simultaneously. Agreement between measurements was tested with the Bland-Altman method. Results Comparisons between pairs of measurements are shown in Figure 1 . Conclusions The two glucose meters evaluated might not be sufficiently reliable to be used in the ICU setting, especially for patients under strict blood glucose control. Moreover, there are marked differences between the equipment and a decrease in precision if capillary or venous samples are used. Introduction Implementing tight glycaemic control (TGC) in the ICU requires accurate blood glucose (BG) monitoring. We evaluated the performance of two bedside glucometers (GM) in ICU patients. Methods Four hundred and fifty-two arterial blood samples were prospectively analysed in 37 adult ICU patients subjected to TGC. Arterial BG was simultaneously determined using a reference test (ABL \u00ae ) and two GM (Accu-Chek \u00ae and HemoCue \u00ae ). Data were Results Correlation between the reference method and both GM in the overall BG range was reasonable, but not perfect (r 2 \u2265 0.93). This was further underlined by BA analysis (Figures 1 and 2) , showing a bias to overestimate BG with GM. In the TGC range (80-110 mg/dl) correlation was low for both GM (r 2 \u2264 0.66). This was confirmed by BA analysis, demonstrating broad limits of agreement: +14.2 and -26.6 mg/dl for Accu-Chek \u00ae and +5.5 and -31.1 mg/dl for HemoCue \u00ae .\n\nConclusions The accuracy of the tested GM in our ICU patients was insufficient for safe clinical practice. Therefore, to avoid potentially harmful hypoglycaemia, caution is warranted when TGC is implemented exclusively based on BG results obtained by GM. Introduction Bedside capillary or arterial blood glucose monitoring is mandatory for ICU patients under tight glycemic control. Pointof-care methods are based on glucose-oxidase (GO) or glucosedehydrogenase (GD) enzymatic methods whereas the laboratory reference method is hexokinase for measuring the plasma glucose levels.\n\nMethods In this prospective observational study, blood glucose was simultaneously measured on the Glucocard Arkray (GO, capillary), on the Accu-chek Inform Roche (GD, capillary and arterial) and on the Rapid-Lab 1265 Bayer (GO, arterial), and each value was compared with the reference laboratory result. Results A total of 262 matched analyses were done in 60 patients. Biases are defined as the glucose laboratory value minus point-ofcare value. The bias, 95% limits of agreement, and numbers of observed discrepancy (d) paired results >20% and >10% are reported in Table 1 .\n\nConclusions GO methods underestimate while GD methods overestimate all blood glucose levels as compared with plasma glucose levels measured by the reference method of hexokinase. Capillary methods have wider 95% limits of agreement than measures carried out on arterial blood.\n\nContinuous glucose monitoring for intensive care patients using whole blood microdialysis Introduction The objective of this study was to investigate whether continuous glucose monitoring for intensive care patients could be implemented using blood microdialysis (MD) as tight glycaemic control reduces mortality and morbidity of critically ill patients. Currently investigated is whether the subcutaneous tissue is an adequate and representative site for glucose monitoring. We have designed and tested a novel system that allows continuous measurement of glucose concentration in whole blood based on MD. Methods Na-heparin is pumped to the tip of a double lumen catheter and the blood-heparin mixture is withdrawn continuously at a mixing ratio of 1:1 at a flow of 4 ml/hour. The blood-heparin mixture is microdialysed in a planar flow-through MD unit and is discarded thereafter. The dialysate is collected and analysed for glucose concentration via Beckman analysis and referred to venous blood samples taken from the reference arm. Eight healthy volunteers underwent a 12-hour investigation including an OGTT. Glucose readings from dialysate and venous blood were obtained in a 15-30 minute interval. Results All eight subjects successfully completed the 12-hour trial. The coefficient of correlation between continuously withdrawn microdialysed blood and venously taken reference blood samples was r = 0.9834 (0.9753-0.9958). The Clark Error Grid Analysis (EGA) revealed that 99.5% of all data pairs are in the A range (220 of 221). Applying the novel Insulin Titration EGA yielded in 100% of data pairs the 'acceptable treatment' area. Conclusions Blood MD based on continuous blood withdrawal and extracorporeal MD is a promising approach to obtain dialysate reliably, safely and continuously for long-term determination of blood glucose concentration with online sensors. The correlation between glucose concentration of dialysate and reference venous blood samples is excellent. The patency of the double lumen catheter in the current form could be improved by introducing Available online http://ccforum.com/supplements/11/S2 Introduction Early adequate nutritional support (NS) in critically ill patients can improve clinical outcome [1] . Although enteral nutrition is considered the best method, it carries a risk for developing ventilator-associated pneumonia, particularly if patients are nursed horizontally [1] . The aim of this prospective audit is to assess the compliance of nutritional practise in our ICU with some aspects of recommendations of Canadian Clinical Guidelines. Method Demographic data, head elevation (HE) from the horizontal position, daily nitrogen and calorie intake were recorded. Daily recommended calorie requirements were calculated according to body weights on admission. The nasogastric tube size and the gastric residual volume threshold (GRVT) before abandoning enteral feeds were also recorded. Results During 2 months 55 patients (male 47%, female 53%) were admitted, including 47 (85%) emergency admissions. Thirtythree (60%) patients stayed in the unit for >48 hours with an average stay of 7.1 days. Thirty-two (58%) patients received NS, and 26 (81%) of these were within 48 hours of admission. Enteral and parenteral routes were used in 26 (81%) and six (18%) patients, respectively. In five (15%) patients both methods were used during the change of route of administration. The daily calorie intake expressed as a percentage of the recommended intake is presented in Table 1 . HE was more than 300 in 70% of the 570 measurements. Blood sugar was between 6.3 and 6.9 mmol/l. Gastro Prokinetics was used in 80%. There was no feeding protocol in the unit and low GRVTs were used before abandonment of the enteral regime. Results Thirty-one patients were included. Figure 1 shows the PDR of all patients under the different prokinetic drugs. Table 1 presents the average percentage of GE improvement over BM with different therapies, and the average improvement of individuals' best combination. Comparing 20 patients, BM first, with 11 at different timings, revealed no difference of baseline or best combination (P = 0.1, P = 0.2, respectively). Conclusions In this population: 1. Metoclopramide is poor in improving GE. 2. The combination of metoclopramide and continuous erythromycin is the most effective. 3. The BreathID is a convenient and novel way to monitor GE in order to study and individually tailor the most effective (up to 85% over BM) prokinetic combination. Introduction For critically ill patients unable to eat, enteral tube feeding (ETF) is the preferred mode of feeding. The study aimed to investigate the amount of enteral feed obtained by patients on ICU in a busy London Teaching Hospital, the efficiency of initiation of feeding, and possible reasons for the failure of the above. Methods A prospective observational study was carried out over 1 month on patients admitted to a general and cardiothoracic ICU, who received ETF. Baseline data including age, reason for admission and illness severity score (SOFA) were documented. Length of time from admission to start of feeding was noted, and the volume of feed delivered to patients was recorded. The quantity of calories delivered to the patient was compared with the patient's ideal nutritional requirement (determined by the ICU ETF protocol). Feeding interruptions were also recorded.\n\nResults Fifty-two patients receiving ETF were observed for a total of 7,349 hours: 67.3% of patients were surgical and 32.7% medical. Patients received a median of 75% of their ideal calorific requirement. Feeding was started a median of 15 hours after admission, and a median of 5% of feeding time was interrupted after ETF had been started. Reasons for interruption included high gastric aspirates, starvation for procedures and displacement/ blockage of feeding tube. The time to start ETF was significantly different according to admission categories (P = 0.033), with abdominal and cardiothoracic surgical patients having the greatest delays. Abdominal surgical patients also had a higher proportion of feeding interruptions due to high gastric aspirates and starvation for procedures. The SOFA score on day 1 significantly correlated with the time taken to start feeding (P = 0.008), length of total feeding interruption (P = 0.012), length of feeding interruption due to high gastric aspirates (P = 0.043), and length of feeding interruption due to starvation for procedures (P = 0.026).\n\nConclusion The majority of patients received a high proportion of their ideal calorific requirement and began feeding within 24 hours.\n\nThe data indicate that patients having had abdominal surgery or the sickest patients may be may be more likely to experience delays in initiation and interruptions to feeding. Introduction Clostridium difficile associated disease (CDAD) is recognized as a major cause of morbidity and mortality among patients in hospital. There have been reported associations between the use of proton pump inhibitors (PPIs) and CDAD in community and hospital settings [1, 2] . The aim of this study was to investigate the effect of introducing PPI prophylaxis in critically ill patients on the incidence of CDAD. Objective A retrospective study to assess the incidence and causal factors associated with long-term dysphagia following intensive care discharge.\n\nMethods A questionnaire was sent out 4 months post ICU discharge to 193 intensive care patients (Level 3 care with a stay of over 48 hours). We reviewed the case notes of those patients who reported swallowing difficulties to establish whether they had undergone, had any characteristics of or received therapies potentially associated with dysphagia.\n\nWe had a 50% response rate to our questionnaire. An overall dysphagia post ICU stay rate of 19.5% was observed. Fever and age over 65 were both common findings as one may expect and showed the highest association with subsequent dysphagia. We did not find any suggestion of a relationship between changing tracheostomy (suggesting repeat procedures) and subsequent difficulty swallowing. One patient within this group subsequently developed a tracheal stenosis. See Table 1 .\n\nConclusion We found the percentage of patients reporting swallowing difficulties post percutaneous tracheostomy (PCT) (Portex Blue Line Ultra tracheostomy tube) to be higher than one would expect. This may be confounded by neurological injury necessitating the need for a PCT, but we feel this may be an area of concern meriting further investigation given frequent PCT in ICU practice.\n\nIntestinal corticotropin-releasing factor is decreased in shocked trauma patients and may affect gut function \n\nThe reasons for the typical bowel dysfunction following traumatic injury are unclear. Corticotropin-releasing factor (CRF) in peripheral blood/tissue may induce intestinal barrier dysfunction via receptor-mediated mechanisms independently of the hypothalamic-pituitary-adrenal axis. This mechanism seems to involve interactions of CRF with enteric nerves and mast cells, which results in increased gut intercellular tight junction permeability to macromolecules, as well as increased epithelial cell apoptosis leading to loss of mucosal integrity. We investigated whether blood and intestinal tissue CRF is associated with postoperative gut dysfunction in shock. Methods CRF analysis was performed on full-thickness bowel specimens obtained from shocked trauma patients requiring emergency abdominal surgery for penetrating injury, and from patients undergoing small bowel resection during elective bowel surgery. Venous blood was taken before anaesthesia, intraoperatively and on postoperative day 1. CRF extracted from tissue and blood was quantified using radioimmunoassay. On day 1 postoperatively, intestinal permeability was tested by urinary lactulose:mannitol (L:M) measurement. Institutional ethical approval was granted and patients gave written informed consent. Results Trauma patients (n = 6, male/female = 6/0, age 27 \u00b1 10.2 years, ISS 23 \u00b1 6.8) were younger than elective patients (n = 6, male/female = 4/2, age 52.8 \u00b1 7.7 years, P < 0.0006), and had significantly lower mean tissue [CRF] (0.034 \u00b1 0.015 x 10 -3 % total protein (TP)) than elective patients (0.117 \u00b1 0.075 x 10 -3 %TP, P = 0.023). The median (IQR) intraoperative blood CRF level was higher in trauma patients (86.7 (5.5) pg/ml vs 59.8 (9.6) pg/ml, P = 0.03) than elective patients. In trauma patients this correlated negatively with postoperative L:M (r = -0.9, P = 0.037), although intestinal permeability was greatly and equally increased in both groups (combined mean \u00b1 SD L:M, 0.58 \u00b1 0.55).\n\nConclusions CRF is detectable in bowel tissue following trauma and is significantly lower in trauma vs elective surgery patients, while CRF in blood may be a factor associated with gut barrier changes following shock and emergency laparotomy. \n\n(EN). In the ICU, however, the use of sedation, 24-hour feeding and proton pump inhibitors can make the standard confirmatory methods recommended by the UK National Patient Safety Agency (UKNPSA) [1] unreliable, and result in the need for multiple chest X-rays (CXR), increased cost and feeding delay. We studied the role of the Cortrak \u00ae (Viasys MedSystems, USA) against standard practice, and assessed the following outcomes: time required to confirm correct NGT position, time to starting feeding, and potential cost savings.\n\nMethods We enrolled patients admitted to our ICU who required NGT insertion for EN. A Corflow NGT with Cortrak stylet was inserted, and the position monitored using the Cortrak magnetic sensor. The Cortrak system tracks the trail of the stylet tip as it progresses towards the stomach and provides a visual representation of the NGT position on a video screen, allowing rapid determination of insertion success. The position of the NGT was also assessed using pH paper and/or CXR, as appropriate, according to the standard UKNPSA guidelines. Data were analysed using a paired t test and time-to-event analysis.\n\nResults Fifty-two patients were recruited and 57 insertions were analysed: 32 first insertions, and 25 reinsertions. Gastric content was successfully aspirated in 40% (23/57) of insertions, and 14% (8/57) had pH 6 or above. Forty-six CXRs were requested, with three patients requiring multiple CXRs. The Cortrak correctly confirmed the position of NGT in all 57 insertions. The time required to confirm the NGT position was significantly less with the Cortrak than with conventional methods (mean \u00b1 SE Cortrak: 9.6 \u00b1 1.7 min; pH paper: 11.6 \u00b1 1.7 min; CXR: 122 \u00b1 23 min; P < 0.0001). There was a 1. Background The aim of this study was to determine the effect of propranolol on infections and clinical parameters during the acute phase postburn. Severe thermal injury is followed by a period of hypermetabolism that is directly proportional to the size of insult sustained. Infection and multiorgan failure are now the leading cause of death from severe thermal injuries. Propranolol, an anticatabolic agent, improves hypermetabolism postburn. However, there is evidence that propranolol worsens immune function and increases the incidence of infection in critically ill patients. Methods Sixty-six patients with burns >40% total body surface area were enrolled into the study and randomized to receive standard burn care (controls, n = 33) or standard burn plus propranolol for more than 21 days (propranolol, 0.5-1.5 mg/kg every 6 hours, n = 33). Biopsies were taken three times a week for microbiological determination. Clinical parameters were collected and blood was drawn at regular intervals throughout the hospital course and analyzed for IGF-1, IGFBP-3, and HGH. Patients underwent weekly resting energy-expenditure measurements. Statistical analysis was performed using analysis of variance with Bonferoni's correction and Student's t test where applicable. Results Propranolol treatment reduced heart rates by 10% and significantly improved stroke volume throughout the acute hospital stay compared with controls (P < 0.05). Resting energy expenditure was significantly decreased in the propranolol group when compared with controls at discharge (P < 0.05). Infection rates on admission were the same for both groups (17% propranolol vs 22% control). The incidence of infection throughout hospital course was significantly lower in the propranolol group (60%) compared with controls (87%) (P < 0.05). Propranolol significantly increased IGF-I, IGFBP-3, GH, and prealbumin, while it significantly decreased CRP and fatty acids (P < 0.05).\n\nConclusions Following a severe burn, propranolol attenuates infections, inflammatory markers and fatty acid levels while improving cardiac work and endogenous anabolic hormone levels.\n\nWe suggest that propranolol is a safe and efficacious modulator of the postburn response.\n\nThe study investigates the effect of early enteral immunonutrition on patient recovery after extensive elective surgery in the upper abdomen [1, 2] . It investigates the speed of patient recovery administering early enteral immunonutrition combined with total parenteral nutrition [3] .\n\nThe total of 40 patients who had undergone this type of surgery were involved in this study. Near the end of the surgery procedure a percutaneous jejunostomy was performed in 20 patients (G1), and enteral nutrition started on the first postoperative day with small doses of immunonutrient (Reconvan) 10 ml/hour. After every 12 hours the tolerance was estimated (abdominal distension, diarrhoea, vomiting). After every 24 hours the immunonutrient dose was increased by 20 ml/hour until we reached the maximum of 80 ml/hour. In the first three postoperative days the patients were also administered total parenteral nutrition, and after that only enteral nutrition. The other group of 20 patients (G2) was administered only parenteral nutrition from the first postoperative day. Preoperatively, every patient was measured for body weight, body height and body mass index, and using laboratory tests we established the levels of albumin, transferine, blood urea nitrogen and creatinine. On the third and ninth postoperative days we repeated the same laboratory tests, and measured the daily loss of nitrogen by excretion of urea in urine. Results and discussion Patient recovery was faster in G1. The average patient stay in ICU was 5 \u00b1 1 days (G1) vs 10 \u00b1 2 days (G2). The average hospital stay was 22 \u00b1 3 days (G1) vs 29 \u00b1 5 days. Peristalsis was detected on the third day as an average (G1) vs 4.5 days (G2). A decrease in pulmonary complications was achieved in G1 (one pleural effusion) vs G2 (eight pleural effusions). Laboratory tests show that patients in G1 are in lower catabolism compared with G2 patients. Conclusion Early enteral immunonutrition through jejunostomy is an efficient and safe method of patient nutrition with fewer postoperative complications, and also accounts for a hospital cost decrease of 50%. Introduction A large randomized trial is needed to evaluate the safety and efficacy of glutamine (GLN) and antioxidant (AOX) supplements. However, high doses of such nutrients via enteral and parenteral routes early in the course of critical illness is often interrupted by high illness acuity and other treatment priorities. The purpose of this pilot trial was to evaluate the feasibility of delivering high-dose GLN and AOX supplements early on in the course of critical illness, and to estimate recruitment for the larger REDOXS study.\n\nMethods In six Canadian centers, using a 2 x 2 factorial design, we randomized mechanically ventilated adults who had two or more organ failures within 24 hours of ICU admission to one of four groups: (1) GLN (0.35 g/kg/day i.v. plus 30 g enterally), (2) AOX (500 \u00b5g selenium i.v. and 300 \u00b5g selenium, 20 mg zinc, 10 mg \u03b2-carotene, 500 mg vitamin E, and 1,500 mg vitamin C enterally), (3) both AOX + GLN, and (4) placebo. Supplementation was continued for a minimum of 5 days up to 28 days and was provided independent of nutrition support. We recorded the time from ICU admission to randomization, the time to start of supplements and nutrition support parameters. Results From April 2005 to April 2006, 80 patients were randomized (average 2.1/site/month). The median time from ICU admission to randomization was 18.2 hours (range 11.6-21.1 hours). All patients received parenteral supplements, the median (range) time to start was 2.7 hours (2.0-3.8 hours) and 78/80 (98%) received enteral supplements with a median (range) of 2.6 hours (1.9-4.5 hours) from randomization. The mean duration of supplements was 11.1 days (enteral) and 12.2 days (parenteral). The mean volumes of enteral and parenteral supplements received were 84% (range 45-102%) and 93% (range 54-100%) prescribed volumes, respectively. The average prescribed energy and protein intakes were 1,802 kcal/day and 86 g protein/day but the average daily percentage energy and protein received from nutrition support was only 65% (range 4-95%) and 62% (range 2-97%) of that prescribed, respectively. Conclusion In critically ill patients with organ failure we provided adequate amounts of study supplements via both enteral and parenteral routes in the early phases of acute illness, independent of nutrition support. We estimated recruitment of at least two patients/site/month for our future trial.\n\nIntroduction Standard selenium (Se) substitution (30-75 \u00b5g/day; 0.4-0.9 \u00b5mol/l) in the critically ill is not sufficient for a sustained plasma level (0.58-1.82 \u00b5mol/l; 46-143 \u00b5g/l). Standard Se substitution keeps the plasma level in the range 0.28-0.42 \u00b5mol/l. High-dose Se substitution correlated with a decrease in mortality of patients with SIRS. The influence of high-dose substitution on selected parameters, MAP and mortality in the critically ill were evaluated in a prospective randomized clinical trial.\n\nMethods One hundred and twenty-three patients (78 males, 45 females, median age 62.7 and 60 years, respectively) were randomized into group A (SOFA 19.27) and group B (SOFA 10.23). Group A received standard Se substitution: 30-75 \u00b5g NaSelenite i.v./day. Group B received high-dose Se substitution according to a protocol: 1,000 \u00b5g at day 1, followed with 500 \u00b5g at days 2-14 of NaSelenite i.v. The plasma levels of Se, prealbumin, albumin, CRP, PCT, cholesterol, gluthathionperoxidase GSHPx, D-dimer, creatinine clearance and leucocytes were examined daily. MAP trends and 28-day mortality were evaluated as clinical markers.\n\nThe Se plasma level was significantly higher in high-dose Se-substituted patients (0.56 \u00b5mol/l vs 0.88 \u00b5mol/l, P < 0.001). GSHPx was significantly higher in high-dose Se-substituted patients (4,864 U/l vs 6,097 U/l, P < 0.001). No significant differences were found in the level of albumin, prealbumin, CRP, PCT, leucocytes, fibrinogen, cholesterol, D-dimer and creatinine clearance and MAP. The 28-day mortality was lower in a high-dose Se-substituted patients (33% vs 37%), but not significantly. Conclusion The critically ill have an increased demand for Se, which is essential for synthesis of Se enzymes and Se proteins. The increased demand for Se is not covered by standard substitution. High-dose Se substitution (500-1,000 \u00b5g/day) normalizes its plasma level and increases the GSHPx plasma level. High-dose Se substitution has no adverse reactions. The decrease of 28-day mortality in high-dose Se-substituted patients is not significant. The trial on high-dose Se substitution further continues. Introduction Nutrition therapy is an integrant aspect of ICU support and can influence outcomes. A delay to starting nutrition support after 24 hours of ICU admission is associated with increased morbidity and mortality [1] , and certain lipid emulsions can exacerbate the inflammatory cascade. For an appropriate evaluation of the impact of these and other recent research findings, information regarding the use of parenteral nutrition (PN) in the ICU is needed. Methods This is the interim analysis of a multicenter, prospective, cohort study aimed to obtain information regarding the use of PN. Data were collected during 3 months from ICU patients over 18 years of age on the use of PN in 20 adult ICUs in Brazil using a web-based clinical research form.\n\nOne hundred and sixty-six patients were included in this analysis. Among the main results, 63.69% were males and 77.78% were considered malnourished. The mean SOFA score was 6.21, with a mean APACHE II score of 19.39. In total, 97.23% of the PN used in Brazil were manufactured by third-party companies and this was associated with a significant delay in the beginning of the infusion (median time 29.76 hours), and elevated in-ICU (50%) and inhospital (55.17%) mortality rates. A total 24.29% of the patients were immunosuppressed. The most used lipid source was long-chain triglycerides/medium-chain triglycerides (80.69%).\n\nConclusions The use of PN in Brazil is associated with a significant delay in the start of infusion and high mortality rates. The most used lipid emulsion (long-chain triglycerides/medium-chain triglycerides) has been associated with more apoptosis [2] and compromised lymphocyte proliferation [3] . The overall findings of these study indicate that strategies to reduce the delay in start of PN and the use of better lipid sources must be adopted to provide better assistance for patients in need of PN in Brazil.\n\nIntroduction Ultrasound (US) significantly facilitates central venous catheterization, reducing the percentage of failure, the percentage of accidental arterial puncture, and the percentage of complications (haematoma, haemothorax, pneumothorax). Nonetheless, it is not clear whether US guidance (USG) (so-called 'dynamic' or 'real-time' US techniques: that is, venipuncture under direct US control) may be better than US assistance (USA) (socalled 'static' or 'indirect' US techniques: that is, US imaging of the vein, with or without skin marking, and then blind venipuncture). Methods From February 2005 to September 2006, our CVC Team adopted the following protocol for internal jugular vein (IJV) catheterization: (a) both IJVs were evaluated to assess position, dimensions, and other features known to affect the risk of catheterization; (b) then, a decision was made whether to continue with USA or USG; (c) the IJV was accessed via the low lateral Jernigan approach; (d) after two failed USA attempts, USG venipuncture was adopted; (d) when IJVs were not available, USG venipuncture of other central veins was the second choice; and (e) fluoroscopy was used only in paediatric patients, but all patients had a postoperative chest X-ray to rule out pneumothorax and malposition. Results In 20 months, 821 central venous catheters (CVCs) were inserted in adults (181 short-term CVC + 218 tunnelled + 316 ports) and in paediatric patients (age range 20 days-13 years, average 5.5 years: 20 short-term + 84 tunnelled + two ports). In adults, the procedure started as USA in 522 and as USG in 299 cases: a shift from USA to USG was necessary in 8%. USG was the first choice in all paediatric cases. The IJV was successfully cannulated in most adult patients, with very few exceptions (innominate vein in 12 cases, axillary vein in two cases, femoral vein in one case, all by USG). In one paediatric patient, the CVC was inserted in the subclavian vein, via a supraclavicular USG approach. Complications were: failure 0%; pneumothorax 0%; haemothorax 0%; accidental arterial puncture 1.1% (1.7% USA vs 0.3% USG); haematoma 0.4% (only for USA); malposition (0.8%, exclusively with the left IJV). Conclusion In conclusion, (a) we had a minimal incidence of complications, (b) USG was associated with a relevant reduction of the risk of accidental arterial puncture and haematoma, if compared with USA, and (c) choosing the left IJV was associated with a higher risk of malposition.\n\nCatholic University Hospital, Roma, Italy Critical Care 2007, 11(Suppl 2):P159 (doi: 10.1186/cc5319) Introduction In the critically ill, a reliable peripheral or central venous access is of paramount importance. Nonetheless, access may be difficult or may carry a significant risk of complications (pneumothorax, central line infection, etc.). Peripherally inserted venous catheters -either central (PICC) or peripheral (midline catheters (MC)) -are associated with a low risk of catheter-related bacteremia; also, using the ultrasound guidance and the microintroducer technique (UG + MIT), they can be inserted in any patient, regardless of the availability of superficial veins. We have reviewed our experience of 56 peripherally inserted catheters in 53 patients in different ICUs (surgical ICU, trauma unit, coronary unit, neurosurgical ICU, stroke unit, pediatric ICU, etc.); all catheters were positioned at the mid-arm, in the basilic vein or in the brachial veins, using UG + MIT. We assessed the feasibility of this technique in the acutely ill and the rate of complications.\n\nWe inserted 16 PICC and 40 MC in patients requiring prolonged venous access (estimated >15 days); nine were septic, six had coagulopathy, 21 had tracheostomy. We used both silicone and polyurethane 4 Fr catheters. Procedures were performed by a team of trained physicians and nurses. Catheter insertion was easy in most cases, and immediate complications were few (no failure; one hematoma; no arterial or nerve injury). Late complications were: one local infection; three thrombosis (two requiring removal); four cases of damage of the external catheter (due to poor nursing or to inappropriate use of the catheter during rx procedures), all easily repaired; one dislocation; no catheter occlusion; no catheter-related bacteremia. Most catheters stayed in place for a prolonged time (range 9-65 days, median 19 days); only three were removed because of complications. Conclusion Our experience with PICC and MC was characterized by an extremely low rate of infective and thrombotic complications. Venous access was achieved in any patient, even with limited availability of peripheral veins. The use of US-inserted PICC and MC should be considered when central access is not advisable or is contraindicated. Introduction Ultrasound has been introduced in the insertion of central venous lines to reduce the complications associated with the conventional landmark technique [1] . We compared both techniques; we noted the number of attempts, the duration of insertion and complications. Methods Thirty patients were randomly selected, from the operating theatre and ICU, who required placement of a central venous catheter. The central venous catheter placement was performed by two experienced anaesthetists with more than 6 years experience in anaesthesia and intensive care. In 15 patients the internal jugular venous catheter placement was performed using the external anatomical landmark technique, and in the other 15 patients the placement was under ultrasound guidance. The duration of insertion was recorded from the moment the needle touched the skin until insertion of the catheter and removal of the guide wire. The numbers of attempts as well as immediate or delayed complications were recorded. Results The central venous catheter placement was successfully performed from the first attempt in both groups. There were no immediate or delayed complications noted; however, the mean time of insertion was longer in the ultrasound-guided group (4.55 min) compared with the external anatomical landmark group (2 min) ( Figure 1 ). Discussion Some studies have been designed to evaluate ultrasound-guided central venous catheter placement compared with the conventional method based on external anatomical landmarks. These studies demonstrated the superiority of ultrasound-guided central venous line placement over the external anatomical landmark technique. However, there was no time gain demonstrated in ultrasound-guided placement [2] . On the other hand, a number of studies have expressed several reservations concerning the systematic use of ultrasound guidance for central line placement [3] . In our patients we found that the use of ultrasound neither altered the rate of complication nor the number of attempts in central venous catheter placement. Also the duration of placement of the central line catheter using the external We recently demonstrated in isolated blood perfused rat lungs subjected to i.t. LPS-induced pulmonary oedema that the continuous measurement of inspiratory resistive work is a good indirect indicator of progressive lung oedema [1] . Here we extend these findings to two other types of pulmonary oedema: hydrostatic oedema induced by elevation of the left atrial pressure, and alveolar oedema (ALV) by infusing normal saline into the trachea at two different infusion rates (2 and 4 ml/hour for 120 min). See Table 1 . Our results indicate that the continuous measurement of inspiratory resistive work is a good indicator of both permeability and hydrostatic lung oedema, but not of pure alveolar oedema (absence of interstitial oedema \n\nThis study aimed to establish the prevalence of home ventilatory and respiratory support within the catchment area of Frimley Park Hospital in Surrey. The number of patients receiving respiratory support at home has been increasing nationally since 1990 [1] ; however, no local data exist. This trend is likely to continue as domiciliary ventilation gains popularity for the treatment of obstructive sleep apnoea and certain groups of COPD patients [2] . Method A postal survey was sent out to practice managers in the local catchment area. They were asked to provide data for: patients on home ventilatory support for any reason, patients with long-term tracheostomies, patients with COPD who are on home oxygen or who you would classify as end stage, and the total number of patients registered to the practice. This was followed up with a telephone call approximately 2 weeks later. Many were then emailed the same questionnaire. A further two telephone calls to each practice were made as necessary in order to obtain data. Results Out of 67 surgeries contacted, we achieved a response rate of 65%. Thirty-three practices (49%) were able to provide complete data, six (9%) provided partial data, and a further five (7%) were unable or unwilling to provide any data. Twenty-three (34%) practices did not respond. A total of 318,130 patients were listed by the responding practices. Of these: 23 patients live with long-term tracheostomies, a prevalence of 1 in 13,800; 65 patients receive mechanical respiratory support at home, a prevalence of 1 in 4,900; and 207 patients receive oxygen therapy at home, a prevalence of 1 in 1,500. Discussion This study suggests that the Frimley Park Hospital population of 350,000 currently contains about 100 individuals requiring mechanical respiratory support at home. This is of concern as currently there is no formal support for any of these high-risk patients other than ventilator maintenance. Simple problems precipitate hospital admission and rapidly trigger outreach or intensive care review. The current position is clearly unsatisfactory and must be addressed by PCTs if patient numbers increase. Results The frequency of MV was 30%; the overall and specific mortality rates were 15% and 50%, respectively. The mean (\u00b1SD) age was 57 \u00b1 21 years; 52% were males; the mean APACHE II score was 22.2 \u00b1 8.2; 69% were medical patients; the mean duration of MV was 11 \u00b1 7.9 days; 93% were on invasive MV. A multivariable analysis was performed to identify the variables associated with death. These included sepsis (P = 0.02), MV duration (P < 0.001), renal failure (P = 0.006) prior to MV, and the following variables that occurred during the MV period: sepsis (P = 0.004), acute lung injury/acute respiratory distress syndrome (P = 0.001), renal failure (P < 0.001), haematological failure (P = 0.02) and vasoactive drug use (P < 0.001). It should be noted that selected ventilatory monitored variables were included in the multivariate model. However, they were not associated with mortality in our study sample.\n\nOur results indicate a frequency of patients on MV of 30% with an elevated specific mortality rate (50%). Sepsis, MV duration, renal failure prior to MV, and sepsis, acute lung injury/acute respiratory distress syndrome, renal failure, haematological failure and vasoactive drug use during the MV period are risk factors for mortality in 28 days after starting MV. Identification of these factors may allow early interventions to attempt to mitigate these poor outcomes.\n\nThe Background Hypercapnia and elevated intraabdominal pressure from carbon dioxide (CO 2 ) pneumoperitoneum can adversely affect respiratory mechanics and arterial blood gases. We tested the hypothesis that adaptive pressure ventilation-synchronised intermittent mandatory ventilation (APV-SIMV) may provide better pulmonary mechanics, CO 2 homeostasis and pulmonary gas exchanges with less frequent ventilatory settings (tidal volume (TV), respiratory rate (RR)) and lower peak inspiratory pressure (P peak ) and plateau pressure (P plat ) than pressure-controlled synchronised intermittent mandatory ventilation (P-SIMV) in patients undergoing laparocopic cholecystectomy (LP). Method The study group consisted of 40 patients (APV-SIMV n = 20, P-SIMV n = 20). LP was performed under total intravenous anesthesia. After induction of anesthesia, a RR of 12 breaths/ minute, and an inspiratory:expiratory rate of 1:2 and PEEP of 6 cmH 2 O were set for both groups. APV-SIMV was started with a target TV of 8 ml/kg. P-SIMV was started with the inspiratory pressure (P ins ) that will provide 8 ml/kg TV. The settings were changed until target parameters to maintain normocapnia and normoxia were achieved (ETCO 2 30-35 mmHg, PaCO 2 35-45 mmHg and SaO 2 >90%). When the target parameters could not be achieved, the first RR was increased by 2 breaths/ minute up to 16 breaths/minute, then the volume or pressure was titrated to induce 1 ml/kg increases in TV up to 10 ml/kg. The initial FiO 2 was set to 50%. FiO 2 was increased with increments when the SaO 2 fell below 90%. PaO 2 /FiO 2 , static compliance, VD/VT, P peak and P plat , ETCO 2 , inspiratory and expiratory resistances, and arterial blood gas analysis were recorded before, during and after pneumoperitoneum. Statistical analysis were carried out using the chi-square test, paired test and independent samples test when appropriate.\n\nResults Demographic data were similar between groups. Pneumoperitoneum caused significant decreases in static compliance and arterial pH, and increases in P peak and P plat , VD/VT and ETCO 2 in both groups. However, APV-SIMV resulted in fewer setting changes, lower peak and plateau pressures, VD/VT, and ETCO 2 levels when compared with P-SIMV (P < 0.025).\n\nConclusion APV-SIMV may provide better results then conventional P-SIMV in patients undergoing LP.\n\nThe influence of cycling-off criteria and pressure support slope on the respiratory and hemodynamic variables in intensive care unit patients \n\nThe clinical use of ventilators is limited due to a huge variety of different ventilation methods. The clinician -often under high cognitive load from the complicated technical equipment on an ICU -just uses a small subset of available parameter settings. The aim of the present study was to develop a closed-loop ventilation controller based on mathematical models and fuzzy logic.\n\nThe system was designed to track a desired end-tidal CO 2 pressure (PaCO 2 ), to find a PEEP leading to maximum estimated respiratory system compliance and to maintain the arterial oxygen saturation (SaO 2 ) at an optimal level. We developed a program in LabView (National Instruments, Austin, TX, USA) on a laptop that is able to read the internal data of a ventilator (Evita 4; Dr\u00e4ger Medical, Germany) in real time. Respiratory signals (for example, SaO 2 ) are acquired from monitoring. Discrete measurements (for example, PaO 2 ) are either assumed constant until next measurement or are interpolated using a model-based approach evaluating, for example, the etCO 2 data. The course of etCO 2 following the setting of optimal frequency was evaluated to calculate the time required for equilibration of etCO 2 .\n\nResults A module automating the initial settings of the ventilator according to local ICU rules is realized. Modules were added that optimize breathing frequency with respect to PaCO 2 /etCO 2 and FiO 2 according to SO 2 whenever no PaO 2 is available. A lung simulator (Michigan Instruments Inc., Grand Rapids, MI, USA) connected with the LS4000 (Dr\u00e4ger Medical) was used to evaluate the system. Exemplary results are presented in the figures, which show the minute volume/etCO 2 relationship ( Figure 1 ) and a parametric fit of etCO 2 data ( Figure 2 ). The adjustment of the frequency is based on the current etCO 2 model. Conclusion Automation is a 'sine qua non' to achieve optimal patient individualized ventilation support. Our system is enabled to evaluate a therapeutic strategy and to base the settings of the ventilator on current trends/drifts observed in the data.\n\nThe Conclusion NIPPV was not associated with better outcome in our population of hematological patients with acute respiratory failure. NIPPV followed by IPPV was an independent predictor of mortality. Introduction The rapid shallow breathing index (RSBI) is the ratio determined by the frequency (f) divided by the tidal volume (VT). An RSBI <105 has been widely accepted by healthcare professionals as a criteria for weaning to extubation and has been integrated into most mechanical ventilation weaning protocols. We hypothesized that the converse of using the RSBI for weaning might be useful in predicting the need for noninvasive ventilation. (Figure 1 ). The CO2SMO Plus! with the ETCO 2 /flow sensor was used for obtaining bedside measurements. Patients would breathe through the ETCO 2 /flow sensor for 60 seconds with nose clips.\n\nThe threshold value for RSBI that discriminated best between no NIV and the need for NIV was determined in 61 patients. Thirty-five patients who did not require ventilatory support had a mean RSBI of 105, and 26 patients with NIV had a mean RSBI of 222 (P = 0.0001). A receiver-operating-characteristic curve was constructed based upon the dataset in increments of 10 for the RSBI (Figure 2 ). An RSBI > 120 yielded a sensitivity of 0.81 and a specificity of 0.74 for determining the need for NIV. A likelihood ratio positive (LR+) of 3.14 further illustrates the formidable predictive value of the 120 RSBI. , low tidal volume ventilation (TV = 6 ml/kg) [2] , prophylaxis for stress ulcer (SUP) [3] , and deep vein thrombosis (DVTP) [4] . (Table 1) . On average, low tidal volume ventilation was adopted. DVTP and SUP were well implemented without training. There was no effect on frequency of pneumonia, ICU length of stay, or survival. Introduction The aim of the study was to compare the combination of intermittent mandatory ventilation plus pressure-support ventilation (SIMV+PSV) with intermittent trials of spontaneous breathing (ITSB) using a T-tube as two methods of weaning in a surgical ICU. Methods A total of 104 patients who had been ventilated for more than 48 hours in the postoperative period from October 2005 to October 2006 were enrolled in the study. After fulfilling the weaning checklist they were randomly assigned into two groups: SIMV+PSV group (n = 53), and ITSB group (n = 51). In patients assigned to the SIMV+PSV group, the ventilator rate was initially set at 6-8 breaths/minute plus PSV of 15 cmH 2 O and then both reduced, if possible, by 2 breaths/minute and 2 cmH 2 O each time.\n\nPatients able to maintain adequate ventilation with SIMV of 2 breaths/minute and PSV of 5 cmH 2 O for at least 2 hours without signs of distress were extubated. Patients assigned to the ITSB group were disconnected from the ventilator and allowed to breathe spontaneously through a T-tube circuit. The duration of the trials was gradually increased. Between the trials, assist-control ventilation was provided for at least 1 hour. Patients able to breathe on their own for at least 2 hours without signs of distress were extubated. Results Until the first attempt was made for weaning, all patients received assist-control ventilation because of haemodynamic instability. The following underlying conditions were present: chronic obstructive pulmonary disease in 67 patients, neuromuscular disorders in nine patients, acute lung injury as a result of surgery in 14 patients, asthma in six patients and miscellaneous causes in eight patients. The duration of mechanical ventilation before weaning was 2.5 \u00b1 0.5 days in the SIMV+PSV group vs 2.4 \u00b1 0.4 days in the ITSB group (P = 0.02) and the duration of weaning was 6.2 \u00b1 0.23 hours vs 8.3 \u00b1 0.44 hours in the two groups, respectively (P < 0.01). Patients who remained extubated for 48 hours were classified as having successful extubation -the rate of successful extubation in the first 24 hours of starting weaning was higher for the SIMV group (79.2%) than in the ITSB group (64.7%, P < 0.01). The total duration of mechanical ventilation was 3.3 \u00b1 0.3 days vs 5.2 \u00b1 1.1 days and the ICU length of stay was 5.6 \u00b1 1 days vs 7.5 \u00b1 1.7 days in the two groups, respectively (P < 0.01).\n\nConclusions The use of SIMV+PSV as a weaning method in the surgical ICU lead to shorter duration of weaning, a higher rate of successful extubation, a shorter duration of mechanical ventilation and less ICU stay than the use of ITSB.\n\nPredicting successful weaning in a cohort of elderly patients Introduction Acute illness adversely affects a patient's circadian rhythms. Minimising the delayed restoration of these rhythms may have patient benefits. The aims of this study were to investigate the acute effects of exogenous melatonin on the rest-activity rhythms of patients recovering from critical illness, and furthermore to analyse the rhythms and relationship between plasma melatonin and cortisol levels.\n\nMethods A randomised controlled trial in 24 critically ill patients weaning from mechanical ventilation. Ethics committee approval was granted and all patients provided written consent. Twelve patients in each group received placebo or 10 mg exogenous melatonin at 21:00 hours for four nights. Twelve plasma samples were taken periodically from the first 18 of these patients over a 24-hour period. Actigraphy was used to monitor patient activity. Rhythm analysis of plasma levels and activity data used single cosinor analysis and nonparametric parameters, respectively. Results Both groups were well matched. There were no significant differences between the groups in any of the rest-activity measures, which were abnormal and comparable with those previously reported [1] . There was a weak inverse correlation between plasma melatonin and cortisol levels (r = -0.22, P = 0.015). Seven of 18 patients had a circadian rhythm of plasma cortisol levels, while only two patients had a normal acrophase. Four of the nine placebo patients had a circadian rhythm of melatonin, but only one of these had a normal amplitude and acrophase. Results After excluding patients whose weaning was interrupted by transfer or a decision to withdraw treatment, we had 89 weaning attempts to analyse. These represented 43 successful (S) and 46 unsuccessful (US) weans. Comparison of mean \u00b1 SD ages (S 61 \u00b1 14.3 years, US 57.3 \u00b1 16.1 years, P = 0.28) and APACHE scores (S 16.2 \u00b1 4.9, US 17.7 \u00b1 6.5, P = 0.23) for the two groups showed no major differences. Logistic regression demonstrated that the worst FiO 2 prior to weaning and the duration of ventilation prior to weaning were both significantly associated with an unsuccessful SC weaning attempt (P = 0.002 and P = 0.005, respectively). ROC curve analysis suggested patients with an FiO 2 below 0.47 and a duration of ventilation prior to weaning of below 43 hours were more likely to be successfully weaned.\n\nConclusions SC proved most successful in those patients who had a lower worst FiO 2 prior to weaning and a lower duration of ventilation prior to commencing weaning.\n\nAssessing the impact of introducing the 'ventilator bundle' on outcomes for mechanically ventilated patients Background The concept of bundles was developed by the Institute for Healthcare Improvement. Individual bundle elements are built on evidence-based practice, and the bundle concept is that when these elements are executed together they produce better outcomes than in isolation. There is, however, limited evidence linking the use of bundles to demonstrable changes in patient outcomes. As a preliminary analysis to inform a multicentre evaluation, we explored the effect of the introduction of the 'ventilator bundle' on the outcomes for mechanically ventilated patients in a single critical care unit. Methods Data were extracted for mechanically ventilated admissions from a single unit participating in the Case Mix Programme that was an early adopter of the ventilator bundle. A risk prediction model was developed using data from admissions during the 3.5 years prior to the introduction of the bundle and applied to admissions during the 3 years since introduction to estimate the cumulative excess mortality (observed minus expected deaths). Results There were 762 ventilated admissions prior to the introduction of the bundle and 618 since. The cumulative excess mortality plot suggested a reduction in mortality after introduction of the bundle (Figure 1 ) but this was not statistically significant (relative risk reduction 10.9%, 95% confidence interval -10.2% to 31.8%).\n\nInterpretation The results suggest that it will be beneficial to carry out a multicentre evaluation of the ventilator bundle in Case Mix Programme units, and will inform the design of this study.\n\nHemodynamic changes due to expiratory positive airway pressure by facial mask in the postoperative period of cardiac surgery Methods We collected data on the baseline demographic, physiological characteristics and outcomes of 1,299 newborns treated with nCPAP in 57 neonatal ICUs in Poland over a 2-year period. We conducted a stepwise MLR of 481 newborns with the two most common indications for use. We evaluated three outcomes: need for intubation in newborns treated electively with nCPAP (RDS), weaning failure requiring reintubation in the mechanically ventilated newborns (weaning), and bad outcome. Results In the RDS group of patients we found that nCPAP failure was highly significantly related to estimated gestational age and clinical risk index for babies (CRIB). While in our population less mature RDS newborns were only slightly less likely to avoid intubation, the MLR model showed that, controlling for initial CRIB, they were less than one-half as likely to avoid intubation. Failure of nCPAP in weaning was highly significantly related to only pH, prior to beginning nCPAP. Bad outcomes were highly related to estimated gestational age and CRIB in the RDS group, but not the weaning population.\n\nConclusions We believe that understanding the risk of both nCPAP failure and also bad outcomes for a specific patient will enhance clinical decision-making. That is, for patients with the highest risk of poor outcome or nCPAP failure, more aggressive use of intubation and surfactant might be warranted. Likewise, such aggressive therapy might also be avoided for those with a seemingly low chance of poor outcome.\n\nIntroduction Advocates of airway pressure release ventilation (APRV) suggest that this mode is lung-protective for patients with ALI/ARDS, while providing additional benefits of spontaneous breathing, including improved haemodynamics, decreased need for sedation, and better patient comfort. However, there are few available data on the clinical experience with APRV.\n\nMethods We conducted a retrospective audit of consecutive patients receiving APRV from January 2004 to August 2006 in three academic ICUs in Toronto. APRV was initiated at the discretion of the attending physician; a protocol guiding the implementation of APRV was introduced in July 2006. We recorded data describing: baseline characteristics; how APRV was used; its potential ramifications including oxygenation and sedation/analgesia doses; and outcomes.\n\nResults Thirty patients, all with ALI/ARDS, received 39 trials of APRV during the study period -median age 52 years, 60% male, 50% pulmonary ALI risk factor, median APACHE II score 28. They had ALI for a median of 4.5 days with a median 135 hours of CMV before APRV. They received a median of 38 hours APRV. By 12 hours, oxygenation improved significantly (P/F ratio from 103 to 159, P < 0.01), with a concomitant decrease in FiO2 requirements (from 0.70 to 0.50, P < 0.0006). At 72 hours, the median P/F ratio had improved to 196 on a median FiO 2 of 0.40 (both P < 0.01). Administration and dosages of sedatives (midazolam equivalents, propofol) and analgesics (morphine equivalents) did not change significantly over the period from 24 hours before to 24 hours after APRV initiation. There were two episodes of barotrauma during APRV; neither required therapeutic drainage. The 30-day mortality was 13/30 (43%), most commonly due to multiorgan failure and withdrawal of life-support.\n\nConclusions In our patients APRV use appeared safe, led to improved oxygenation, but did not change needs for sedation/analgesia. Future studies are needed to determine the optimal timing and methods for ARPV use; these should be followed by randomized trials to confirm safety and document the effects of APRV on patient-centered outcomes.\n\nIntroduction A reduced content of biophysically active large surfactant aggregates is a common finding in acute inflammatory lung disease. Cyclic surface area changes and a carboxylesterase activity (surfactant convertase) are thought to mediate this subtype conversion. However, data concerning regulation of surfactant convertase are scarce. We therefore investigated the expression and activity of lung surfactant convertase and HMSE-1, a potential macrophage-derived human convertase, under normal and acute inflammatory conditions. Methods Convertase activity in lavage fluid (BALF) was assessed using the in vitro cycling assay. The relative large surfactant aggregate content was determined by phospholipid quantification in the pellet following centrifugation at 48,000 x g. Esterase activity was assessed by means of a chromogenic substrate assay. Expression of both convertase and HMSE upon LPS challenge was assessed by real-time (TaqMan) PCR in murine alveolar macrophages, murine primary type II cells, and the human monocytic cell line U937, respectively. Results Lavage fluid from ARDS patients displayed an increased esterase activity when compared with BALF from healthy controls. In addition, a pronounced large to small aggregate conversion was observed for BALF from LPS-challenged mice or BALF from ARDS patients. Incubation with LPS resulted in a significant increase in convertase gene expression in primary mouse type II cells as well as in HMSE-1 gene expression in U937 cells and monocytes from peripheral blood. No convertase expression was found in cultured murine alveolar macrophages. Conclusions An increased convertase activity was found under acute inflammatory conditions of the alveolar compartment, and type II cells seem to be a relevant source of this increased convertase activity. However, leakage of esterase activity from the vascular space and other inflammatory cells cannot be ruled out. \n\nMethods In the present study we generated transgenic mice that express a surfactant protein B-urokinase fusion protein (SPUC) in the distal respiratory epithelium under the control of the 3.7 kb human SP-C promotor. Survival was determined in a lethal ALI model (inhalative LPS administration) in SPUC mice compared with wild-type mice of the same genetic background. Furthermore, the outcome, lung function, collagen content and histology were assessed in the model of bleomycin-induced pulmonary fibrosis.\n\nResults Transgenic mice showed an improved survival after inhalative LPS or bleomycin administration as compared with wildtype mice. The fibrotic response to inhalative bleomycin challenge was markedly attenuated in transgenic mice, as evident by reduced histological appearance of fibrosis, improved pulmonary compliance and reduced lung hydroxyproline content. As potential underlying mechanisms for the attenuated fibrotic response we observed an improvement in alveolar surface activity, a decrease in pulmonary fibrin deposition, increased hepatocyte growth factor levels and decreased gelatinase activity in the BAL fluids of transgenic mice as compared with control animals.\n\nConclusions Lung-specific expression of a surfactant protein B-urokinase fusion protein protects against ALI after inhalative LPS challenge and prevents fibrosis associated with bleomycininduced lung injury. \n\nThe results of the ARDS Network trial [1] demonstrated a significant reduction of mortality by using a mechanical ventilation protocol with tidal volumes (VT) of 6 ml/kg predicated body weight. Additionally, a computer-driven weaning protocol was successfully performed and a reduction of mechanical ventilation duration could be demonstrated [2] . The implementation of the ARDS Network protocol in routine ICU practice remains modest [3] . A possible reason is the increased organisational and temporal burden. An automated execution of the protocol would help to propagate its day-to-day use. To test the ability to automate such a complex protocol, we designed a pilot study in porcine acute lung injury using an experimental medical expert system capable of continuously controlling respiratory parameters and global as well as regional ventilation with electrical impedance tomography (EIT). Methods After induction of saline lavage-induced lung injury in pigs (n = 3), automated mechanical ventilation was initiated. The medical expert system used a closed-loop fuzzy controller with a rule base of if/then rules based on the ARDS Network protocol reference card. The protocol's algorithmic rules and therapeutic goals (oxygenation, pH, I:E, VT) were continuously controlled and ventilatory settings electronically adjusted accordingly. The medical attendant personnel was constantly informed with status messages about the decisions made. During the trial, all measurements were made using an online blood gas monitor (TrendCare Satellite; Diametrics Medical Inc., UK), a monitor for hemodynamic parameters (Sirecust 1281; Siemens, Germany), a capnograph (CO 2 SMO+; Respironics, Inc., USA), and an EIT prototype system (EIT Evaluation Kit; Draeger Medical, Germany). Subjects were ventilated for between 40 and 90 minutes.\n\nThe computer-driven ventilator settings could stabilise the ventilation of the lung-injured subject in the predefined thresholds. Compared with the beginning of the study, a reduction in ventilation pressure and PaCO 2 could be observed. Despite the initial low PaO 2 /FiO 2 ratio (<200 mmHg) of the subjects, FiO 2 could be decreased by the system in the given time without penetrating the thresholds for oxygenation. Conclusion Robust execution of an automated ARDS Network protocol with an electronically controlled ventilator is possible and leads to pulmonary stabilisation. Further trials have to be undertaken before this successful approach can be realised in ARDS patients. Introduction The optimal duration of prone position ventilation (PPV) in acute respiratory distress syndrome (ARDS) is uncertain. It has been pointed out that pulmonary ARDS patients respond less than extrapulmonary ARDS patients. Objective To study effects of continuous long-term PPV on gas exchange, PEEP, lung injury score and multiorgan failure in pulmonary ARDS patients. Materials and methods The design was a prospective (cohort). We studied 42 PPV periods in 33 pulmonary ARDS patients. Measures were taken in the supine position before PPV and at 1 hour after PPV, and then every 6 hours until the end of PPV. Statistical values are expressed as the median and interquartile range. Wilcoxon and Kruskal-Wallis tests were used. P < 0.05 was considered significant. Results The mean age was 44 (25-57) years, the initial lung injury score (LIS) was 3.1 (2.75-3.6), and PPV was maintained for 91 (51-117) hours. The PaO 2 /FIO 2 ratio was 125 (99-181) mmHg before PPV and 256 (170-298) mmHg after 1 hour of PPV (P = 0.001). This difference with the supine PaO 2 /FIO 2 ratio was sustained until the end of PPV. Initial values of PEEP were set at 15 (12-18) cmH 2 O by constructing a PEEP-compliance curve; there were no differences in PEEP values along the study. Initial values of PaCO 2 were 47 (41-69) mmHg and there were no significant differences along the study period. After 24 hours of PPV, the LIS was significantly decreased in comparison with the supine value before PPV: 3 (2.25-2.7) vs 2.5 (2.25-2.75), P = 0.001. There were no significant complications. Conclusions PPV had a positive effect on gas exchange even after 6 hours. This effect lasts through the PPV period. Because of its effect on the LIS, a duration of 24 hours for continuous PPV could be useful in this patient setting.\n\nIntroduction There is great controversy concerning protective ventilation in ARDS. Recruitment maneuvers and PEEP titration sufficient to avoid collapse and tidal recruitment are the major goals of the maximal recruitment strategy (MRS). Objectives To describe clinical and demographic data. To evaluate the incidence of complications related to transportation and to the MRS. Methods Forty-three patients with ARDS were transported to CT and submitted to the MRS, which consisted of 2-minute steps of ventilation with a fixed PCV = 15 cmH 2 O and progressive PEEP levels (10-45-25-10 cmH 2 O), RR = 10, I:E = 1:1, and FiO 2 = 1.0. Opening (recruitment) and closing (PEEP titration) pressures were determined according to the least amount of collapse observed at the CT, and were used to ventilate the patients afterwards.\n\nResults Clinical data are presented in Table 1 . There were no complications due to transportation and one patient developed pneumomediatinum after the protocol. Introduction The analysis of the nonlinearity of respiratory compliance to guide ventilator settings in ALI and ARDS is well established. The pressure dependency (or volume dependency respectively) of respiratory resistance of these patients is mostly ignored. This study was performed to investigate the pressure dependency of resistance in ALI and ARDS over a wide range of pressures.\n\nMethods Twenty-one patients with ALI or ARDS were analyzed. Ventilation was interrupted by a respiratory manoeuvre: the volume was increased from ZEEP in steps of 100 ml with constant inspiratory flow until the plateau pressure reached 45 cmH 2 O. Each step was followed by a hold of 3 seconds. Inspiratory resistance during each step was determined by a least-squares fitting procedure. Results Resistance decreased from 10.7 \u00b1 5.1 cmH 2 O\u00b7s/l at 5 cmH 2 O to 8.1 \u00b1 4.0 cmH 2 O\u00b7s/l at 40 cmH 2 O (P < 0.05). Figure 1 shows individual absolute values and means \u00b1 SD of all patients. Most of the decrease was found up to 20 cmH 2 O; at higher pressures, changes were not uniform. The average relative changes in inspiratory resistance (\u00b1SD) of all patients are shown in Figure 2 .\n\nConclusion Inspiratory resistance in ALI and ARDS is not constant. Especially at higher pressures, individual resistance may change unpredictably. The assumption of a constant resistance should therefore be avoided.\n\nAvailable online http://ccforum.com/supplements/11/S2 Introduction Alveolar microscopy seems to provide important insight into alveolar dynamics during mechanical ventilation [1, 2] . The utility of this method is limited due to high efforts needed to evaluate sequences of images with respect to alveolar geometry. The evaluation -done by hand -is time consuming, places a high cognitive load on the examiner and is error prone. Reproducibility of results is low. This project aims to establish a computer-assisted tool that provides semi-automatic evaluation of video sequences acquired with alveolar endoscopy. Methods We developed a computer program based on Matlab (Mathworks, Natick, MA, USA), which analyses video sequences acquired with an alveolar endoscope (Sch\u00f6lly, Denzlingen, Germany) [2] . The user has to provide a pointer to the alveoli that shall be traced and whose changes in size and shape are to be determined. Filters, smoothing splines and expectation-driven fine tuning is performed to achieve robust and predictable results of the intratidal change in alveolar geometry.\n\nResults Animal studies related to alveolar mechanics during artificial ventilation were conducted. Figure 1a shows a plot of a frame taken from a video obtained during an experiment performed on a healthy anesthetized rat. Overlaid circles indicate identified boundaries of a selected alveolus. Figure 1b presents a trace of alveolar diameter during a tidal breath. Evaluation of successive frames allows one to compensate for motion artifacts and to analyze the intratidal changes in alveolar geometry.\n\nConclusion Given a synchronization with respiratory data, this tool will allow one to quantify pressure-related changes of alveolar size.\n\nThus it will allow one to monitor the alveolar distension in a variety of animal models (for example, lavage-induced ARDS) and to correlate these findings, for example, with outcome.\n\nIntroduction Alveolar recruitment and maintenance of lung volume are important goals in the treatment of acute lung injury (ALI) and essential for improving oxygenation. The most usual employed strategy to achieve this goal is the use of positive end-expiratory pressure (PEEP). Recruitment and collapse are highly dynamic phenomena that are difficult to monitor. Dynamic effects of regional ventilation can be monitored by electrical impedance tomography (EIT) at the bedside [1] . We investigated the ability of EIT for providing a useful tool to detect dynamic changes of regional breath by breath recruitment at the bedside during an incremental and decremental PEEP trial in experimental lung injury. In addition, we analyzed pressure-volume (P-V) curves computed by EIT data.\n\nMethods ALI was induced in six pigs by repetitive lung lavage. After stabilization of the lung injury model (> 1 hour) a stepwise PEEP trial was performed consisting of 2-minute steps of tidal ventilation (10-30 cmH 2 O; 30-5 cmH 2 O). During the PEEP trial subjects were ventilated pressure-controlled. Global ventilatory and gas exchange parameters were continuously recorded. Offline we analysed EIT data by computing the amount of breath by breath recruitment (\u2206V EIT) at each pressure level before and after lung lavage. Nondependent and dependent regions of interest were defined in the tomograms. \u2206V EIT was defined as the mean increase or decrease in end-expiratory global impedance per breath. Results Ventilatory parameters clearly showed a recruitment of nonaerated lung areas at the descending part of the pressure ramp. The shape of the P-V curve from EIT data, in particular the increasing slope (lower level > upper level), reflected the recruitment of poorly ventilated lung regions. The flattening of the curve at higher pressures, especially at the upper level, reflected less amount of recruitment but more overdistension. Regional pulmonary recruitment/derecruitment was very high in the lower level. These phenomena were more impressive after induced lung injury. S79 of ventilator settings and respiratory mechanics is crucial for further developments of protective lung ventilation. Up to now, the nonlinearity of compliance has mainly been the focus of interest. We hypothesized that airway resistance also changes intratidally. Therefore, this study was performed to analyze the dependence of resistance on tidal gas volume. Methods After induction of anesthesia and tracheotomy, the lungs of 14 surfactant-depleted piglets were ventilated at zero endexpiratory pressure with three different tidal volumes (8, 12, 16 ml/kg) in a randomized order. In addition, baseline measurements (12 ml/kg) were performed before saline lavage. Before any change of the ventilator settings a recruitment maneuver was performed. The nonlinear intratidal airway resistance was analyzed using the SLICE method [1] . Results Figure 1 shows the intratidal resistance before lavage (grey) and after surfactant depletion (black) plotted against the alveolar pressure. Each curve in the diagram represents the intratidal course of resistance for one ventilator setting. Resistance is increased after surfactant depletion and is intratidally declining before and after lavage.\n\nConclusion The analysis of resistance shows a dependence on intratidal volume. The nonlinear course of intratidal resistance can be interpreted as a volume-related caliber effect leading to an increase of cross-sectional area of the large and small airways. Introduction Alveolar recruitment maneuver (ARM) using high airway pressures has been shown to re-expand atelectasis and to improve gas exchanges after general anesthesia; however, ARM may lead to lung stretching-induced inflammatory response. The objective of this study was to evaluate plasma cytokine behavior after an ARM in healthy volunteers. Methods After obtaining ethical committee approval and informed consent, a basal blood sample was collected in 10 healthy volunteers. Continuous positive airway pressure (CPAP) was noninvasively applied (BiPAP Vision \u00ae ; Respironics, USA) using a total face mask. CPAP was increased by 3 cmH 2 O from 5 to 20 cmH 2 O every five breaths. At CPAP of 20 cmH 2 O, an inspiratory pressure of 20 cmH 2 O above CPAP was implemented during 10 breaths. After that, CPAP was stepwise decreased in an inverse fashion. Pulse oximetry, arterial pressure and heart rate were measured before and after ARM. Additional blood samples were drawn at 30 minutes, 2 and 12 hours. TNF\u03b1, IL-1\u03b2, IL-6, IL-8, IL-10 and IL-12 were measured by the flow cytometry technique (Cytometric Bead Array BD\u2122 Kit). The highest cytokine value at 30 minutes or 2 hours after ARM was considered the peak value measurement. Data were analyzed using a paired t test and oneway RM ANOVA. P < 0.05 was significant.\n\nResults Four men and six women with a mean age of 26 \u00b1 1 years and mean BMI of 23.8 \u00b1 3.6 kg/m 2 were studied. No changes were observed in heart rate or MAP after ARM, while pulse oximetry increased from 97.2 \u00b1 0.8% to 98.4 \u00b1 0.7% (P = 0.009). As shown in Figure 1 , ARM induced a significant increase in the peak plasma level concentration of all cytokines that returned to basal levels within 12 hours. No adverse effects were observed during and after ARM.\n\nConclusions Despite beneficial effects in reversing atelectasis, ARM-induced lung stretching was associated with an inflammatory response in healthy volunteers.\n\nIntroduction Recruitment/derecruitment (R/D) seems to play an important role in the development of VILI [1] . Many clinicians base their determination of PEEP settings during mechanical ventilation of ARDS/ALI patients on an estimate of alveolar recruitability [2] . This project aims to establish an online tool that provides estimates of R/D in patients at the bedside. approaches [1, 3, 4] . Our model is fitted (currently offline) to patient data acquired during controlled mechanical ventilation. For data acquisition the internal respiratory data of a ventilator (Evita 4; Dr\u00e4ger Medical, L\u00fcbeck, Germany) is read in real time. The simulation assumes a quantitative partition into pressuredependent and time-dependent recruitment. Pure pressure-related approaches (for example [1] ) are not able to describe transients (for example, a volume shift after a change in PEEP).\n\nResults A multistep optimization process is performed to reduce the difference between measured data and model prediction. At any moment during a tidal breath or during some respiratory maneuver the current state of the model can be visualized. The inflated volume splits up into extension of open alveoli and into temporal or pressure-dependent recruitment. Distribution of these compartments over time during a tidal inflation is depicted in Figure 1a . The pressure vs time and flow vs time curves are shown in Figure 1b . Conclusion The fitting of recruitment models provides interesting insight into not directly observable R/D. It may be used for monitoring trends and drifts in recruitment. Currently results rely on certain assumptions; for example, distribution and quantity of superimposed pressure. With modern imaging techniques (for example, CT, EIT) a validation of the fitted models will come into reach and will be performed as a next step.\n\nIntroduction Results FRC increase in group A is statistically significant (W) (step 1: 1,525 \u00b1 360 ml; step 2: 1,937 \u00b1 583 ml, P < 0.05 vs step 1; step 3: 2,592 \u00b1 659 ml, P < 0.05 vs step 2 and P < 0.01 vs step 1) while the FRC increase in group B is not significant (step 1: 1,697 \u00b1 210 ml; step 2: 1,757 \u00b1 367 ml; step 3: 1,982 \u00b1 365 ml); the FRC of group A is statistically higher than the FRC of group B in step 2 (P < 0.05 MW) and in step 3 (P < 0.01 MW). The ratio increase in group A is statistically significant (W) (step 1: 256 \u00b1 133; step 2: 407 \u00b1 187, P < 0.01 vs step 1; step 3: 379 \u00b1 169, P < 0.05 vs step 1) while the ratio increase in group B is not significant (step 1: 194 \u00b1 50; step 2: 253 \u00b1 83; step 3: 276 \u00b1 73); the ratio of group A is statistically higher than the ratio of group B in step 2 (P < 0.01 MW) and in step 3 (P < 0.05 MW). The C stat increase in both groups is not significant, but in group A C stat is statistically higher than C stat of group B in every step (P < 0.05 MW) (step 1: 38 \u00b1 2 ml/cmH 2 O for group A vs 28 \u00b1 7 ml/cmH 2 O for group B; step 2: 44 \u00b1 6 ml/cmH 2 O vs 36 \u00b1 8 ml/cmH 2 O; step 3: 47 \u00b1 5 ml/cmH 2 O vs 36 \u00b1 8 ml/cmH 2 O).\n\nConclusion With the limit of low sample size, these preliminary data suggest that the FRC evaluation system is a good parameter to optimize pulmonary recruitment and seems to be in a position to overcome the C stat limit for the evaluation of pulmonary recruitable parenchyma. Introduction The purpose of this study is to report our clinical experience with high-frequency oscillatory ventilation (HFOV) for rescuing trauma patients with acute respiratory distress syndrome (ARDS) and severe hypoxemia despite optimal conventional ventilation. Experimental and clinical data suggest mechanical ventilation can contribute to mortality in ARDS, and modern ventilatory strategies require protective measures such as low tidal volume, low airway pressure and fraction of inspired oxygen (FIO 2 ), which is not always possible with conventional ventilation. HFOV could be an alternative to achieve protective ventilation and adequate oxygenation.\n\nMethods We retrospectively analyzed nine trauma patients who presented with ARDS criteria and failed conventional mechanical ventilation requiring HFOV. The mean airway pressure was initially set 3-5 cmH 2 O higher than that for conventional ventilation and was subsequently adjusted to maintain oxygen saturation >90% and FiO 2 <0.6. The PaCO 2 target range was 35-60 mmHg with a pH >7.25. We collected demographic data, injury severity scale (ISS), APACHE II score, time to HFOV, time spent on HFOV, ventilation settings and arterial blood gas before and after HFOV and mortality. Results Fifteen out of the total 28 patients were R and 13 were NR. Both the groups were similar prior to HFOV in terms of APACHE II score, number of organ failures, PEEP and plateau pressures, and duration of ventilation before HFOV. The baseline PO 2 /FiO 2 ratio and improvement in it at 6 hours and 24 hours in the R group were statistically significantly higher as compared with Available online http://ccforum.com/supplements/11/S2\n\nS82 that in the NR group. The difference in improvement in the oxygenation index (OI) of the two groups at 6 and 24 hours was also statistically significant. The rate of improvement in the PO 2 /FiO 2 ratio and OI in NR was slower than that in R, and this difference was statistically significant (trend test). See Figure 1 .\n\nConclusion A lower PO 2 /FiO 2 ratio and higher OI prior to HFOV and slow improvement in the PO 2 /FiO 2 ratio and OI at 6 and 24 hours on HFOV are significant negative outcome predictors of HFOV in ARDS.\n\nMonitoring slow recruitment manoeuvres with highfrequency oscillatory ventilation in adult acute respiratory distress syndrome patients using electrical impedance tomography However, the changes in lung volume during a RM (lung recruitability) are difficult to quantify at the bedside, and the use of CT is impractical in patients on HFOV. We studied the effects of a standardised protocol of slow RM (SRM) on regional lung volumes assessed noninvasively by electrical impedance tomography (EIT survived to discharge to the ward. An admission pH of less than 7.20 was found to be significantly associated (P = 0.09) with failure of treatment.\n\nConclusion Although we believed that the unit's approach to HFOV was one of 'treatment' rather than 'rescue', our results suggest we are still using HFOV in a 'rescue' mode. While our results support the findings of other studies that earlier initiation of HFOV shows a trend towards improved outcome in adult patients with ARDS, further studies are still required to identify appropriate parameters for selecting patients in a timely manner who may benefit from HFOV. However, progressive acidosis in ARDS appears to be a relatively more important predictive criterion than parameters of failing oxygenation and ventilation.\n\nIntroduction Inadvertent endobronchial intubation and one-lung ventilation (OLV) with a standard endotracheal tube may lead to serious complications, such as a nonventilated lung, pneumothorax and hypoxemia. Auscultation of breath sounds was found to be VCV, volume controlled ventilation.\n\ninaccurate for the detection of OLV with a high margin, up to 60% error [1] . Vibration response imaging (VRI) is a novel technology that measures vibration energy from the lungs and displays regional intensity in both visual and graphic format. The time from the start of the procedure to display takes less than 1.5 minutes.\n\nWe investigated the effectiveness of VRI to detect OLV using a double-lumen endotracheal tube in lung surgery patients. Methods Double-lumen tubes were placed at the time of surgery. Tracheal and endobronchial lumens were alternately clamped to produce unilateral lung ventilation of the right and left lungs. VRI was performed after each occlusion. Two images were excluded a priori (prior to analysis) due to technical failure (external artifact).\n\nThe right and left lung distribution of vibration intensity is shown in Figure 1 . The mean percentage change of vibration intensity clearly demonstrates the increased vibration in ventilated lungs (89.1 \u00b1 5.47% vs 10.9 \u00b1 5.4%, P < 0.05) ( Figure 2 ). Conclusions Auscultation is insensitive to endobronchial intubation and chest radiography may not be immediately available. VRI offers the potential to rapidly and noninvasively determine endobronchial intubation. Currently VRI is performed in the sitting position, but the capability of supine imaging will soon be available. Introduction Airflow into a mechanically ventilated patient is easily measured in the inspiratory limb of the ventilator. Regional airflow inside the lungs, up to this point, is a black box. Vibration response imaging (VRI) is a novel technology that measures vibration energy from the lungs to create a real-time structural and functional image of regional vibration during respiration. Sophisticated surface skin sensors are placed on the subject's back to record, analyze and display vibrations noninvasively. Our goal was to assess the correlation of vibration measured at the chest wall with airflow into the lungs. Methods To assess the effect of constant inspiratory flow on lung vibration, VRI was performed on a mechanically ventilated patient on assist volume control, and airflow in the tubing was recorded concurrently. To assess the effect of increasing flow rates on lung vibration, healthy subjects were recorded several times with VRI while taking tidal volumes of 200-1,300 ml at the same respiratory rate. The inspiratory tidal volume was recorded.\n\nAvailable online http://ccforum.com/supplements/11/S2 \n\nIn the mechanically ventilated patient, when there is minimal flow, the vibration was at its lowest. When flow begins at the ventilator, the vibration measured over the lungs increases and when the flow stops, the vibration decreases. An inspiratory hold was performed to separate inspiratory from expiratory vibrations ( Figure 1 ). As the subject takes increasing tidal volumes, the vibration during the breath cycle increases linearly. A sample subject is shown in Figure 2 (R 2 = 0.81). Conclusion Vibration measured using VRI correlates with lung airflow. Given the difficulty in assessing airflow in the lungs, measuring lung vibration could potentially serve as a surrogate for regional lung airflow.\n\nThe Gliding-SLICE method: an enhanced tool for estimation of intratidal respiratory mechanics Introduction Focusing on lung-protective ventilation, the analysis of nonlinear dynamic respiratory mechanics appears crucial. Based on the SLICE method we developed the Gliding-SLICE method as a tool to determine respiratory system mechanics. This tool was tested in a nonlinear water-filled two-chamber lung model.\n\nThe classic SLICE method [1] determines parameters of the respiratory system for abutted volume ranges. The Gliding-SLICE method enhances this method by moving a window of analysis along the volume axis. This way, a quasi-continual course of intratidal mechanics can be determined. To test the new method we build up a physical model that consists of two connected chambers filled with water. During inspiration water is displaced from one chamber to the other resulting in a counter pressure.\n\nUsing wedges of certain shapes we simulated volume-dependent nonlinear compliances. Results Using the Gliding-SLICE method we determined a nonlinear course of compliance in a patient ( Figure 1 ) and in model data ( Figure 2 ). Conclusion The Gliding-SLICE method allows one to calculate mechanical parameters of the respiratory system quasi-continually. This allows a more intuitive interpretation of data. The method is not limited to principle constrictions but can be enhanced by ventilatory maneuvers; for example, for separated view on inspiratory and expiratory respiratory mechanics. S85 decrease after PEEP was reduced from 15 to 5 cmH 2 O but decreased after PEEP was reduced from 5 to 0 cmH 2 O. Conclusion Accurate measurements of FRC are obtained during a constant breathing pattern that is easier to obtain during controlled ventilation in comparison with pressure-support ventilation. In patients with ALI/ARDS, the FRC decreased during each PEEP reduction, but whether the largest change in FRC indicates the optimal PEEP needs further research. Methods The percentage of V non (%V non ) relative to the total lung volume was quantified in CT-image series (n = 21) of sheep with gross anesthesia-induced atelectasis. This was performed for different combinations of number and anatomical location of CT slices and the results were compared with the %V non of the entire lung (lung). The combinations were: one juxtadiaphragmatic slice (juxta), three apical, hiliar and juxtadiaphragmatic slices (3old), and three consecutive juxtadiaphragmatic slices (3new). The correlation between %V non and the arterial oxygen partial pressure (PaO 2 ) was examined for all combinations. The PaO 2 was measured at the time of the CT and transformed logarithmically (lnPaO 2 ) to linearize the relation between PaO 2 and %V non . Linear regression and Bland-Altman plots were used for statistical analysis.\n\nResults The R-squared (R 2 ) values for the correlation between lnPaO 2 and %V non of lung and the slice combinations juxta, 3new and 3old were 0.61, 0.60, 0.57 and 0.55, respectively. The %V non of lung correlated best with the %V non of slice combinations juxta and 3new (R 2 = 0.96 and 0.95, respectively). Comparison of these slice combinations with lung also resulted in the least bias in the Bland-Altman analyses (6.3 and 5.9%, respectively). R 2 for the correlation between lung and 3old was 0.93, and the bias for lung vs 3old in the Bland-Altman analysis was 6.8%. Conclusion Depending on the precision required, the use of single juxtadiaphragmatic CT slices can help to speed up the analysis process and thereby propel the clinical implementation of CTderived information. Our data suggest that juxtadiaphragmatic slices may be better suited than the 'traditional' combination of apical, hiliar and juxtadiaphragmatic slices. Introduction Respiratory physiotherapy is ever more utilized for the treatment of critical patients. However, it is known that there are few studies on the effect on gas exchange in patients with acute lung injury (ALI) and acute respiratory distress syndrome (ARDS) and physiotherapy techniques. The purpose of this study was to assess the effect of a pulmonary expansion and disobstruction maneuver with a closed system on the gas exchange of patients with ALI and ARDS. Methods The patients with the diagnosis of ALI and ARDS who met the inclusion criteria were randomized to one of the two groups: those of the intervention group were subjected to a pulmonary expansion and bronchial disobstruction maneuvers, for approximately 10 minutes by the association of the following physiotherapy techniques: sighs, side-lying position, expiratory rib-cage compression and endotracheal suctioning with a closed system and after observed for 10 minutes; the patients of the control group did not receive any treatment, they were only observed for 20 minutes. Ventilatory parameters and arterial blood gases were measured before (Time 1) and 10 minutes after the procedures (Time 2 Methods The nebulizer device consists of a cylindrical glass housing that, at the bottom, ends up in a spherical lower housing part that serves as the dry powder reservoir. A gas inlet portion with a nozzle at its end is coaxially aligned with the housing, almost reaches the bottom of the dry powder reservoir, and induces aerosol generation when gas pressures between 1 and 2 bar are applied. The upper portion of the housing contains a cap with an aerosol exit port. Several nozzles ensure a discharge of unsuitably large aerosol particles. Aerosol characteristics were determined by laser diffractometry. The efficacy of an inhalative rSP-C surfactant application was assessed in three animal models of acute lung injury, including rabbits with acute lung injury due to either repetitive lavage with prolonged and injurious ventilation, or due to inhalative application of bleomycin at day 4, and bleomycinchallenged, spontaneously breathing mice.\n\nThe generated aerosol had a mass median aerodynamic diameter of 1.6 \u00b5m, with 85% of all particles being smaller than 5 \u00b5m, and the average mass of surfactant being nebulized under these conditions was approximately 1 g/min. Biochemical and biophysical studies showed that the composition and surface tension reducing properties of the rSP-C surfactant remained unaltered after nebulization. In both rabbit models, administration of 130 mg/kg body weight rSP-C surfactant resulted in a far-reaching restoration of gas exchange and compliance. In bleomycinchallenged, spontaneously breathing mice, surfactant aerosolization resulted in a restoration of compliance.\n\nConclusions Nebulizer characteristics and results from the in vivo studies suggest that the herein-described dry powder nebulizer might proffer for surfactant therapy of ARDS. Objectives (1) To study the effects of BAL on respiratory mechanics in mechanically ventilated patients with suspected pneumonia, and (2) to find out whether these effects are related to the extension of radiographic infiltrate and preceding respiratory mechanics measurements.\n\nMethods BAL was performed with 150 ml sterile isotonic saline in three aliquots of 50 ml. Respiratory mechanics (static compliance and airway resistance) was measured using the rapid airway occlusion technique immediately before and after the BAL and 90 minutes later. The heart rate, arterial blood pressure and body temperature were recorded continuously in all patients. Patients were classified according to the presence of unilateral or bilateral infiltrates. Results Fifty-eight critically ill patients undergoing mechanical ventilation were included. Following the BAL, compliance of the respiratory system (Crs) decreased from 50.9 \u00b1 36.1 to 35.6 \u00b1 14.8 ml/cmH 2 O (P < 0.01) and airway resistance increased from 16.2 \u00b1 7.6 to 18.1 \u00b1 11.3 cm H 2 O/l/seg (P < 0.05); 90 minutes later, both parameters had returned to pre-BAL values (P = not significant). Patients who showed >20% decrease in Crs had higher pre-BAL Crs than patients with less severe decrease (55.8 \u00b1 20.1 vs 36.9 \u00b1 14.1; P < 0.001). On the contrary, neither pre-BAL airway resistance nor the extension of the radiographic infiltrates were related to the changes in respiratory mechanics.\n\nConclusions (1) BAL in mechanically ventilated patients can lead to a significant but transitory deterioration on pulmonary mechanics characterized by a decrease in Crs and an increase in airway resistance.\n\n(2) Patients with better initial Crs showed the more severe affectation. Clinical examination (CE) and chest X-ray (X-ray) have limited sensibility and specificity. Contrast-enhanced computed tomography (CT scan) is the gold standard. CT scan has limitations: it takes time to be performed, implies transport of severely injured patients, and has ionising effects. Thoracic ultrasonography (US) can be quickly performed at the bedside in the emergency room. It has good diagnosis accuracy in ARDS patients [2] . The purpose of this study is to evaluate the diagnosis accuracy of US in severely injured patients in the emergency room. Methods We prospectively evaluated 90 patients (median age: 41 (7-89) years) who were admitted to the emergency room of the Grenoble University Hospital over a period of 9 months. Pneumothorax, hemothorax and alveolar consolidation were diagnosed by CE, X-ray and US. The physician who performed the US was not involved in the patient's management. Results During 123 missions, 15 adult patients underwent prehospital endotracheal intubation (cardiac arrest n = 9, multiple injuries n = 4, drug poisoning n = 1, pulmonary edema n = 1) with the Bonfils intubation fiberscope, the use of which was either planned (n = 13) or unplanned (n = 2). All intubations were successful in the first attempt, even in two cardiac arrest victims who had an unexpected difficult airway (Cormack&Lehane grade IV under direct laryngoscopy). In those patients with multiple injuries the cervical immobilization collar did not need to be unfastened or removed for endotracheal intubation. Sufficient retropharyngeal space -which is mandatory for sufficient use of the Bonfils -was created by a digital jaw thrust maneuver in the first three patients. Using a standard Mackintosh laryngoscope blade significantly enhanced ease of insertion of the Bonfils fiberscope and visualization of the glottic aperture, thereby decreasing the procedure time from 35-40 seconds to 20-25 seconds.\n\nConclusion Despite this first promising series of in-the-field use, physicians and paramedics should familiarize themselves with the Bonfils device under optimal clinical conditions before using it under emergency or prehospital conditions. In our experience, the learning curve with the Bonfils device is steep, and 10 intubations supervised by an instructor usually prove effective for achieving sufficient skills to use the Bonfils on one's own and under less optimal conditions. In summary, we believe that the Bonfils fiberscope will prove its value as an additional airway management device in both, emergency and prehospital settings.\n\nIntroduction This study was designed to assess the ability of ICUs to deal with the unanticipated difficult intubation. The ICU is a location in which the incidence of difficult intubation has been found to be significantly higher than in theatre (8-22.5% vs 1.5%).\n\nMethod We contacted all adult general ICUs in the South of England and invited the physician responsible for airway management to take part in a structured interview. The interview was designed to follow the Difficult Airway Society (DAS) guidelines. We designed six equipment-related questions that identified a unit as achieving the minimum levels of equipment necessary. These included availability of laryngoscopes, capnography, LMA/ILMA, and rescue techniques. Introduction Occasionally, rescuers are confronted with a hard situation to establish tracheal intubation compared with doctors in the anesthetic room. Especially in the confined space, the tracheal intubation must enter technical difficulties with any supporting device. This may be caused by the fact that there was no device developed specially from a standpoint in the clinical emergency use. Objective The AirWay Scope (AWS) is one of the newest intubation devices, manufactured using modern technology to alleviate the tracheal intubation in emergency scenes. The AWS is equipped with a full-colored CCD, a LCD monitor and a specially configured introducer guiding a tracheal tube into the glottis (Figure 1 ). The aim of this study is to confirm the potential of the AWS as an intubation-supporting device in emergency scenes. Method Six doctors in the emergency department were enrolled in this study. All doctors have experienced using the AWS in cases on the operation Available online http://ccforum.com/supplements/11/S2\n\nAirway equipment available on ICUs. A score of 6/6 is considered the minimum.\n\nConclusion With its portability, easy handiness, excellent visual information and the tube-guiding function of the introducer, the AWS may have potential to alleviate the various difficulties in intubation in emergency scenes, even in a confined space. Results The number of unsuccessful placement attempts was none in the UBBR group (0/10) and one in the CBBR group (1/10), two in the UBBL group (2/10) and five in the CBBL group (5/10). A fiberoptic aided technique should be more appropriate for the leftsided blocker in both groups. There was no statistical difference in bronchial blocker malpositions, the lung to collapse and the number of complications among the two groups. Furthermore, for elective thoracic surgical cases, once the lung was isolated, the management seemed to be similar for both groups. Discussion This study demonstrates that the wire-guided bronchial blocker (Uniblocker\u2122) provides a high torque control and can be easily manipulated into the desired site of the lungs.\n\nIn conclusion, our study shows that the Uniblocker\u2122 is more useful than the Coopdech bronchial blocker tube. is a new tracheal tube designed for use for mechanical ventilation in the critically ill. The LoTrach has been shown to prevent the ubiquitous problem of micro-aspiration of secretions associated with conventional HVLP cuffs [1] . Aspiration prevention is achieved by the properties of the LoTrach's low-volume low-pressure (LVLP) cuff, and these have been previously described [1, 2] . It is important that, alongside achieving aspiration prevention, there is also tracheal wall pressure control. Results Conventional HVLP cuffs do not prevent leakage of fluid past the cuff, hence the negative slope on the graphs. The LoTrach cuff does not leak and therefore the line is horizontal (see Figure  1 ). The clinical study shows that both the LoTrach and Portex cuffs demonstrate a gross air leak at equal and acceptable tracheal wall pressures at 33.4 and 29.7 cmH 2 O, respectively (see Table 1 ). Introduction Lung injury and generalized edema from a burn and resuscitation complicates airway management and patient care. The need for long-term ventilation and multiple surgeries warrant early tracheostomy. Percutaneous techniques are well described; however, the burned and swollen neck increases all of its recognized complications. We report a modified semi-open technique for performing percutaneous tracheotomies (PT) in acutely burned patients, which we consider safer. Methods We reviewed the medical records of 20 patients admitted to a regional burn center requiring tracheostomy for prolonged mechanical ventilation. The procedure took place in the OR if burn excision was planned; otherwise it was performed at the bedside. The Blue Rhino tracheostomy kit was used for all PT. Major differences from other approaches included dissecting down to the pretracheal fascia, allowing the trachea to be seen and palpated; bleeding was controlled using an electrocautery, and blood vessels were retracted from the field or ligated. The trachea was palpated as the endotracheal tube was withdrawn into the proximal trachea and a flexible bronchoscope was used only to confirm the proper placement of the guidewire. Proper placement of the tracheal tube was confirmed by capnography. In patients with a deep trachea due to severe neck swelling, a proximal-long tracheostomy tube was substituted for the standard one. In the event that the airway or ventilation became compromised, this technique could be converted rapidly to an open procedure.\n\nOf 350 patients admitted to the burn center from July 2005 to December 2006, 20 (6%) required a tracheostomy. Eighteen were performed percutaneously, 13 at the bedside. The total burn surface area averaged 46% (range 2-95%). PT were performed within an average of 10 days from admission (range 0-32 days). Overall mortality in the tracheostomy group was 35%. There were no short-term complications associated with this method. Conclusion PT can be performed safely in severely burned patients using a semi-open percutaneous technique. Exposing the trachea and palpating the trachea avoids the risk of losing the airway and permits immediate access to the trachea in the event of an untoward loss of the airway. We believe that this method is safer than the more commonly used technique requiring bronchoscopic visualization.\n\nIntroduction A retrospective analysis of a year cohort of tracheostomies discharged from intensive care in a specialist cardiothoracic centre was undertaken to analyse whether facilitated outreach-led discharge was safe.\n\nMethods A retrospective analysis of the ICU database was undertaken to identify all patients who had a tracheostomy (percutaneous or surgical) inserted in the ICU, and a chart review of patients discharged from the ICU with a tracheostomy in situ was performed. The following variables were collected: patient demographics; diagnosis; number of days of tracheostomy in situ; number of days on noninvasive ventilation (CPAP); and tracheostomy-related complications. A review of the risk management database was performed to identify any tracheostomy-related reported adverse events. Results One hundred and eight tracheostomies were performed in intensive care in the 2-year period. Sixty-two patients were discharged with tracheostomy in situ and were reviewed by the outreach team for a cumulative total of 710 days until decannulation. There were 383 days whereby patients with a tracheostomy in situ had been noninvasively ventilated. There were three reported critical events relating to tracheostomy and no deaths. Conclusion More than 60% of patients who had a tracheostomy inserted are discharged from critical care with a tracheostomy in situ. With the support of the outreach team these patients were successfully managed in Level 2 and Level 1 areas. This reduced the requirement for critical care (Level 3) bed-days. There was a low rate of complications. We concluded that outreach services can facilitate early and safe discharge of tracheostomy patients from critical care. Indication of levosimendan therapy was acute heart failure due to myocardial infarction in 23 cases and acute progression of chronic heart failure (NYHA III-IV) in 18 cases. After a 10-minute bolus levosimendan infusion was administered at rate of 0.1 \u00b5g/kg/min for 6 hours and 24 hours in each group, respectively. We investigated the occurrence of sustained ventricular or supraventricular arrhythmias for the first 48 hours from the beginning of infusion.\n\nThe ratio of hypertension, diabetes, earlier myocardial infarction and ACBG were 58%, 27%, 32% and 15%, respectively, in the monitored population (13 females, 28 males; mean age: 68 years). Three ventricular arrhythmias and one supraventricular arrhythmia were observed during the 48-hour period, all of them occurred in acute heart failure patients with acute myocardial infarction. Parallel usage of catecholamines (noradrenalin and/or dopamine) and levosimendan therapy was observed in three cases, in one of them ventricular tachycardia was observed 3 hours after starting levosimendan infusion. No arrhythmia was observed in chronic heart failure patients. The incidence of proarrhythmic effects during levosimendan therapy was 9.75% of the whole analysed population and was 17.4% at acute heart failure during acute myocardial infarction. Conclusion With these results the authors would like to draw attention to the proarrhythmic effects of levosimendan during acute heart failure therapy, especially in the case of parallel usage with catecholamines. Introduction Myocardial depression in sepsis, among other factors, is due to calcium (Ca 2+ ) desensitization in the myofilament. So using a Ca 2+ sensitizer drug may play a beneficial role in this situation. Levosimendan has a dual mechanism; it causes Ca 2+ sensitization through binding to Troponin C and opening of ATPdependent K + channels in vascular smooth muscle.\n\nMethods A prospective observational case-series study extending over a period of 18 months from November 2004 to April 2006. We analyze the data of 18 patients receiving levosimendan for myocardial depression due to severe sepsis and compare them with our historical data in the previous year of the same group of patients regarding mortality. All those patients were included in the study who had a pulmonary artery catheter (PAC) and who after initial resuscitation (early goal-directed therapy (EGDT)) did not respond to treatment and their cardiac index (CI) was <2.2. Each patient than received an infusion of levosimendan at 0.1 \u00b5g/kg/min without a loading dose. Hemodynamic parameters such as the CI, mixed venous saturation (SvO 2 ) and mean arterial pressure (MAP) were recorded at 0, 12, 24 and 48 hours. Noradrenaline was used to maintain a MAP above 65 mmHg. Patients were followed for 30 days to document the 7th-day and 30th-day mortality. SPSS 11 was used for statistical analysis. The Student t test was used as a test of significance.\n\nThe average age was 67.6 \u00b1 10.39 years and the APACHE II score was 26.33 \u00b1 2.37. Patients were divided into three subgroups: survivors, 7th-day and 30th-day mortality groups. There was no significant difference in these subgroups regarding age and APACHE II score. Levosimendan group 7th-day and 30thday mortality was 33% and 66% as compared with historical data of 37% and 71%, respectively. The change in CI in the survivor group was significant (P = 0.021), from 2.11 \u00b1 0.17 to 3.8 \u00b1 0.28, while in the 7th-day and 30th-day mortality groups it was insignificant. SvO 2 increased in the survivor and 30th-day mortality groups significantly (P = 0.011 and P = 0.035, respectively). It did not show any significant improvement in the other group. MAP also showed significant improvement in the survivor group (P = 0.026) and insignificant in others. Conclusion It is evident from our study that levosimendan improves hemodynamic response in septic patients. Although it improves the mortality, we cannot say with full confidence that these improved hemodynamic parameters are responsible. Randomised control trials are needed to answer this question, which are underway.\n\nThe elderly acute coronary syndrome patient: a neglected population? \n\nThe incidence of acute myocardial infarction (AMI) in old patients is increasing due to rapid aging of the population. This is of particular concern because AMI in patients who are \u226580 years old is associated with high mortality. However, the role of reperfusion therapies is not clear in these patients.\n\nObjective To evaluate the mid-term and long-term prognosis of octogenarian patients with ST-elevation myocardial infarction (STEMI) treated with primary coronary angioplasty (PCA). Methods We studied retrospectively, from January 2000 to March 2005, 73 patients \u226580 years with STEMI treated with PCA. At the end of follow-up, we assessed the incidence of death, myocardial infarction and necessity of new procedures of revascularization of the treated vessel.\n\nThe average age was 84 \u00b1 3.6 years, 39 (58%) were women. The location of the AMI was anterior in 56%, and 25% were diabetic. The average follow-up time was 19 \u00b1 17 months. During the follow-up, 43 patients developed events, most of them (n = 28) consisting of death (23 by cardiac death). However, most of these events occurred in the first month after the admission, the mortality between 1 month and 3 years being low ( Figure 1 ). Conclusion Our data show that the octogenarian patients with STEMI treated by primary PCA developed a very high mortality. However, this mortality especially concentrates in the first month after the procedure, being low between 1 month and 3 years. Purpose Although invasive management of ST-segment elevation myocardial infarction (STEMI) has improved the clinical outcome, early mortality remains an important issue. Our purpose is to assess the utility of the initial electrocardiographic (ECG) pattern in detecting patients who are at increased risk despite the current recommendations of revascularization. Methods We analyzed 446 consecutive patients (age 61.9 \u00b1 13.8 years, 76.5% male) admitted in the first 12 hours of STEMI to our coronary unit. Exclusion criteria were left bundle branch block at admission or previous myocardial infarction. Most patients (87%) were treated with primary angioplasty. Patients treated with thrombolytics and with early reperfusion criteria were programmed to coronary angiography the following day. Two groups were defined according to the presence of ST-segment elevation (STE) together with distortion of the terminal portion of the QRS in two or more adjacent leads (group 1) or the absence of this pattern (group 2) (Figure 1) . Results ST elevation with distortion of the terminal portion of QRS were present at initial ECG in 55 patients (23%). This group had a lower incidence of Rentrop grade 2/3 than group 2 (P = 0.006, Figure 1 ). Moreover, group 1 had higher enzyme release, worse maximal Killip class and more frequently the combined variable death/shock. Group 1 more often had proximal occlusion of the infarction-related artery and nonreflow. Multivariate analysis found ECG to be an independent predictor of outcome. Available online http://ccforum.com/supplements/11/S2\n\nResults Patients with hemoglobin values between 140 and 159 g/l were used as the reference; cardiovascular mortality increased as hemoglobin levels fell below 140 g/l or rose \u2265160 g/l. The inhospital 30-day mortality was 25.0% in patients with hemoglobin concentrations <100 g/l, 20.4% in patients with hemoglobin concentrations of 100-119 g/l, 10.6% in patients with hemoglobin concentrations of 120-139 g/l, 4.3% in patients with hemoglobin concentrations of 140-159 g/l, and 8.5% in patients with hemoglobin concentrations of 160 g/l or greater. The increase in risk of complications associated with a low hemoglobin concentration was more pronounced in patients with anemia than in patients without. Compared with patients with hemoglobin concentrations of 140-159 g/l, those with hemoglobin concentrations <140 g/l had more inhospital complications and those with hemoglobin concentrations \u2265160 g/l also had more arrhythmia and pneumonia (P < 0.001, respectively). As expected, a significant inverse correlation between hemoglobin concentrations and ages (r = -0.51; P < 0.001) was observed, and a significant positive correlation between hemoglobin concentrations and albumin concentrations and in the patients with acute myocardial infarction. Conclusion It is demonstrated in this study that a reverse J-shaped relationship between baseline hemoglobin values and major adverse cardiovascular events is observed in patients with acute myocardial infarction. There is a greater incidence of patients with a hemoglobin concentration on admission in the elderly population than that in the younger one.\n\nAnaemia at the moment of admittance is associated with higher heart failure and mortality among patients with acute coronary syndrome \n\nThe prevalence of anaemia (Hb < 11 g/dl in women and Hb < 12 g/dl in men) in patients with an ACS was 15.4%. This group was characterised by the following: woman (P < 0.0001), higher age (P = 0.0001), less weight (P = 0.01), higher frequency of high blood pressure (P = 0.0001), diabetes mellitus (P = 0.0001), history of ischaemic heart disease (P = 0.002) and peripheral artery disease (P = 0.0001). This group presented a major proportion of the NSTEMI (P = 0.015), higher level of renal dysfunction (77% to 32%, P = 0.0001), and microalbuminuria (61% to 32%, P = 0.0001). Patients with anaemia presented a worse intrahospital prognosis: major incidence of cardiac insufficiency (42% to 20%, P = 0.0001), refractory angina pectoris (14% to 6%, P = 0.01), more electric complications (12% to 9%, P = 0.01) and a higher mortality (14% to 7%, P = 0.009). The presence of anaemia was an independent predictor of cardiac insufficiency and death at the moment of admittance to the CCU (OR = 2.20, 95% CI = 1.10-4.35; P = 0.002) Conclusion The presence of anaemia is a powerful predictor of a worse prognosis in patients with ACS. Anaemia is associated with other factors of a worse prognosis such as renal dysfunction, peripheral artery disease and diabetes mellitus.\n\nLong-term prognostic impact of anemia in patients with ST-elevation acute myocardial infarction treated by primary coronary angioplasty in the other group (P < 0.001). After controlling for a variety of baseline clinical, laboratory, and angiographic variables, anemia was a strong and independent predictor of death or rehospitalization for heart failure or acute coronary event (HR 1.96, 95% CI 1.21-3.17, P = 0.006). Figure 1 shows that patients with anemia present a worse prognosis. Introduction The aim of this study was to investigate the relationship between blood pressure and plasma magnesium levels in patients referred to the emergency department with hypertensive attack. Epidemiological evidence on the effects of magnesium on blood pressure is inconsistent. Metabolic and experimental studies suggest that magnesium may have a role in the regulation of blood pressure. Magnesium regulates various ion channels in many tissues, including those of the cardiovascular system. Magnesium is the second most abundant intracellular cation, and the important element that has numerous biological functions in the cardiovascular system. Furthermore, magnesium acts as a calcium antagonist, regulating the calcium metabolism. Methods Patients were included who were taken to the emergency department due to hypertensive attack. Their age, gender, systolic and diastolic blood pressure were recorded. In order to see the plasma magnesium levels, venous blood samples were taken. The results were compared with a chi-square test. The values P < 0.05 were accepted as significant.\n\nResults Seventy-three patients (35 of whom were female, 38 males) were included in the study. The average age was 47 \u00b1 6.3 (ranging from 33 to 68 years). The average blood pressure of the patients was found as systolic 200 \u00b1 10 (range 185-240) mmHg, diastolic 105 \u00b1 7 (range 95-110) mmHg. The average plasma magnesium levels were 1.4 \u00b1 0.3 (range 0.9-2.2) mg/l. The plasma magnesium levels were low in 29 patients (ranging from 0.9 to 1.7 mg/l). There was a negative relationship systolic blood pressure and plasma magnesium level (P < 0.05). In addition, there was a negative relationship diastolic blood pressure and plasma magnesium level (P < 0.05). Conclusion Low plasma magnesium levels would be an important factor for elevated blood pressure and hypertensive attack.\n\nS99 surgery patients as well as severely hypertensive patients.\n\nClevidipine is an ultrashort-acting, vascular and arterial-selective calcium antagonist currently under development for treating acute hypertension.\n\nMethods We analyzed data from two double-blinded, placebocontrolled trials (ESCAPE-1 and ESCAPE-2) that evaluated the ability of clevidipine to control BP in high-risk cardiovascular surgery patients. In addition, we evaluated the design of a recently initiated trial that analyzes clevidipine in severe hypertension (VELOCITY trial).\n\nIn both ESCAPE-1 and ESCAPE-2, clevidipine demonstrated a statistically significant decrease in mean arterial pressure from baseline (P < 0.0001) compared with placebo at the 5-minute time point. A BP lowering effect was observed within 1-2 minutes with clevidipine, with the median time to achieve target systolic blood pressure (SBP) of 6 and 5.3 minutes, respectively (see Figure 1 ). In the patients with acute severe hypertension, the VELOCITY trial studies the percentage of patients in whom the SBP falls below the lower limit of a patientspecific predetermined target range at the initial dose of 2.0 mg/hour within 3 minutes of initiating the infusion, as well as the percentage of patients who reach the prespecified target SBP range within 30 minutes of the beginning of the study drug. Conclusion In both the ESCAPE-1 and ESCAPE-2 studies, clevidipine demonstrated the ability to precisely achieve target blood pressure reductions in a short period of time, in a high-risk patient population. Further analysis of the rapid decreases noted with clevidipine is being conducted in patients with acute severe hypertension in the VELOCITY trial.\n\nThe inflammatory process of acute pericarditis (Pc) may involve the epicardium and cause myocardial damage, as reflected by cardiac Troponin I (TnI) release. Studies performed with TnI demonstrated that the temporal pattern of this release is similar to that of an acute myocardial infarction (AMI); however, the true prognostic significance of TnI remains unknown in this setting. Introduction Early risk stratification is essential in the management of patients with an acute coronary syndrome (ACS).\n\nMeasurements of renal function such as serum creatinine and estimation of creatinine clearance carry independent prognostic information in this population. Cystatin C is a new and better marker of renal function than creatinine. The aim was therefore to evaluate the prognostic value of cystatin C in this population. Methods Four hundred and twenty-eight patients with an ACS, admitted to our coronary care unit (CCU), were studied prospectively. Sixty-three per cent presented a non-ST-segment elevation myocardial infarction (NSTEMI) and 37% a ST-segment elevation myocardial infarction (STEMI). During their hospitalization we registered cardiovascular risk factors: we determined the presence of microalbuminuria (>3 mg/dl) in a 24-hour urine sample. We also took blood samples during the first 24 hours of their admittance to the CCU for a complete hemogram, levels of total cholesterol, HDL cholesterol, LDL cholesterol, triglycerides, creatinine, creatinine clearance (Cockroft-Gault equation), glucose, HbAc1, high-sensibility C-reactive protein, Cystatin C and a follow-up of levels of Troponin, CK and CK-MB. All patients were submitted to a coronary angiography in the first 72 hours to give a clinical score to their coronary artery disease (disease of one, two or three arteries). Results We determined the Cystatin C level in 59 patients (16 females and 43 males). In 36% (21 patients) we found normal levels (<0.95; 0.80 \u00b1 0.9), called group 1. In the other group (group 2) we found higher levels of Cystatin C (>0.95; 1.63 \u00b1 0.77). Patients in group 2 presented a higher age, a higher frequency of high blood pressure, worse Killip class score at the moment of admittance, higher inflammatory activity (leucocytosis, P = 0.001 and higher levels of C reactive protein, P = 0.005), higher grade of renal dysfunction (P = 0.001) and anaemia (P = 0.06). Patients in group 2 presented a worse intrahospital prognosis with a higher incidence of cardiac insufficiency (45% to 14%, P = 0.01), ventricular arrhythmias (29% to 5%, P = 0.05), pericardial effusion (18% to 0%, P = 0.05) and a higher mortality (21% to 5%, P = 0.08). In the multivariant analysis, Cystatin C was an independent predictor of cardiac insufficiency (OR = 4.5, 95% CI 1.1-20.8, P = 0.05). \n\nConclusion Higher levels of Cystatin C (>0.95) in patients with an ACS indicate a worse intrahospital prognosis and also a higher inflammatory activity and renal dysfunction.\n\nThe reduction of the glomerular filtration rate and the presence of microalbuminuria at the moment of admittance reduce the prognostics of patients with an acute coronary syndrome Introduction Determination of the glomerular filtration rate (GFR) in patients with an acute coronary syndrome (ACS) has an important prognostic value. The presence of microalbuminuria (MA) is a known risk factor in patients with hypertension and diabetes. We know less about the effect of reduction of the GFR on patients with an ACS. Method Four hundred and twenty-eight patients with an ACS, admitted to our coronary care unit (CCU), were studied prospectively. Sixty-three percent presented a non-ST-segment elevation myocardial infarction and 37% a ST-segment elevation myocardial infarction. During their hospitalization we registered cardiovascular risk factors; we determined the presence of MA (>3 mg/dl) in a 24-hour urine sample. We also took blood samples during the first 24 hours of their admittance to the CCU for a complete hemogram, levels of total cholesterol, HDL cholesterol, LDL cholesterol, triglycerides, creatinine, creatinine clearance (Cockroft-Gault equation), glucose, HbAc1, high-sensibility Creactive protein and a follow-up of levels of Troponin, CK and CK-MB.\n\nResults Thirty-nine percent of the patients with an ACS presented a GFR less than 60 ml/minute, and 36% presented MA at the moment of admittance to the CCU. Forty-four percent of the patients with a GFR less than 60 ml/minute also presented MA; on the contrary, only 32% of the patients with a GFR more than 60 ml/minute did so (P = 0.01). This group contains significantly more women (P = 0.001), more history of ischemic brain events and peripheral artery disease (P = 0.03), worse Killip score at the moment of admittance (P = 0.001), more development of cardiac insufficiency (P = 0.003) and a higher mortality during hospital stay (P = 0.03). The intrahospital survival of patients with GFR less than 60 ml/minute and MA was 79%, to 96% in patients without MA and a GFR more than 60 ml/minute (P = 0.01; Log-rank test = 6). Patients with a GFR less than 60 ml/minute but without MA presented an intrahospital survival of 85%. In the multivariant analysis a GFR less than 60 ml/minute (OR = 2.0; 95% CI 1.13-3.53) and the presence of MA (OR = 2.30; 95% CI 1.37-3.86) were independent predictive factors of cardiac insufficiency and mortality.\n\nConclusions The presence of a GFR less than 60 ml/minute and MA at the moment of admittance of a patient with an ACS identifies a group of patients with a bad prognosis. Future studies can reveal whether an improvement of the renal function can be beneficial for this group of patients. Background Elevated circulating levels of TNF\u03b1, brain natriuretic peptide (BNP) and cardiac Troponin I (cTnI) have been connected with adverse prognosis in patients with chronic heart failure (CHF). However, there are scant data about the predictive value of these biomarkers in combination. Methods A total of 577 consecutive patients (mean age: 73 \u00b1 9 years), who were hospitalized for acute decompensation of NYHA class III/IV (65.3% of ischemic etiology) low-output (mean LVEF: 22 \u00b1 5) CHF, were studied. Biochemical markers were measured upon admission. The incidence of 31-day death was the prespecified primary endpoint.\n\nThe incidence of the primary endpoint was 17.7%. By multivariate Cox analysis, including baseline characteristics and the study biomarkers, elevated circulating levels of TNF\u03b1 (RR = 2.1; P < 0.001), BNP (RR = 3.5; P < 0.001) and cTnI (RR = 3.8; P < 0.001) were independently associated with the primary endpoint. When the patients were divided according to the number of positive biomarkers (estimated by ROC analysis) there was a significant gradual increase in the rate of the primary endpoint with increasing of the number of the positive biomarkers (4.1%, 10%, 21.5% and 53.5% 31-day mortality rate for patients with zero, one, two and three positive biomarkers, respectively; P < 0.001) (Figure 1 ).\n\nThe present results suggest that in patients hospitalized due to acutely decompensated severe low-output CHF, serum levels of TNF\u03b1, BNP and cTnI can be used in combination for enhanced early risk stratification.\n\nAvailable online http://ccforum.com/supplements/11/S2 Introduction Over the past decade, coronary revascularisation has helped reduce mortality and morbidity rates from coronary artery disease. In addition to revascularisation, long-term prognosis is dependent on successful implementation of secondary prevention, in particular the use of aspirin, statins, angiotensin-converting enzyme (ACE) inhibitors and, in many, \u03b2-blockers. Previous studies have highlighted the under-utilisation of secondary preventative strategies in this patient population. A focused review of secondary preventative medication at the time of revascularisation provides an excellent opportunity to ensure optimal use of these agents. Our aim was to identify the proportion of patients undergoing nonemergency surgical revascularisation discharged on these four secondary preventative medications.\n\nMethods A retrospective analysis of our inhouse cardiothoracic surgical database was performed. All patients had undergone surgical revascularisation between January 2003 and November 2006. Only patients undergoing coronary artery bypass grafting were included. Results A total of 2,749 consecutive patients were included in the analysis, mean age 65.5 years (\u00b19.2). In total, 2,302 isolated coronary artery bypass grafting procedures and 447 combined procedures were performed. See Table 1 . Introduction Under normal resting conditions, the oxygen delivery (DO 2 ) matches the overall metabolic demands of the organs, the oxygen consumption (VO 2 ) is about 25% of the DO 2 , and energy is produced basically through the aerobic mechanism. In cardiac surgery with extracorporeal circulation (ECC), several factors (for example, hemodilutional anemia, myocardial stunning resulting in a low cardiac output (CO)) can determine an imbalance between O 2 demand and DO 2 and may affect the outcome. Below the critical DO 2 there is a linear decrease of both VO 2 and CO 2 production (VCO 2 ), but due to the anaerobic VCO 2 the respiratory quotient increases. This study is aimed to evaluate the role of O 2 and CO 2 derived parameters to predict postoperative morbidity in cardiac surgery.\n\nMethods Eight hundred and twenty-seven consecutive adult patients who underwent coronary surgery were studied. We selected 38 intraoperative and postoperative O 2 and CO 2 derived parameters, which could be associated with postoperative morbidity. Postoperative data were collected in the first 3 hours after admission to the ICU. The influence of each predictor on outcome was analyzed. Morbidity was defined as one or more of the following events: cardiovascular, respiratory, neurological, renal, infectious, and hemorrhagic complications. Univariate and multivariate analyses were performed. ROC curve analysis was also used to define the best predictive variables. Results Seventeen (34%) patients had VS, 11 (65%) men and six (35%) women. Longer aortic clamping time (P = 0.007) and CPB time (P = 0.013) were associated with VS. These patients showed a higher cardiac index at 4 hours (P < 0.001) and lactic acid within the first 24 hours. Seven of these patients (41%) fulfilled vasoplegic shock criteria (P < 0.001). We found higher levels of IL-6 at 0 hours (P = 0.02) and 4 hours (P = 0.001), and soluble TNF receptor at 0 hours (P = 0.044). At ICU admission (0 hours) there was a higher coagulation activation: INR (P = 0.005), fibrinogen (P = 0.001), antithrombin (P = 0.007); lower levels of plasminogen activator inhibitor-1 (P = 0.023) as well as lower plasminogen activator inhibitor-1/tissue-plasminogen activator ratio (P = 0.021), and higher levels of D-dimer (P = 0.041); lower levels of C3 (P = 0.023), B factor (P = 0.013), C4 (P = 0.015) as well as a significantly higher decrease between preoperative and 0-hour levels of C1-inhibitor, C4, C3 and B factor. Lower levels of leptins at 0, 4 and 24 hours were found. Vasoplegic patients showed higher blood losses along all time points (Figure 1 ), higher incidence of excessive bleeding (60% vs 40%; P = 0.011) and required more hemoderivatives during the ICU stay, plasma (P = 0.016) and platelets (P = 0.002).\n\nConclusions VS post-CPB was associated with activation of serin protease systems, which leads to higher blood loss and excessive bleeding. Introduction Vasoplegic syndrome (VS) after cardiac surgery with cardiopulmonary bypass (CPB) can vary from mild to severe complication and it appears with an incidence ranging between 5% and 15%. The etiology is not completely elucidated but risk factors such as temperature and duration of cardiopulmonary bypass and preoperative treatment with angiotensin-converting enzyme (ACE) inhibitors have been associated [1] . We wanted to investigate the possible role of several genetic polymorphisms in patients with VS after elective CPB.\n\nMethods We performed a nested case-control study of 50 patients undergoing CPB, 27 (54%) men and 23 (46) women, mean age 66.5 (SD 9.6) years. VS was defined as systemic vascular resistance index lower than 1,600 dyn\u2022seg/cm 5 /m 2 and a cardiac index greater than 2.5 l/min/m 2 within the first 4 hours after surgery. We recorded data related to hemodynamic parameters at different postoperative time points, at ICU admission (0 hours), 4 and 24 hours after surgery, and the polymorphism of the following genes: plasminogen activator inhibitor-1 (PAI-1) and \u03b2-TNF + 250. In addition, 23 neutral markers were genotyped to follow genomic control strategies that would detect spurious associations due to population substructure. We used the Pearson chi-squared test and binary logistic regression. SPSS version 12.1 was used.\n\nWe observed 17 (34%) patients with vasoplegia criteria, 11 (65%) men and six (35%) women, age 67 (61-72) years. The only one associated with VS was the PAI-1 polymorphism, and its distribution in the study population was: 4G/G genotype in 10 (20%) patients, 4G/5G in 26 (52%) patients, and 5G/G in 14 Introduction Body mass index (BMI) has been described as a risk factor for coronary artery disease, but association with postoperative bleeding after cardiopulmonary bypass (CPB) has been found in several studies recently. Nevertheless the strong relationship between a low BMI and excessive bleeding remains unexplained. We sought to investigate the BMI role on postoperative bleeding and its relationship with leptin levels, coagulation, fibrinolysis and complement parameters. Methods We performed a nested case-control study of 26 patients, who did not receive antifibrinolytic prophylaxis. We used Bray's classification for BMI: lower than 27 kg/m 2 ; 27-30 kg/m 2 ; higher than 30 kg/m 2 . Variables were collected preoperatively, at ICU admission (0 hours), and at 4 and 24 hours after surgery. Excessive bleeding was defined as blood loss higher than 1 l in the first 24 hours after intervention. Figure 1 ). BMI presented a direct correlation with leptins, fibrinogen and plasminogen activator inhibitor-1 (PAI-1) on arrival, meanwhile 24-hour bleeding showed an inverse correlation with the same parameters and BMI (Table 1) . Patients with BMI < 27 kg/m 2 had significantly greater coagulation, fibrinolysis and complement activation. Therefore these patients required significantly greater hemoderivatives. Conclusions Lower BMI was associated with higher postoperative bleeding and lower procoagulant factor levels. 49) ), respiratory control ratio (11 (7-15) vs 10 (9-11)) (not significant). Hepatic mitochondrial state 4 was higher (27 (16-31) vs 19 (13-22)) and respiratory control ratio lower (3 (3-4) vs 5 (4-6)) in the hemorrhage/endotoxemia group, compared with controls. Background Sepsis-induced multiple organ failure may crucially depend on the development of mitochondrial dysfunction and consequent cellular energetic failure. We investigated whether hepatic mitochondrial dysfunction was present in a clinically relevant porcine model of fluid-resuscitated septic shock. Methods Anesthetized and ventilated pigs (40 \u00b1 3 kg) were randomly assigned to septic shock by fecal peritonitis (F, n = 3) or control (C, n = 3) after placement of portal/hepatic vein catheters and portal vein and hepatic artery flow probes. F and C received 8 \u00b1 13 ml/kg/hour and 5 \u00b1 7 ml/kg/hour ringer lactate + starch, respectively. The mean arterial pressure (MAP), total liver flow (TLF), hepatic O 2 delivery (DO 2,h ) and hepatic O 2 consumption (VO 2,h ) were recorded at baseline (BL), 12 and 24 hours (ml/kg/min). Activities of mitochondrial respiratory chain enzymes (complex I-IV) were assessed by spectrophotometry in snapfrozen liver samples. Data are presented as the mean \u00b1 SD. Results Hyperdynamic circulation developed in F with increasing DO 2,h and decreasing VO 2,h ( Table 1) . Complex II activity significantly decreased from 19.3 \u00b1 4.2 to 9.5 \u00b1 2.6 (P < 0.05 vs BL and between groups) in F compared with C. Complex I-III-IV function decreased in parallel in F. Monitoring of the mitochondrial NADH redox state (an indicator of intracellular oxygen levels) together with microcirculatory blood flow (TBF) and with oxygenation (HbO 2 ) could serve as a preferred approach to evaluate tissue O 2 balance or viability. We hypothesize that in the presence of reduced oxygen delivery and extraction, blood flow will be redistributed in order to protect the most vital organs by increasing their regional blood flow, while O 2 delivery to the less vital organs will diminish. Thus, the NADH redox state of less vital organs could serve as an indicator of overall O 2 imbalance as well as an endpoint of resuscitation. We have therefore developed an optical device embedded in a Foley catheter to provide real-time data on the NADH redox state, TBF and HbO 2 in critically ill patients. The CritiView is a computerized optical device that integrates hardware and software in order to provide real-time information of S107 tissue viability [1] . A modified three-way Foley catheter that contains a fiberoptic probe connects the CritiView to the mucosal side of the urethral wall. We have used this device in five female pigs that underwent graded hemorrhage, and in four patients who were monitored during aortic abdominal aneurysm operations. These preliminary swine model and human studies confirm the feasibility of collecting information about mitochondrial function from the urethral wall. The main effects of graded hemorrhage started when the blood volume decreased by 30%. At 40% blood loss, minimal levels of TBF and HbO 2 were correlated to the maximal NADH levels. The values of the three parameters returned to baseline after retransfusion of the shed blood. Aortic clamping in patients led to a significant decrease in TBF and HbO 2 while NADH levels increased. After aortic declamping, the parameters recovered to normal values.\n\nOur preliminary results show that the CritiView may be a useful tool for the detection of O 2 imbalance and the development of an emergency metabolic state in nonvital tissues. \n\nThe outcome of liver transplantation is steadily improving. There is still need for earlier detection of complications such as hepatic artery thrombosis and rejection. In an earlier in vitro study we showed that the CMA microdialysis system with a 100 kDa pore size membrane can be used to measure the selected cytokines and complement. We monitored patients undergoing liver transplantation with microdialysis continuously for a week postoperatively, and analyzed both parameters to detect ischemia, and cytokines and anaphylatoxins to explore whether rejection was detected earlier than with the standard methods. Methods Twenty patients undergoing 22 liver transplantations were included. Two microdialysis catheter were introduced in the liver and one in subcutaneous tissue. We analyzed metabolic parameters (glucose, pyruvate, glycerol and lactate), and IL-6, IL-8, MCP-1, IP-10, and C5a. Results Fourteen patients had an uneventful course postoperatively, judged clinically and by routine biochemical markers and ultrasound Doppler. These patients had a median lactate starting at 3.5 mM (2 hours after reperfusion) falling to below 2 mM during the first 24 hours, and thereafter staying low. The L/P ratio (a specific measure of ischemia) dropped from about 20 to below 10. These patients had a steady rise in IP-10 from 200 to 3,000 pg/ml, and also a slight raise in IL-6 initially. Case 1. The male patient had a steadily increasing L/P ratio during the 7 days of microdialysis measurements, indicating an insufficient blood supply. He underwent surgery 5 days later and a hepatic artery thrombosis was found. A biopsy was done during the operation showing an acute rejection. There was a significant rise in IP-10 to 13,000 pg/ml 7 days before the diagnosis of rejection. Case 2.\n\nThe female patient had an acute rejection verified by biopsies on day 10 postoperatively. Her IL-8, IP-10 and C5a increased 10-fold to 100-fold in the liver 3 days earlier than an increase in liver enzymes and 5 days before the rejection was verified by biopsy. Conclusion We have described the normal course of the four cytokines IL-6, IL-8, MCP-1 and IP-10 and complement C5a after liver transplantation, as well as metabolic parameters to detect ischemia. In two patients with rejection we found a large increase in IP-10, IL-8 and complement split-product C5a in the liver but not in the subcutis 3-5 days before any other parameter of liver injury. Objective To evaluate skeletal muscle microcirculation by nearinfrared spectroscopy (NIRS) in patients with chronic heart failure (CHF). Background Skeletal muscle microcirculation is impaired in patients with CHF, and this impairment seems to correlate with disease severity. Methods We evaluated 49 patients with CHF (mean age: 58 \u00b1 12 years) and 12 healthy volunteers. Of the CHF patients, 14 had end-stage heart failure (ESCHF) and were undergoing treatment with intermittent inotropic agent infusion during the period of the study protocol. The thenar muscle tissue oxygen saturation (StO 2 %) was measured noninvasively by NIRS before, during and after 3-minute occlusion of the brachial artery (occlusion technique).\n\nResults Patients with ESCHF (n = 14) and CHF (n = 35) presented a significantly lower tissue oxygen saturation (StO 2 ) than healthy subjects (75 \u00b1 6%, 77 \u00b1 8% and 85 \u00b1 5%, P = 0.001 respectively). The oxygen consumption rate during the occlusion of the brachial artery differed significantly between patients with ESCHF, CHF and healthy subjects (22.4 \u00b1 9%/min, 29 \u00b1 10%/min and 38.1 \u00b1 11.1%/min, P = 0.001 respectively). The reperfusion rate differed significantly between patients with ESCHF, CHF and healthy subjects (302 \u00b1 136%/min, 393 \u00b1 134%/min and 480 \u00b1 133%/min, P = 0.002 respectively).\n\nConclusions Peripheral muscle microcirculation assessed by NIRS is impaired in CHF patients. The degree of dysfunction is associated with disease severity and is acutely partially reversed with inotropic agent infusion. Introduction Cardiac surgery patients are at low risk for postoperative complications, but these may involve multiple organ failure with a high mortality rate. These complications may be related to occurrence of organ ischemia and reperfusion during and just after surgery. We investigated whether microcirculatory flow alterations occur during cardiac surgery. Methods We observed 10 consecutive patients who underwent cardiac surgery with cardiopulmonary bypass (CPB). The microcirculation was studied using sidestream dark field (SDF) imaging. The sublingual capillary flow was estimated using a semiquantitative microvascular flow index (MFI) in small (diameter 10-25 \u00b5m), medium (25-50 \u00b5m), and large (50-100 \u00b5m) sized microvessels (0 = none, 1 = intermittent, 2 = sluggish, 3 = continuous flow). SDF imaging was performed at least three times per time period (that is, at baseline, after starting CPB and after surgery) in each patient. Data are presented as the median and interquartile range.\n\nThe MFI decreased in all sizes of microvessels <15 minutes after starting CPB in comparison with baseline (P < 0.05, Table 1 ). After starting CPB, the mean arterial pressure (MAP) was lower (61 mmHg (53-65 mmHg)) than at baseline (100 mmHg (92-118 mmHg); P = 0.01). After return to the ICU, the MFI increased (P < 0.05) and returned to baseline values in all microvessels.\n\nConclusions SDF imaging can be used as a bedside tool to evaluate sublingual microcirculatory changes during cardiac surgery. Despite maintaining common circulatory parameters during CPB, the nonpulsatile status, hypothermia, and the temporary drop in MAP after starting CPB were associated with decreased sublingual MFI, which normalized after surgery. Further studies should reveal whether these changes are related to outcome.\n\nIntroduction Complications of oesophagectomy with gastric tube reconstruction include leakage and stenosis. This can be explained by compromised local perfusion, although it is unclear to which extent local and systemic factors contribute to this process. The aim of this study was to observe the microvascular blood flow in an unaffected, distant tissue during the perioperative period. Methods Twelve patients were included. Anesthesia consisted of thoracic epidural analgesia, restrictive peroperative fluid therapy (net peroperative fluid balance below 4 l) and early extubation. In the ICU, fluid infusion was adjusted in order to maintain hourly urine production of 0.5 ml/kg. The mean arterial pressure was maintained at or above 60 mmHg with administration of noradrenalin if necessary. Microcirculation was visualized in the sublingual tissue with the MicroScan, a sidestream dark field imager. Data were collected at five time points: immediately after induction, after gastric tube reconstruction, directly postoperative, and days 1 and 2 postoperatively. Video data collected with the MicroScan were analysed according to semiquantitative analysis described by Boerma and colleagues [1] . We divided the vessels into three categories: small (5-10 \u00b5m), medium (10-15 \u00b5m) and large (>15 \u00b5m). By dividing the images into four quadrants and categorizing the flow per vessel-size per quadrant, we calculated the microvascular flow index (MFI) Results See Figure 1 . \n\nSublingual microvascular perfusion. S109 Introduction Microcirculatory dysfunction has been hypothesized to play a key role in the pathophysiology of multiple organ failure, and consequently to patient outcome. The objective of the present study was to investigate differences in reactive hyperemia response and oscillation frequencies in survivors and nonsurvivors of patients with multiple organ dysfunction syndrome (MODS). Methods Twenty-nine patients (15 survivors; 14 nonsurvivors) with two or more organ failures were eligible for study entry. All patients were hemodynamically stabilized, and demographic and clinical data were recorded. A laser Doppler flowmeter was used to measure the cutaneous microcirculatory response. Reactive hyperemia and oscillatory changes in the Doppler signal were measured during 3 minutes before and after a 5-minute period of forearm ischemia during hyperemia.\n\nResults Nonsurvivors demonstrated a significantly higher MODS score when compared with survivors (P = 0.004). Norepinephrine requirements were higher in nonsurvivors (P = 0.018). Nonsurvivors had higher arterial lactate levels (P = 0.046), decreased arterial pH levels (P = 0.001), and decreased arterial PO 2 values (P = 0.013) when compared with survivors. A higher oscillation frequency of skin microvasculature at rest (P = 0.033) and after an ischemic stimulus (P = 0.009) was observed in nonsurvivors. No differences were observed in reactive hyperemia response between groups. The flowmotion frequency observed in reactive hyperemia was associated with the severity of the MODS (P = 0.009), and -although not statistically significant -arterial lactate concentration (P = 0.052). Conclusion An increased skin microvascular oscillation frequency during rest and after an ischemic stimulus is associated with increased mortality in patients suffering from MODS. We suggest that the underlying mechanism of the increased flowmotion could be a response of the skin microvasculature to hypoxia or to an impaired oxygen utilization of the skin tissue. Introduction Intermittent positive pressure ventilation (IPPV) may be accompanied by alteration of microcirculation [1, 2] ; however, the effect of IPPV is not mentioned in the interpretation of the results of studies evaluating microcirculation using orthogonal polarization spectral or sidestream dark-field imaging. This study aimed to evaluate the effect of IPPV on microcirculation in the skeleton muscles and in the serosa of the small intestine in rats. Methods Ten animals were tracheostomized and prepared for microcirculation study; after tissue preparation, five rats were allowed to breath spontaneously (Group SB = spontaneous breathing), and five rats (Group IPPV) were connected to a small animal ventilator (IPPV: FiO 2 0.21, respiratory rate 60/min, tidal volume 10 ml/kg, inspiratory time 50% of respiratory cycle, and 2 cmH 2 O PEEP). Sidestream dark-field images were obtained from the quadriceps femoris muscle (QFM) and serosa surface of the ileum. The arterial blood pressure and rectal temperature were also recorded. The functional capillary density (FCD) and small and medium vessels rate were analysed offline using AVA V1.0 software (AMC, University of Amsterdam, The Netherlands), P \u2264 0.05.\n\nThe FCD was decreased significantly in QFM in rats with IPPV with respect to Group SB (184 \u00b1 27 resp. 197 \u00b1 61 cm/cm 2 ), but the FCD of the intestinal serosa was not affected by IPPV (265 \u00b1 46 resp. 267 \u00b1 25 cm/cm 2 ). There were no differences in mean blood pressure and temperature between groups (128 \u00b1 7 Torr and 36.6 \u00b1 0.1\u00b0C in Group SB, or 128 \u00b1 10 Torr and 36.5 \u00b1 0.1\u00b0C in Group IPPV).\n\nConclusion The use of IPPV should be taken into account in the interpretation of the studies examining the changes in microcirculation in rats. \n\nWe examined sublingual microvascular changes in experimental human endotoxemia. Changes in microcirculation and mitochondrial dysfunction appear to be key mechanisms in sepsis, since they can lead to regional mismatch of oxygen supply and demand. Lipopolysaccharide (LPS) can be used to induce endotoxemia as a model of sepsis, but the effects on microcirculatory perfusion have not been tested before, particularly after tolerance induction during repeated challenges of LPS. Methods Six healthy volunteers received an intravenous injection of 2 ng/kg Escherichia coli LPS to induce endotoxemia on five consecutive days. Microvascular perfusion was sublingually measured using sidestream darkfield imaging just before, and 2 and 4 hours after LPS injection on day 1. All measurements were repeated on day 5 of LPS administration. Sublingual capillary flow was estimated using a semiquantative microvascular flow index (MFI) in small (10-25 \u00b5m), medium (25-50 \u00b5m) and large-sized (50-100 \u00b5m) microvessels (no flow, 0; intermittent flow, 1; sluggish flow, 2; and continuous flow, 3 Introduction Shock is defined currently as tissue oxygen metabolic disorders. It is most important to understand oxygen metabolic disorders in individual tissue. Microdialysis allows the determination of the metabolic condition in regional tissue and it appears ideal to determine the regional metabolic tissue conditions during endotoxemia.\n\nObjective This study was designed to assess the regional metabolic tissue conditions on markers of tissue metabolism (lactate in regional tissue: tissue lactate (TL)) and tissue partial oxygen pressure (PtO 2 ) during severe endotoxemia and to compare them with variables determined by standard monitoring (hemodynamics, blood gas analysis, blood lactate (BL)).\n\nwere used for this study. The rats in the control group (n = 6) were injected with saline of 2 ml intraperitoneally, and the rats in the experimental group (n = 6) were treated with intraperitoneal injection of lipopolysaccharide (LPS) of 40 mg/kg. The hemodynamic parameters, arterial blood gas analysis, BL and PtO 2 were measured in both groups. TL and pyruvate in subcutaneous tissue were measured using microdialysis. These parameters were measured every 50 minutes until 400 minutes after LPS was administered.\n\nIn the control group, all parameters were not changed during the observation period of 400 minutes. In the experimental group, the mean arterial pressure (MAP) remained fairly stable until 300 minutes after injection of LPS, and the MAP gradually decreased subsequently. While the MAP was maintained, the PtO 2 gradually decreased linearly ( Figure 1 ). TL increased with time linearly. Meanwhile, BL did not change from 150 to 250 minutes; after 300 minutes it increased abruptly in the experimental group ( Figure 2 ). Conclusions In our experimental endotoxemia model it has been shown that partial pressure of oxygen in subcutaneous tissue decreased even if systemic blood pressure was maintained. Boekstegers and colleagues [1] revealed that mean skeletal muscle PO 2 was increased in patients with sepsis compared with patients with limited infection. We obtained conflicting results to those of Boekstegers and colleagues. The reason for this is unknown. BL abruptly increased during 50-150 minutes, probably from abnormal metabolism induced by LPS in whole-body organs. It is considered that BL did not show a rise during 150-250 minutes due to metabolization of lactate in liver and muscle. TL, which is insusceptible of lactate metabolism by other organs, may reflect abnormality of tissue metabolism precisely. Shunting of the microcirculation contributes to the pathology of sepsis and septic shock. In this study, we hypothesize that shunting of the microcirculation occurs after superior mesenteric artery (SMA) ischemia (occlusion) and reperfusion, and we explore functional consequences using intravital microscopy. Spontaneously breathing animals (rats) (n = 30) underwent occlusion of the SMA for 0 (controls), 30 or 60 minutes followed by reperfusion (4 hours) with normal saline. Leukocyte-endothelial interactions in mesenteric venules were quantified in an exteriorized ileal loop using intravital microscopy. Abdominal blood flow was recorded continuously, and arterial blood gases were analyzed at intervals. Continuous SMA blood flow measurements were performed in comparable groups without exteriorizing an ileal loop. Adherent leukocytes increased shortly after reperfusion in ischemia groups, and plateaued in these groups. The centerline velocity and shear rate in the recorded venules were significantly reduced after reperfusion down to low-flow/no-flow in animals undergoing 60 minutes of mesenteric artery occlusion compared with animals with 30 minutes occlusion and controls, whereas perfusion of the SMA and ileal vessels persisted. The microcirculatory changes in animals with 60 minutes occlusion were accompanied by progressive metabolic acidosis, substantially larger volumes of intravenous fluids needed to support arterial blood pressure and significantly reduced survival (30%). In the groups with continuous SMA blood flow measurements, SMA blood flow increased in SMA occlusion for 60 minutes and subsequent reperfusion causes perfusion abnormalities in the mesenteric microcirculation as often seen in sepsis and septic shock with increased microcirculation shunting, progressive metabolic acidosis and increased mortality.\n\nTo detect these significant changes requires prolonged observation periods and might help to find new treatments to improve the poor prognosis of mesenteric ischemia.\n\nIntroduction Targeting oxygen delivery in the postoperative period has been shown to reduce hospital length of stay and complications [1] . Using a near-infrared spectroscopy device such as the Inspectra\u2122 325 allows the measurement of tissue oxygen saturation (STO 2 ) noninvasively as well as a rudimentary measure of blood flow beneath the probe. It is plausible, then, that changes in oxygen delivery (DO 2 ) during postoperative optimisation may be reflected in changes in STO 2 and provide a noninvasive surrogate of DO 2 . Methods All adult patients admitted to the ICU after surgery who underwent protocolised haemodynamic optimisation were included. All patients had STO 2 recorded over the thenar eminence using an Inspectra\u2122 325 for the first 8 hours of their stay.\n\nWe found a significant correlation between the changes in STO 2 and oxygen delivery index (DO 2 I) over the first 8 hours of intensive care stay (n = 40, correlation coefficient of 0.947, P = 0.0001, Figure 1 ). We classified patients who achieved DO 2 I > 600 ml/min/m 2 as responders. These responders had higher STO 2 values by 3 hours of optimisation, a change that remained significant throughout the duration of the study (Figure 2 ). Conclusion Changes in STO 2 during postoperative optimisation appear to mirror changes in DO 2 I and may allow more widespread use of noninvasive tissue oxygenation devices in surgical optimisation. Methods We randomized 36 healthy subjects undergoing maxillofacial surgery to receive general anesthesia with a sevofluoraneremifentanil (Group S) or a propofol-remifentanil association (Group P). We collected noninvasive measures of hemoglobin concentration from the gastrocnemius muscle of the subjects using a NIRS device (NIMO, NIROX srl, Italy), which performs quantitative assessments of the [HbO 2 ] and [Hb] exploiting precise absorption measurements close to the absorption peak of the water. Data were collected during a series of venous occlusions at different cuff pressures, before and after 30 minutes from induction of general anesthesia. The muscle blood volume and microvascular compliance were obtained with a process previously described elsewhere [1] . Data were analyzed with a one-way analysis of variance test.\n\nResults Demographic data of the 36 subjects were similar in both Groups S and P. General anesthesia reduced the heart rate and mean arterial pressure and increased the total muscle blood volume in both groups (Group S: from 2.4 \u00b1 0.9 to 3.2 \u00b1 1.2 ml/ 100 ml; Group P: from 2.4 \u00b1 1.2 to 3.5 \u00b1 1.8 ml/100 ml; P < 0.05). During general anesthesia, despite no differences in muscle blood volume between the two groups, sevofluoraneremifentanil significantly decreased microvascular compliance (from 0.15 \u00b1 0.08 to 0.09 \u00b1 0.04 ml/mmHg/100 ml; P = 0.001) whereas propofol-remifentanil did not (from 0.15 \u00b1 0.08 to 0.16 \u00b1 0.11 ml/mmHg/100 ml; P = 0.39).\n\nAvailable online http://ccforum.com/supplements/11/S2\n\nConclusion General anesthesia affects the microvascular bed of skeletal muscle. An association between opioid and ipnotic agents increases the muscle blood volume, whereas microvascular compliance is reduced only by the sevofluorane-remifentanil association. Hypothesis Tissue oxygen saturation (StO 2 ) and the rate of tissue deoxygenation during stagnant ischemia can early and reliably detect inadequate tissue oxygenation and assess prognosis in medical emergency room patients. Introduction Early recognition of patients with inadequate tissue oxygenation facilitates early diagnostic evaluation and treatment that was correlated with improved outcome. Near-infrared spectroscopy is noninvasive and in the emergency setting is a rapidly appliable method for measuring StO 2 .\n\nMethods In a prospective observational study we included 340 consecutive medical emergency room patients. On admission, StO 2 and the rate of tissue deoxygenation during stagnant ischemia were measured by the near-infrared spectroscopy method (InSpectra tissue spectrometer; Hutchinson Technology Inc., The Netherlands) and correlated with clinical signs of shock, lactate and outcome. Results Three hundred and forty patients were included. Of 137 patients admitted, 16 (11.7%) were admitted to the ICU and 14 (10.2%) died in the hospital. The StO 2 was higher in patients who were not admitted compared with patients with LOS > 7 days (80.2 \u00b1 8.7% vs 76.9 \u00b1 9.2%, P = 0.009). Tissue deoxygenation was faster (16.7 \u00b1 7.0%/min vs 12.9 \u00b1 5.6%/min, P = 0.014) in survivors. Tissue deoxygenation was slower in the group of patients with clinical signs of shock compared with all patients (11.8 \u00b1 6.0%/min vs 16.5 \u00b1 7.0%/min, P < 0,05). Age, lactate and rate of tissue deoxygenation but not StO 2 were significant predictors of death (Table 1 ). There was weak but significant correlation between StO 2 and age (P < 0.0001, r = -0.28), StO 2 and lactate (P = 0.035, r = -0.12) and StO 2 and systolic blood pressure (P < 0.0001, r = 0.26). Methods Microcirculatory hemoglobin oxygen saturation (\u00b5HbO 2 , %) and blood flow (flow, AU) were measured using an O 2 C \u00ae (Lea Medizintechnik GmbH, Giessen, Germany) probe applied to the buccal mucosa and to the thenar eminence. Measurements were obtained simultaneously at two depths, superficial (2 mm) and deep (6 mm), every 2 seconds for 5 minutes and were recorded for later analysis. The procedure was repeated on another occasion at least 1 week apart.\n\nWe studied 20 healthy subjects; 10 males and 10 females (mean age = 38 \u00b1 18 years, range 21-74 years). Both \u00b5HbO 2 and flow measurements were consistently higher when measured from the deep tissue layers (6 mm) than those measured from the superficial layers, regardless of the site of measurement. Buccal mucosal \u00b5HbO 2 ranged from 78% to 96% and varied only minimally (CV: 4-7.5%), whereas there was a marked variability in flow measurements (CV: 29-63.9%). The reproducibility of buccal mucosal \u00b5HbO 2 and flow measurements were moderate to good (that is, intra-individual reliability, ICC: range 0.7-0.87, P < 0.05). However, only measurements from the superficial mucosal layers showed a moderate to good degree of inter-individual agreement (that is, inter-individual reliability, ICC: range 0.68-85, P < 0.001). LDF and VLS values measured on the thenar eminence were highly variable, were not reproducible, and the inter-individual agreement was poor.\n\nConclusion O 2 C \u00ae provides reliable measurement of buccal \u00b5HbO 2 and microvascular flow. Skin measurements on the thenar eminence are highly variable and unreliable.\n\nCentral venous pressure in a femoral access: a true evaluation? Introduction Traditionally invasive haemodynamic measurements of pulmonary artery occlusion pressures have been used to assess the volume status in critically ill patients. The vascular pedicle, as seen on chest X-ray scan, is the mediastinal silhouette of the great vessels. We hypothesized that the vascular pedicle width (VPW) on supine, portable chest X-ray scans could be used to predict intravascular volume overloaded status in critically ill patients. Methods We conducted a prospective, blinded observational trial where both pulmonary artery occlusion pressures (PAOP) and VPWs were measured in patients admitted to the ICU. We used measurements of PAOP \u2265 18 mmHg as indicative of a fluid overloaded state, and measurements of PAOP < 18 mmHg as normal or low volume states. Standardized, portable chest X-ray scans in the supine position were obtained within 1 hour of PAOP measurement. Receiver-operating characteristics (ROC) curves were constructed using different cutoffs of the VPW measurement to identify sensitivities and specificities for each value (see Figure 1 ). Results Measurements were obtained from 50 patients. Using ROC-derived cutoffs, a VPW measurement of 74.5 mm was found to have a sensitivity of 83% and a specificity of 78% for correctly predicting a fluid overloaded state.\n\nConclusions These results suggest that serial measurements of the VPW can reliably be used to predict intravascular volume overload in the ICU. [3] 38.8%, [4] 15.1%, [5] 4.5%. The mean self-rate was 2.8 \u00b1 0.9, with that of residents (2.8 \u00b1 0.9, n = 129) being similar to that of specialists (2.7 \u00b1 0.8, n = 111), P < 0.31. Conclusions The ability of physicians to predict advanced cardiopulmonary parameters based on clinical evaluation and conventional monitoring alone has considerable limitations and is not improved by experience.\n\nIntroduction Many therapeutic decisions are made in the ICU on the basis of clinical judgment and conventional monitoring alone, although these may be inadequate for a reliable estimate of hemodynamic status. We therefore measured the effects of more advanced cardiopulmonary parameters (ACP) on major therapeutic decisions.\n\nMethods Cardiopulmonary assessment was done in critically ill patients from 12 European ICUs independently by one to four physicians per patient just before the use of the PiCCO monitor (Pulsion, Germany). Following cardiopulmonary evaluation and prediction of ACP (reported elsewhere), each physician suggested a therapeutic plan before and after the first set of PiCCO measurements (cardiac output, systemic vascular resistance, global enddiastolic volume, stroke volume variation, and extravascular lung water), and then self-rated the accuracy of his original therapeutic plan. 18 males) requiring advanced invasive hemodynamic monitoring due to cardiovascular instability were included in a prospective observational study in a university hospital setting with a 24-bed medical ICU and a 14-bed anaesthesiological ICU. Volume-based hemodynamic parameters were determined using the thermal-dye transpulmonary dilution technique. Simultaneously, the IVC diameter was measured throughout the respiratory cycle by transabdominal ultrasonography.\n\nWe found a statistically significant correlation of both inspiratory and expiratory IVC diameter with central venous pressure (P = 0.004 and P = 0.001), extravascular lung water index (P = 0.001 and P < 0.001), intrathoracic blood volume index (P = 0.026 and P = 0.05), the intrathoracic thermal volume (both P < 0.001), and the paO 2 /FiO 2 oxygenation index (P = 0.007 and P = 0.008, respectively). Conclusions Sonographic determination of the IVC diameter is useful in the assessment of volume status in mechanically ventilated septic patients. This approach is rapidly available, noninvasive, inexpensive, easy to learn and applicable in almost any clinical situation without doing harm. IVC sonography may contribute to a faster, more goal-oriented optimization of fluid status and may help to identify patients in whom deleterious volume expansion should be avoided. It remains to be elucidated whether this approach influences the outcome of septic patients.\n\nIntroduction The hypothesis is that measurement of the cardiac index (CI) is accurate between the ultrasound cardiac output monitor (USCOM) and the esophageal Doppler monitor (EDM). The EDM is a minimally invasive device that has demonstrated strong correlation with cardiac output measurements obtained by thermodilution. A disadvantage of the EDM is the need for probe placement in the esophagus, effectively limiting its use to Available online http://ccforum.com/supplements/11/S2 Method We included 110 consecutive adult patients immediately after cardiac surgery in a prospective, single-center study taking place in the ICU. CO measurements obtained from NICOM and PAC-CCO were simultaneously recorded minute by minute ( Figure  1 ). We evaluated the accuracy, precision, responsiveness, and reliability of NICOM for detecting CO changes. Tolerance for each of these parameters was specified prospectively. Results A total of 65,888 pairs of CO measurements were collected. Mean reference values for PAC-CCO ranged from 2.79 to 9.27 l/min. During periods of stable PAC-CCO (slope < \u00b110%, 2SD/mean <20%), the correlation between NICOM and PAC-CCO was R = 0.82; bias was +0.16 \u00b1 0.52 l/min (+4.0 \u00b1 11.3%), and the relative error was 9.1 \u00b1 7.8%. In 85% of patients the relative error was <20%. During periods of increasing CO, slopes were similar with the two methods in 96% of patients and intraclass correlation was positive in 96%. Corresponding values during periods of decreasing CO were 90% and 84%, respectively. Precision was always better with NICOM than with PAC-CCO. During hemodynamic challenges, changes were 3.1 \u00b1 3.8 minutes faster with NICOM (P < 0.01) and amplitude of changes were not different (not significant). Finally, sensitivity of the NICOM for detecting significant directional changes was 93% and specificity was 93%. \n\nIntroduction Reliable continuous hemodynamic monitoring of critically ill patients is essential for effective volume management and adequate administration of vasoactive drugs. The PiCCO system allows continuous measurement of the cardiac index using arterial pulse contour analysis. Calibration of this system by transpulmonary thermodilution is recommended every 8 hours. In this study we compared the difference of the continuous measurement of the cardiac index using the arterial pulse contour analysis (CIpc) with the cardiac index acquired by the transpulmonary thermodilution (CItd) when calibrating the system. Methods Our study includes 140 measurements in 10 critically ill patients (eight males, two females, age 37-84 years, mean 64.1 \u00b1 13.0 years) requiring hemodynamic monitoring with the PiCCO system. Five patients had septic shock, three hepatorenal syndrome and two acute heart failure. First the CIpc was recorded immediately before the next calibration and afterwards the CItd was measured three times, which resulted in a simultaneous calibration of the pulse contour algorithm of the PiCCO system. We performed a mean of 14 \u00b1 9.4 measurements per patient. The time-lag between the measurements was 12 hours 54 minutes \u00b1 7 hours 47 minutes.\n\nThe comparison of the CIpc immediately before calibration and the calibration-derived CItd resulted in a correlation coefficient of 0.84 with a P value of 0.02. In the Bland-Altman analysis the CIpc was a mean 0.14 l/min/m 2 lower than the CItd. The standard deviation was 0.72 l/min/m 2 . There was no correlation of the timelag between the calibrations and the difference of CIpc and CItd (r = -0.03; P = 0.13). Conclusion The PiCCO system allows a reliable continuous measurement of the cardiac index using the pulse contour analysis. In our study we could not find an increased difference of CIpc and CItd even with longer time periods between the calibrations using transpulmonary thermodilution. Because calibration is easy to achieve and additional data for the intrathoracic blood volume and the extravascular lung water are obtained, a 12-hour period between the calibrations is reasonable. The precision has been looked into previously and strategies to improve it have been made (that is, averaging three or four measurements over the respiratory cycle) yet not much is known about the precision of transpulmonary techniques in terms of repeatability. This study aims to look into the coefficient of variation (CV) of the lithium dilution technique in a mixed (medical/surgical) intensive care population and propose a method to improve its precision.\n\nWe performed four consecutive lithium dilution cardiac output determinations on 70 critically ill patients requiring haemodynamic monitoring. The heart rate (HR), central venous pressure (CVP) and mean arterial pressure (mAP) were documented in conjunction with cardiac output estimation. Data were excluded if a \u00b15% change in HR, CVP or mAP occurred during the sequential measurements. The CV ((SD/mean cardiac output) x 100) was calculated for single measurements and for the average of repeated measurements. In order to clinically accept the precision of the technique, we aimed to obtain a CV below 10%.\n\nResults Sixty-five series were suitable for analysis. The CV showed a normal distribution and no correlation with the magnitude of the mean cardiac output. The mean CV for single lithium dilution was 12.3%. The CV for the average of n lithium dilutions was 8.6% for n = 2, 7.1% for n = 3, 6.1% for n = 4.\n\nConclusions The CV for one lithium dilution was higher than clinically acceptable (12.3 > 10%). The average of two lithium dilution measurements improves the precision by 30% and shows an excellent CV (that is, 8.6%). When measuring cardiac output with LiDCO an average of two lithium dilution curves provide an excellent precision, and we suggest that in this population (medical/surgical) this approach should always be used when calibrating the pulse pressure algorithm (PulseCO) at the baseline. Results Six patients were studied (five males, one female). All were intubated but none were receiving muscle relaxation. Mean age was 60.6 \u00b1 21.8 years. The mean APACHE II score was 20.8 \u00b1 2.3. Ten paired results were obtained (see Table 1 ). The mean difference of LiDCO from PiCCO overall was -5.49 \u00b1 21.3%. The correlation coefficient r = 0.73 (P = 0.0166). Conclusion There was general agreement between LiDCO and PiCCO. There were significant differences (>30%) in only three out of 10 measurements. Combining the results of >1 lithium calibration may improve accuracy. These results are promising but a larger trial will be required.\n\nIntroduction Measurement of extravascular lung water (EVLW) obtained with transpulmonary thermodilution (PiCCO system) can help the physician to guide fluid management of critically ill patients [1] . The thoracic fluid content (TFC) is a parameter deriving from the electric conductivity of the thorax, determined from intravascular, alveolar and interstitial fluids [2, 3] . Introduction A new device may be used in intensive care to measure the cardiac output (CO) by arterial pulse pressure waveform analysis, but few studies have evaluated the reliability of this method and the correlation with other methods of CO determination. The aims of this study were to evaluate the CO obtained using the Flow Trac\u2122 Vigileo\u2122 and the correlation with CO obtained by transthoracic echocardiography (TTE). Materials and methods Ten critical care patients admitted to a general ICU were enrolled in the study. All patients were mechanically ventilated (tidal volume 6-8 ml/kg, plateau pressure < 30 cmH 2 O) and connected to an integrated monitoring system (Flow Trac\u2122/Vigileo\u2122; Ewdards Lifescience, Irvine, CA, USA) that attaches to an arterial cannula. After haemodynamic stabilization the CO was calculated from an arterial pressure-based algorithm that utilises the relationship between pulse pressure and stroke volume. At the same time a TTE examination was performed (Hewlett Packard, SONO 1000) and the CO was calculated by Doppler measurement of the left ventricular outflow area (LVOT) and the velocity-time integral (VTI LVOT), assuming stroke volume = cross-sectional area x VTI. Every patient had two CO determinations by TTE during Flow Trac\u2122 measurement. A regression analysis and Bland-Altman analysis were used to compare the two methods of CO determination. Results A total of 40 CO determinations were performed in 10 patients. Table 1 reports the main results. Figure 1 and use in severe sepsis compared with 2003 can be seen in Table 1 . \n\nMethods Arterial and central venous blood gas analysis was done on admission (T1) and after 8 hours (T2) of admission to the ICU. The HLOS and the incidence of complications were determined for patients with low (<70%) or normal (\u226570%) ScVO 2 . Results Sixty-three postoperative patients were screened and 23 patients were analysed. Patients were excluded if they did not have a central line positioned in the superior vena cava or blood had not been sampled at both time points. Patients with pre-ICU HLOS > 5 days (n = 10), acute spinal cord injury (n = 3), or admitted for postoperative airway management (n = 4) were omitted. ScVO 2 was low in 7/23 patients at T2 and six of these had lower gastrointestinal surgery. The HLOS (median (IQR)) was longer in those with low ScVO 2 at T2 (17 (37.8) v 9.5 (5.0) days, P = 0.04). The incidence of complications was not different. There were no differences between the ScVO 2 groups at T2 with respect to age, gender, standard base excess, lactate, haemoglobin, mean arterial pressure or central venous pressure. The volume of colloid the two groups received in the 8-hour observation period was not different although there was a trend for the low group to receive more crystalloid (P = 0.08). Conclusion A significant proportion of patients had a low ScVO 2 , which was associated with increased HLOS. The results provide a basis for the trial of postoperative early goal-directed therapy for high-risk surgical patients admitted to our ICU. However, ScvO 2 is also affected by arterial oxygen saturation (SpO 2 ), oxygen consumption, and hemoglobin (Hb) according to the formula: ScvO 2 = SpO 2 -(VO 2 / Q*1/Hb). The aim of this study was to investigate the relative influence of hypoxemia and anemia on the measurement of ScvO 2 . Methods A database of 700 pairs of arterial and central venous blood gases drawn from 300 patients admitted to the ICU of a university hospital was considered. After assessing for the technical adequacy of sampling (defined as a discrepancy of hematocrit and blood glucose between arterial and venous samples lower than 5%), 462 couples were selected for analysis. Samples were then clustered according to ScvO 2 : <70% (low), \u226570% (high). SpO 2 , partial pressure of oxygen (PaO 2 ) and Hb were considered. Venous to arterial difference of partial pressure of CO 2 (DpCO 2 ), arterial to venous difference of oxygen content (DavO 2 ) and the oxygen extraction ratio (ER) were also considered as measures of perfusion adequacy. Differences between low and high ScvO 2 samples were estimated by Mann-Whitney rank sum test (Sigma Stat, SPSS), accepting P < 0.05 as significant. Data are presented as median (25th-75th percentile). As expected, DpCO 2 , DavO 2 and ER were different between high and low ScvO 2 groups (P < 0.001). However, while Hb was similar (P = 0.670), SpO 2 and PaO 2 were significantly lower when ScvO 2 was below 70% (P < 0.001). Normalization of ScvO 2 to SpO 2 (ScvO 2 /SpO 2 ) allowed one to overcome the effects of hypoxemia. Values were: 0.662 (0.603-0.693) in the low group and 0.794 (0.757-0.828) in the high group (P < 0.001).\n\nConclusions When considering ScvO 2 as a surrogate measure of perfusion adequacy, it is mandatory to consider the relative effect of hypoxemia. Anemia was less relevant in our case mix.\n\nIntroduction Increasing oxygen delivery in high-risk surgical patients led to a dramatic reduction in both mortality and morbidity. Yet, it is still not widely practised due to logistical difficulties associated with its use. We aimed to evaluate whether pulse power analysis calibrated by the lithium dilution technique, a pragmatic minimally invasive technique, can be used to optimize the oxygen delivery index (DO 2 I) in high-risk patients during major surgery. Methods Lithium indicator dilution and pulse power analysis were used to measure cardiac output and to calculate DO 2 I (LiDCOplus system). We prospectively evaluated the oxygen delivery pattern and perfusion variables of 26 high-risk patients (LiDCO group) submitted to major surgeries and goal-directed therapy during surgery and 8 hours postoperatively, aiming to maximize the DO 2 I to levels higher than 600 ml/min/m 2 using dobutamine and either 'restrictive' (4 ml/kg/min) or 'liberal' (12 ml/kg/min) strategies of intraoperative fluid management (partial results). Postoperatively both groups received 1.5 ml/kg/min lactated ringer. Fluid challenge with 250 ml colloid was done in the presence of signs of hypovolemia and additional fluids were given if necessary. Patients were considered responders if they achieved the therapeutic goal. A historical group of 42 high-risk surgical patients in whom the therapeutic goals were to keep a mean arterial pressure between 80 and 110 mmHg, a central venous pressure between 6 and 12 cmH 2 O, hematocrit > 30% and urine output > 0.5 ml/kg/hour in the first 24 hours after ICU admission was used as control. \n\nResults Median doses of 10 \u00b5g/kg/min and 7.5 \u00b5g/kg/min dobutamine were used intraoperatively and postoperatively, respectively. A total of 75% and 84% of the patients were responders during surgery and postoperatively. However, a much better pattern of DO 2 I during surgery was seen in the liberal group than in the restrictive group ( Figure 1) . The values for arterial lactate and central venous oxygen saturation (ScvO 2 ) on ICU admission and 24 hours later for both groups are shown in Table 1 . Significantly lower arterial lactate and higher ScvO 2 were seen in optimized patients (P < 0.05 vs control group). Major complications occurred in 50% of the patients in the historical control group (21/42) and in 15% of the LiDCO group (4/26) (RR 0.15, 95% CI 0.037-0.600, P < 0.05).\n\nConclusion The use of a therapeutic approach guided by DO 2 I calculated by the LiDCO plus system, intraoperatively and postoperatively, seems to be a feasible and practical approach to guide oxygen delivery optimization therapy during major surgery in high-risk patients. Better perfusion and a much lower rate of complications were seen in optimized patients.\n\nOxygen delivery to carbon dioxide production ratio for continuously detecting anaerobic metabolism in trauma patients Conclusions Our findings showed that, for a DO 2 > 9 ml/kg, the SVO 2 , RQ, and DO 2 /VCO 2 ratio may be used interchangeably. For a DO 2 < 9 ml/kg, the DO 2 /VCO 2 ratio seems a more reliable predictor of AM than SvO 2 and RQ. The DO 2 /VCO 2 ratio can be simply and quickly calculated at the bedside because pulse wave analysis allows the DO 2 to be frequently calculated, and because the CO 2 analyzer provides VCO 2 values continuously. Combined gas exchange and pulse wave monitoring might be a valuable and a useful approach to detect AM in trauma patients.\n\nWe evaluated whether changes in, routinely measured, mean radial artery pressure (MAP) due to passive leg raising (PLR) can be used to assess preload dependence in nonspontaneous breathing patients. We therefore compared the changes in cardiac output (CO) with changes in MAP, pulse pressure (PP) and systolic pressure (SP) as well as the stroke volume variation (SVV) before PLR. Methods In this prospective, intervention and response study, 30\u00b0P LR of both legs was performed in 20 supine patients receiving mechanical ventilation after elective cardiothoracic surgery. The thermodilution cardiac output (COtd), heart rate, central venous pressure (CVP), MAP, PP, SP and SVV measurements were performed before, during and after PLR. Results The COtd, MAP, CVP, PP and SP increased after PLR. No change in heart rate and systemic vascular resistance was observed. We found a significant correlation between PLRinduced changes in COtd versus SVV during baseline (slope = 0.902, P = 0.003), changes in MAP (slope = 0.499, P = 0.003), PP (slope = 0.190, P = 0.024) and SP (slope = 0.276, P = 0.021). Changes in CVP were not correlated to changes in COtd. The area under the receiver operating curves was larger than 0.7 but not different for MAP, PP, SP and SVV. Conclusion Not only baseline SVV but also PLR-induced changes in MAP, PP and SP are reliable parameters to assess preload dependence in cardiac surgery patients. In the clinical setting we prefer the MAP approach, based on simplicity, availability and robustness. Introduction It is known that corticosteroid therapy improves the hemodynamic state in patients with septic shock and relative Available online http://ccforum.com/supplements/11/S2 S126 adrenal insufficiency. This effect is partially due to a direct action on vascular tone in the more vasoplegic patient, so they may be more hypovolemic. We tried in this study to determine whether pulse pressure variation measured just before the adrenocorticotropin test can predict the adrenal state.\n\nMethods During a period of 3 years (January 2001-December 2003) we realized a prospective observational study. All patients having septic shock were enrolled. Patients with arrhythmia were excluded. We measured hemodynamic data (mean arterial pressure (MAP), pulse pressure variation (\u2206PP)), then we realized an ACTH short test (injection of 250 mg ACTH with dosage of the cortisol level before the injection, at 30 and 60 minutes).\n\nResults One hundred and one patients were enrolled. Age was 48 \u00b1 17 years. SAPS II = 45 \u00b1 16, APACHE II score = 18 \u00b1 8, MAP = 52 \u00b1 12 mmHg, lactate = 3.5 \u00b1 2 mmol/l, and basal cortisol level (BCL) = 278 \u00b1 143 \u00b5g/l. We divided all patients into two groups using the \u2206PP cutoff: <12% (n = 30) and \u226512% (n = 71). There is no difference in the two groups in age, SAPS II, and MAP. Patients with low \u2206PP (<12%) have a significantly (P = 0.01) low BCL: 204 \u00b1 127 \u00b5g/l vs 291 \u00b1 133 \u00b5g/l, a low increase of cortisol level in response to ACTH: 264 \u00b1 144 \u00b5g/l vs 369 \u00b1 142 \u00b5g/l (P = 0.02), and a low maximum variation after the ACTH test: 59 \u00b1 52 \u00b5g/l vs 79 \u00b1 63 \u00b5g/l (not significant). The relative adrenal deficiency (\u2206max < 90 \u00b5g/l) is more frequent in patients with low \u2206PP: 80% vs 60%. Survival is lower in the low \u2206PP group, 13% vs 40%. Discussion Patients with low \u2206PP seem to be of poor prognosis because they have a low BCL, a low maximum cortisol increase after the ACTH test, and a high death rate. Annane and colleagues [1] found that nonsurvivors have low MAP, high lactate level, high basal cortisol level and low maximum cortisol level increase after the test compared with survivors. This finding is in contrast to our patients. Conclusion Patients with low \u2206PP before realizing the ACTH test tend to have more probability of adrenal deficiency, have more probability to receive corticosteroid and have poor prognosis. \n\nThe new sequential leg compression device (SCD) (Tyco, Mansfield, MA, USA) pneumatically applies sequential compression to the lower limb, while maintaining a pressure gradient throughout the compression cycle [1, 2] . We hypothesized that the SCD Response System could shift blood volume toward the thoracic compartment comparable with a volume challenge, with an increase in preload index such as the intrathoracic blood volume index (ITBVI) and stroke volume index (SVI). The aim of the study was to evaluate the relationships between changes in SVI (\u2206SVI) induced by SCD and \u2206SVI induced by rapid fluid loading (RFL) in critically ill patients. Methods Twenty-seven patients (mean age 60 \u00b1 9.1 years) admitted to the ICU were studied. Each patient received conventional monitoring plus hemodynamic-volumetric monitoring (PiCCO System; Pulsion Medical Systems, Munich, Germany). The heart rate, mean arterial pressure, central venous pressure, cardiac index, ITBVI, and SVI were recorded in the supine position before and after treatment with the SCD Express Compression System \u00ae . The same data where collected before and after a RFL performed with 3 ml/kg hydroxyethyl starch 6%. The relationships between \u2206SVI induced by SCD and \u2206SVI induced by RFL were analyzed by linear regression analysis. Statistical significance was considered to be at P <0.05. Results Linear regression analysis between \u2206SVI induced by SCD (\u2206SCD) and \u2206SVI induced by RFL (\u2206RFL) showed r 2 = 0.50 (P = 0.0002). When analyzed in a subgroup of spontaneously breathing versus mechanically ventilated patients, the relationships observed were respectively r 2 = 0.41 (P < 0.01) ( Figure 1 ) and r 2 = 0.73 (P < 0.007) (Figure 2 ). Conclusions The SCD Response System could shift blood volume toward the thoracic compartment comparable with RFL better in mechanically ventilated than in spontaneously breathing patients. Larger population studies are needed to confirm these preliminary data. S127 amplitude, as they both depend on the stroke volume. We designed a prospective study to evaluate the correlation between respiratory arterial PP variation and POP waveform amplitude variations in ventilated patients and the influence of acute lung injury (ALI) in this relationship. Methods Sixty patients were included in the study. Thirty-nine (65%) had diagnosis of ALI and 21 (35%) had normal gas exchange, defined as a relation of PaO 2 to FiO 2 (P/F) below and above 300, respectively. Respiratory variation in arterial PP and POP waveform amplitude were recorded simultaneously on a beatto-beat basis, and mean values of two measures for each parameter were compared for correlation and agreement. Results Respiratory variation in POP waveform amplitude could accurately predict variation in arterial PP with a sensibility of 83.3%, specificity of 85.7%, positive predictive value (PPV) of 71.4 and negative predictive value (NPV) of 92.3. The area under the ROC curve was 0.88 (0.79-0.97) with a best cutoff value of 14% to predict a variation in arterial PP of 13%. The kappa index of agreement was 0.65 (P < 0.001). Eighteen (30%) patients had variations in arterial PP above 13%, and 21 (35%) showed variations in POP waveform amplitude above 14%. In patients without ALI (P/F > 300) the sensibility was 100%, specificity was 93.3%, NPV was 100% and PPV was 80%. In the group with ALI (P/F < 300) the kappa index measure of agreement was 0.55, and in the group without ALI the kappa index was 0.85. PEEP levels were not different between the groups. Conclusion Respiratory variation in arterial PP above 13% can be accurately predicted by a variation in POP waveform amplitude of 14% with good correlation and agreement. Our results confirm the findings of a recent trial and suggest that the correlation is even stronger when ALI is absent. These findings raise potential clinical applications of respiratory variation in POP waveform amplitude for haemodynamic management of patients without an arterial catheter. Introduction Goal-directed intraoperative fluid therapy reduced the hospital stay after major surgery [1] . Aortic vascular surgery is associated with excessive blood loss and massive fluid shift [2] . We found that postoperative urea and creatinine improved when intravascular fluid volume was maintained using transoesophageal Doppler. Methods We randomly selected 40 patients who underwent elective infrarenal aortic surgery (aortic aneurysm repair/aortobifemoral grafting). All patients' cardiac output was continuously monitored using a transoesophageal Doppler probe (EDM\u2122; Deltex Medical, Inc., Irving, TX, USA). The corrected flow time (FTc) was recorded immediately after induction as a baseline and recorded again pre-extubation. A target FTc of 375-425 ms was aimed for. The estimated total blood loss was calculated for each patient at the end of surgery. Preoperative and 24-hour postoperative urea and creatinine were recorded for comparison.\n\nThe mean baseline FTc was 278 ms, and the mean target FTc was 405 ms. The mean average blood loss was 3.77 l/patient. The mean preoperative urea and creatinine were 5.9 mmol/l and 95.3mmol/l, respectively. The mean 24-hour postoperative urea and creatinine were 5.23 mmol/l and 76.77 mmol/l, respectively. See Figure 1 . Conclusion Goal-directed intraoperative fluid therapy aiming for FTc of 375-425 ms as a target improved the 24-hour Available online http://ccforum.com/supplements/11/S2\n\nIntroduction New information is emerging as a basis for reconsidering albumin as a key homeostatic molecule in the critically ill. We are only beginning to understand the full spectrum of albumin properties and action in healthy individuals and hypoalbuminaemic patients. Besides its function in the regulation of colloidal osmotic pressure, albumin has an important antioxidant capacity and a role in the transport of a wide range of drugs, hormones, ions, amino acids, and fatty acids. Few comparative studies have as yet been performed, and they address only a restricted number of parameters mostly defined by the European Pharmacopeia.\n\nObjective To study and compare the main albumin functions of eight preparations of pharmaceutical-grade albumin, using a battery of different techniques. Two additional albumin preparations, a preparation without stabilisers and a recombinant albumin from Pichia pastoris, were also included in the study.\n\nThe following biochemical and physicochemical parameters were investigated: total protein concentration (Biuret assay) and albumin antigen (nephelometry); quantitative analysis of contaminating proteins by nephelometry, levels of polymers and fragments by gel filtration chromatography on Superose 6, the binding affinity of exogenous ligands for Sudlow's site I (warfarin) or site II (dansylsarcosine) by steady-state spectrofluorimetry, the reactivity of Cys34 with Ellman's reagent, and the esterase-like activity using p-nitrophenyl acetate as substrate by spectrophotometry. Results All pharmaceutical-grade products show a purity ranging from 95% to 108%. The main contaminant proteins are prealbumin, transferrin, \u03b1-1 acid glycoprotein, haptoglobulin, and retinol-binding protein. All of them are in conformity with the European Pharmacopeia specifications. The warfarin-binding capacity of the 10 albumin preparations was studied. An average binding constant of 2.6 (\u00b10.3) x 10 5 M -1 (n = 1) was found. The presence of stabilisers reduced the binding of dansylsarcosine significantly (by 27-40%). The esterase-like activity toward pnitrophenyl acetate and the reactivity of Cys34 differed from product to product. Interesting is the absence of free Cys34 in the recombinant albumin. Conclusion Significant differences were observed between the 10 different human albumin preparations, recombinant or not. We confirm that the presence of stabilisers such as tryptophan derivatives significantly reduces the binding capacity of Sudlow's site II. Two important physiological properties of albumin, the esterase-like and antioxidant activities, were also found to be modified to different extents in all pharmaceutical-grade products in comparison with the albumin without stabiliser. The benefits of albumin administration should be considered carefully, taking into account the different functions and properties of albumin. Aims and methods A clinical scenario was used to assess current knowledge among medical staff regarding i.v. fluid therapy. 'An 85year-old lady is brought into A&E semiconscious. Temperature 32\u00b0C, blood pressure 90/50 mmHg and BM 6.5 mmol/l. Arterial blood gases (ABG) on room air: pH 7.12 pO 2 10.8 kPa, pCO 2 2.6 kPa, HCO 3 -12 mmol/l, O 2 saturation 94% and base excess -19'. Medical staff were asked to complete a questionnaire relating to the case under supervised conditions. Results Eighty-seven questionnaires were completed by seven SpR/consultants, 48 F2/senior house officers, 13 F1 and 19 finalyear medical students. ABG interpretation was correct in 80/87 (92%). Only 52/87 (59.8%) could calculate the anion gap and only 1/87 listed fluid as a cause of a metabolic acidosis. Eighty-three staff (93.4%) knew that a metabolic acidosis caused an increased respiratory rate. Normal saline was the first-choice fluid for resuscitation in almost 60% (52/87) cases. The chloride concentration of normal saline was known by 12/87 staff (13.8%). The serum chloride concentration was known by 28/87 staff (32%). Conclusion The majority of medical staff prescribe normal saline as their first-choice intravenous fluid. Many medical staff are unaware of the electrolyte composition of normal saline, the phenomenon of hyperchloraemic metabolic acidosis, or how to differentiate hyperchloraemic metabolic acidosis from lactic acidosis by calculating the anion gap. A good understanding of fluid therapy is important for all medical staff.\n\nIntroduction Postoperative outcomes may be improved if cardiac output and oxygen delivery are maintained at optimal levels. Trials of the use of dopexamine for this purpose have yielded inconsistent results. This may relate to the use of high doses in some trials. A meta-analysis of data from these trials may therefore identify a benefit of low-dose dopexamine on postoperative mortality and length of stay. Methods A comprehensive literature review was performed to identify published randomised trials of perioperative dopexamine infusion in patients undergoing major surgery. Individual patient S129 data were obtained, allowing a meta-regression approach to explore mortality outcomes after correction for age and dose of dopexamine. A Cox proportional hazards model was constructed to examine the length of stay. Results Five studies fulfilled the inclusion criteria [1] [2] [3] [4] [5] . Low-dose dopexamine (\u22641 \u00b5g/kg/min) was associated with a 49% reduction in 28-day mortality (6.3% vs 12.3%; OR = 0.51 (95% CI 0.29-0.89), P = 0.008). The length of postoperative stay was also reduced in the low-dose dopexamine group compared with control (median 13 vs 15 days, HR 0.75 (95% CI 0.65-0.88), P = 0.004). High-dose dopexamine (>1 \u00b5g/kg/min) was not associated with a difference in mortality (14.5% vs 12.3%; OR = 1.18 (95% CI 0.67-2.08), P = 0.37) or length of stay (median 17 vs 15 days, HR 1.10 (95% CI 0.90-1.34), P = 0.37) when compared with controls. Conclusions Perioperative use of low-dose dopexamine decreases mortality and duration of hospital stay in patients undergoing major surgery. The blood pressure waveform carries information about the cardiac contraction and the impedance characteristics of the vascular bed. Here, we demonstrate that the start of isovolumic ventricular contraction is persistently reflected as an inflection point in the pressure wave as recorded in the aortic root (TPIC) as well as in the carotid artery distension waveform (TDIC) as it travels down the arterial tree. In a group of six patients with normal pressure gradients across the aortic valve after valve replacement, the TPIC had a small delay with respect to the onset of isovolumic ventricular contraction (<10 ms). In a group (n = 21) of young, presumably healthy, volunteers, the inflection point occurred persistently in the carotid distension waveform, as recorded by means of ultrasound, before the systolic foot (intersubject delay between inflection point and systolic foot: mean \u00b1 SD = 40.0 \u00b1 9.4 ms, intrasubject SD 4.6 ms). Retrograde coronary blood flow during isovolumic ventricular contraction may be the origin of the persistent end-diastolic pressure and distension perturbation. This study shows that the duration of the isovolumic contraction can be reliably extracted from the carotid artery distension waveform.\n\nIntroduction Following the introduction of a new bariatric surgical service in Sheffield, we aimed to assess the impact upon critical care services and examine how this has changed as the service has evolved. Method All admissions for bariatric surgery between 1 April 2003 and 30 April 2006 were reviewed retrospectively. These procedures were performed on two sites, the Royal Hallamshire Hospital (RHH) and Thornbury Hospital (TH). The critical care admissions and length of hospital stay (LOS) were reviewed. Results A total of 497 patients were identified as having had bariatric surgery. After review of hospital and critical care admission data, a total of 473 were identified with complete data. Of these, 94 (19.9%) were open procedures (OP), 260 (55.0%) laparoscopic bandings (LB) and 119 (25.1%) laparoscopic gastric bypasses (LGB). The age range was 16-68 years. The average hospital LOS for OP was 6.8 days, for LGB 4.0 days and for LB 1.9 days. Surgical procedures and HDU admissions increased annually (2003) (2004) (2005) (2006) from 74 to 249, and 21 to 107, respectively. As a proportion, open procedures declined from 60% to 7%, and laparoscopic interventions increased (LB from 40% to 63% and LGB from 0% to 30%). There were a total of 14 admissions to the ITU by 10 patients, of which seven had undergone an initial OP. No admissions were elective and eight patients required further surgical interventions. HDU admissions occurred on both sites, with 148/277 (53.4%) of patients admitted to HDU at TH, and 53/196 (27.6%) at RHH. At TH only three patients required level 2 care, and 95 were discharged within 26 hours. At RHH, 16 patients required level 2 care, and 38 were discharged within 26 hours. Discussion The requirement for ITU admission in this surgical group is, and has remained, low, despite a significant increase in bariatric surgical procedures. This increase is predominantly laparoscopic surgery. HDU activity has increased as the service has expanded; however, 90.4% of this is level 1 care, particularly at TH, where admission to the HDU is a matter of policy rather than clinical necessity. Availability of a level 1 facility would significantly decrease the requirement for HDU provision -an important consideration when introducing a new bariatric service. Available online http://ccforum.com/supplements/11/S2\n\nPatients and methods Ten healthy subjects (seven females, three males; age 27.3 \u00b1 4.5 years) were investigated in a supine position before and after application of continuous positive airways pressure (CPAP) of 10 cmH 2 O by nasal mask. The study was performed using sonographic equipment with a multiprobe (convex 3.5-5 MHz; sector 2.5-3.5 MHz) and color-Doppler capability (Hitachi H 21) . IVC was visualized by a two-dimensional echographic sector probe and M-mode was used to measure the inspiratory and expiratory diameters at the origin of the suprahepatic veins. HF is composed of portal flow (PF) and hepatic artery flow (HAF). Portal velocity, assessed near the liver hilum, was used as a measure of PF, and the left intrahepatic branch resistivity index (RI) was used as a measure of HAF. Measures were repeated twice for each value of intrathoracic pressure by two different examiners and the mean value was given for the statistical analysis. Results are given as the mean \u00b1 SD. Data were evaluated by paired t test and P < 0.05 was taken as statistically significant.\n\nResults CPAP determined a reduction of portal vein velocity: 30.0 \u00b1 9.1 cm/s vs 19.7 \u00b1 5.0 cm/s (P = 0.01). IVC diameters are increased by CPAP: inspiratory diameter 9.49 \u00b1 2.5 cm vs 12.05 \u00b1 3.9 cm (P = 0.002), expiratory diameter 16.46 \u00b1 2.9 cm vs 18.08 \u00b1 3.65 cm (P = 0.05).\n\nConclusions The results of this study demonstrate that, in healthy subjects, variation of intrathoracic pressure by CPAP influences venous return. HF reduction could be due to an increased IVC pressure, as displayed by the bigger diameters measured during CPAP, other than a diaphragmatic descent. Ultrasonography is able to detect this effect and could be useful in a more complete evaluation of patient haemodynamic status in various clinical settings. Introduction Critically ill patients with severe sepsis or septic shock have a very high mortality rate. The aim of our study was to investigate the impact of intra-abdominal hypertension (IAH) on the outcome of patients with or without severe sepsis/septic shock. Methods Two hundred and fifty-three mechanically ventilated patients admitted to the general ICU of Tartu University Hospital were prospectively studied. Patients who had severe sepsis or septic shock at admission or developed it during their first week of stay were compared with patients not suffering from severe sepsis. IAH was defined as sustained intra-abdominal pressure above or equal to 12 mmHg developing within the first week in the ICU. Results Severe sepsis or septic shock was observed in 123 patients (48.6%). The ICU mortality among these patients was 33.3% compared with 18.5% in nonseptic patients (P = 0.005). IAH developed in 95 patients (37.0%). The incidence of IAH was higher among septic patients (45.5% vs 28.5%, P = 0.004). Those septic patients who developed IAH had a mortality rate of 50.0% compared with 19.4% in septic patients without IAH (P < 0.001). Mortality among nonseptic patients was not different between the patients with or without IAH (18.9% vs 18.3%). Development of IAH was a significant risk factor for death in septic patients (OR 4.15; 95% CI 1.87-9.26), but not in nonseptic patients (OR 1.04; 95% CI 0.39-2.77).\n\nConclusion Development of IAH significantly increases the risk of death in patients with severe sepsis or septic shock, but not in nonseptic patients. The abdominal pressure-volume relation can be described by a linear relation giving an elastance (E) and a pressure at zero volume (P V0 ). The goal of this study was to measure this relation in a large group of patients with different characteristics looking for the factors that influence and explain this relation. It is believed that obese persons have higher abdominal pressures and it is unclear whether muscle relaxation lowers it. A large group of 70 patients, ASA class I or II, between 21 and 75 years old and scheduled for laparoscopic surgery were included in this study with approval from the hospital ethical committee. Anaesthesia was induced with propofol 200 mg, sufentanil 20 \u00b5g, and sevoflurane 1.5 Mac in 50% O 2 /N 2 O. Some patients were fully muscle relaxed with nimbex 20 mg while others not. Patients were asked to empty the bladder before surgery. The stomach was emptied by suction through a gastric tube. An Olympus insufflator UHI-3 was initialised and the abdomen was inflated with a stepwise flow to 7, 10, 13 and 16 mmHg. When the pressure was reached, flow was stopped and the actual pressure and volume measured giving four data points. E and P V0 were calculated by fitting to a linear relation. The following recorded determinants were evaluated by regression analysis for their effect: age, length, weight, BMI, sex, gravidity and muscle relaxation. P V0 increases significantly with body weight and decreases significantly with muscle relaxation. \n\nThe aim of this study is to clarify how Japanese citizens are interested in the importance of immediate cardiopulmonary resuscitation (CPR) and defibrillation, and how they understand that importance. In Japan, the out-of-hospital emergency medical service system has been established with the ambulance service and an emergency life saving technician (ELST) belonging to the fire department.\n\nPatients' records were reviewed for the past 2 years. In Yokohama (3,700,000 people), the cardio-S131 pulmonary arrest (CPA) patient is transferred to the nearest ED of the selected 11 hospitals with adequate ability of CPR and cerebral resuscitation. We perform ultrasound, chest X-ray, and blood examination including Troponin in all CPA patients, and cerebral plane CT (40%) or chest CT (7%). CT was not performed in patients with a clearly known aetiology.\n\nWe treated 624 CPA patients in the past 2 years, 38% were cardiac and 62% were noncardiac aetiology (3% subarachnoid haemorrhage and 5% acute aortic dissection). Restricted in cardiac aetiology patients, 13% showed a ventricular fibrillation (VF) as a first monitored rhythm and 33% showed a VF during resuscitation. In all patients, 50% of VF were witnessed. In witnessed patients, 17% were witnessed by the ELST during transfer and 81% by a layperson, most of whom are patients' families and patients' friends. Fifty-three per cent were witnessed in the patients' home (35% in patients' private room, 1% in bathroom and 7% in lavatory), 4% in an aged people's residence, 1% in a hotel, restaurant, office, and 3% on the road. Only 48% of CPA patients underwent bystander CPR, and 51% of witnessed CPA patients (24% of all CPA patients) underwent bystander CPR by the witness; most patients underwent bystander CPR in the patients' home by the patients' families. Conclusions In Japan, CPA patients were witnessed mainly in their home by their families or their friends. The aetiology of some CPA patients is noncardiac (subarachnoid haemorrhage or acute aortic dissection, etc.). However, only 24% CPA patients underwent bystander CPR by the witness. \n\nClinical and experimental studies have demonstrated a marked activation of blood coagulation and fibrin formation after prolonged cardiopulmonary resuscitation (CPR). Several experimental studies suggest that thrombolysis therapy acts directly on thrombi or emboli but also enhances microcirculatory reperfusion. In this retrospective study we investigated the extent of blood coagulation and fibrin formation via the plasma D-dimer level, an indicator of endogenous fibrinolytic activity, in patients who underwent inhospital and out-of-hospital cardiac arrest from nontraumatic causes. Conclusions Vasopressin was superior to epinephrine in patients with asystole (better ROSC with admission, 24-hour survival and discharge from hospital). Vasopressin followed by epinephrine was more effective than epinephrine alone in the treatment of refractory cardiac arrest.\n\nIntroduction We evaluated the time course and relationship of proinflammatory cytokines and procalcitonin (PCT) serum levels after cardiopulmonary resuscitation (CPR). We hypothesized that an increase of cytokine levels would precede a marked increase in PCT levels and that PCT would be the best predictor of the final neurologic outcome. Methods Data were prospectively collected from 71 patients. Blood samples were taken after admission to the hospital and after 6, 12, 24, 72 and 120 hours. PCT, IL-6, IL-8 and TNF\u03b1 levels were measured using automated assays. On day 14 patients were divided into two neurologic outcome groups according to the Cerebral Performance Categories (CPC 1-3: bad; CPC 4-5: good). Differences between groups were evaluated using a t test.\n\nROC curves were computed to analyze the predictive value of the markers for a bad outcome. Results There was an early and significant increase in TNF\u03b1, IL-6 and IL-8 after admission to the hospital (14.4 \u00b1 5.2, 185 \u00b1 248 and 89 \u00b1 81 \u00b5g/l) and in the ensuing 6 hours (15.6 \u00b1 8.7, 209 \u00b1 239 and 176 \u00b1 232 \u00b5g/l) in patients with bad neurologic outcome. Initially, PCT levels were indistinguishable between the groups; however, a striking increase was observed in patients with bad neurologic outcome peaking after 24 hours (16.7 \u00b1 30.0 vs 6.9 \u00b1 2.1 \u00b5g/l; P < 0.013). PCT values after 24 hours were the best predictor for a bad neurologic outcome with an area under the curve of 0.91 (cutoff value: 0.44; sensitivity 100%/specificity 62%). Conclusion TNF\u03b1, IL-6 and IL-8 serum levels are significantly elevated in the early phase after successful CPR in patients with bad neurological outcome. PCT increases are subsequently found and have a high prognostic value for the neurologic outcome.\n\nIntroduction Mortality among patients surviving to be discharged following inhospital cardiac arrest (IHCA) is high. The present study assesses whether this might be explained by differences in patient factor or in factors at resuscitation. Methods An analysis of IHCA data collected from one Swedish tertiary hospital and from five Finnish secondary hospitals over a 10-year period. The study was limited to patients surviving to be discharged from the hospital. Multiple logistic regression analysis was used to identify factors associated with survival at 1 year from the arrest. . There was no difference in initial rhythm, delay to defibrillation or delay to return of spontaneous circulation between survivors and nonsurvivors at 12 months. Conclusion Several patient factors, mainly age, functional status and co-morbid disease, influence long-term survival following IHCA. Location of the arrest also influences survival, but the initial rhythm, the delays to defibrillation and return of spontaneous circulation do not. Introduction Mild hypothermia (32-34\u00b0C) is a promising new therapy for patients resuscitated from cardiac arrest. Animal studies suggest that early and fast cooling is crucial for beneficial effect on neurological outcome. Inducing mild hypothermia immediately after successful restoration of spontaneous circulation (ROSC) in the out-of-hospital setting remains a challenge. Therefore, a novel cooling-blanket (EMCOOLSpad \u00ae ), independent of any energy source during use, was developed. The aim of the study was to evaluate feasibility and safety of out-of-hospital surface cooling with EMCOOLSpad \u00ae in patients successfully resuscitated from cardiac arrest. Methods We included patients successfully resuscitated from outof-hospital cardiac arrest with an oesophageal temperature (T es ) >34\u00b0C. The EMCOOLSpad \u00ae consists of multiple cooling units (12 mm thick), filled with a mixture of graphite/water, which are stored in a cooling box at -3\u00b0C in the ambulance car. Cooling was initiated as soon as feasible by the first treating paramedics and emergency physicians, and was continued in the emergency room. The cooling-blanket was removed when the T es reached 34\u00b0C. The target temperature of T es 33\u00b0C was maintained for 24 hours. Data are presented as the median and interquartile range (25-75%).\n\nResults From September 2006 to December 2006, 10 patients, weighing 70 (64-93) kg, were included in the study. Cooling was initiated 14 (7-20) minutes after ROSC. The cooling-blanket decreased the T es from 36.5 (36.2-36.7)\u00b0C at the start of cooling to 34.0\u00b0C within 61 (47-93) minutes, and to target temperature T es 33\u00b0C within 83 (61-119) minutes, resulting in a cooling rate of 2.6 (1.6-3.6)\u00b0C/hour. Hospital admission was 45 (40-53) minutes after ROSC, and T es 33\u00b0C was achieved 78 (32-107) minutes after admission. In eight patients, precooled parts of the coolingblanket had to be applied repeatedly on the chest and abdomen to maintain the target temperature of T es 33\u00b0C for 24 hours. No skin lesions were observed. Conclusion Noninvasive surface cooling with the EMCOOLSpad \u00ae immediately after resuscitation from cardiac arrest, in the out-ofhospital setting, was shown to be feasible and safe. Whether early cooling, as compared with delayed cooling in the hospital, will improve neurological outcome needs to be determined in a prospective randomized trial.\n\nMild hypothermia induction following cardiac arrest using a water-circulating cooling device \n\nThe use of mild hypothermia for comatose survivors of cardiac arrest has been endorsed by the American Heart Association and the International Liaison Committee on Resuscitation [1, 2] . Unintentional overcooling is common with some techniques such as cool intravascular fluid or the use of ice packs.\n\nNowadays, the maintenance of hypothermia can be facilitated with new technology to avoid unintentional overcooling. Methods A 77-year-old male with a history of hypertension, previous replacement of aortic valve and a right coronary artery bypass was admitted to our ICU after cardiac arrest. He suffered a collapse while walking. The emergency service arrived within 5 minutes. The initial cardiac rhythm was ventricular fibrillation. The estimated time to return to spontaneous circulation was 20 minutes. The patient arrived in the hospital 50 minutes after collapse and was immediately admitted to the ICU. Thirty minutes after ICU admission, he was unconscious with a Glasgow coma score of 5. Hypothermia was induced by the Artic Sun 2000 cooling system (Medivance, Louisville, CO, USA), and the goal temperature was obtained 105 minutes after induction. The body temperature was monitored continuously with a Foley catheter. Hypothermia was maintained for 24 hours at 33\u00baC and rewarming to the target temperature of 37\u00baC was achieved over 12 hours. No electrolyte imbalances or coagulopathies were observed. No overcooling was observed at any moment. The patient was extubated on day 6 after admission and discharged from the ICU on day 10 without neurological sequelae.\n\nConclusions Careful monitoring of temperature is important during use of therapeutic hypothermia because unintentional overcooling below 32\u00b0C may place the patient at risk for serious complications such as arrhythmias, infection, and coagulopathy. Cooling with a water-circulating cooling device is fast and safe. Clinicians should work to institute protocols for mild hypothermia treatment for such patients as a part of their critical care treatment.\n\nIntroduction Induction of mild hypothermia (MH) in patients resuscitated from cardiac arrest can improve their outcome. However, benefits and risks of MH induction in patients who remain in cardiogenic shock after the return of spontaneous circulation (ROSC) are unclear. We therefore analysed a group of all cardiac arrest survivors who were indicated for MH induction in our coronary care unit (CCU) and compared the outcome of patients with cardiogenic shock syndrome after ROSC with the outcome of those who were relatively haemodynamically stable. Methods We performed retrospective analysis of all consecutive cardiac arrest survivors treated by MH in our CCU from November 2002 to August 2006. They were classified into two groups, according to whether they met the criteria for cardiogenic shock or not after ROSC and just before MH initiation. Primary outcome measures were inhospital mortality, and the best inhospital and discharge neurological result. Predicted mortality was evaluated by the APACHE II score, and neurological outcome by Cerebral Performance Category score. MH was initiated as soon as S134 possible after ROSC and patients were cooled to body temperature 32-34\u00baC for 12 hours. Results From 50 consecutive patients, 28 fulfilled criteria of cardiogenic shock before MH initiation (group A), and 22 were relatively hemodynamically stable (group B). While predicted mortality was 83.1 \u00b1 13.1% in group A and 63.2 \u00b1 19.0% in group B (P < 0.001), real inhospital mortality was 55.6% in group A and only 18.2% in group B patients (P = 0.009). The best inhospital neurological outcome was found favourable in 71.4% patients in group A and in 86.3% in group B (P = 0.306). Favourable discharge neurological outcome was reached in 100% in group A and in 94% in group B (P = 1.000). Patients in both groups did not differ in rate of complications.\n\nConclusions While inhospital mortality in cardiac arrest survivors treated by MH was expectably higher in those with cardiogenic shock than in stable patients, favourable neurological outcome was frequent and comparable in both groups of patients. Moreover, MH application was safe in both groups. Therefore, induction of MH should be considered also in cardiac arrest survivors with cardiogenic shock syndrome after ROSC.\n\nChanges in urinary 8-hydroxy-2-deoxyguanosine in patients with global brain ischemia undergoing brain hypothermia therapy: comparison of whole body and selective head cooling \n\nOxygen free radicals play an important role in global brain ischemia after cardiac arrest. Brain hypothermia therapy is effective in suppressing free radical expression. The aim of this study was to assess free radical expression under brain hypothermia, and to compare the expression between whole body and selective head cooling. Methods The subjects were 12 patients treated with mild brain hypothermia (34 \u00b1 1\u00b0C) after resuscitation following cardiac arrest in our ICU; five patients received whole body cooling and seven patients received selective head cooling. We examined the hemodynamic changes and the urinary concentration of 8-hydroxy-2-deoxyguanosine (determined by HPLC) during brain hypothermia therapy. Furthermore, we compared the prognosis at 28 days after admission to the ICU.\n\nThe induction time for whole body cooling was significantly shorter than that for selective head cooling. The rewarming time for head cooling was significantly shorter than that for whole body cooling. The mean arterial pressure and heart rate were both stable in the head cooling group. The urinary 8-hydroxy-2-deoxyguanosine concentrations decreased significantly in both groups, but data were significantly lower in the whole body cooling group compared with the selective head cooling group. Five and seven patients, respectively, exhibited good recovery 28 days after admission, in the whole body and selective head cooling groups. Conclusions Mild brain hypothermia therapy suppressed the production of free radicals following global brain ischemia. Whole body cooling had a stronger effect of suppression of free radicals compare with selective head cooling. It is considered that selective head cooling exhibits neuroprotection similar to whole body cooling.\n\nReduction of magnetic resonance spectroscopy brain temperature by convective head cooling in healthy humans This pilot study assessed the effect of forced convective head and neck cooling on brain temperature, measured by magnetic resonance spectroscopy (MRS), in five healthy adult humans (three males). Following a 10-minute baseline, subjects received 30 minutes head cooling followed by 30 minutes head and neck cooling. The cooling device delivered air at 11\u00baC and 15 m/s through a hood and separate neck collar made of a double layer of nylon sheeting, the inner layer pierced with holes. Subjects wore a windproof waistcoat taped round the base of their neck and were wrapped in blankets from the base of the neck down. Bilateral foot warming with chemical hot packs was used to encourage heat loss in the presence of normothermia. MRS temperature data were collected at the level of the basal ganglia over the baseline and the last 10 minutes of each cooling intervention. MRS detects naturally occurring brain metabolites and interpretation of the relative frequencies of N-acetyl aspartate and water allows estimation of tissue temperature in 1 cm 3 voxels. For assessment of regional cooling, voxels lying within the region formed by joining the tips of the lateral ventricles were defined as 'core', voxels within approximately one voxel of the brain surface were defined as 'outer', and all other voxels were defined as 'intermediate'. The oesophageal temperature was measured continuously with a fluoroptic thermometer. The mean baseline-corrected MRS brain temperature over all voxels reduced by -0.45\u00baC (SD 0.23\u00baC, P = 0.01, 5% CI -0.74 to -0.17\u00baC) with head cooling and -0.37\u00baC (SD 0.30\u00baC, P = 0.049, 95% CI -0.74 to 0.00\u00baC) with head and neck cooling. Head cooling reduced the mean baseline-corrected MRS brain temperature in core voxels in all subjects. The formal test for gradient was not significant (P = 0.43; 95% CI -0.15 to 0.29\u00baC). Head and neck cooling reduced the temperature in core voxels in three subjects; the test for gradient was not significant (P = 0.07; 95% CI -0.03 to 0.58\u00baC). The mean baseline-corrected oesophageal temperature reductions for the last 10 minutes of each intervention were -0.16\u00baC (SD 0.04\u00baC) with head cooling and -0.36\u00baC (SD 0.12\u00baC) with head and neck cooling. Forced convective head cooling reduced the MRS brain temperature at an equivalent of 1.35\u00baC per hour in healthy subjects, and the reduction was apparent across the brain. Methods and results A total of 50 patients were examined after successful CPR. Twenty-nine patients received 4\u00b0C cold infusions after arrival in the heart catheter laboratory. Retrospective data of S135 21 patients who had received a volume substitute by means of drips from ambient temperature served as a control group. After admission to the ICU, both groups were immediately connected to an external cooling device (CoolGard \u00ae or Thermo Wrap \u00ae ) and were cooled to a target temperature of 33\u00b0C (bladder temperature).\n\nThe average temperature at admission did not differ in both groups (35.5 \u00b1 0.9\u00b0C vs 35.89 \u00b1 0.8\u00b0C). In the group with initial cooling by means of 4\u00b0C cold infusions, a significant temperature decrease could be reached during the invasive coronary diagnostics to admission to the ICU of an average 0.84\u00b0C (35.88 \u00b1 0.9\u00b0C vs 35.04 \u00b1 0.9\u00b0C, P < 0.0001). The middle chill duration up to the achievement of the target temperature after admission was significantly shorter with the combined method (341 \u00b1 113 min versus 553 \u00b1 342 min, P < 0.01). The period to the achievement of the target temperature after the beginning of the external cooling device with the group of the combined method was significantly shorter (163 \u00b1 91 min versus 342 \u00b1 258 min, P < 0.01).\n\nConclusions The combined method with initial cooling with 4\u00b0C cold solutions shows a sure and actual prestationary cooling procedure to the introduction or realisation of mild hypothermia and offers the possibility to reach the purpose temperature significantly faster. Preclinical introduction of mild hypothermia by means of 4\u00b0C cold solutions could be a beneficial criteria in the future treatment, and probably affects the outcome of these patients. Introduction Therapeutic hypothermia (TH) following cardiac arrest is associated with several complications including symptomatic bradycardia, coagulopathy, and pneumonia [1] . Furthermore, hyperthermia is associated with poor outcome following brain injury. The incidence of these complications may be increased by excessive temperature fluctuations. We sought to compare complications between two techniques used to induce TH; surface cooling (SC) using ice packs, and endovascular cooling (EV), using the Coolgard\u2122 system (Alsius Corp., USA). Methods A retrospective review was performed of all cardiac arrest patients undergoing TH and surviving \u226548 hours between June 2005 and November 2006. Results Thirty-five patients underwent our TH protocol (SC group = 21, EV group = 14). The incidence of overcooling (<32\u00b0C) in the SC group was significantly higher than the EV group (10 vs 1, P = 0.01), whilst a trend towards more episodes of symptomatic bradycardia (SC 9 vs EV 2, P = 0.07) and rebound hyperthermia (SC 9 vs EV 2, P = 0.07) was also present. The incidence of pneumonia (SC 7 vs EV 4, P = 0.77) and coagulopathy/bleeding (SC 2 vs EV 3, P = 0.32) were similar between groups. Conclusions (1) SC is associated with a significantly higher incidence of overcooling than EC and may be associated with an increase in complications such as symptomatic bradycardia. (2) SC may also be associated with an increase in rebound hyperthermia. Introduction Hypothermia may be therapeutically beneficial in stroke victims; however, it provokes vigorous shivering. Buspirone, a partial serotonin 1A antagonist, and dexmedetomidine, an \u03b1 2 agonist, linearly reduce the shivering threshold (triggering core temperature) with minimal sedation and respiratory depression. We tested the hypothesis that buspirone and dexmedetomidine synergistically reduce the shivering threshold without producing substantial sedation or respiratory depression. Methods We studied four healthy male volunteers (18-40) on 4 days: (1) control (no drug); (2) buspirone only (60 mg orally); (3) dexmedetomidine only (target plasma concentration 0.6 ng/ml); and (4) combined buspirone and dexmedetomidine in the same doses. Lactated Ringer's solution (3\u00b0C) was infused via a central venous catheter to decrease tympanic membrane temperature by \u22482.2\u00b0C/hour; the mean skin temperature was maintained at 31\u00b0C. An increase in oxygen consumption more than 25% of baseline identified the shivering threshold. Sedation was evaluated using the Observer's Assessment Sedation/Alertness scale. Two-way repeated-measures analysis of variance was used to identify interactions between drugs. Data are presented as means \u00b1 SDs; P < 0.05 was statistically significant.\n\nThe shivering thresholds were 36.4 \u00b1 0.5\u00b0C on the control day; 34.9 \u00b1 0.6\u00b0C (P < 0.01 from control) on the buspirone only day; 36.1 \u00b1 0.6\u00b0C (P < 0.01 from control) on the dexmedetomidine only day; and 34.2 \u00b1 0.5\u00b0C (P < 0.01 from control) on the combined buspirone and dexmedetomidine day. The calculated mean difference between the thresholds on the combined and the control days was 1.9 \u00b1 0.4\u00b0C, while the measured mean difference derived from the difference between the combined and control days was 2.3 \u00b1 0.4\u00b0C. There was only trivial sedation with either drug alone or in combination. The respiratory rate and end-tidal PCO 2 were well preserved on all days. Conclusion Buspirone and dexmedetomidine act synergistically to reduce the shivering threshold with only mild sedation and no respiratory depression. This combination might be a valid treatment to prevent shivering in stroke patients during therapeutic hypothermia.\n\nThe bispectral index (BIS) is calculated from frontotemporal electroencephalogram (EEG), and the suppression ratio (SR) estimates the percentage of EEG suppression. We monitored the BIS and SR during therapeutic hypothermia (TH) and compared them with neurological outcomes of encephalopathic survivors of out-of-hospital cardiac arrest (OHCA). Methods Thirty-two patients with anoxic encephalopathy after OHCA received 18 hours of TH at 32-34\u00b0C. BIS monitoring was initiated at the onset of TH, and neuromuscular blockade (NMB) was dosed in response to shivering. Blinded BIS and SR data were recorded after the first dose of NMB, and compared with the Cerebral Performance Category (CPC) at discharge and 6 months. CPC 1 or CPC 2 was considered a good outcome (GO). Results Fourteen out of 32 patients (44%) survived, 11 (34%) with GO. Five of the remaining 18 patients died before neurological evaluation at 72 hours, and one patient recovered neurological function but died of cardiogenic shock. No survivor recalled the period of NMB. First NMB was administered a median of 5 hours after cardiac arrest or 87 minutes after initiation of TH, at 35.6 \u00b1 1.7\u00b0C. Patients with GO had a higher first post-NMB BIS (39 \u00b1 6 vs 13 \u00b1 14, P < 0.001) and a lower SR (10 \u00b1 12 vs 69 \u00b1 29, P < 0.001) than those with CPC 3-5. Initial NMB reduced frontotemporal electromyogram (EMG) power from 52 \u00b1 8 to 27 \u00b1 1 db, P < 0.001. In 17 of the patients with downloaded EEG data, an increase in EMG power of 17 dB (IQR 10-27) from baseline was associated with clinically detectable shivering. Epileptiform discharges were noted on the monitor during NMB in two patients, and seizure activity was confirmed by formal EEG in both.\n\nConclusions In cardiac arrest survivors receiving TH, a higher post-NMB BIS score and a lower SR are very early predictors of neurological outcome. The potential benefits of monitoring BIS and SR, as well as EMG power for early recognition of shivering, and continuous frontotemporal EEG to detect seizures, warrant further study. Electroencephalogram (EEG) is an appropriate monitoring tool in intensive care because it is linked to cerebral metabolism, is sensitive to ischaemia/hypoxia, can detect neuronal dysfunction at a reversible stage and is the best method to detect seizure activity. Scientific data have proved utility of continuous EEG monitoring in intensive care [1, 2] . But there is a paucity of data relating to single recordings of EEG especially in general ICUs. A retrospective chart review of patients who had bedside EEG in a medical-surgical ICU was done. Data were collected with a focus on: indication for requesting EEG, technical difficulties during the study, the report and its influence on subsequent clinical management. Forty-two charts were reviewed. The indications were: evaluation of persistent comatose state (n = 27), to diagnose/exclude seizure activity and nonconvulsive status epilepsy (n = 12), and as an adjunct to support clinical diagnosis of suspected brain death prior to formal testing (n = 3). Movement artifacts led to technical difficulty in four studies. EEG confirmed: moderate to severe nonspecific brain dysfunction as the cause for persistent comatose state by the presence of either diffuse slowing with theta/delta activity, absence of cerebral activity, continuous rhythmic and semi rhythmic lateralized/bilateral epileptiform discharges, burst suppression pattern or continuous bilateral slow U-shaped waves; anoxic brain damage by absence of changes in electrical signals following external application of noxious stimuli; and seizure activity by epileptiform discharges. Twelve reports stated that use of sedation interfered with EEG interpretation.\n\nThe following clinical decisions were made based on the EEG report in conjunction with clinical findings: initiating withdrawal of life support or 'do-not-resuscitate orders' in patients diagnosed to have hypoxic ischaemic encephalopathy (n = 15); adding/ escalating or stopping antiseizure drugs based on the presence/ absence of seizure activity (n = 12); and continuing supportive care in comatose patients diagnosed to have metabolic encephalopathy/prolonged sedation effect as the cause for coma (n = 8).\n\nBased on these results it can be concluded that, despite limitations such as motion artifacts and influence of sedation on electrical signals, EEG impacts on clinical decision-making processes in critical care. Hence it is beneficial, and more widespread use would improve its diagnostic potential.\n\nIntroduction Aneurismal subarachnoid hemorrhage (SAH) is an extremely severe illness associated with a high mortality rate and permanent severe neurological dysfunction in two-thirds of all affected patients. One of the major complications of SAH is vasospasm-associated cerebral ischemia. Clinical and experimental data suggest that vasospasm is linked to the inflammatory response associated with SAH. The goal of this study was to investigate the expression of Pentraxin3 (PTX3), a prototypic long pentraxin protein induced by proinflammatory signals in the brain, in SAH patients to test the hypothesis that SAH is followed by an upregulation of PTX3, and establish a temporal relationship between the expression of PTX3 and the induction of vasospasm.\n\nWe also attempted to establish that PTX3 is detectable in cerebrospinal fluid (CSF). Methods We studied eight severe SAH patients admitted to our neuroscience ICU with a median World Federation Neurosurgical Score of 4 and a Fisher score of 4. Arterial, jugular venous blood and CSF samples were routinely obtained every 12 hours for 7 days. PTX3 levels were measured by ELISA in plasma and CSF samples.\n\nResults Compared with plasma levels of PTX3 in normal volunteers (<2 ng/ml [1] ), SAH induced a marked increase in plasma PTX3 expression. During the first 48 hours following SAH (acute phase), PTX3 arterial and jugular venous levels increased to 36.93 \u00b1 24.32 ng/ml and 33.64 \u00b1 28.76 ng/ml, respectively, and then subsequently decreased concomitantly with the reduction of the inflammation (48-96 hours: subacute phase). PTX3 is detectable in the CSF: mean CSF levels of PTX3 were 4.07 \u00b1 3.64 ng/ml during the acute phase and 0.69 \u00b1 0.44 ng/ml during the subacute phase (t test: P < 0.05 compared with the acute phase). In the presence of vasospasm (four patients), we detected a second peak of PTX3 (4.03 \u00b1 2.85 ng/ml) in CSF samples (t test: P < 0.05 compared with the subacute phase) that was not detectable in plasma. Conclusions SAH is characterized by the production of PTX3 and the induction of vasospasm is associated with an upregulation of PTX3 in the CSF that is not detectable in plasma. Methods Retrospective analysis of prospectively gathered data of 102 patients with SICH treated in our ICU during the past 8 years.\n\nOn admission the following data were registered: vascular risk factors (high blood pressure, diabetes mellitus), age, gender, APACHE II score, GCS, hemorrhage characteristics (location, side, volume, mass effect), surgical procedure, MODS, blood pressure (systolic, diastolic, mean), pulse pressure, pulse rate, laboratory parameters (hemoglobin, white cell and platelet count, INR, serum values for Na, glucose, lactate, creatinin, bilirubin). Also registered were length of stay (LOS), duration of mechanical ventilation (MV), time of intubation (TT) and patient outcome. Haemodynamic instability was defined as low mean blood pressure and support with vasoactive and inotrop drugs. Statistical evaluation was performed using univariate and multivariate logistic regression, Student's t test Pearson's chi-square test and Fisher's exact statistic were used. Patients who where operated on had higher mortality but were also more severely ill. Conclusion Age, severity of illness, ICH score, hypoxemia, haemodynamic instability, and increased temperature are directly related with the outcome of patients with SICH. Gender, LOS, MV, TT, and MODS did not influence mortality.\n\nIntroduction Recently, the medical treatment in acute stroke has been making rapid progress. Especially, in the ischemic stroke of acute stage, the efficacy of thrombolysis, systemic t-PA or local transarterial urokinase infusion has been proved. However, the effective treatment time is still quite limited. The patients must be brought to the stroke center as soon as possible. We analyzed the reason why most stroke patients delay coming to the stroke center.\n\nWe extracted the problems and proposed some solutions.\n\nThe clinical subjects consisted of 1,112 consecutive patients with ischemic stroke in the acute stage, hospitalized in our hospital between April 2003 and September 2006. We investigated the clinical course, especially the time from the onset to the physical examination, and radiological examinations (CT, MRI, MRA and/or cerebral angiography). The mean age was 72.3 years. Among them, 334 patients were classified as atherothrombosis, 232 were cardiac embolism, 439 were lacunar infarction and 107 were transient ischemic attack. Only 19 patients underwent acute thrombolytic therapy.\n\nResults Two hundred and forty-one patients (21.7%) were hospitalized within 3 hours from the onset, and 365 patients (32.9%) were within 6 hours. Among them, only 438 were admitted by ambulance. We found the following results. The main reason for the delayed admission is through another hospital, not a stroke center. The patients denied their symptoms are not so rare. The patients or their family often hesitate to request the emergency car.\n\nConclusions The most significance point for rapid diagnosis and therapy is that people must doubt 'stroke' at first. We should further educate citizens to the warning signs of stroke and also the necessity of emergency admission using an emergency car. In addition, we should justly build a core stroke center in the district and centralize the patients.\n\nThe intraaortic balloon pump has been shown to improve cardiac output and diastolic coronary flow. The aim of our study was to determine the effects of intraaortic counterpulsation (IABP) on cerebral blood flow velocities measured on the middle cerebral artery. Methods In 11 cardiac surgery patients receiving IABP postoperatively, blood flow velocities in the middle cerebral artery were assessed by transcranial Doppler (TCD). In each patient, measurements of V max , V mean and V min were performed at four different pump settings: without support (WS), and at pump assist pulse with ratio 1:1, 1:2 and 1:3.\n\nResults Repeated-measures analysis of variance: P = 0.0006, considered extremely significant variation of TCD measurements among IABP settings. Comparing all pairs of V max , V mean and V min values, we found that V max and especially V mean are significantly greater at the 1:2 and 1:3 pump settings, but not at the 1:1 setting. We also found that the end diastolic velocities (V min ) were significant lower during the pump deflation. None of our patients had a significant diastolic flow velocity reversal during the pump deflation. Conclusions Left ventricular support with IABP significantly changed the flow velocity pattern of our patients. The pump significantly increased the V max and the V mean at the 1:2 and 1:3 settings because of pump inflation during the diastole. We suggest that the velocities did not change at the 1:1 setting because the end-diastolic flow velocities reduce during every pulse, according to pump deflation. Background Patients with critical illness can acquire a syndrome of weakness and dependence on mechanical ventilation that has been linked to peripheral nerve and muscle injury. Our aim was to systematically review published data on the diagnosis, risk factors and outcomes of patients with critical illness neuromuscular abnormalities (CINMA). Methods MEDLINE, EMBASE, CINAHL, and the Cochrane Library were searched, and studies were included if they reported on ICU patients > 16 years old who were evaluated for CINMA clinically and electrophysiologically, and they contained sufficient data to quantitatively measure the association between CINMA and clinically relevant exposures and/or outcomes. Two reviewers independently extracted data on study methodology and quality, methods for diagnosing CINMA, and CINMA prevalence, risk factors, and outcomes. Results In 1,421 ICU patients who were evaluated in 24 studies, 655 (46%) were diagnosed with CINMA. All enrolled patients were receiving protracted mechanical ventilation, had sepsis, or had multiple organ failure. Diagnostic criteria for CINMA were heterogeneous and few reports explicitly differentiated between the polyneuropathic, myopathic and mixed types of CINMA. CINMA was linked in several studies to hyperglycemia, the systemic inflammatory response syndrome, sepsis, renal replacement therapy, and catecholamine administration. In contrast, across studies there was no consistent relationship between CINMA and patient age, gender, severity of illness, multiple organ failure, and use of glucocorticoids, neuromuscular blockers, aminoglycosides, or midazolam. Mortality was not increased in patients with CINMA, but mechanical ventilation and ICU and hospital stays were prolonged. Conclusions The risk of CINMA is nearly 50% in a subset of ICU patients with sepsis, multiorgan failure, or protracted mechanical ventilation, but there were no data to support CINMA as an independent predictor of death. The impact of frequently cited risk factors is uncertain, but emerging data indicate glycemic control decreases CINMA risk in vulnerable patients.\n\nGram-negative bacteremia is an independent predisposing factor for critical illness polyneuromyopathy Introduction Critical illness polyneuromyopathy (CIPM) is a major clinical problem in the ICU resulting in prolonged ICU stay and increased morbidity and mortality. Objective To investigate risk factors of CIPM involved, in a general multidisciplinary ICU. Patients and participants Four hundred and seventy-four (323 males/151 females, age 55 \u00b1 19) consecutively admitted patients in a 28-bed university multidisciplinary ICU were prospectively evaluated. All patients were assigned admission APACHE II (15 \u00b1 7) and SOFA (6 \u00b1 3) scores and were subsequently evaluated for newly developed neuromuscular weakness. We examined muscle strength according to the Medical Research Council scale, deep tendon reflexes, sensory function and muscle wasting. Laboratory values and medical therapy were recorded daily. Other potential causes of new-onset generalized weakness after ICU admission were excluded before the diagnosis of CIPM was established. Out of the 474 patients, 185 remained in the ICU for \u226510 days.\n\nResults Forty-four (23.8%) out of those 185 patients developed generalized weakness that met the criteria for CIPM. Patients with S139 CIPM had a higher admission APACHE II score (18.9 \u00b1 6.6 vs 15.6 \u00b1 6.4, P = 0.004) and SOFA score (8.4 \u00b1 2.9 vs 7.1 \u00b1 2.9, P = 0.013). Multivariate logistic regression analysis showed that risk factors independently associated with the development of CIPM were severity of illness at the time of admission to the ICU, administration of aminoglycoside antibiotics and high blood glucose levels. Analysis according to severity of illness stratification revealed the emergence of Gram-negative bacteremia as the most important independent predisposing factor for CIPM development in less severely ill patients. Conclusions CIPM has a high incidence in the ICU setting. Our study revealed the important association that Gram-negative bacteremia, aminoglycosides, hyperglycemia and severity of illness have with CIPM development. \n\nWe examined interhospital cooperation after critical and emergency care for life-threatening cervicalthoracic-abdominal trauma (n = 501) and thoracoabdominal emergency diseases (n = 236) who were treated with intensive care in our CCEC and were able to be discharged or transferred to another acute treatment hospital.\n\nOf the trauma patients directly transferred to our center, 48% were transferred to the 'affiliated hospitals', whose medical staffs were dispatched from the 'departments' in our university, 17% were transferred to the nonaffiliated hospitals, and 34% were directly discharged from our center. Of emergency disease patients, 28% were transferred to the affiliated hospitals, 20% were transferred to other hospitals, and 52% were directly discharged. Patients staying in our center for more than 14 days tended to be transferred to the affiliated hospital. Of trauma patients indirectly transferred from other hospital to our center, 30% and 11% were transferred to the affiliated and nonaffiliated hospitals, and 19% were directly discharged. Of emergency disease patients, these values were 21%, 7%, and 13%, respectively. Patients staying in our center for more than 14 days tended to be transferred to the affiliated hospital. Discussion and conclusion These results are thought to be a common situation in a typical urban city in the world. Now, the interhospital cooperation between city hospital and referral hospital does not function well because of poor understanding of retransfer to the previous hospital, resulting in dysfunction of the management of critical patients in the local medical area. It is important to construct a new interhospital-cooperation system based on the local medical area.\n\nThe objective was to determine the relationship of early hypothermia to multiple organ failure and mortality in severely injured trauma patients. Methods This prospective observational study was performed at seven Level I trauma centers over 16 months. Severely injured patients with hypoperfusion and a need for blood transfusion during the early hospital course were followed with near-infrared spectroscopy-derived tissue oxygen saturation (StO 2 ) and clinical variables. Outcomes including multiple organ dysfunction syndrome (MODS) and 28-day mortality were evaluated. Hypothermia was defined as temperature < 35\u00b0C within the first 6 hours.\n\nResults Hypothermia was common (43%, 155/359). Hypothermic patients were more likely than normothermic patients to develop MODS (21% vs 9%, P = 0.003), but did not have increased mortality rates (16% vs 12%, P = 0.28). The maximum base deficit (Max BD) in hypothermic patients did not discriminate between those who did or did not develop MODS (9.8 \u00b1 4.6 mEq/l vs 9.4 \u00b1 4.4 mEq/l, P = 0.56) but had good discrimination for mortality in both hypothermic and normothermic patients. Significant predictors of MODS using multivariate analysis included minimum StO 2 (P = 0.0002) and hypothermia (P = 0.01), but not Max BD (P = 0.09). Predictors for mortality with multivariate analysis included minimum StO 2 (P = 0.0004) and Max BD (P = 0.01), but not hypothermia (P = 0.74). Hypothermia remained a significant risk factor for MODS when fluid/blood infusion volumes were included in the multivariate model. Conclusions Hypothermia is common in severely injured trauma patients and is a risk factor for MODS but not mortality. Minimum StO 2 predicts MODS and mortality in normothermic and hypothermic patients, while the predictive effect of BD for MODS is blunted in the presence of hypothermia.\n\nThe nonlactate gap: a novel predictor of organ failure and mortality following major trauma were measured on admission. We derived the calculated ion gap using a simplified Stewart-Figge equation, and subtracted the measured serum lactate from the calculated ion gap to obtain the nonlactate gap (NLG).\n\nResults See Table 1 . The NLG discriminated survivors from nonsurvivors (P = 0.008, analysis of variance). An NLG above 2 mmol/l was associated with an increased risk of mortality (P = 0.010, Fisher's exact test). No patient with an NLG less than 2 mmol/l died; 32.4% of the patients with an NLG above 2 mmol/l died. A NLG above 2 mmol/l also correlated strongly with organ failure (Multiple Organ Dysfunction Syndrome score P = 0.011, Sequential Organ Failure Assessment score P = 0.011, Mann-Whitney U test). \n\nWe describe the NLG for the first time, and quantify it using simple bedside calculations derived from routine blood investigations. The NLG is an excellent marker for organ failure and death following major injury, and should be used to guide trauma resuscitation.\n\nPrehospital hypotension that persists on arrival at the emergency department is a powerful predictor of mortality following major trauma Objective Outcome following major injury is time dependent. Early identification of high-risk patients allows rapid decision-making and correction of life-threatening disorders. Complex scoring systems are of limited value during major trauma resuscitation. Our aim was to evaluate the utility of a single blood pressure during the prehospital phase in combination with the blood pressure on arrival at the emergency department.\n\nMethods Data were collected prospectively on 1,111 patients admitted to a Level 1 South African trauma unit over a 1-year period. Patients were subdivided into two groups according to the combination of their prehospital (PH) and emergency department (ED) blood pressure. Hypotension was defined as a systolic blood pressure less than 90 mmHg. Mortality was defined as death within 30 days.\n\nThe mortality in patients (n = 1,031) with normal PH and ED blood pressure was 5.4%. The mortality in patients (n = 80) with PH and ED hypotension was significantly higher at 45% (P < 0.0001, chi-square test) (Table 1) .\n\nThe combination of prehospital and emergency department systolic blood pressure is a simple yet extremely powerful predictor of mortality following major trauma and should be used as a triage tool to rapidly identify the highest risk patients. S141 extravasation site computed tomography (CT). In this study we investigated the site and extent of contrast medium extravasation on CT findings and its effect on treatment and predicting clinical outcome in trauma patients. Methods Fifty patients admitted to our emergency department with blunt abdominal trauma showing contrast medium extravasation on abdominal-pelvic CT scan were included in our study for 33 months. Patients were prospectively collected and medical records were reviewed and analyzed retrospectively. The patients' clinical and laboratory findings, abdominal sonographic (FAST) findings, and CT findings were reviewed. Extravasation sites were classified as intraperitoneal, retroperitoneal, intrapelvic and correlated with post-treatment complications, mortality and morbidity rates.\n\nResults The incidence of extravasation site was intraperitoneal in 33 cases (66%), retroperitoneal in 13 cases (26%), and intrapelvic in four cases (8%). The frequency of injured vessels showing extravasation was 18 (36%) hepatic vessels, nine (18%) splenic vessels and six (12%) iliac vessels. There was no correlation between the extravasation site and ICU or total hospitalization duration (P > 0.523). Sixteen patients with intraperitoneal extravasation required surgical intervention, six patients underwent angiography with embolization. In patients with retroperitoneal extravasation, nine were treated conservatively and two with embolization. Over all there were no significant differences between the extravasation site and treatment modality. The intraperitoneal group had the highest mortality with 13 deaths (11/33, 39%) and the highest early mortality rate (10/13, 76%) in the first 24 hours (P = 0.001).\n\nConclusion CT findings in patients with blunt abdominal trauma showed no significant correlation between the contrast medium extravasation site and treatment modality, ICU hospitalization duration, or final results. However, patients with intraperitoneal extravasation required more aggressive transfusion with packed red cells and had a higher mortality rate in the first 24 hours. Techniques suggested during primary survey include Xray of C-spine, chest and pelvis. This can be time consuming and radiation intensive. In comparison with conventional multiple radiographs, Lodox (Statscan), a full-body digital radiology device, performs a.p. and lateral whole-body examinations in 3-5 minutes with about one-third of the irradiation and without the necessity for lifting patients. This is the first device installed in Europe. Methods This paper describes our experience with the use of a new low-dose X-ray technique as part of our modified ATLS algorithm, where single-total a.p./lateral body radiographs have been implemented as adjuncts to primary survey in favour of several conventional X-rays. Results There were 94 patients (males = 59; females = 35) between 4 October and 9 December 2006; age range from 1 to 86 years. The ISS ranged from 3 to 75 (ISS > 16 in 54/94 patients). The average time for obtaining LODOX radiographs was 3.5 minutes (range 3-6 min). The mean time in the resuscitation room (during primary and secondary surveys) was 28.7 minutes with the new technique compared with 29 minutes before implementing LODOX (median time 27 min to 24 min). In 54/94 patients an additional full body CT scan was performed as adjunct to secondary survey. In only 14/54 patients were additional conventional X-rays necessary to visualize the skeleton. Conclusion The implementation of a modified ATLS algorithm using LODOX allows a complete a.p. and lateral whole-body examination without a significant increase in the time taken for resuscitation.\n\nSince we are at the very beginning of a learning curve we are confident that in future the time for the ATLS primary survey can be markedly reduced. The LS imaging system seems to be a useful tool for rapid screening and management of trauma patients.\n\nPeripheral oxygen extraction predicts organ failure and mortality following major trauma Table 1 . Peripheral, but not central, oxygen extraction with a threshold of 150 ml oxygen extracted per litre of blood distinguished survivors from nonsurvivors on admission to the trauma ICU. Low peripheral oxygen extraction (<150 ml) had an odds ratio for risk of death of 5.3 (P = 0.016, Fisher's exact test) and was associated with higher organ failure scores (P = 0.044, Mann-Whitney U test). A trend of increasing peripheral oxygen extraction was also a strong predictor of mortality (P = 0.019, Mann-Whitney U test) and organ failure (P = 0.003, Mann-Whitney U test). Conclusions With an arterial and venous blood sample, and a simple equation, we have for the first time demonstrated that absolute and serial peripheral oxygen extraction are powerful predictors of organ failure and mortality following major injury. S142 management enable surgical stabilization in the early phase of trauma care. Logistical organization and accessibility to several therapeutic solutions can influence a physician's decisions regarding a trauma patient. The aim of this study is to investigate whether the timing of surgery and method of stabilization in trauma patients with femoral fracture can influence the incidence of pulmonary complication, MOF and the length of stay in the ICU. Method In a retrospective study performed at a Level I trauma center, we considered all adult patients with major trauma (ISS > 15) and femoral shaft fracture admitted between January 2003 and July 2006. Patients were separated into two groups according to the management strategies for the femoral fracture: group 1, no surgery within 72 hours after primary admission; group 2, surgical stabilization within 72 hours (DCO). To compare the two groups we considered age, ISS, RTS, TRISS, SAPS II, GCS, comorbidity, and other associated surgery. Parameters of evaluation were: mortality in the ICU, ICU length of stay, respiratory failure and length of ventilation, and daily SOFA collected for 8 days.\n\nStatistics were determined with the Student t and chi-squared tests; P < 0.05 was considered significant.\n\nWe identified 48 patients, 24 for each group. The groups were comparable regarding all the considered parameters except for GCS at admission (group 1, 8.63 \u00b1 5.12; group 2, 12.2 \u00b1 3.99; P = 0.01) and TRISS (group 1, 62.04 \u00b1 34.55%; group 2, 82.37 \u00b1 18.60%; P = 0.01). We observed in group 2 a significant decrease of mortality (5 vs 0; P = 0.02), incidence of ALI-ARDS (13 vs 4; P = 0.01) and pneumonia (18 vs 6; P = 0.01), a decrease of SOFA score (mean SOFA score: 7.58 \u00b1 4.11 vs 3.97 \u00b1 2.39, P < 0.001; maximum SOFA score: 9.83 \u00b1 4.36 vs 5.62 \u00b1 2.97, P < 0.001; days with SOFA >6: 3.79 \u00b1 3.08 vs 2.16 \u00b1 2.18, P = 0.04).\n\nWe observed an improvement of respiratory parameters and SOFA score in patients treated with DCO. Furthermore, patients with worse neurological conditions at admission do not undergo orthopaedic surgery because it could worsen the cerebral perfusion (risk related to transfer to a far operating room). The physician's decisions (and therefore the patient's prognosis), in our experience, are limited by access to optimal therapeutic solutions that could improve the clinical course of the patient.\n\nHead trauma: risk factors for early brain death -our experience \n\nIn a group of patients that evolved in brain death after head trauma, we evaluated the risk factors for an early brain death (in the first 3 days) among the parameters collected prehospital, in the emergency room (ER) and during the ICU stay. Method All the consecutive patients admitted to the ER of Careggi Hospital that evolved to brain death after head trauma during the period January 2004-June 2006 were considered (n = 54). The following parameters were considered for the study: prehospital phase: hypoxemia (SaO 2 < 95%), hypotension (SAPS II < 90 mmHg), orotracheal intubation, fluids (>1,000 or \u22641,000 ml), and GCS; ER phase (ATLS approach): hypoxemia, hypotension, orotracheal intubation, fluids, GCS, blood lactate, pharyngeal temperature, and ISS; ICU stay: SAPS II, daily SOFA score, blood lactate, core temperature, glycaemia, and ScVO 2 (>75% or \u226475%). On the basis of the timing of brain death, the patients were divided into two groups: group 1 (n = 27), brain death occurred in the first 3 days; group 2 (n = 27), brain death occurred in the days after. Statistics were determined with the Student t and chi-squared tests; P < 0.05 was considered significant. Results The significant differences between the two groups are reported in Table 1 . A strict relationship exists between early brain death and prehospital treatment. During the ICU stay low levels of ScVO 2 and high levels of glycaemia are related to early brain death. Conclusion The results of the study confirm that prehospital hypotension is the main risk factor for an early evolution to brain death in head trauma. Also, patients that have prolonged hypoperfusion and neurohormonal imbalance after the postresuscitation phase present an increased risk of brain death.\n\nSensitivity and specificity of a triage score dedicated to trauma patients in a tertiary-level hospital: preliminary results Then a craniotomy and traumatic brain injury (TBI) was made. A regional cerebral blood flow catheter (RoCBF) was placed under the dura. After the TBI the intraabdominal hemorrhage was made by pulling the titch (rupture of the aorta). The animals were assigned into two groups: group A (fluid resuscitation) and group B (hypotensive resuscitation). The animals that survived after 1 hour of hemorrhage were managed by surgical checking and with 1 hour more of fluid resuscitation. Results See Table 1 : RoCBF and SjO 2 before and after the surgical checking of the hemorrhage. Conclusion In group B there was complete restoration of cerebral blood flow and brain oxygenation, after the surgical checking of hemorrhage. Hypotensive resuscitation causes significant reduction in mortality in patients with closed intraabdominal trauma and coexisting head injury, without putting cerebral function in jeopardy.\n\nHyperoxemia improves cerebral autoregulation in severe traumatic brain injury The intrinsic autoregulation mechanisms of the cerebral vessels that normally maintain a constant cerebral blood flow (CBF) relatively independently from the cerebral perfusion pressure variations are frequently impaired in the severely traumatized. It has been shown that hyperventilation can restore the cerebral autoregulation and controlling intracranial pressure, but much less attention has been given to the effects of the hyperoxic state in the restoration of cerebral autoregulation. The purpose of the study was to compare the autoregulatory response to hyperventilation vs hyperoxia in severe traumatic brain injury.\n\nAvailable online http://ccforum.com/supplements/11/S2 Introduction The goal of the study was to evaluate the neurobehavioral effects of the C1-inhibitor (C1-INH), an endogenous inhibitor of complement and contact-kinin pathways, following controlled cortical impact (CCI) brain injury in mice.\n\nMethods Mice were anesthetized and subjected to CCI brain injury. At 10 minutes postinjury, animals randomly received an intravenous infusion of either C1-INH (15 U) or saline (equal volume, 150 \u00b5l). A second group of mice received identical anesthesia, surgery, and saline to serve as uninjured controls. The neurobehavioral motor outcome was evaluated weekly (for 4 weeks) by performing a neuroscore, and cognitive function was evaluated at 4 weeks postinjury using the Morris water maze. Results Consistently, brain-injured mice receiving C1-INH showed attenuated neurological motor deficits during the 4-week period compared with injured mice receiving saline (Figure 1 ). At 4 weeks postinjury we observed a trend towards a better cognitive performance in mice receiving C1-INH compared with mice receiving saline (n = 8 per group, P = 0.08).\n\nConclusion Post-traumatic administration of the endogenous complement inhibitor C1-INH significantly attenuates neurological motor deficits associated with traumatic brain injury.\n\nHaemostatic activation markers in brain injury for mortality prediction: comparison of blood samples from the jugular bulb and central venous line Introduction Norepinephrine used in clinical routine to increase cerebral perfusion following severe traumatic brain injury (TBI) may activate \u03b1 2 -adrenergic receptors on platelets, thereby possibly promoting formation of microthrombosis and inducing additional brain injury. Methods Arterial and jugular venous platelets isolated from norepinephrine-receiving TBI patients (n = 11) and healthy volunteers (n = 36) (cubital vein) were stimulated in vitro with increasing Available online http://ccforum.com/supplements/11/S2\n\nChanges in P-selectin expression in isolated platelets stimulated in vitro with norepinephrine or TRAP. +P < 0.001 vs low-dose norepinephrine; *P < 0.001 vs controls; #P < 0.001 vs first week.\n\nnorepinephrine concentrations (10 nM to 100 \u00b5M); thrombin receptor activator peptide (TRAP) served as positive control. P-selectin expression was determined by flow cytometry (FACS).\n\nResults Following TBI, the number of unstimulated P-selectinpositive platelets was significantly decreased in the second week by 60%. During the first week, the in vitro stimulatory effect was significantly reduced; in the second week, however, norepinephrine-mediated effects exceeded changes in controls and the first week without a difference between arterial and jugular venous platelets (Figure 1 ). . A large-bore catheter (8.5 mm ID) was inserted in the right superior vena cava (for injection of preformed blood clots) and large-bore catheters were inserted in the inferior vena cava via the S147 femoral vein and in the aorta via the femoral artery (for accessing CPB). We measured the mean arterial pressure (MAP), cardiac output (CO), blood gases, pulmonary artery pressure measurement (MPAP), end-tidal CO 2 (ETCO 2 ) and blood gases. Then 100-300 ml preformed blood clot was injected until systemic circulation ceased (systolic AP < 25 mmHg) and a 5 minute interval was allowed before start of CPB (flow rate of 4-7 l/min). Heparin 10,000 IE was given i.v., and rt-PA 10 mg as an i. \n\nThe aim of the present study was to examine the coagulation status of patients in the early postburn period. Method Coagulation and fibrinolysis parameters -antithrombin III (ATIII), protein C (PrC), free protein S (PrS), plasminogen activator inhibitor 1 (PAI-1), tissue plasminogen activator (t-PA), thrombin/ antithrombin complexes (TAT), plasmin/\u03b1 2 -antiplasmin complexes (PAP), fibrin degradation products (F1.2) -were measured at ICU admission and daily thereafter for 7 postburn days. Results Forty-five patients were screened (nine nonsurvivors and 36 survivors). All patients had a severe deficiency of the coagulation inhibitors on admission. Normalization of these levels in survivors was observed at day 5 for ATIII and PrC, and at day 7 for PrS. All patients had elevated levels of TAT during the investigation period, but survivors had significantly lower levels at day 7 postburn (6.2 \u00b1 4.9 vs 11.4 \u00b1 4.5 \u00b5g/l, P < 0.001). PAP levels were within the physiological range in both groups at day 1, remained low in survivors, but raised significantly in nonsurvivors at day 7 (19.3 \u00b1 14 vs 80.9 \u00b1 10.4 \u00b5g/l, P = 0.003). The t-PA levels were elevated permanently only in nonsurvivors. PAI-1 levels were increased at day 1 in both groups, but returned to normal values at day 5 in survivors. The degree of PAI-1 activation was significantly higher than this of t-PA. The F1.2 levels were permanently elevated and there was no statistically significant difference in both groups. A logistic regression analysis revealed that ATIII and PrS at days 3, 5 and 7, Pr C at days 5 and 7 and TAT at day 7 were independent predictors of ICU death. Conclusion Our findings indicate the early postburn dysregulation of the hemostatic balance characterized by the activation of procoagulant pathways. Although fibrinolysis was activated, the inhibition of fibrinolysis was more pronounced at the same time.\n\nThe coagulation inhibitors and TAT levels seem to be early predictors of ICU mortality. The TEG parameter of fibrinolysis (LY30) was significantly increased during CPB. The frequency of aprotinin administration was 12% TEG, 10.7% control. No correlation between positivity of fibrinolysis and peroperative/postoperative blood loss and red blood cells (RBC) and fresh frozen plasma (FFP) transfusions were recorded. No correlation between aprotinin administration and peroperative/postoperative blood loss and RBC transfusion were recorded. Positive correlation between aprotinin administration and FFP transfusion were recorded Conclusion Fibrinolysis was usually not associated with serious bleeding. There was no positive effect of aprotinin to reduce bleeding or transfusion therapy. FDP and D-dimers are not useful to detect fibrinolysis in cardiac surgery. S148 intrinsic/extrinsic activation, FIBTEM = EXTEM with inhibition of platelets) were from Pentapharm (Munich, Germany). Results The clot formation time (CFT) and maximum clot firmness (MCF), but not clotting time (CT), were strongly correlated with the fibrinogen level and platelet count. Surgery significantly decreased the ROTEM haemostatic activity, but normalised in most patients within 14-18 h postoperation. Lowest haemostatic activity (dramatic increase in CT and CFT, decrease in MCF) was seen when patients were conditioned for cardiopulmonary bypass (CPB). When connected to CPB, the CT and CFT turned to recover, but MCF in EXTEM remained unchanged and MCF in FIBTEM declined further indicating continuous fibrinogen consumption. In 12.5% of our patients, postoperative MCF in FIBTEM was reduced to <9 mm indicating a need for fibrinogen substitution. Low postoperative activity in ROTEM \u00ae was associated with high postoperative blood loss. The positive predictive value and specificity of FIBTEM were clearly superior to those of the APTT or prothrombin time. Up to 50% of patients had an increased haemostatic activity in preoperative ROTEM \u00ae , and this was associated with high CRP levels and intraoperative blood loss.\n\nConclusions ROTEM \u00ae is a valuable tool to monitor perioperative haemostasis. The decreases in haemostatic activity and postoperative bleeding are probably due to anticoagulant therapy as well as fibrinogen and platelet consumption. An increased preoperative haemostatic activity is probably due to an acute phase reaction associated with advanced atherosclerosis, and the high intraoperative bleeding in these patients might be due to the atherosclerotic vessels rather than due to an insufficient haemostasis. Objective Cardiac surgery, in particular when done with cardiopulmonary bypass (CPB), is frequently associated with excessive intraoperative and postoperative bleeding. Here we employed the Technothrombin \u00ae thrombin generation assay (TGA; Technoclone) to monitor changes in perioperative haemostasis. Methods One hundred and forty-eight patients (age 66.0 \u00b1 9.8 years; 103 males, 45 females) with elective cardiac surgery and a CPB time >45 minutes (mean 114 \u00b1 56 min) were enrolled. Arterial blood samples were obtained preoperative, postoperative and 14-18 hours postoperative and centrifuged within 30 minutes after withdrawal, for 5 minutes at 5,000 x g. Plasma samples were stored until analysis at -80\u00b0C. Thrombin generation (TG) induced by the TGA RC low reagent (71.6 pM tissue factor) was measured in 96-well multiplates using a FLUOstar OPTIMA fluorescence reader (MWG Labtech). Results Due to cardiac surgery, the lag-phase and time-to-peak of TG increased by 45% and 35%, respectively. In parallel, the peak thrombin concentration and maximum slope decreased by 42% and 51%, respectively (P < 0.000001). Both the lag-phase and time-to-peak returned to basal values within 14-18 hours postoperative, but the peak thrombin and maximum slope of TG rose above the preoperative values (+39% and +68%, P < 0.0005). The on-pump time was positively correlated with lag phase and time to peak and negatively correlated with peak thrombin and maximum slope of TG when measured at 14-18 hours postoperative, but there was no correlation at early postoperative. With respect to classical coagulation parameters, significant correlations were observed between TG and activated partial thromboplastin time at preoperative and 14-18 hours postoperative, and between TG and prothrombin time at postoperative (P < 0.025-0.001). At 14-18 hours postoperative there was a significant correlation of TG with platelet as well as leukocyte counts (P < 0.025).\n\nConclusions The data provide clear evidence for a marked decrease of TG during cardiac surgery followed by an excess restoration in the postoperative phase. Factors released from platelets and leukocytes (procoagulant microvesicles?) might contribute to the enhanced TG observed at 14-18 hours postoperative.\n\nIntroduction Orgaran \u00ae (danaparoid sodium) is a low-molecularweight, nonheparin glycosaminoglycan antithrombotic that is currently a first-line treatment for heparin-induced thrombocytopenia (HIT Conclusions Our findings of high incidence of positive heparin antibodies may be mainly due to unfractioned heparin flushes. The use of heparin flushes with the new high-quality catheters and monitoring kit is questionable. Since low doses of unfractioned heparin could lead to the production of antibodies and subsequently to HIT, further studies should examine the risk-to-benefit ratio of the use of unfractioned heparin flushes in the ICU setting.\n\nIntroduction Transfusion-related acute lung injury, due to plasma from female donors containing antileucocyte antibodies, may be a common contributor to the development of acute lung injury (ALI) in the critically ill. In July 2003 the English Blood Service stopped using female donor plasma for the manufacture of fresh frozen plasma (FFP). Patients undergoing repair of ruptured abdominal aortic aneurysm (AAA) receive large amounts of FFP and often develop ALI. We investigated whether the change to male-only FFP was associated with a change in incidence of ALI in patients undergoing emergency AAA repair. Methods A before-and-after, observational, single-centre study. Subjects were 211 consecutive patients undergoing open repair of a ruptured AAA between 1998 and 2006. Primary outcome was development of ALI (PaO 2 /FiO 2 < 300 and bilateral pulmonary infiltrates on chest X-ray) in the first 6 hours after surgery. Secondary outcomes were time to extubation, and survival at 30 days. Chest X-rays were examined independently by two radiologists who were blinded to the study hypothesis.\n\nResults One hundred and twenty-nine patients were operated on before and 82 after the change in FFP procurement. Groups were well matched, with respect to age, sex, co-morbidities and severity of illness, and received similar volumes of i.v. fluids and blood products from admission to 6 hours postoperatively (mean units of FFP, 8.6 before and 8.39 after, P = 0.833). The maximum tidal volume, PEEP, and CVP were similar in both groups. Norepinephrine was given to 8.5% of patients in the before group compared with 24.4% after (P = 0.001), otherwise inotrope use was similar. Primary outcome: there was significantly less ALI following the change to male-only FFP (36% before vs 21% after, P = 0.042). Secondary outcomes were not statistically different between groups; however, patients with ALI in either group had a poorer 30-day survival (59% vs 80%, P = 0.005). Conclusion Exclusion of female-donor FFP was associated with a statistically significant reduction in the incidence of ALI in patients undergoing repair of a ruptured AAA.\n\nIntroduction Octaplex is a new prothrombin complex concentrate that is indicated for treatment or perioperative prophylaxis of bleeding in patients with deficiency of the prothrombin complex coagulation factors, such as deficiency caused by treatment with vitamin K antagonists or by liver failure, when rapid correction of bleeding is required. The study was conducted to demonstrate both prevention of bleeding and achievement of haemostasis in acute bleeding and to obtain further information about the safety of administration of Octaplex. Conclusion Our results support the evidence that recombinant factor VII is an effective haemostatic agent that can be used in patients with uncontrolled bleeding in postcardiopulmonary bypass surgery. Despite its high cost, there is an advantage in terms of effectiveness to support its use. However, its potential thromboembolic risk remains a concern and current evidence may restrict its use only as a rescue therapy following failed conventional treatment.\n\nIntroduction Recombinant activated factor VII (rFVIIa) (NovoSeven \u00ae ; Novo Nordisk, Denmark) was developed primarily for the treatment of bleeding episodes in hemophilic patients. Little information is available on the use of rFVIIa in other situations, such as intractable postsurgical or post-traumatic bleeding. In the field of cardiac surgery, only a very few cases of treatment with rFVIIa are described. Because of the difficulty in performing randomized trials in this setting, information based on case studies is very valuable Methods We studied 22 consecutive patients treated with rFVIIa due to refractory postoperative bleeding. rFVIIa was given only after all other options, including revision, to stop bleeding was failed. The amount of bleeding, the number of transfused units of red cells, platelets and other blood products were recorded both before and after administration of rFVIIa.\n\nCritical Care March 2007 Vol 11 Suppl 2 27th International Symposium on Intensive Care and Emergency Medicine\n\nMean hourly drain output 6 hours pre and 2 hours post NovoSeven administration. Clinical databases and electronic medical records were reviewed, collecting data elements needed to assess study objectives. One hundred and twenty patients were identified. Twenty-seven patients were excluded because they lacked documentation of RF7a administration, were treated for neurologic indications or had incomplete medical record data. Ninety-three patients were analyzed. RF7a effectively achieves hemostasis in patients with intractable bleeding, reducing blood product transfusions within 6 hours of treatment ( Figure 1 ). Our findings suggest differences in PRBC transfusion reduction between RF7a doses. We observed no additional reduction PRBC transfusions in patients administered doses greater than 60-90 \u00b5g/kg ( Figure 2 ). Effects of RF7a on surgical re-exploration and other potential related adverse events (stroke, AMI, VTE, etc.) are forthcoming. Our study, like others evaluating RF7a for this indication, are limited by the retrospective scope. Randomized trials comparing RF7a doses are under way. Although RF7a therapy is costly, minimal reductions in surgical reexploration may offset the cost of RF7a therapy provided that adverse events are not increased.\n\nThe S154 low platelet count. Maternal mortality was reported to be as high as 24%. Two classifications of the HELLP syndrome are widely used (Tennessee [1] and Mississippi [2] ). The aim of this study is to determine mortality of HELLP syndrome as defined by each classification and try assessing the most relevant. Patients and methods Prospective data collection as part of the APRiMO study (Assessment of Prognosis and Risk of Mortality in Obstetrics). Included were all obstetric patients transferred from a referral center for high-risk pregnancies in our independent multidisciplinary ICU. The study period was January 1996-September 2004. Demographic data, obstetric history, morbid events, length of stay (LOS), severity of illness scoring systems and organ dysfunction scores at day 1 of admission were collected. Exclusion criteria were LOS < 4 hours. The main outcome of interest was survival status at ICU discharge. Two groups were compared: patients with HELLP syndrome as defined alternatively by the two classifications (Group I), and patients without hepatic dysfunction (Group II). Results are expressed as the mean \u00b1 standard deviation. P < 0.05 was considered significant. Discrimination of the classifications was assessed by the area under the receiver operator characteristic curve (AuROC). Calibration was assessed by the Hosmer-Lemeshow (HL) goodness-of-fit test. Data were computed on SPSS 11.5, Win-XP compatible.\n\nResults and discussion Differences between Group I and Group II were statistically significant concerning obstetric hemorrhagic complication (P < 0.001), incidence of acute renal failure (P = 0.01), mortality (P = 0.001), LOS (6.5 \u00b1 7 days vs 4.4 \u00b1 4 days, P = 0.001), SAPS-Obst score (24.5 \u00b1 8 vs 16.8 \u00b1 7, P < 0.001). The Mississippi classification discriminated well, but calibrated badly. In contrary, the Tennessee classification was a poor discriminator but calibrated very well. See Table 1 . \n\nThe results of the present study complement the findings of previous studies on timing of stroke in pregnancy [1, 2] . We found that preeclampsia/eclampsia and intracranial vascular malformations were the major causes of stroke in pregnancy, which agrees with other findings [2, 3] . Our study shows a high mortality rate of 35%, which indicates that careful management of at-risk patients during the first postpartum weeks is warranted.\n\nIntroduction Renal dysfunction is associated with increased morbidity and mortality in intensive care patients. The aim of this study was to investigate the use of laboratory markers in an ICU, especially glomerular filtration rate (GFR) markers, and to compare two GFR markers, creatinine and Cystatin C. A secondary aim was to assess the frequency of reduced GFR in this patient group using the creatinine and Cystatin C estimated GFRs as several pharmaceuticals are prescribed according to renal function. The aim of this study was to evaluate FEU in the follow-up of ARF due to prerenal azotemia in order to predict the necessity of renal replacement therapy (RRT). The prospective study took place at the ICU of Stadtspital Waid, Z\u00fcrich. All patients admitted starting from 19 February 2006 were evaluated for ARF according to the RIFLE classification. ARF due to prerenal azotemia was defined as ARF combined with FEU of less than or equal to 35%. FEU was calculated as [(urine urea/blood urea)/(urine creatinine/plasma creatinine)] x 100. Urine specimens were taken and FEU was calculated daily until complete or partial renal recovery was reached or the criteria for RRT were met. The goal of therapy was reconstitution of renal function by treatment of the underlying condition. RRT was initiated according to the usual criteria. Statistics were determined using Fisher's exact test.\n\nAvailable online http://ccforum.com/supplements/11/S2\n\nFractional excretion of urea (FEU) in the follow-up of acute renal failure (ARF) due to prerenal azotemia. Data presented as mean \u00b1 SD. n = number of patients.\n\nBy 7 December 2006, 15 patients met the inclusion criteria for ARF due to prerenal azotemia (nine males, six females). The mean age was 71 \u00b1 11 (SD) years for male patients and 58 \u00b1 31 years for female patients. Twelve out of the 15 patients responded to conservative management and had complete or partial renal recovery. Three patients needed RRT. Two of them refused RRT and died during the course of the disease. During the first 48 hours after initiation of conservative therapy, FEU remains less than or equal to 35% in all three patients who needed RRT. By contrast, nine out of 12 patients in whom renal function recovered without RRT showed a FEU of more than 35% within the first 48 hours (P < 0.05) (Figure 1 ). In patients presenting with ARF due to prerenal azotemia, an increase of FEU above 35% within the first 48 hours after initiation of conservative therapy for ARF is a valuable parameter to predict renal recovery. After initiation of conservative therapy, measurement of FEU is of no value concerning discrimination of prerenal azotemia and acute tubular necrosis in ARF.\n\nHemodynamic goal-directed intermittent hemodialysis The major concern of IH in septic shock patients is hemodynamic instability. Whether stringent hemodynamic monitoring and maintaining preset goals would reduce these instabilities and deliver optimal RRT is not clear. We undertook a prospective study to evaluate this concept. Methods Preset goals of keeping the mean arterial pressure (MAP) > 75 mm, cardiac output (CO) > 5 l/min and cardiac index (CI) > 2.5 l/min/m 2 throughout the session were attempted to achieve by a stepwise protocol as follows: (1) fluid boluses, (2) increase in vasopressor/inotrope dose, (3) adjustment in the ultrafiltration rate between 250 and 700 ml/hour, and (4) adjustment in the blood flow rate between 150 and 300 ml/minute on a hemodialysis machine. Dopamine, norepinephrine, vasopressin and dobutamine were used alone or in combination to achieve these goals. Hemodynamic monitoring and data collection were done with Datex S-5 and Flo-Trac Vigileo monitors. Introduction Acute renal failure (ARF) is one of the major complications after cardiovascular surgery. To investigate the incidence and prognosis of ARF after cardiac surgery, we performed a retrospective study. Our hypothesis is that ARF is more common in patients who underwent surgery for great vessel diseases than in those who underwent coronary or valve surgery. Methods We enrolled patients over 18 years old who underwent cardiovascular surgery and entered our ICU between 2004 and 2005. The background diseases were classified into two groups: great vessel disease, and coronary/valve disease. We determined ARF when serum creatinine increased by more than 50% of the preoperative values, or when renal replacement therapy was newly started. By reviewing ICU charts, we collected data before, on admission to the ICU and during the ICU stay.\n\nResults ARF occurred more frequently in patients with great vessel disease than in those with coronary/valve disease (33.5% vs 11.2%, P < 0.05). The prognosis of patients with ARF was poorer than those without ARF in both groups (Figure 1 ). Patients with ARF showed a longer operation time, larger intraoperative bleeding and a higher level of blood lactate on admission to the ICU than those without ARF. Patients with ARF showed higher incidence of liver dysfunction, and needed a longer mechanical ventilation and ICU stay.\n\nConclusion ARF is common after cardiovascular surgery, especially after surgery for great vessel disease. ARF was associated with more postoperative organ disorders. were included in a retrospective observational study. The incidence of postoperative renal dysfunction was compared in patients given aprotin, tranexamic acid or no antifibrinolytic agent, using propensity-adjusted multivariable logistic regression. Further analysis was performed comparing patients taking ACE inhibitors preoperatively with those not taking ACE inhibitors. Renal dysfunction was defined as creatinine higher than 200 \u00b5mol/l and/or renal dialysis. Patients with a previous history of renal dysfunction were excluded from the study. Results Using propensity-adjusted multivariable logistic regression (C-index, 0.82), the use of aprotinin in patients taking ACE inhibitors was associated with more than doubling the risk of acute postoperative renal failure in patients undergoing nonemergency cardiac surgery (odds ratio 2.64; confidence interval 1.32-5.27).\n\nTranexamic acid was also associated with a significant increase in the risk of renal failure (odds ratio 1.59; confidence interval 1.09-2.31) in patients taking ACE inhibitors. However, in this study, there was no association between either aprotinin (odds ratio 1.01) or tranexamic acid (odds ratio 1.19) and postoperative renal failure in patients not taking ACE inhibitors. Conclusion In cardiac surgery, there is a significant association between use of the antifibrinolytic drugs aprotinin and tranexamic Available online http://ccforum.com/supplements/11/S2\n\nS158 acid and the occurrence of acute postoperative renal dysfunction, in patients taking ACE inhibitors. The potential blood-saving benefits of antifibrinolytic drugs should be weighed up against this serious postoperative complication.\n\nIntroduction A number of preoperative patient-specific risk factors contribute to increased perioperative cardiovascular risk (myocardial infarction, heart failure, death) in patients undergoing major noncardiac surgery [1] . Renal impairment is one predictor. Creatinine values are a poor reflection of true renal function.\n\nMeasures of the glomerular filtration rate provide a more accurate measure of renal function; as such, the true prevalence of renal impairment in this population may be significantly higher than previously appreciated. Our aim was to identify that proportion of elective vascular patients undergoing abdominal aortic surgery that have CKD, assessed by the estimated glomerular filtration rate (eGFR, ml/min/1.73 m 2 body surface area; creatinine, \u00b5mol/l) [2] . Methods A retrospective analysis of our 'ABC study' database was undertaken. The ABC study is an ongoing study looking at preoperative risk assessment using cardiopulmonary exercise testing in patients undergoing elective aortic surgery.\n\nResults Sixty-six patients were included in the analysis. All patients were Caucasians, 62 males (94%). No patient had a pre-existing diagnosis of chronic renal impairment. No patients had stage 4/5 CKD (severely reduced kidney function, eGFR < 30). Moderately reduced kidney function (eGFR 30-59, CKD stage 3) was seen in 20 (30%) patients, and mildly reduced kidney function (eGFR 60-89, CKD stage 2) in 44 (67%). Only two patients had normal kidney function. The mean total cholesterol for the cohort was 4.6 mmol/l (\u00b11 mmol/l). See Table 1 . \n\nThe majority of vascular patients undergoing elective aortic surgery in our unit have impaired renal function that is not accurately reflected by creatinine values. Management of patients with stage 2 and 3 CKD is primarily cardiovascular risk assessment with aggressive treatment of modifiable vascular risk factors [3] . The full impact of risk factor modification on perioperative outcome in vascular patients requires further detailed investigation.\n\nIntroduction Gc globulin, a hepatically synthesized actin binding protein, is known to decrease in both acute and chronic liver disease, and low levels are associated with poor prognosis. Neutrophil gelatinase-associated lipocalin (NGAL), a member of the lipocalin family of proteins, has been shown to be an early biomarker of ischaemic renal damage. We prospectively investigated the use of these proteins as markers of severity of illness and prognostication in acute and acute-on-chronic liver failure requiring intensive care. Methods NGAL and Gc globulin were measured on admission to our unit using a sandwich ELISA technique (AntibodyShop \u00ae ) in 17 patients with acute liver failure (ALF) and 11 patients with acute-onchronic liver disease (ACLD). Biochemical and physiological variables were collected prospectively and entered into a physiological database (ICARE). All measurements were taken on day 1 of admission. Results are expressed as the median and interquartile range. Results The mean number of PDRs/patient was three. The mean PDR was 17% (range: 3.3-51%). In two out of 38 patients, the PDR could not be detected due to hemodynamic instability. PDR < 5% was a predictor value for irreversible liver failure (P = 0.000). In nine (25%) out of 36 patients, the PDR was <5%. Of those nine, two patients recovered its synthetic function and seven (78%) patients developed irreversible liver failure (four died of liver failure and three underwent liver transplantation) (see Table 1 ).\n\nConclusions ICG-PDR < 5% is a significant predictor of irreversible liver failure. It is a good complement of such scores for decision-making.\n\nIntroduction Desferrioxamine (DFX) is a clinically approved iron chelator used to treat iron overload. It has also shown beneficial effects in experimental acute liver failure (ALF) by inhibiting oxidative damage [1] . Lung dysfunction commonly complicates ALF. Ironmediated processes have been shown to contribute to it [2] . We hypothesized that inhibition of oxidative reactions by means of iron chelation could attenuate lung injury after ischemic ALF. This study aimed to analyse the incidence of relative adrenal insufficiency (RAI) in these patients, to identify factors associated with relative adrenal insufficiency and to describe how adrenal responsiveness affects outcome. Methods In a prospective observational multicenter study, a short Synacthen test (SST) was performed within 5 days after admission to the hospital in 25 patients with severe acute pancreatitis, after signed informed consent was obtained. The incidence of RAI, defined as an increment after SST of less than 9 \u00b5g/dl, was the primary endpoint of the study. Serum cortisol was measured at baseline and 30 and 60 minutes after 250 \u00b5g adrenocorticotropic hormone administration.\n\nThe median baseline cortisol level was 26.6 \u00b5g/dl, and increased to 43.2 \u00b5g/dl and 48.8 \u00b5g/dl after 30 and 60 minutes, respectively. RAI was found in 16% of all patients, and in 27% of patients with organ dysfunction. Patients with RAI were more severely ill and had higher SOFA scores from day 4 through day 7 after admission. All patients with RAI developed pancreatic necrosis, and all of them needed surgical intervention. Mortality was significantly higher in patients with RAI (75% vs 10%, P = 0.016). Patients who died had a lower increment in cortisol levels after the SST than patients who survived. Conclusion RAI is frequent in patients with severe acute pancreatitis and organ dysfunction. It occurs in patients with more severe pancreatitis and is associated with an increased mortality rate.\n\nThe relative sensitivity of serum lipase versus amylase for radiological image-positive pancreatitis K Introduction Pulmonary edema is a common consequence of hemorrhagic shock resuscitation. The type and amount of fluid used in resuscitation may be important determinants of the amount of edema formed. Ringers' lactate (RL) and normal saline (NS) remain common resuscitative fluids. These experiments were designed to measure the extravascular lung water (EVLW) after resuscitation from hemorrhagic shock with RL vs NS, to determine whether the fluid type results in differences in the amount of EVLW, and to determine whether there exists a threshold amount of fluid that results in the development of edema. Methods This was a randomized controlled trial using 20 female Yorkshire crossbred pigs. Animals were mechanically ventilated. Anesthesia was maintained using 2% isofluorane in 100% oxygen. Continuous hemodynamic monitoring, blood sampling, and determination of EVLW by single indicator transpulmonary dilution was done using a PiCCO plus monitor (Pulsion Medical System, Munich, Germany). The animals underwent a midline celiotomy, S163 suprapubic Foley catheter placement, and splenectomy. The spleen was weighed and, based on randomization, either LR or NS solution was infused to replace three times the spleen weight in grams. Following a 15-minute stabilization period, a standardized Grade V liver injury (injury to a central hepatic vein) was then created using a specialized clamp. Following 30 minutes of uncontrolled hemorrhage, we blindly randomized the swine to receive either NS or RL resuscitation at 165 ml/min. Resuscitation fluid was administered to achieve and maintain the baseline mean arterial pressure (MAP) for 90 minutes post injury. Results All animals spontaneously stopped bleeding within 12 minutes of injury after losing approximately 25% of their blood volume. There were no differences in initial blood loss between the two groups -estimated blood loss (mean \u00b1 standard error) RL \n\nThe incorporation of lipid emulsions in parenteral diets is a requirement for energy and essential fatty acid supply, and may prevent many metabolic disturbances associated with intravenous feeding amino acids and glucose alone in critically ill patients. For different parenteral fat emulsions, a significant impact on the immune system has been shown. In this study, the toxicity of soybean oil-based emulsion and olive oil-based emulsion on leukocytes from healthy volunteers was investigated.\n\nMethods Twenty-four volunteers were recruited and blood samples were collected before infusion of a soybean oil-based emulsion or olive oil-based emulsion, immediately afterwards and 18 hours later. The cells were studied immediately after isolation, and after 24 hours or 48 hours in culture. The following determinations were made: composition and concentration of fatty acids in plasma, lymphocytes and neutrophils, and lymphocyte proliferation. The toxicity was determined by plasma membrane integrity, DNA fragmentation, phosphatidylserine externalization, mitochondrial depolarization, production of reactive oxygen species and neutral lipid accumulation. Results Both lipid emulsions decreased lymphocyte proliferation and induced cell death, but the effects of soybean oil-based emulsion were more pronounced. Soybean oil-based emulsion provoked apoptosis and necrosis, whereas olive oil-based emulsion caused neutrophil and lymphocyte necrosis only. Evidence is presented that lipid emulsion is less toxic to neutrophils than to lymphocytes. The mechanism of cell death induced by this lipid emulsion involved mitochondrial membrane depolarization and neutral lipid accumulation, but did not alter production of reactive oxygen species.\n\nConclusions Olive oil-based emulsion can be an alternative to soybean oil-based emulsion, avoiding leukocyte death and the susceptibility of patients to infections.\n\nAvailable online http://ccforum.com/supplements/11/S2 and 15 g/dl, group B (n = 104) with Hb levels between 9 and 12 g/dl, and group C (n = 48) with Hb levels below 9 g/dl. We calculated Pearson's coefficients between the SID and bicarbonate and between chloride and bicarbonate in these three groups of patients.\n\nResults Correlation strength between the SID and HCO 3 was high and significant even at a Hb concentration below 9 g/dl (see Table 1 ). Pearson's coefficients for chloride and bicarbonate showed a moderate but significant inverse correlation in group A and group B; eventually this correlation was completely lost in group C. Immediate incidents were reported in 2.5% of the cases. The posttransfusion haemoglobin average was of 9.15 g/dl. Discussion and conclusion The evolution of the blood transfusion was remarkable, since the use of total blood in the 1980s, with the acquisition of the first techniques of separation of the blood components. The transfusion practice in Tunisia is far from being to the standards. The results obtained make it possible to transmit to the clinician the failures of the system, to better include how to prescribe a blood product, to follow its effectiveness and its possible side effects, and to apprehend the impact of the innovated biotechnologies to improve quality of transfusion medicine in coherence with the security requirements. These persons answered questions about sedation practices, use of sedation scorings systems, and withdrawal symptoms.\n\nResults Twenty-nine (82.9%) out of a total of 35 possible hospitals answered, including 113 (57.7%) answers out of a total of 196 possible answers. Ninety-seven per cent of the physicians were specialists in anaesthesiology. Eighty-seven per cent of the nurses were certified intensive care nurses. Forty-seven per cent were from university hospitals. Twenty-six per cent had a sedation protocol, 37% of the physicians and 14% of the nurses. Only one-third of the ICUs had a protocol for sedation. Sixty-eight per cent having a protocol used it always or often, whereas 32% never use it. Sixtyseven per cent had a sedation scoring system in their departments. The scoring systems used was: Ramsay 49%, Sedation Agitation Score 10% and own (locally made) scoring system 41%. Twentytwo per cent answered that the scoring systems was always used, 58% often and in 20% the scoring systems was seldom used. Forty per sent use the 'wake-up call' test, 63% physicians and 37% nurses. Sixty per cent answered 'no we do not use' the wake-up call test, 47% physicians and 53% nurses. Withdrawal symptoms were experienced more than three times as frequently by nurses compared with physicians (31% vs 9%). Five times as many experienced withdrawal symptoms in the group not having a sedation and analgesia protocol (84% vs 16%). Conclusions There is still a great educational potential for improving the use of sedation protocols and implementing sedation scoring systems and the wake up test in Danish ICUs. This potential could perhaps reduce the incidence of withdrawal symptoms. Effort should also be placed in implementing the sedation protocol in the ICU, illustrated by the differences in numbers of doctors and nurses having a sedation protocol. We compare the efficacy, adverse events, and recovery duration of etomidate and propofol for use in procedural sedation in the emergency department (ED). A randomized nonblinded prospective trial of adult patients undergoing procedural sedation for painful procedures in the ED was made. Patients received either propofol or etomidate. Doses, vital signs, nasal end-tidal CO 2 (etco 2 ), pulse oximetry, and bispectral electroencephalogram analysis scores were recorded. Subclinical respiratory depression was defined as a change in etco 2 greater than 10 mmHg, an oxygen saturation of less than 92% at any time, or an absent etco 2 waveform at any time.\n\nClinical events related to respiratory depression, including an increase in supplemental oxygen, the use of a bag-valve-mask apparatus, airway repositioning, or stimulation to induce breathing, were noted. Etomidate and propofol appear equally safe for ED procedural sedation. Etomidate had a lower rate of procedural success and induced myoclonus in 20% of patients (see Table 1 ). Background The aim of this study is to describe maturational aspects of propofol pharmacokinetics following single intravenous bolus administration in childhood. Methods Seventy propofol blood-time profiles were collected in nine neonates (mean weight 2.4, range 0.91-3.8 kg) by arterial blood samples up to 24 hours after administration of a single intravenous bolus of propofol (3 mg/kg over 10 s) before elective chest tube removal. Concentration-time curves obtained for every individual neonate were interpreted by two-stage analysis as twocompartment and three-compartment open models. These newly collected observations following intravenous bolus administration of propofol in preterm and term neonates (n = 9) were combined with individual pharmacokinetic estimates in toddlers (n = 12) and young children (n = 10) [1, 2] . Data were reported by the median and range. The Wilcoxon test or linear correlation were used to analyse the pharmacokinetic findings in neonates, toddlers and young children.\n\nResults The blood-concentration curves obtained for every individual patient were interpreted by two-stage analysis as a threecompartment open model in a cohort of 31 patients with a median weight of 11.2 (range 0.91-24) kg and a median postmenstrual age of 108 (range 27-405) weeks. The median clearance was 36.8 (range 3.7-78.1) ml/kg/min, the median apparent volume of distribution at steady state (V ss ) was 7.6 (1.33-15.6) l/kg and the median final serum elimination half-life was 377 (range 27-1134) minutes. Median clearance was significantly lower in neonates compared with toddlers and older children (P < 0.01) and these differences remained significant after allometric scaling (ml/kg 0.75/min). A significant correlation between V ss and postmenstrual age (r = 0.61, 95% CI 0.32-0.8, P < 0.004) was observed. Conclusions Propofol disposition is significantly different in neonates compared with toddlers and young children, reflecting both ontogeny and differences in body composition. Based on the reduced clearance of propofol, accumulation during repeated administration and longer recovery time are more likely to occur in neonates. Background Acute pain is common after cardiac surgery and can keep patients from participating in activities that prevent postoperative complications especially respiratory complications. Accurate assessment and understanding of pain are vital for providing satisfactory pain control and optimizing recovery. This study was performed to find the location, distribution, and intensity of pain in a sample of adult cardiac surgery patients during their postoperative ICU stay. Methods In a prospective study, pain location, distribution (number of pain areas per patient), and intensity (0-10 numerical rating scale) were documented on 250 consecutive adult patients on the first, second and third postoperative day (POD). Patient characteristics (age, sex, size, and body mass index) were analyzed for their impact on pain intensity. There were 140 male and 110 female patients, with a mean \u00b1 SD age of 65.7 \u00b1 13.5 years.\n\nThe maximal pain intensity was significantly higher on POD 1 and 2 (3.7 \u00b1 2 and 3.9 \u00b1 1.9, respectively) and lower on POD 3 (3.2 \u00b1 1.5). The order of overall pain scores among activities (P < 0.001) from highest to lowest was coughing, moving or turning in bed, getting up, deep breathing or using the incentive spirometer, and resting. After chest tubes were discontinued, patients had lower pain levels at rest (P = 0.01), with coughing (P = 0.05). Age and sex was found to have an impact on pain intensity, with patients <60 years old and male patients having a higher pain intensity than older patients on POD 2 (4. \n\nIn patients with ischemic coronary artery disease the 'sympathomimetic' effects of ketamine can cause myocardial damage. However, the S-isomer of ketamine may have various advantages. We studied the cardiovascular stability and safety of intravenous anesthesia with S-(+)-ketamine for coronary artery bypass graft surgery (CABGS). Methods After approval of the local ethics committee and written informed consent, 315 patients scheduled for elective 'on-pump' CABGS were enrolled in the study. Patients were randomly allocated to three anesthetic protocols: sufentanil-sevofluoranepropofol (SSP), sufentanil-propofol (SP), and S-(+)-ketaminemidazolam-propofol (KMP). Standard invasive hemodynamic monitoring was performed using a pulmonary artery catheter and hemodynamic variables were reported. Measurements were taken after induction of anesthesia, after weaning from cardiopulmonary bypass, and 6 hours postoperatively. Serial plasma troponin T levels were taken: before induction of anesthesia, after surgery, and 6 and 24 hours postoperatively. All cardiovascular adverse events were recorded (such as electrocardiographic signs of ischemia, myocardial infarction, 28-day mortality). S173 factors in sex, age, trauma or nontrauma, whether or not PMCT, cause of death and final diagnosis. We especially compared the two groups: PMCT(+) and PMCT(- This abstract outlines the use of simulated critical care telephone calls into the education of trainees. We hope others may consider it for their centres. The Capital Health Region provides advanced healthcare for 2 million people, but spread over 9,800 km. We therefore rely heavily on transportation of critically ill patients to a single urban centre. In addition to geographic and climatic factors, bed pressures complicate how we triage, stabilize, transport and receive those patients. A major strategy is the 'Critical-Care-Line': a 24-hour telephone service with teleconference capabilities and contact numbers. However, experience suggests it takes practice to become proficient with its use. Given the importance of optimal communication, we arrange simulated calls. Senior trainees are paged during a normal workday by the Critical-Care-Line: just as they will be once in independent practice. The facilitator then assumes the role of a referring doctor in a small town. Peer-reviewed cases are used that include pertinent teaching points. Applicable staff at the teaching centre are briefed of this exercise and asked to act as they normally would. For example, emergency physicians, internists, senior nurses and administrators are notified that they may be brought into the call, depending on whether the trainee decides to involve other services (for example, if he/she decides a patient requires further work-up before deciding upon ICU or if he/she decides to bring the patient to emergency if no ICU bed is currently available). All calls are recorded to aid debriefing. This method allows us to ascertain how trainees ask focused histories, offer practical advice based upon the variable skill set of referring physicians, and deal with complex ethical decisions (for example, if a family wishes to override a patient's previous wish; or how aggressively to treat the terminal patient for whom no prior discussions have occurred). It allows us to test the trainees' knowledge, but more importantly we can determine how well that knowledge is applied in everyday practice. In Canada, the Royal College of Physician and Surgeons has decreed that trainees become not just medical experts, but also proficient communicators, collaborators, and managers [1] . These goals, while laudable, have been very difficult to capture without novel approaches such as the one outlined. This simple and costfree addition to our training has been very well received. Initial success means it will now be expanded throughout acute care specialist training. Conclusion Although this study is small with only two observers who were investigated, it clearly shows that there is a significant intra-individual and inter-individual variability in the classification of monitoring events done by experienced clinicians. These findings are supported by studies into image analysis that also found high intra-individual and inter-individual variability. High inter-observer and intra-observer variability is a challenge for clinical studies into new alarm algorithms. Our findings also show a need for reliable classification methods.\n\nThe goal of this program is to develop an experimental tool able to record, store and analyse data issued from critical care patients. Due to technical limitations and medical constraints, information systems able to manage such data flow are difficult to deploy. Methods Data recording is done through a laptop connected to the medical devices, allowing analogical and digital signal transmission through a high-speed network. Several servers are dedicated to specialised tasks: mass storage, model generation, artificial intelligence (AI), telecommunications, and security. A 3 Teraflops supercomputer is dedicated to intensive computation when necessary. Twenty applications are dedicated to elective tasks, most of them running using the Linux operating system. The 'Aiddiag' data-acquisition software is a standalone application adapted to patient data recording from the biomedical devices and caregiver's inputs. It has a friendly designed user-interface touchscreen at the bedside and was adapted according to caregivers' feedback. Data are also stored in a repository and a selective secondary extraction is possible. Online and offline analysis by the AI engine is allowed. Software had to consider time specifications and uses distributed computation to achieve high workload tasks. We complied to the French legal patient data management constraints. Results After 2 years, our system is fully deployed. It recorded more than 2,500 patient-hours over a 3-month period. Signal loss is less than 1%. Our tool allows recording of more than 40 digital signals, eight analogical signals sampled at a rate of 1 kHz, and caregiver comments and actions. CPU resources of the laptop are available for supplemental AI developments during data acquisition. Transfer of data to the repository is either a hotplugautomated process or delayed with 5 days of buffering in the laptop. Automated artefacts' cleaning allows time-series analysis (GARCH method) to extract behavioural models after intensive computation. The AI engine is used for medical guideline implementation (that is, severe brain trauma care algorithms) and later comparison with caregiver's behaviour. Remote use of our system is possible and schedulable, allowing other research teams to work on the data. Limitations have been detected during intensive calculation. Fine-tuning of the network will suppress these limitations. Conclusion ISIS is the first program to achieve an easy-to-use recording tool able to build a very large medical repository. Data analysis methods and AI-controlled automated complex medical guidelines are under evaluation.\n\nIntroduction As a result of centralisation of PICU services in the United Kingdom, transfer of critically ill children has become common over the past decade. It is not uncommon to receive multiple retrieval requests simultaneously, thus a tool to prioritise the urgency of this would be beneficial. Our aim was to develop such a tool and assess its inter-rater repeatability.\n\nMethods The tool was developed by three senior medical staff of the South Thames Retrieval Service (operating from the PICU at Evelina Children's Hospital, London with 1,000 calls per annum from 24 district general hospitals, resulting in 600 retrievals). A modified Delphi method was used, which comprised an iterative process including a literature review, knowledge of the underlying conditions and a review of retrievals performed by the service over the previous 7 years (n = 3,669). Inter-rater agreement was assessed using the weighted kappa statistic, and was measured between various pairings of junior and senior medical staff (n = 28 combinations) on 50 retrieval episodes.\n\nThe final tool comprised five categories (three levels of severity each) allowing for a range of scores from 0 to 15 ( Figure 1 ). Three levels of urgency were defined: semi-urgent (score <8), urgent (score 8-10), immediate (score >10). Overall the tool showed a good to very good strength of inter-rater agreement (kappa scores ranging from 0.65 to 0.88; Figure 2 ). There were no obvious differences between levels of staff seniority.\n\nThe score showed acceptable agreement, fullfilling the first step of validation. \n\nThe South Thames Retrieval Service (STRS) is a specialised paediatric intensive care retrieval service, integrated into the Evelina Children's Hospital, the lead centre in the South Thames Region, London. Over the last 5 years a number of initiatives have been adopted to reduce mobilisation times (the time from retrieval acceptance and activation to departure from the lead centre) and improve service delivery to surrounding district general hospitals (DGHs). The aim of this study was to evaluate whether these initiatives led to a reduction in mobilisation time between January 2002 and December 2006. Methods The STRS covers 24 DGHs within an 80 mile range serving a population of 1.6 million children in the South Thames region of Greater London. All calls to the service were logged on a detailed database. Retrieval requests for potential PICU patients were triaged and coordinated via a retrieval-specific telephone line. Once accepted, the onsite retrieval team was mobilised and dispatched via a dedicated ambulance to the DGH. Mobilisation includes assimilating and checking pre-packed equipment bags (ventilators, drugs, intubation kit, monitors, and so on) and organising a team of at least one retrieval nurse, doctor and ambulance driver. Details of each retrieval request to the STRS, including the time of the call, were captured on a database containing the patient demographic and clinical details. The interval between accepting the patient for retrieval and team departure from the unit was termed the 'mobilisation time' (minutes). Data were analysed over two time periods, before (n = 976 retrievals) and after 2004 (n = 1,785), coincident with a dedicated ambulance and driver on site. Nonparametric tests were used for continuous data (Kruskall-Wallis test or Mann-Whitney test) and the chi-squared test for categorical 2 x 2 comparisons. Results A total of 2,761 retrievals (median age 12 months, 78% ventilated) were performed and included for analysis during the study period, 33 were excluded (missing mobilisation times (n = 30) or elective transfers (n = 3). Figure 1 shows the process introduced to improve mobilisation times with a dedicated onsite ambulance service in 2004. There was a significant reduction in Available online http://ccforum.com/supplements/11/S2 There was also a significant increase in the incidence of sub-30-minute mobilisation times, which almost doubled after 2004 with the availability of an onsite dedicated ambulance service (14.3% to 25.9%, P < 0.0001). Conclusion There has been a significant decrease in the mobilisation time of the STRS over the last 5 years. Although the presence of an onsite ambulance service in 2004 had a significant impact on reducing retrieval mobilisation times, a number of other factors and initiatives contributed to steadily reducing mobilisation times over the study period.\n\nAmbulance transport is associated with a higher mortality than private transport following major penetrating trauma in a semi-urban environment Aims The use of private transportation has been associated with improved outcomes in urban trauma patients. The need for patient stabilization at the scene needs to be balanced with the need for early operative intervention, and therefore the need for rapid transportation to hospital. Our aim was to assess the relationship between the mode of transport to hospital and outcome in a semiurban trauma environment. Methods Data were collected prospectively on 1,396 patients admitted to a Level 1 South African trauma unit over a 1-year period. The Revised Trauma Score was used to assess injury severity and physiological derangement at the time of admission, and to allow comparison between the groups. Mortality was defined as death within 30 days.\n\nThe mortality in the blunt trauma patients (n = 527) was higher in the ambulance transport group, but this was not statistically significant. However, the mortality in the penetrating trauma patients (n = 808) was significantly higher in the ambulance transport group (P = 0.020, chi-square; Table 1 ) despite similar Revised Trauma Scores (Table 1) .\n\nThe use of ambulance transportation is associated with a 3.7-fold increase in mortality following penetrating injury. This may be related to longer times in the field resulting in delay to definitive care in hospital.\n\nThe In Southend Hospital a track and trigger system has been used since 2005 to alert nurses to abnormal physiological parameters in order to trigger urgent medical review of the unwell patient. It is recognised that the respiratory rate is a particularly useful predictor of significant deterioration and should be measured with every set of observations. This audit aimed to assess the use of the track and trigger system on the medical wards and ensure that deteriorating critically ill patients are promptly reviewed. Methods Patient observation charts were reviewed for a specified 24-hour period. Data were gathered on the frequency and type of observations taken. For patients who met criteria to trigger a review, further data were abstracted about the nature of the deterioration and the promptness of the review. Results One hundred and sixty patient-days of observations were evaluated over seven medical wards. Twenty-nine patients met the trigger criteria and in 16 cases this represented a deterioration. Doctors were called in two cases. Observations were recorded with different frequency on different wards. One ward managed to record the respiratory rate with every set of observations. Conclusions Documented deteriorations in physiological observations did not trigger medical review. This may be a communication failure or failure to recognise recorded observations as abnormal. For this process to work well relevant observations must be recorded regularly and accurately. The respiratory rate was not consistently recorded between wards and the frequency of measurement of observations was variable. Further education and training is needed to improve recording of the respiratory rate and work needs to be done to establish why doctors were not called appropriately. Concerns about the volume of work generated by the system are unfounded. A positive predictive value of 55% is acceptable and 29 'triggers' in a 24-hour period are manageable. Median retrieval mobilisation times. Background Critical care outreach services (CCOS) have been introduced in the United Kingdom with aims to: avert or ensure timely admission to critical care; enable discharge from critical care; and share skills with ward staff. We aimed to assess the impact of the introduction of CCOS at the critical care unit level, as characterised by the case mix, outcome and activity of critical care unit admissions. Methods An interrupted time-series analysis was carried out using data from 108 units participating in the Case Mix Programme that had completed a survey on CCOS provision. Individual patientlevel data were collapsed into monthly time series for each unit (panel data). Population-averaged panel-data models were fitted using a generalised estimating equation approach. Various outcomes reflecting the stated aims of CCOS were considered for three groups of admissions: all admissions to the unit; admissions from the ward; and unit survivors discharged to the ward. The primary exposure variable was the presence of a formal CCOS with secondary exposures of CCOS activities, coverage and staffing, identified from the survey data.\n\nOf 108 units in the analysis, 79 (73%) had a formal CCOS introduced between 1996 and 2004. For admissions from the ward, the presence of a CCOS was associated with significant reductions in: the proportion of admissions receiving cardiopulmonary resuscitation during the 24 hours prior to admission (odds ratio 0.84, 95% confidence interval 0.73-0.96); the proportion of admissions between 22:00 and 06:59 (0.91, 0.84-0.97); and the mean ICNARC physiology score (absolute reduction 1.2, 0.3-2.1). No significant effects of CCOS on outcomes including hospital mortality and readmission to critical care were identified for patients discharged to the ward.\n\nInterpretation The results of this study were mixed. While some differences in the characteristics of patients admitted to critical care units were found to be associated with the introduction of CCOS, there was no evidence for an impact on the outcomes of patients discharged from critical care. It was not possible to identify any clear characteristics for an optimal CCOS. Introduction Blood groups may be related to differences in inflammatory responses [1] . We looked at blood group as a risk factor for ICU mortality in general and for patients with sepsis. Methods Data were retrospectively collected from all 11,553 patients that were admitted from 1997 to 2005 to our medical/ surgical ICU. Results ICU mortality and SAPS II score for different blood groups are shown in Table 1 . P values are given for the difference between blood groups A and O. No differences were found for age, gender, and reason for admission. No influence of rhesus blood group type was seen on mortality. \n\nContribution of genomic variations within human \u03b2 \u03b2defensin 1 to incidence and outcome of severe sepsis multifunctional mediator in infection and inflammation, which has been largely explored in ex vivo studies. The lack of fully representative genetic animal models increases the importance of analyzing the impact of defensin gene polymorphisms on the courses of infectious and inflammatory diseases such as sepsis. This study was designed to investigate whether DEFB1 genomic variations are associated with incidence and outcome of severe sepsis. Six reported polymorphisms were detected in 211 patients with severe sepsis and 157 control individuals using diverse analytic methods. Linkage disequilibrium (LD), haplotype frequency, and statistical power for this association study were analyzed. The -44G-allele and -44G-allele carrying genotypes were significantly associated with incidence and outcome of severe sepsis. There was enough statistical power (1 -\u03b2 > 0.8 at type I level of 0.05) to demonstrate a significant contribution of the -44G allele to severe sepsis. The -20G allele and GG genotype were associated with susceptibility to severe sepsis, while the -1816Gallele and -1816G-allele carrying genotypes influenced the outcome of severe sepsis. SNPs -20A/G, -44C/G and -52A/G were in strong LD. Haplotype -20A/-44C/-52G showed a protective role against severe sepsis, whereas haplotype -20G/-44G/-52G served as a risk factor for fatal outcome of severe sepsis. The present findings have important implications in the understanding of the role of DEFB1 in the pathophysiology of severe sepsis, and DEFB1 genomic variations may offer a new means of risk stratification for patients with severe sepsis. \n\nWe examined the association of TNF\u03b1 promoter single nucleotide polymorphisms and haplotypes with gene expression in terms of mRNA levels and with outcome in a cohort of patients with severe sepsis. Methods Sixty-two Irish Caucasian patients presenting with severe sepsis were enrolled. Blood sampling was carried out on day 1 and on day 7. Mononuclear cells were isolated and TNF\u03b1 mRNA quantified using the technique of quantitative real-time polymerase chain reaction (QRT-PCR). DNA was extracted and assayed for four TNF\u03b1 promoter polymorphisms. Haplotypes were inferred using PHASE software. Results Twenty-seven patients died. Patients carrying an A allele at position -863 produced more TNF\u03b1 mRNA on day 1 than C homozygotes (P = 0.037). There was a trend for patients homozygous for the G allele at position -308 to produce more TNF\u03b1 mRNA on day 1 than those carrying an A allele (P = 0.059). Carrier status for haplotype 1 (with A at position -863 and G at position -308) was associated with greater TNF\u03b1 mRNA levels on day 1 (P = 0.0374). Carrier status for haplotype 4 (with C at position -863 and A at position -308) was associated with a nonsignificant decrease in TNF\u03b1 mRNA levels on day 1 (P = 0.059). When directly compared, haplotype 1 was associated with significantly greater levels of TNF\u03b1 mRNA than with haplotype 4 on day 1 (P = 0.02). Patients homozygous for the A allele at position -308 were more likely to succumb to severe sepsis than those carrying the G allele (P = 0.01).\n\nConclusion These results contradict previous in vitro functional studies on the TNF2 allele. This may be secondary to the method of quantification of in vivo gene expression with QRT-PCR providing more accurate and sensitive data when compared with prior ELISA-based assays. Indeed, the extrapolation of functionality from in vitro functional genetic tests after lipopolysaccharide stimulation may be of questionable value. We conclude that genotypic analysis does have a place in risk stratification in sepsis and that genetic variants at positions -863 and -308, or sites in linkage disequilibrium with these variants, may influence TNF\u03b1 production. \n\nThe classic response to isolated endotoxin challenge entails secretion of IL-1 and TNF\u03b1. The purpose of this study was to longitudinally characterize the cytokine response to sepsis in critically ill systemic inflammatory response syndrome (SIRS) patients. Methods Uninfected, critically ill trauma patients with SIRS were evaluated daily for sepsis. Patients were divided into two groups: pre-septic = SIRS patients who developed sepsis, and uninfected SIRS = SIRS patients remaining uninfected. Plasma samples and whole blood (PAXgene) obtained at study entry and daily for 3 days prior to sepsis were analyzed for differential gene expression between groups (Affymetrix Hg_U133 2.0 plus microarray, false discovery rate < 0.5%, P < 0.005) and quantitative plasma protein TNF and IL-1 levels (Immunoassay, Luminex\u2122, elevated if > 3 SD above the mean for normals). Gene expression data are the median fold change between groups (uP = pre-septic > uninfected). Results Gene expression on 90 patients and protein measurements on 142 patients were available. Protein levels of both subtypes of TNF and IL-1 were not elevated at any time point in either group. IL-1\u03b1 was noted to have differential gene expression 24 hours before sepsis. No differences were noted in gene expression for TNF\u03b1, TNF\u03b2, or IL-1\u03b2. Differential gene expression for only two TNF family members (TNFSF10 and TNFSF13b) was noted. However, differential gene expression for TNF and IL-1 receptors and IL-1 receptor antagonist was prominent (Table 1) . Background Myocardial dysfunction is common among critically ill septic patients. Elevated levels of cardiac biomarkers are predictors of mortality in acute coronary syndrome and in unselected critically ill patients. Our aim was to evaluate the role of the cardiac markers NT-proBNP, Troponin T (TnT) and myoglobin as predictors of inhospital and 6-month all-cause mortality in patients admitted to a general adult ICU with severe sepsis/septic shock. Methods Serial plasma samples were taken for five sequential days on all patients admitted with severe sepsis/septic shock. Samples were analysed for NT-proBNP, TnT and myoglobin.\n\nResults Samples were analysed on 49 patients. Elevated myoglobin was the only predictor of ICU mortality. Age, myoglobin and NT-proBNP levels predicted hospital mortality. Predictors of 6month mortality were age, peak TnT, peak myoglobin and peak NT-proBNP levels. The APACHE II score did not predict mortality. A score was established dependent on TnT (<0.1 = 1, \u22650.1 = 2), age (<65 years = 1, \u226565 years = 2), BNP (<10,000 = 1, >10,000 = 2), and myoglobin (<750 = 1, >750 = 2). Patients were placed into tertiles (score = 4&5, 6, 7&8) to produce survival curves (Figure 1 , P < 0.01).\n\nIn critically ill patients with severe sepsis/septic shock a score based on age and increased plasma levels of cardiac biomarkers can help risk-stratify patients and predict short-term (<6 months) outcome.\n\nThe calculated ion gap: a novel predictor of mortality in the critically ill surgical patient We evaluated these anions as a novel marker of outcome.\n\nMethods We prospectively evaluated 109 consecutive patients admitted to a surgical high-dependency unit (HDU). Regional Ethics Committee approval was obtained. Serum electrolytes, albumin, phosphate and lactate were measured on admission and days 1 and 2. We derived the calculated ion gap (CIG) using our simplified modification of the Stewart-Figge equations.\n\nThe CIG on day 1 predicted mortality (P = 0.001, analysis of variance). A CIG > 10 mmol/l correlated very strongly with mortality. The mortality in patients with a CIG < 10 mmol/l (n = 86) was 4.7%. The mortality in patients with a CIG > 10 mmol/l (n = 23) was 26.1% (P = 0.006, chi-square test). There were no differences in CIG with respect to mortality on admission or day 2 (P = 0.273 and 0.104, respectively). The mean hospital stay was significantly longer in patients with a CIG > 10 mmol/l (46.6 vs 18.7 days, P = 0.015, t test) ( Table 1) . Conclusion We describe the CIG for the first time in the critically ill surgical patient, and quantify it using simple bedside calculations derived from routine blood investigations. Failure to normalise the CIG by day 1 after admission to the HDU is an excellent marker for mortality and length of hospital stay, and should be used to guide resuscitation. \n\nLung nitroxidative stress as a prognostic factor in ventilated septic patients Introduction During sepsis and mechanical ventilation, nitric oxide (\u2022NO) is produced by lung cells. We study whether pulmonary \u2022NO production is a prognostic factor in mechanically ventilated septic patients.\n\nWe studied 50 patients with sepsis within the first 48 hours of sepsis. Operating room patients served as control a group (ORCG). Nitrite and nitrate (NO x -) and 3nitrotyrosine (3NT) in plasma and bronchoalveolar lavage fluid (BALF) were analyzed by the Griess/vanadium chloride method and ELISA, respectively. Results were expressed as median and interquartile range. Receiver operator curves were constructed to compare the predictive value of NO x values in BALF at admission with other variables. Kaplan-Meier analysis was used to compare survival between high and low BALF NO x levels at admission. A P value less than 0.05 was considered significant. Results At study admission in the sepsis group, nonsurvivors had higher levels of BALF NO Introduction Myocyte stretch is the main stimulus of brain natriuretic peptide (BNP) synthesis and release. During septic shock, important variations of volemia can occur and a correlation has been described between the cardiac index and the BNP level [1] . However, the relation between the echocardiographic left ventricule area and the BNP level has never been described. The aim of our study was to evaluate BNP and left ventricule area variations after an acute fluid loading in septic shock. Methods Mechanical ventilated patients with septic shock, and without anterior cardiac disease, were included in our study. A fluid challenge was performed with colloid (500 ml) in 30 minutes. A BNP blood sample was drawn before and 1 hour after fluid loading. The primary endpoint was BNP variation after fluid challenge. Median values (25-75th percentiles) were compared with the Wilcoxon test (P < 0.05). The end-diastolic left ventricule area was recorded before and 1 hour after fluid challenge. Linear regression of BNP variation and left ventricular area variation was determined and r 2 was calculated. \n\nResults Eight patients (median age 68 years; six males/two females; SOFA score = 12) were enrolled in our study. The initial BNP level median increased from 695 (417-2,738) to 715 (478-2,596) \u00b5g/ml after a fluid loading (P = 0.7) (Figure 1 ).We did not find a statistically significant relationship between BNP variation and left ventricule area variation after fluid challenge (P = 0.13) (Figure 2) . Conclusion There is no increase in BNP level in patients with septic shock after fluid challenge. To our knowledge, this preliminary study is the first to evaluate the relationship between BNP and left ventricule area variation in patient with septic shock. Although no statistical significance between left ventricule area variation and BNP variation after fluid challenge, there is a trend to correlation between these two parameters. More patients have to be included to confirm this result.\n\nnominal variables were analyzed with the chi-square test and the risk ratio (RR) with 95% confidence interval (IC95) was performed; intragroup ordinal variables were analyzed with the Wilcoxon test (W), intergroup ordinal variables were analyzed with the Mann-Whitney test (MW). The receiver operative characteristic test was used to discriminate the BNP cutoff value with sensibility, specificity and respective IC95, between group A and group B. P < 0.01 is taken as statistically significant. Results Weaning failure in group A occurred in 50% of patients vs 12% of group B patients (P < 0.01 chi-square test; RR = 4.13, IC95 = 1.36-12.49). BNP value differences in group A are not significant (step 1: 662 \u00b1 147 pg/ml; step 2: 769 \u00b1 171 pg/ml; step 3: 843 \u00b1 167 pg/ml), while BNP value differences in group B are statistically significant (W) (step 1: 130 \u00b1 21 pg/ml; step 2: 236 \u00b1 41 pg/ml, P < 0.01 vs step 1; step 3: 375 \u00b1 75 pg/ml, P < 0.001 vs step 1 and P < 0.01 vs step 2). There are statistically significant differences between group A and group B in every step (P < 0.01 MW). The BNP cutoff value to discriminate group A from group B is 274 pg/ml with sensibility 90 (IC95 = 55-98) and specificity 79 (IC95 = 61-91). Conclusions Risk of weaning failure is increased four times in patients with HD. BNP values of group B patients are higher than normal people probably because the heart of ICU patients is submitted to different kinds of stress; therefore the BNP cutoff value to consider for discrimination of patients with HD from patients without HD in the ICU should be higher. BNP production in ICU patients with good performance of the heart is the right protective response to stress performed by therapy adopted during the ICU stay, this response is absent in patients with HD because their hearts already work in safety mode.\n\nCombining various severity of illness scoring systems to improve outcome prediction: pilot experience in the critically ill obstetric population Z Haddad 1 , C Kaddour 2 , L Skandrani 2 , S Nagi 2 , T Chaaoua 2 , R Souissi 2 1 CHI St-Cloud, France; 2 National Institute of Neurology, Tunis, Tunisia Critical Care 2007, 11(Suppl 2):P458 (doi: 10.1186/cc5618) Introduction No perfect severity score exists to predict ICU mortality, thus the search for new systems is still a preoccupation. Hypothesis Use of many severity of illness scores simultaneously improves mortality prediction.\n\nAn open prospective observational study as part of the APRiMo project [1] . The study period was January 1996-September 2004. Inclusion criteria were critically ill obstetric patients and ICU length of stay >24 hours. Exclusion criteria were those of the used scores. The main outcome of interest was the survival status at ICU discharge. The database was divided into two samples: development and validation datasets. Development database patients were chosen randomly (n = 414) and the remaining patients composed the validation dataset (n = 229). A multivariable logistic regression model was developed to predict mortality associating the Acute Physiology and Chronic Health Evaluation II score [2] , Simplified Acute Physiology Score II [3] , Admission Mortality Prediction Model (MPM-H0) and Day 1 Mortality Prediction Model (MPM-H24) [4] . Discrimination and calibration were assessed by goodness-of-fit C-hat statistics and area under the ROC curve. The developed model was then tested in the validation dataset. Good discrimination was retained if C-hat statistics P > 0.1 and good calibration if area under the ROC curve > 0.8.\n\nSix hundred and forty-three patients enrolled. The overall mortality rate was 11.51%. The new model predicted accurately 99% of survivors and more than 60% of nonsurvivors. Conclusion The 'multiscore' model seems to refine prognosis. This is partly due to mixing of new evaluated parameters. Testing the latest developed generations of scores and also organ dysfunction systems could be interesting.\n\nValidation of logistic organ dysfunction score prediction compared with APACHE II score prediction hospital outcome in Thai patients \n\nWe conducted a prospective observational review of 100 consecutive patients admitted to our ICU. We collected data relating to daily maximum organ dysfunction scores. Outcome was defined in terms of length of ICU stay and ICU mortality.\n\nWe included 100 patients (62 males), mean age 60.9 years. Of these admissions, 45 were elective surgical, 22 emergency surgical, 33 medical. The median Sequential Organ Failure Assessment (SOFA) score on admission was 4.50 (IQR 4). The median maximum SOFA score was 5.00 (IQR 5). The median length of ICU stay was 3.0 days (IQR 3). The overall ICU mortality rate was 14.0%. For patients with a maximum SOFA score \u22648, mortality was 5.1% -vs 45.5% for those whose maximum SOFA score was >8 (P < 0.001). Sixty-four per cent of patients scored their maximum SOFA score on admission. In patients whose SOFA score increased after admission, the mortality was 24.3%. Logistic regression analysis showed the maximum SOFA score bore a stronger correlation with mortality than admission SOFA score. See Figure 1 .\n\nConclusion Maximum and admission SOFA scores are of prognostic value in the intensive care setting; allowing patients with increased risk of mortality and prolonged stay to be identified. like India, and is associated with significant mortality. The outcome of malarial MODS predicted in various studies is extremely variable and dependent on many patient parameters.\n\nObjective We prospectively evaluated the correlation of the APACHE II score, parasite index, procalcitonin (PCT) levels, number of organ dysfunctions/failures and Sequential Organ Failure Assessment (SOFA) score with the outcome of severe malaria. Methods Eleven patients with acute severe malaria with MODS were treated in our ICU in the last 5 months. All these patients were treated with artesunate and/or quinine as per the WHO antimalarial treatment schedule, along with standard ICU care. The APACHE II and SOFA scores were calculated on admission. PCT levels were measured semiquantitatively on admission. The parasite index was confirmed by two pathologists. Results Nine out of 11 patients survived without any residual organ damage, and the remaining two died due to MODS (Figure 1 ). Both these patients had five organ dysfunctions on admission, and their SOFA scores were 18 and 20, respectively. They had a low parasitic index of 1% and 2.5% and their PCT levels were 0.5-2 and >10 (semiquantitative method), respectively. Their APACHE II scores were 16 and 10. Conclusion The pretreatment APACHE II score, parasite index, PCT levels and number of organs involved have variable correlation with mortality and are not consistent predictors of outcome. A higher SOFA score on admission is a more reliable predictor of mortality in malarial MODS.\n\nModel calibration and discriminatory ability: a comparison of four derived variables from the SOFA score and the SAPS II Introduction We sought to compare four derived variables from the SOFA score and the SAPS II in ICU patients in terms of discriminatory ability and model calibration.\n\nPatients and methods Four hundred and fourteen patients were included; they were evaluated on admission and every 48 hours thereafter until ICU discharge or death. Readmission and patients with an ICU stay shorter than 48 hours were excluded. The TMS score was calculated by summing the worst scores for each of the organ systems. Organ failure was defined by a SOFA score \u22653. Median length of ICU stay in patients with an admission SOFA score >8 vs those whose admission SOFA score was \u22648 (P = 0.001).\n\n\u2206SOFA was defined by TMS minus admission SOFA (SOFAi). The maximum SOFA was defined by the worst SOFA value during the ICU stay. Logistic regression modeling techniques were used to describe the association of derived SOFA variables and SAPS II with mortality. ROC curves were used to assess the model's discriminatory ability and we examined the model calibration using the Hosmer-Lemeshow goodness-of-fit test. P < 0.05 was considered significant.\n\nResults Diagnostic categories were: trauma 21.3%, postoperative 19% and medical 59.7%. Global mortality was 34.3%. Survivors had lower average SAPS II (28.1 \u00b1 14 against 48.6 \u00b1 19, P < 0.01), SOFAi score (3.7 \u00b1 3 against 7.2 \u00b1 4, P < 0.01), SOFAmax score (4.6 \u00b1 4 against 10.8 \u00b1 3, P < 0.01), \u2206SOFA (1.6 \u00b1 6 against 4.2 \u00b1 3, P < 0.01), DoMAX (1.6 \u00b1 6 against 4.2 \u00b1 3, P < 0.01) and TMS (5 \u00b1 3 against 11.4 \u00b1 4, P < 0.01), and the difference was statistically significant. Results regarding model calibration and discriminatory ability are presented in Figure 1 . Conclusion The SOFAmax score had the best model calibration and could be used to compare different patient populations in terms of mortality. Introduction Sepsis is associated with progressive organ failure. We sought to describe Sequential Organ Failure Assessment (SOFA) score daily trends in septic patients and tried to correlate those trends with survival. Methods Patients with severe sepsis or septic shock admitted for at least 5 days in a seven-bed medicosurgical ICU of a Brazilian university hospital were studied. The daily SOFA score for each patient was calculated during the first 5 days of admission. Relevant data were prospectively acquired from March 2003 to May 2006 and the latter retrieved from a electronic database. ICU survivors were compared with nonsurvivors using the Mann-Whitney U test. Day-to-day changes were verified within each group using Friedman's test. P \u2264 0.01 was elected as the significance limit. Medians and interquartile ranges (IQRs) were used to describe the sample.\n\nOne hundred and seventy-six patients were studied (71 males (56%), median age 51 (IQR 36-67) years, 78 (44%) with severe sepsis, median length of ICU stay 10 days (IQR 7-16), median admission SOFA 6 (IQR 4-9), median APACHE II score 19 (IQR 13-26), ICU mortality 27.84% (49/176 patients)). The SOFA score and its components scores along the five admission days distinguished the survivors from the nonsurvivors. Considering the SOFA score and its respiratory, neurologic and circulatory components, survivors presented lower scores as the days passed (P < 0.001). Mortality was increasingly higher for those patients who persisted with a SOFA score \u2265 7 as the days passed. Conclusion In the sample studied, the persistence of an elevated SOFA score and its components during the first 5 days of admission predicted a higher mortality. Survival appears to be related to early organ dysfunction recovery. The SOFA score and SOFA-related variables' day-to-day changes in a population of septic patients may have an important prognostic implication and some patterns of daily evolution may distinguish those patients with a more ominous outcome.\n\nCumulative lactate load correlates with cumulative Sequential Organ Failure Assessment score and survival in intensive care unit patients Background Changes in lactate levels are used as a prognostic marker in critically ill patients. However, the relation between the time course of arterial blood lactate clearance and important outcome parameters such as ICU length of stay (LOS), incidence of organ failure and survival rate has not been established.\n\nMethods Case records from all ICU patients admitted between 2002 and 2004 were retrospectively identified in the ICU database. The Sequential Organ Failure Assessment (SOFA) score was calculated daily to assess the time course of organ failure. All lactate levels were extracted and the total cumulative lactate load (area under the curve above the upper normal level of 2.2 mmol/l; cum-lactate), and total cumulative SOFA score (cum-SOFA) were calculated and related to ICU LOS and final hospital survival. Values are the median (interquartile range). Results Observations in 1,711 ICU admissions were analyzed, age was 69 (57-77) years, cum-lactate was 420 (94-419 min\u00b7mmol/l) and cum-SOFA was 11 (4-38). Cum-SOFA was higher in patients with hyperlactatemia (cum-lactate > 0) during the ICU stay (n = 782; 24 (7-71)) than in those without (5 (3-20); P < 0.001). Cum-SOFA correlated with cum-lactate and with ICU LOS, and cum-lactate correlated with ICU LOS (all P < 0.001). In patients who died in the hospital (n = 329), cum-lactate (1,180 (203-3,427) min\u00b7mmol/l) and cum-SOFA (30 (10-95)) were higher than in hospital survivors (n = 1,382; 298 (73-1,154) min\u00b7mmol/l, and 22 (5-67); both P < 0.001). In emergency admissions, cum-lactate (484 (113-2,031)) and cum-SOFA (27 (8-78)) were higher than in planned admissions (131 (37-454)) and (4 (3-28); both P < 0.001), respectively. Conclusion In ICU patients, the cumulative area under the lactate curve correlates with the ICU LOS, cumulative SOFA score, and inhospital mortality. The prognostic value of cum-lactate requires prospective evaluation.\n\nAvailable online http://ccforum.com/supplements/11/S2\n\n*Classified as + if predicted mortality \u22650.5. Domax, the maximum number of organ failures during ICU stay. S187 20.09%; P = 0.045 and P = 0.097). The mortality of patients that were readmitted more than once (from a total 160 patients, 41 died -25.6%) was bigger than the mortality of patients readmitted just once (from a total 696 patients, 131 died -18.96%) (P = 0.12). The length of stay of nonsurviving readmitted patients in other departments before readmission was higher than the length of stay of survivors (mean 7.64 days vs mean 5.71 days). Conclusion Sex and age older than 70 years of patients readmitted to the ICU, readmission during 24 hours, and length of stay in other departments before readmission could be used for simple prediction of mortality of patients readmitted into the ICU. The amount of single patient readmissions to the ICU cannot be used as a predictor of death of patients readmitted to the ICU. Introduction The aim of this study was to assess the role of absolute risk reduction (ARR) to measure ICU performance as an alternative to the standardized mortality ratio (SMR). Methods This retrospective study involves patients admitted to three ICUs of a single tertiary medical center from January 2003 through December 2005. Only the first ICU admission of each patient was included in the study. The ICUs were staffed similarly. We abstracted data from the APACHE III database. For each ICU, the SMR and ARR with their 95% confidence intervals (CI) were calculated. ICU performance was categorized as shown in Table 1 . When comparing ICUs, if the 95% CI of the SMR or the ARR overlap between the units, the performances were considered similar. If there was no overlap, the differences in performance were considered statistically significant. Results During the study period, 12,447 patients were admitted to the three ICUs: 4,334 to the medical ICU, 3,275 to the mixed ICU and 4,838 to the surgical ICU. The predicted mortality rates were 19.5%, 16.0% and 9.0% and the observed mortality rates 14.8%, 9.7% and 4.3% for the medical, mixed and surgical ICUs, respectively. The SMR and ARR in mortality for each ICU are presented in Table 2 .\n\nConclusions ICU mortality performances assessed by SMR and ARR give different results. The ARR may be a better metric when comparing ICUs with a different case mix. Multiple linear regression, and support vector regression with linear and nonlinear (RBF) kernel functions were performed, after selection of relevant data components and model parameters.\n\nPerformances of the prediction models on unseen datasets were analyzed with fivefold cross-validation. Wilcoxon signed-rank analysis was performed to examine differences in performances between prediction models and to analyze differences between real and predicted tacrolimus blood concentrations.\n\nThe mean absolute difference with the measured tacrolimus blood concentration in the predicted regression model was 2.34 ng/ml (SD 2.51). Linear SVM and RBF SVM prediction models had mean absolute differences with the measured tacrolimus blood concentration of, respectively, 2.20 ng/ml (SD 2.55) and 2.07 ng/ml (SD 2.16). These differences were within an acceptable clinical range. Statistical analysis demonstrated significant better performance of linear (P < 0.001) and nonlinear (P = 0.002) SVM ( Figure 1 ) in comparison with linear regression. Moreover, the nonlinear RBF SVM required only seven data components to perform this prediction, compared with 10 and 12\n\nAvailable online http://ccforum.com/supplements/11/S2 S189 increase in APACHE II score, delay to ICU readmission, need of mechanical ventilation and three or more organ dysfunctions were significantly associated with mortality. Conclusions Admission to the ICU is common in lung transplant recipients, and it is associated with a high mortality. Sepsis is the main cause of ICU readmission and the most frequent cause of death. Lung transplant recipients with higher APACHE II score and three or more organ dysfunction present higher mortality. The delay on ICU readmission is also associated with higher mortality. \n\nReadmission to the ICU during the same hospitalization is associated with significant morbidity and mortality and results in a longer length of stay and higher costs. There is therefore growing interest to identify reliable predictors for readmission. The aim of our study was to assess the incidence of ICU readmissions, identify predictors of ICU readmission, and determine patient outcome. Methods We performed a retrospective case-control study. The study population consisted of all patients who were discharged alive from our 28-bed surgical, thoracic-surgical and medical ICU in a university teaching hospital in a 1-year period. A case was defined as a patient readmitted to the ICU within 48 hours after discharge. For each case, three control patients were randomly selected from the study population. The following information was collected: demographic parameters and APACHE II score, parameters of hemodynamic, respiratory and renal function, length of ICU stay, duration of invasive ventilatory support (ventilator time), and time between extubation and discharge. To determine a predictive model, covariate selection was done by the two-sample t test, Mann-Whitney test and univariate logistic regression. From significant (P < 0.10), plausible and clinically relevant variables, a predictive model was generated using multivariate logistic regression.\n\nResults During a 1-year period 1,635 patients were admitted to our ICU. Of 1,393 patients at risk for readmission, 25 (1.8%) readmissions occurred in 23 patients. Nine of the 23 (39%) readmitted patients died during their hospitalization, while the overall ICU mortality was 10.6%. The most important reason for readmission (68% of the cases) was respiratory deterioration. In the univariate analysis, age, ventilator time during first admission and time between extubation and ICU discharge were significant predictors of readmission. In the multivariate analysis, age (OR 1.1; 95% CI 1.00-1.13; P = 0.03) and ventilator time during first admission (OR 1.1; 95% CI 1.00-1.10; P = 0.03) were significant predictors, corrected for patient characteristics. Furthermore, patients who were readmitted had a significant longer duration of total (first and second admission) ventilator time (188 vs 106 hours, P = 0.012), and total ICU stay (400 vs 127 hours, P = 0.009).\n\nConclusion Patients readmitted to the ICU have significant longer overall ventilator time, ICU stay, and a higher ICU mortality. The ventilator time during first admission (especially beyond 300 hours) is an important predictor of readmission. The time it takes to get patients ready for discharge after extubation also differed significantly. The data suggest that elderly patients who have been ventilated for a long period are at particular risk for readmission and should receive additional care before discharge from the ICU.\n\npatient-centered and family-centered quality care. Literature examining patient or family satisfaction in SDUs is limited. Methods We administered a modified version of the previously validated Family Satisfaction with ICU care survey to patients and families of patients who were cared for in the SDUs (18 beds in four separate units) of a tertiary regional referral center. We obtained self-reported levels of patient and family satisfaction with 27 aspects of care related to SDU experience, communication, and decision-making. Responses were converted to item scores, which reflect poor to excellent satisfaction with care (0-100). Results A total of 120 patient surveys (60% response) and 99 family surveys (45% response) were completed. Patients had a mean SDU length of stay of 2.5 days, APACHE II score of 9.9 and an SDU mortality of 2.4%. The highest levels of satisfaction with care were (mean \u00b1 standard deviation item score; presented as aspect of care, patients, families, P value): overall care (aggregate score), 81.1 \u00b1 21.5, 80.1 \u00b1 22.3, NS; concern and caring received from SDU staff, 87.9 \u00b1 17.1, 90.4 \u00b1 5.0, NS; and nurses' skill and competence, 88.7 \u00b1 16.0, 88.8 \u00b1 16.6, NS. The lowest levels of satisfaction were: frequency of communication with physicians, 71.6 \u00b1 27.8, 62.7 \u00b1 32.2, P = 0.03 and decisionmaking (aggregate score), 67.5 \u00b1 29.9, 62.7 \u00b1 30.5, NS. For the decision-making process, a proportion of respondents felt they were not included (patients, families, P value; 40.3%, 42.7%, NS), not supported (31%, 38.6%, NS), and not in control (32.7%, 55.3%, P = 0.001). Patients and families were least satisfied with the frequency of communication with physicians and participation in decision-making. Patients and families were similar in their assessments with the exception of the frequency of communication with physicians and control of the care delivered.\n\nConclusions While most patients and family members were satisfied with care received, these data identify opportunities for improvement. Specifically, attention must be paid to communication and decision-making processes in SDUs.\n\nWe investigated patient management plans to ascertain the total number made, types of plan, priority, personnel responsible and expected time frame, proportion completed and the causes and consequences of failed plans (on the patient, the family and the critical care service). Methods Over seven consecutive days, details of all consultant determined management plans were recorded by a dedicated nurse auditor. A plan was defined as an identifiable do-able, shortterm action. Data on type, (arbitrary) priority, involved personnel and time frame were noted. The auditor later returned at the end of shift to determine whether plans had been completed in the appropriate time frame (successful plan) or not (unsuccessful plan). For unsuccessful plans, the nurse, senior nurse, senior house officer, fellow and consultant were all independently quizzed on causes and consequences (for patient, family, service) from a predetermined list of possibilities.\n\nOf 200 plans, 130 were successful, for three plans data were missing and 67 (34%) plans were unsuccessful. Of unsuccessful plans, 36 were completed late, 22 were never completed and nine had missing data. Thirty-six per cent, 34% and 18% of arbitrarily defined high-priority, medium-priority and lowpriority plans were unsuccessful. Most plans were to be actioned by nurse or senior house officer, and 36% and 28% were unsuccessful, respectively. More unsuccessful plans than successful plans were recorded in the computerised notes, 79% vs 67%. Only 40% of data (staff opinions) on perceptions of causes and consequences were gathered. Patient consequences of failed plans included increased ICU stay in 24%, increased morbidity such as risk of inadequate nutrition in 9%, delayed definitive treatment in 7%, delayed weaning in 6%, increased risk of infection in 5% and no impact on patient in 44%. Consequences for family included no impact in 53%, misinformation given in 8%, delayed patient access in 2%, and delayed communication in 2%. Service consequences were bed blocking/increased workload in 20%, delayed admission of another patient in 14%, cancelled elective operations in 4%, and loss of unit capacity and cohesion in 7%.\n\nConclusions We failed to achieve 100% successful plans. Small numbers and failure to gather more than 40% of staff opinions on causes and consequences of failed plans limit this pilot study. Documentation in (electronic) notes did not improve completion of plans. The process and efficiency of care has an impact on at least aspects of morbidity and length of stay, and deserve further study. Conclusion In octogenarians admitted in the ICU after OHCA, hospital mortality is higher than in the younger group but still an important proportion survives. Non-octogenarians survived more often than predicted by APACHE II. Despite the higher mortality, ICU treatment after out of hospital resuscitation of octogenarians seems worthwhile.\n\nThe costs of intensive care Introduction For most countries, there are no good estimates for the costs of intensive care (IC) although it is known that the ICU is a major inpatient cost driver. The aim of this study was to estimate the real costs of IC in two hospitals in The Netherlands using a micro-costing methodology. Methods The costing study was undertaken at two hospitals in The Netherlands. We conducted a retrospective cost analysis of 200 consecutive patients admitted to a 32-bed mixed adult ICU at an academic hospital during two periods in 2006: 16 April-15 May and 5 June-23 June. For comparison, we collected detailed data at a general hospital that has a 10 bed-adult general ICU for the period 1 January-1 July 2003. The costs were adjusted to 2005 using the general price index. Both times, we applied a micro-costing approach, implying that all relevant resources were identified and valued at a detailed level. Data on resource use of diagnostics, drugs, fluids, materials, admission and discharge were acquired from the computerized Patient Data Management System in both hospitals. Hotel and nutrition costs were collected from the respective financial departments. These costs were divided by the annual number of patient-days to calculate the cost per day. The NEMS or TISS scores in the academic or general hospital, respectively, were used to estimate nursing time per patient per day. The costs of medical specialists were based on the labour costs and the number of ICU days per year. In the academic hospital, time for consultations of medical staff attributable to each individual patient-day was prospectively collected using patient record forms. These costs we assumed to be comparable in the general hospital in the absence of detailed data. Unit costs of diagnostics and consumables were derived from the financial hospital databases. Labour costs were based on standardised costs per minute, which equalled the normative income divided by the number of workable minutes per year.\n\nEstimates of the costs of inpatient-days and nonpatient-related care were based on the annual account for 2005 of the hospitals. Nonpatient-related care (capital, overhead) was appointed to patients using a marginal mark-up percentage.\n\nIn the academic hospital the average total costs per ICU day amounted to \u20ac1,775, compared with \u20ac1,703 in the general hospital. The distribution of the costs by cost component varied.\n\nThe overall costs per day for IC in an academic hospital were slightly higher than the costs for an ICU day in a general hospital. These derived costs fit nicely to the official reference costs of \u20ac1,730 for an ICU day in The Netherlands, which is based on a global top-down approach. Introduction Progress in intensive care has led to an increase in the number of patients who survive severe brain injury and, therefore, the number of patients with impaired consciousness.\n\nAvailable online http://ccforum.com/supplements/11/S2 S195 34% and 75%. Historically, these patients have been regarded as having a poor prognosis once admitted to the ICU. We decided to compare the ICU and hospital mortality of these patients with patients of a similar age and severity of acute illness. Methods Twenty-four patients were admitted to the ICU from August 2004 to August 2006 with a haematological malignancy. These were case-matched using sex, age (\u00b12 years), APACHE II score (\u00b12) and admission diagnosis with patients admitted to the ICU without a diagnosis of haematological malignancy. Eighteen patients were matched to one case control; however, in six patients, two matches were found. Where it was impossible to differentiate between cases on the grounds of diagnosis, age or APACHE II score they were both included. We compared ICU and hospital mortality between the two groups.\n\nResults Patients with a haematological malignancy had an ICU mortality of 50%, and a hospital mortality of 58%. Control patients had an ICU mortality of 60%, and a hospital mortality of 67% (statistically nonsignificant). The length of time to admission between the two groups was significantly longer in the haematology group at 12.4 days, compared with 2.8 days in the control patients (P < 0.05). The level of organ support was the same between the two cohorts.\n\nConclusion We have demonstrated that, for our unit, there was no statistically significant difference in hospital or ICU mortality between the two groups. In fact, the group with a haematological malignancy had a lower mortality than the control group. The presence of haematological malignancy, of itself, does not appear to increase the mortality risk, when compared with a population of patients without haematological malignancy of a similar age, APACHE II score and admission diagnosis. S197 and 17.0 (11.3-33.9) days, respectively. Compared with NICUGP, patients in ICUGP were more likely to have developed postoperative arrhythmia (57.9% vs 12.9%, P < 0.001), were older, of higher ASA status, and more likely to have diabetes, coronary artery disease, hypertension, a higher cancer stage, and to have received more intraoperative blood products. Of 352 patients originally not sent to the ICU, 43 (12.2%) were subsequently admitted to the ICU. These patients had higher APACHE III scores and were more likely to have 'aspiration' documented, although their mortality was not higher than direct ICU admissions. Conclusions After esophagectomy, overall mortality is low, but many patients require ICU admission. Postoperative arrhythmias and aspiration pneumonitis are especially problematic. The data completeness was defined as a ratio of available and required data at ICU admission level. We evaluated the dataset and selected 19 most significant admission scheme variables to be included in completeness ratio calculations.\n\nThe majority of data (77.5%) was collected with the manual system and the remaining 22.5% with an integration software. The mean admission data completeness ratio (CR) increased from 85.3% at 1998 to 97.9% at 2005 (P = 0.01). Between the ICUs, the mean CR varied from 91.6% to 99.6% (P = 0.01). The mean CR of the data collected with the IVT was 98.7% and with the manual system was 95.1% (P = 0.01). The rate of 100% complete records per patient was 48.7% and it increased from 0.0% in 1998 to 71% in 2005. Conclusion Data completeness in the FICQC has improved during the study period, although there is still significant variation between ICUs. Improved data completeness and decreased proportion of missing data are most likely due to the increasingly common use of CIS and automated data collection. We conclude that measuring/reporting the amount of missing data is mandatory when data collection and data management procedures for benchmarking are being developed.\n\nThe number of heart-beating donors in The Netherlands is decreasing. This decrease is only partially compensated for by an increase of nonheart-beating donations, resulting in an increasing shortage of donor organs, especially of donor hearts and lungs. In 2005 compared with 2004 the number of patients waiting for a donor heart increased from 38 to 50 (32%). In lung donation the increase was 37% (from 79 to 108). One approach to reduce this shortage is to maximize the number of organs per donor by optimisation of donor treatment in the ICU. Methods We investigated the possibilities for improvement of donor management in our ICU by a retrospective study in 37 heartbeating organ donors hospitalised in our ICU from 1993 to 2005. There was no protocol for the treatment of organ donors in our institution.\n\nThe heart was donated in 18 of 37 patients (49%). Lung donation was possible in only eight of 37 donors (22%). Most hearts and lungs were rejected for transplantation for valid reasons. In some patients there was room for improvement: in two of the three cases where hemodynamic instability impeded heart and lung donation (one dying from subarachnoidal bleeding and one from ischemic cerebral infarction), hemodynamic instability was closely associated with the moment of cerebral death. In three further patients heart donation was not carried out because of wall movement abnormality or electrocardiogram abnormalities. None of Available online http://ccforum.com/supplements/11/S2 Introduction Critical care research involves data from many countries, but critical care resources in these countries are unknown. We hypothesized that there are large differences in critical care resources between countries.\n\nMethods We identified original research articles on critical care in three high impact factor journals (N Engl J Med, JAMA and The Lancet) published from 2001 to 2005. A list of the countries where data collection occurred was extracted. Eight countries contributed to \u226510 studies. A collaborator in each country was asked to provide baseline critical care information for their country from 2005, or as close to that date as possible.\n\nResults Sixty-two studies involving data from 51 countries were identified. Eight countries contributed data to \u226510 studies during this time period: the USA (26 studies), France (18), the United Kingdom (14), Canada (13), Belgium (12), Germany (10), The Netherlands (10) and Spain (10). Relevant data on baseline hospital and critical care resources for the eight countries identified are presented in Figure 1 (data from Canada not available). Adult ICU beds ranged from 3.3/100,000 population in the United Kingdom to 24.6 in Germany, and represented a range of 1.4% of all acute care hospital beds in the United Kingdom to 11.0% of all beds in the USA. Conclusions Many countries contribute substantially to critical care research. However, the underlying critical care resources vary dramatically among these countries.\n\nAvailable online http://ccforum.com/supplements/11/S2\n\nBaseline critical care resources in eight countries. *Extrapolated from survey data, not full national data."}