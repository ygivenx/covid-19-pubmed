{"title": "Challenges in the analysis of viral metagenomes", "body": "In the last decade, at least seven separate viral outbreaks have caused tens of\nthousands of human deaths (Woolhouse,\nRambaut, and Kellam, 2015), and the ever-increasing density of livestock,\nrate of habitat destruction, and extent of human global travel provides a fertile\nenvironment for new pandemics to emerge from host switching events (Delwart 2007; Fancello, Raoult, and Desnues 2012), as was the case\nfor SARS, Ebola, Middle East Respiratory Syndrome (MERS), and influenza-A (H1N1)\n(Castillo-Chavez et al. 2015). At\npresent we have a limited grasp of the extent of viral diversity present in the\nenvironment: the 2014 database release from the International Committee for the\nTaxonomy of Viruses classified just 7 orders, 104 families, 505 genera, and 3286\nspecies (http://www.ictvonline.org/virustaxonomy.asp); yet, one study\nestimated that there are at least 320,000 virus species infecting mammals alone\n(Anthony et al. 2013).\n\nHigh throughput (or so-called \u2018next generation\u2019) sequencing of viruses during the\nmost recent outbreaks of MERS in South Arabia (Gire et al. 2014; Carroll et al. 2015; Park et al.\n2015) and Ebola in West Africa (Quick, J et al. 2016) has facilitated rapid identification of\ntransmission chains, rates of viral evolution, and evidence of the zoonotic origin\nof these outbreaks. Access to such information during initial stages of an outbreak\nwould offer invaluable insight into when, where, and how an epidemic might emerge,\ninforming intervention and mitigation measures or even stopping it altogether. A\nmajor step towards this goal is therefore to identify existing zoonotic and\nenvironmental pathogens with pandemic potential. This is a significant undertaking,\ndemanding considerable investment and close collaboration between government, NGOs\nand academia, for example, the USAID program PREDICT http://www.vetmed.ucdavis.edu/ohi/predict/index.cfm, as well as on\nthe ground surveillance by local authorities and scientists in areas of the world\nmost at risk.\n\nThe characterization of unknown viral entities in the environment is now possible\nwith modern sequencing; however, current tooling for exploiting these data\nrepresents a practical and methodological bottleneck for effective data analysis.\nPractically, most available software tools are inaccessible to the majority of\npotential users, demanding expertise and computing resources often lacked by the\nresearchers from diverse backgrounds involved in sample collection, sequencing, and\nanalysis. There is a need for robust and intuitive analytical tools without\nrequirements for fast internet connectivity, which may be unavailable in remote or\ndeveloping regions. More fundamentally, the intended scope of published analytical\ntools and workflows is often less than clear, and given the diverse applications of\nviral sequencing, it can be difficult to gauge the relevance of newly published\ntools without first testing them. For example, a fast sequence classifier might fail\nentirely to detect a novel strain of a well-characterized virus, and equally might\nperform well with Illumina sequences yet deliver poor results for data generated\nwith the Ion Torrent platform. Furthermore, results arising from these analyses\nshould be replicable, intelligible, and useful to the end user, with provision for\nquality control and error management. Software tools that target expert users should\nbe tested, documented and robustly distributed as packages or containers so as to\nstreamline the processes of installation and generating results.\n\nMethodologically, most genomic sequence analysis software is not well suited for\nviral genomes. Generic tools that are able to address the challenges posed by viral\nsequences are often applicable only in limited circumstances. Choosing between\napproaches is made difficult due to an abundance of disparate yet functionally\nequivalent methodologies and in general a lack of rigorous benchmarks for viral\ndatasets. While there is much ongoing research in this area, both the sensitive\ndetection of previously characterized viruses and viral discovery remain key\nchallenges open for innovation. Here we survey the landscape of available approaches\nfor analyzing both known and unknown viruses within genomic and metagenomic samples,\nwith focus on their practical and methodological suitability for use by a broad\nspectrum of researchers seeking to characterize viral metagenomes.\n\nWithin metagenomes the proportion of viral nucleic acids is typically far lower than\nthat of host or other microbes, limiting the amount of signal available for analysis\nafter sequencing. To mitigate this issue, enrichment and amplification approaches\nare widely used prior to sequencing viral samples. Size filtration or density-based\nenrichment by centrifugation are two effective methods for increasing virus yield,\nalthough such methods may bias the observed composition of viral populations (Ruby, Bellare, and Derisi 2013).\nAlternatively, PCR amplification may be used to generate an abundance of specific\nviral sequences present in a sample, a widely used strategy, which was employed in\nthe identification and analysis of MERS coronavirus (Zaki et al. 2012;\nCotten et al. 2013, 2014),\nalthough effective primer design can be challenging in the presence of high genomic\ndiversity in the target viral species. Conversely, an excess of sequencing coverage\ncan lead to the construction of overly complex and unwieldy de novo\nassembly graphs in the presence of high genomic diversity, reducing assembly\nquality. Using in silico normalisation (Crusoe et al. 2015), excess coverage may be reduced by\ndiscarding sequences containing redundant information. This approach increases\nanalytical efficiency when dealing with high coverage sequence data, and we have\nshown that it can benefit de novo assembly of viral consensus\nsequences. Another in silico strategy for increasing analytical\nefficiency by discarding unneeded data is to filter sequences from known abundant\norganisms through alignment with one or more reference genomes using an aligner or\nspecialist tool (approaches reviewed in Daly\net al. 2015).\n\nThere are several sequencing technologies in widespread use that are capable of\nreading hundreds of thousands to billions of DNA sequences per run (Reuter, Spacek, and Snyder 2015). The\ncurrent market leader, Illumina, manufactures instruments capable of generating\nbillions of 150 base pair (bp) paired end reads (see \u2018Glossary\u2019) per run, with read\nlengths of up to 300 bp. The Illumina short read platform is widely used for\nanalyses of viral genomes and metagenomes, and, given sufficient sequencing\ncoverage, enables sensitive characterization of low-frequency variation within viral\npopulations (e.g. HIV resistance mutations as low as 0.1% (Li et al. 2014)). Ion Torrent (ThermoFisher) is capable\nof generating longer reads than Illumina at the expense of reduced throughput and a\nhigher rate of insertion and deletion (indel) error (Eid et al. 2009). Single molecule real-time sequencing\ncommercialized by Pacific Biosciences (PacBio) produces much longer (>10 kbp)\nreads from a single molecule without clonal amplification, which eliminates the\nerrors introduced in this step. However, this platform has a high (\u223c10%) intrinsic\nerror rate, and remains much more expensive than Illumina sequencing for equivalent\nthroughput. The Nanopore platform from Oxford Nanopore Technologies, which includes\nthe pocket sized MinION sequencer, also implements long read single molecule\nsequencing, and permits truly real-time analysis of individual sequences as they are\ngenerated. Although more affordable than PacBio single molecule sequencing, the\nNanopore platform also suffers from high error rates in comparison with Illumina\n(Reuter, Spacek, and Snyder 2015).\nHowever, the technology is maturing rapidly and has already demonstrated potential\nto revolutionize pathogen surveillance and discovery in the field, as well as\nenabling contiguous assembly of entire bacterial genomes at relatively low cost\n(Feng et al. 2015; Quick et al. 2015; Hoenen et al. 2016). Hybrid sequencing strategies using\nboth long and short reads leverage the ability of long reads to resolve repetitive\nDNA regions while benefitting from the high accuracy of short reads, at the expense\nof additional sequencing, library preparation and data analysis (Madoui et al. 2015).\n\nModern de novo assemblers generally leverage either de Bruijn\ngraphs or read overlap graphs as part of the approach known as overlap layout\nconsensus (OLC). Figure 1\nillustrates the differences between the two methods. OLC assemblers use the\nsimilarity of whole reads in order to construct a graph wherein each read is\nrepresented by a node, and subsequently merge overlapping reads into consensus\ncontigs (Deng et al. 2015). OLC is\nrelatively time and memory intensive, scaling poorly to millions of reads and\nbeyond. However, the fewer, longer reads generated by emerging single molecule\nsequencing technologies tend to be well suited to OLC assembly, which can be\neasily implemented to tolerate long and noisy sequences (Compeau, Pevzner, and Tesler 2011). Older, notable,\nde novo assemblers implementing OLC include CAP3 (Huang and Madan 1999) and Celera\n(http://www.jcvi.org/cms/research/projects/cabog/overview/),\nwhile MHAP (Berlin et al. 2015),\nCanu (Berlin et al. 2015), and\nMiniasm (Li 2016) represent the\ncurrent state of the art. There also exist a number of OLC assemblers intended\nfor use with viral sequences: VICUNA was designed for short, non-repetitive and\nhighly variable reads from a single population (Yang et al. 2012), and PRICE (Ruby, Bellare, and Derisi, 2013) iteratively\nassembles low to moderate complexity metagenomes (e.g. Runckel et al. 2011; Grard et al. 2012;) using a similar algorithm to the\nactively developed consensus assembler IVA (Hunt et al. 2015), which like VICUNA is designed for\nsingle virus populations rather than metagenomes (see Table 1 for additional details on programs). \n\nA de Bruijn or k-mer graph represents a set of reads in terms of\nits k-mer composition, where k-mers are\nsubsequences of a length k, specified by the user. Each\nk-mer is assigned to an edge in a graph, where the nodes\nare k-1 prefixes and suffixes of the k-mer.\nThe assembler identifies the path through the graph in which each edge is\nvisited only once (reviewed in Compeau,\nPevzner, and Tesler 2011). De Bruijn graphs are much more efficient\nto construct than overlap graphs and are suited to large numbers of short reads,\nand where coverage is high, since redundant k-mers occupy\nnegligible random access memory (RAM). However, with this efficiency comes a\nlack of error tolerance in identifying overlaps, less tolerance of repeated\nsequences in comparison to overlap graphs, and a loss of read coherence, meaning\nthat k-mers originating from different reads may be\nco-assembled. Examples of assemblers using de Bruijn graphs include SOAPdenovo\n(Luo et al. 2012), ALLPATHS\n(Butler et al. 2008), SPAdes\n(Bankevich et al. 2012), and\nABySS (Simpson et al. 2009).\n\nTypical de novo assemblers are designed to reconstruct genomes\nwith uniform sequencing coverage across their length. This is problematic for\nmetagenomes (including viromes) where coverage typically varies considerably\nboth among different genomes and within individual genomes. To address this\nproblem, dedicated metagenome assemblers have been developed. Omega (Haider et al. 2014) is an OLC-based\nmethod that uses a minimum cost flow analysis of the OLC graph to generate\ninitial contigs, merging these to create longer contigs and scaffolds using\nmate-pair information. Genovo (Laserson,\nJojic, and Koller 2011) is another OLC-based method that generates a\nprobabilistic model for the dataset and subsequently uses an iterative approach\nto reconstruct the most likely genome contigs. MEGAHIT (Li et al. 2015) prioritizes speed, leveraging a\nsuccinct de Bruijn graph to rapidly reconstruct high complexity metagenomes,\nsuch as those of soil or seawater, on a single computer. Noteworthy is the\niterative de Bruijn graph assembler SPAdes, which although not initially\nintended for metagenome assembly, has been widely adopted for its effectiveness\nin assembling variable coverage metagenomes of limited complexity. MetaSPAdes\n(Nurk et al. 2016) is a\nmetagenome-specific release of the SPAdes pipeline with refinements to its graph\nsimplification and repeat resolution algorithms, counter-intuitively capable of\nleveraging rare strain information so as to improve its consensus reconstruction\ncapabilities. Other de Bruijn graph metagenome assemblers based on their genomic\ncounterparts include Ray-Meta (Boisvert et\nal. 2012), MetAMOS (Treangen\net al. 2013), MetaVelvet (Namiki et al. 2012; Afiahayati, Sato, and Sakakibara 2015), and IDBA-UD (Peng et al. 2012).\n\nFor example, unlike the genome assembler Velvet, MetaVelvet\u2019s de Bruijn graph is\ndecomposed into many subgraphs (using coverage difference and graph\nconnectivity), and scaffolds are built independently for each subgraph.\nMetaVelvet-SL addresses limitations with MetaVelvet, using supervised learning\nto detect and classify chimeric nodes within the de Bruijn graph. IDBA-UD\npartitions a de Bruijn graph into isolated components, constructs a multiple\nalignment, and subsequently identifies variation within these partitions using\nmultiple depth relative thresholds to remove erroneous k-mers.\nRay Meta (Boisvert et al. 2012)\nextends the massively distributed assembly model of Ray to variable coverage\nmetagenomes, while MetAMOS (Treangen et\nal. 2013) is both a metagenomic extension and successor to the AMOS\ngenome assembler.\n\nWe recently proposed a method based on numerical sequence representations and\ndigital signal processing data transformation (SPDT) approaches to reduce the\nsize of working datasets, permitting fast and sensitive read alignment and\nde novo assembly of diverse viral populations (Tapinos et al. 2015). SPDT methods,\nsuch as the discrete Fourier transform (DFT) (Agrawal, Faloutsos, and Swami 1993), and discrete\nwavelet transform (DWT) (Percival and\nWalden 2006) (Fig. 2),\nare used to reduce sequences into lower dimensional space, preserving only\nprominent data characteristics. Analysis is subsequently performed with these\nlower dimensionality transformations, enabling faster data comparison. Since\nSPDT methodologies such as the Fourier and wavelet transforms are applicable\nonly to numerical sequences, nucleotide sequences must first be numerically\ntransformed with one of several techniques including real number representations\n(Chakravarthy et al. 2004),\ncomplex number representations (Anastassiou 2001), the DNA walk (Lobry 1996), and the Voss method (Voss 1992). \n\nAlthough metagenome assemblers generally outperform single genome assemblers in\nreconstructing different genomes simultaneously, the complexity of this task\nstipulates their tendency to collapse variation at or beneath strain level into\nconsensus sequences. Even to this end, their effectiveness may be limited as a\nconsequence of extreme variation within specific RNA virus populations due to\nmutation and recombination, and low and/or uneven sequencing coverage across a\nparticular genome. Furthermore, it should be noted that de novo\nassembly is particularly sensitive to the quality of input sequences, meaning\nthat problems during sample extraction, enrichment and library preparation can\nbe highly detrimental to downstream analyses. Of key importance therefore are\nquality control methods for detecting, and where appropriate correcting,\nproblems associated with contamination (Darling et al. 2014; Orton et\nal. 2015), primer read-through and low quality reads (reviewed in\nLeggett et al. 2013).\n\nViral genomes and metagenomes comprising high intraspecific variation can be\nchallenging targets for assembly, giving rise to complex assembly graphs and\nfragmented assemblies. This is often the case for clinical samples from HIV and\nHepatitis C patients, in which high rates of mutation and long durations of\ninfection can contribute to extreme population divergence, but can also be observed\nin environmental samples. Where such diversity exists, alignment based probabilistic\npopulation reconstruction approaches can be effective, permitting the reconstruction\nof individual viral variants into \u2018haplotypes\u2019 exceeding read length. This problem\nhas been well studied, and tools such as ShoRAH, QuRE, and PredictHaplo (Giallonardo et al. 2014) are designed\nfor haplotyping viral populations. ShoRAH (Zagordi et al. 2011) extracts local alignments of a specified window\nlength, reconstructs haplotypes for each \u2018cluster\u2019 in that window, and removes\nmutations from sequences in the cluster not matching the reconstructed haplotype\nusing a model-based probabilistic clustering algorithm. QuRe (Prosperi and Salemi 2012; Prosperi et al. 2013) removes nucleotide substitutions\nand indels with a Poisson model and reconstructs haplotypes using a heuristic\nalgorithm based on a multinomial distribution. Both approaches have the advantage of\nreporting probabilities for the reconstructed haplotypes. PredictHaplo is notable\nfor taking into account the read pairing information in Illumina data. A limitation\nof all of these approaches; however, is their reliance upon a single reference\nsequence with which to perform the initial alignment, a process which assumes a\ndegree of sequence similarity which may not always be observed in diverse regions,\nsuch as regions encoding envelope proteins, of RNA virus genomes. This can be\nmitigated through construction of a data-specific template through iterative\nreference mapping and consensus refinement strategies (Archer et al. 2010; B\u0159inda, Boeva, and Kucherov 2016). Other possibilities\nfor broader utility of these approaches include the use of multiple viral reference\nsequences, either through consideration of multiple linear sequences or by direct\nalignment of sequences to a variation graph [https://github.com/vgteam/vg], an emerging approach for modeling\ngenomic variation.\n\nViral identification approaches typically depend on similarity searches against a\ndatabase using an aligner such as BLAST (Altschul et al. 1990). Comprehensive databases (e.g. GenBank) or\nsmaller custom databases containing for example, only viral sequences of\ninterest may be used, although the latter can generate misleading results.\nProViDE (Ghosh et al. 2011) uses\nvirus-specific alignment parameters and thresholds to assign viruses at\ndifferent taxonomic levels from BLAST matches to a protein database. VIROME\n(Wommack et al. 2012) is a\nmultifaceted tool integrating results from searches of several sequence and\nfunction databases. MEGAN (Huson et al.\n2011) is a generally applicable metagenomic classifier, which uses\nBLAST results to infer the LCA for a given sequence and provides functional\nanalyses through a graphical interface. Automatic pipelines which combine\nvarious homology search strategies to identify a final set of viral reads\ninclude VirusHunter (Zhao et al.\n2013), a Perl script that automates viral identification using BLAST\nprior to assembly; MetaVir (Roux et al.\n2011), a web application that compares users\u2019 datasets to published\nviral sequences; and VirSorter (Roux et\nal. 2015), which identifies prophages and viruses by comparison with\ncustom datasets. With the exception of web applications, however, these are not\nintuitive tools for the majority of users, requiring manual configuration and\ninstallation of software dependencies. Furthermore, similarity search approaches\nare in general extremely resource-intensive, and performing sensitive BLAST-like\ndatabase searches with millions of reads is intractable without use of\nspecialist computational resources. To address this problem, tools have emerged\nleveraging optimized search algorithms and prebuilt databases so as to increase\nthe tractability of classifying millions of reads. For example, Kraken (Wood and Salzberg 2014) and Clark\n(Ounit et al. 2015) are fast\nexact k-mer matching approaches that use prebuilt databases of\nviruses, bacteria, human, and fungi, although custom databases may also be\nbuilt. One Codex is a proprietary web-based metagenome analysis platform with an\nintegrated fast k-mer matching engine (similar to that of\nKraken) which is both fast, very easy to use, and free for academic use (Minot,\nKrumm, and Greenfield). Lambda (Hauswedell, Singer, and Reinert 2014) and Diamond (Buchfink, Xie, and Huson 2015) are\nsensitive and heavily optimized BLAST-like aligners which leverage alphabet\nreduction to permit protein searches three to five orders of magnitude faster\nthan BLAST, offering prebuilt database indexes for common applications.\n\nAlthough exhaustive BLAST-like methods can detect homology in divergent\nsequences, these methods are in general limited by the relatively few validated\nviral sequences deposited in public databases, the high diversity within viral\nfamilies which can obscure relatedness, and the lack of a defined set of core\ngenes common to all viruses that can be used to distinguish species (e.g. the\n16S gene for bacteria) (Fancello, Raoult,\nand Desnues 2012). These features make it difficult to assign\nsimilarity thresholds for classification that are applicable to all potential\nviruses in a sample (Simmonds\n2015). Comparison methods that do not rely on sequence similarity include\nPhyloPythia (McHardy et al. 2007),\nwhich uses nucleotide frequencies to classify reads, and PHYMM (Brady and Salzberg 2009), which uses\ninterpolated Markov models to find variable length oligonucleotides that\ncharacterize species in the NCBI RefSeq database. Although these approaches are\nless accurate than BLAST searches, PHYMMBL (Brady and Salzberg 2011) combines PHYMM and BLAST\nand outperforms either one on its own. Alignment-free comparison approaches, for\nexample, based on dinucleotide frequencies, codon usage patterns, or small but\nconserved regions of family wide ubiquitous genes, may be more robust to the\nlimitations of the database than sequence similarity searches. These features\nmay also reduce the computation required and highlight evolutionary\nrelationships otherwise obscured by high sequence variability.\n\nA fundamental challenge in the classification of viral sequences with any of\nthese methods remains their limited representation within curated sequence\ndatabases. While the rate at which new viruses are being added to NCBI\u2019s RefSeq\ncollection has increased considerably, from a year average 0.34 species/day in\n2010 to 2.5 species/day in 2015 (Fig.\n3), our documented understanding of the extent of viral diversity\nremains superficial (Anthony et al.\n2013). Reads of true viral origin are therefore liable to be missed\nin many cases. The rate of database growth also highlights the need to maintain\nfrequently updated search indexes for sequence classification, construction of\nwhich often demands specialist servers equipped with hundreds of gigabytes of\nRAM. Even if up-to-date indexes are maintained inside a public repository, their\nfile sizes are substantial, demanding users have access to a fast internet\nconnection. Consequently, complete outsourcing of sequence classification to\nremote web services is a compelling prospect for those with adequate internet\nconnections but without powerful computing hardware, increasing scope for\nconducting analyses with portable computers. \n\nWe see several barriers to realizing the goal of active, on-the-ground surveillance\nand early detection of viruses with epidemic potential. \n\nThe future of the field is promising, with emerging technologies showing potential to\neliminate certain challenges. Single molecule sequencing, for example, permits the\nsequencing of whole viral genomes as single reads, with forthcoming portable and\nsmartphone operated sequencers promising potentially revolutionary analyses in the\nfield. Innovative analytical approaches are constantly being published, and it is\nevident that the motivation, creativity and expertise needed to meet these\nchallenges exists within the community. Broader communication among developers and\nend users is essential, and in conjunction with well-funded international\ninitiatives directed at this goal, intelligent viral surveillance could soon be\nrealized."}