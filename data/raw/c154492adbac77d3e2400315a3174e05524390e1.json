{"title": "Sources and contamination routes of microbial pathogens to fresh produce during field cultivation: A review", "body": "Foodborne diseases are rife in many regions of the world, with at least 1 in 10 people falling ill yearly from consumption of contaminated food and 420, 000 deaths occurring as a result, according to the World Health Organisation (WHO) (2015) . Foodborne diseases have exerted pressure on medical services, contributed to economic and political distress, exacerbated malnutrition and led to human suffering. There are several agents such as chemicals, pathogens, and parasites, which may adulterate food at different points in the food production and preparation process (Allos et al., 2004) . Many of these agents have been extensively characterized and investigated by numerous studies (Farber and Peterkin, 1991; Zhao et al., 2001; Le Loir et al., 2003; Ehling-Schulz et al., 2004; Adzitey et al., 2013; Botana, 2014) . Strategies and protocols to prevent occurrence (and outbreak) of foodborne diseases have been devised and implemented by many researchers, regulatory bodies, and governments. However, despite the considerable progress achieved scientifically, foodborne diseases continue to occur, representing a significant cause of morbidity and mortality globally (Mead et al., 1999; Murray et al., 2013) . Although foodborne diseases are more common in developing countries particularly in Africa and South East Asia with specific groups of people such as children, the immunocompromised, pregnant and aged being particularly at risk, foodborne diseases are not limited to these regions or groups of people (WHO, 2007) . For instance, according to the Centres for Disease Control and Prevention (CDC), between 2001 and 2009, there were 38.4 million episodes of domestically acquired foodborne gastroenteritis caused by unspecified agents in the United States alone (CDC, 2009) . Approximately 17.8 million acute gastroenteritis occurred, and there were at least 473,832 hospitalizations in the US each year and 215 779 hospitalizations caused by the 24 known gastroenteritis pathogens. An estimated 5072 persons died of acute gastroenteritis each year, of which 1498 deaths were caused by the 24 known foodborne pathogens (Scallan et al., 2011) . Health Canada (2011) estimates that 11e13 million cases of foodborne illnesses occur in Canada every year.\n\nAlthough the conventional notion is that foodborne diseases typically originate from meat and poultry products, vegetables and fruits have been implicated in various foodborne outbreaks (Westrell et al., 2009; Lynch et al., 2009; [European Food Safety Authority (EFSA), 2013] . A significant increase in foodborne disease outbreaks or cases associated with consumption of fresh produce has been reported. This increase has been largely due to a general increase in produce consumption, globalization of the produce industry and more effective surveillance (Tauxe et al., 1997; Lederberg et al., 2003; Havelaar et al., 2010) . Increased consumption of fresh produce is likely due to global government efforts to promote healthy eating, the associated health-promoting benefits of consuming fresh produce and ease of access to fresh local produce (Pollack, 2001; Regmi, 2001; Berger et al., 2010; Painter, 2013) . Since fresh produce is mostly eaten raw or after minimal processing, pathogen contamination constitutes a potential health risk (Callej on et al., 2015; Li et al., 2017) . There are numerous factors capable of compromising the microbiological integrity of produce along the farm to fork continuum, all of which have potentially fatal outcomes. However, pre-harvest hazards to produce have been recognized as important because usually, once pathogen contamination is established in the field, it can be challenging to decontaminate produce. There are numerous circumstances that can undermine the safety of produce on farms. Many of these arise because agriculture has grown more intensive over the years, and produce fields are often located near animal production zones thus entwining the ecological connections between wild animals, livestock and produce (Strawn et al., 2013a,b) . This, in many cases, predisposes fruits and vegetables to pre-harvest hazards. Some important pre-harvest hazard sources to produce include the use of contaminated soil, irrigation water and manure for produce cultivation. Wild animals and insects have also been implicated as vehicles of pathogens to produce.\n\nTo ensure produce safety on a sustainable scale, it is imperative to correctly understand the routes of entry, fate, transport, establishment, and survival of pathogens in the agricultural environment such as soil, irrigation water and manure. The knowledge gap in this regard is being filled rapidly, as many studies have attempted to explain the behavior of foodborne pathogens in agricultural media and describe the associations among pathogens, produce and the agrarian environment. In this review, the extent of the produce contamination problem is discussed as well as the sources and routes of contamination of soil, irrigation water, fruits, and vegetables. Also, the various mechanisms and strategies through which bacterial pathogens become established on fruits and vegetables are briefly examined.\n\nThe nutritional and health benefits of consuming fruits and vegetables have been recognized and widely publicized. This has elicited changes in human dietary habits, with many consumers incorporating more fruits and vegetables into their meals. Consequently, the global production of fruits and vegetables has surged exponentially in recent decades. The increased demand for produce has led to modifications such as increased use of soil amendments, utilization of alternative water sources and increased imports and exports in agriculture-spanning across agronomic practices, processing, preservation, packaging, distribution, and marketing . Some of these modifications, however, have great potential to compromise the safety of fruits and vegetables. The biological hazards that are most relevant to fresh produce safety are either zoonotic or human in origin and can be classified into sporeforming bacteria, non-spore forming bacteria, viruses, parasites and prions (James, 2006) . Most studies/surveillance efforts have identified bacterial contaminants in produce-borne illness outbreaks. There is, therefore, a disproportionately higher abundance of information regarding bacterial contamination in the literature. This may be because bacterial species have in fact caused many more outbreaks, but other microbial groups-viruses and parasites have been understudied. The most commonly implicated etiologic agents are presented in Table 1 . Although data and information available on outbreaks associated with fresh produce are diverse and patchy, the available research evidence indicates that the foodborne illness burden due to contaminated produce has increased, in recent decades. In the United States, between 1996 and 2010, approximately 23% of total foodborne illness outbreaks were produce related (Jung et al., 2014) . In Europe, from 2007 to 2011, produce was linked with 10% of the outbreaks, 35% of the hospitalizations and 46% of the deaths (EFSA, 2017) . In Australia, fresh produce was linked to 4% of all foodborne disease outbreaks informed from 2001 to 2005 (Lynch et al., 2009 . Specific produce items are more commonly linked to foodborne illness incidents; for example, leafy greens such as lettuce and spinach, as well as fresh herbs such as parsley and basil are conventional sources of bacterial infections (WHO, 2008; Berger et al., 2010; Denis et al., 2016) . Berries, green onions, melons, sprouted seeds, and tomatoes are similarly high-level priority produce items (Olaimat and Holley, 2012; Denis et al., 2016) . In the US, between 2006 and 2014, 16 of 68 multistate foodborne outbreaks were associated with vegetables (CDC, 2014) . A list of recent produce-related outbreaks is presented in Table 2 .\n\nMost industrialized nations especially the United States have extensive and exhaustive datasets indicating the magnitude of outbreaks, the extent of severity and casualties incurred, the implicated pathogen and produce item as well as documented preventive protocols to avoid future outbreaks. Unfortunately, however, the same is not true of many other countries especially African Countries, the majority of which are still grappling with other challenges and hence, lack the resources to efficiently track and trace foodborne illness incidents (WHO, 2000) .\n\nMany conventional foodborne detection methods are time consuming and laborious, and advanced techniques have therefore been developed and optimized as alternatives to or for use in combination with these traditional techniques. Many of these are rapid, sensitive, reliable and standardized. They can be categorized into nucleic acid based, biosensor-based and immunological based methods (Croci et al., 2008; Adzitey et al., 2013; Law et al., 2014) . Typical examples include simple polymerase chain reaction (PCR), multiplex PCR, real-time PCR, nucleic acid sequence-based amplification (NASBA), loop-mediated isothermal amplification (LAMP) and oligonucleotide DNA microarray. Other examples are optical, electrochemical and mass-based biosensors, and enzyme-linked immunosorbent assay (ELISA) and lateral flow immunoassay (Law et al., 2014; Gilchrist et al., 2015) . These advances in epidemiological investigation approaches and techniques have made it possible to explore the crucial associations between produce and pathogens. In spite of this, however, prompt identification of implicated produce vehicles, location or point of contamination in fresh produce associated outbreaks is still a significant challenge. One prime constraint is the relatively short shelf life of fresh produce, which is often discarded by the time an outbreak is identified (Strausbaugh and Herwaldt, 2000; Lynch et al., 2009) . Therefore, most of the time, the real source of contamination is not ascertained causing investigators to speculate or assume a source. This is, however, dangerous because, in addition to the possibility of being wrong, there is empirical evidence that once a particular transmission pathway is identified, repeated investigations are bound to be biased in causation (Lynch et al., 2009) . Another important consideration is that usually, outbreaks receive widespread attention if the event (i) has severe public health impacts (ii) is unusual or sudden (in that the etiological agent and/produce type are unanticipated; making the circumstances of the outbreak unique and (iii) poses a significant risk of international spread with consequences for international travel or trade. Invariably, the smaller, 'less significant' outbreaks are never investigated. More importantly, foodborne illness incidents occur sporadically in populations, and these cannot be captured in routine epidemiological surveillance or outbreak investigations (Scallan et al., 2011) . This means that the data available may not be a valid representation of the problem. It is likely that the foodborne illness burden related to consumption of contaminated produce is still largely underestimated.\n\nThe possible routes and sources of produce contamination are numerous, and intensive efforts have been made to accurately understand the exact mechanisms through which pathogens are introduced into fresh produce (Kotzekidou, 2016) . Sources and routes of produce contamination vary for different production zones. This is because each farm has a distinct combination of environmental risk factors such as topography, land-use interactions, and climate. Combinations of these peculiar environmental risk factors influence the frequency and transmission of foodborne pathogens and subsequently impact the risk of produce contamination (Strawn et al., 2013b) . Primarily, pathogens may contaminate produce 'on-field' via various routes including; atmospheric deposition, uptake from contaminated soils and groundwater (Harris et al., 2003; Lynch et al., 2009; Mei Soon et al., 2012) , use of raw (or poorly treated) manure and compost, exposure to contaminated water (irrigation or flooding), transfer by insects, or by fecal contamination generated by livestock or wild Table 1 The most commonly implicated etiological agents in fresh produce borne illnesses (Brackett, 1994; Buck et al., 2003; Heaton and Jones, 2008; Jung et al., 2014; Callej on et al., 2015) . animals (Cooley et al., 2007; Uyttendaele et al., 2015) . A schematic representation of the main entry points for pathogens to humans via produce is provided in Fig. 1 .\n\nThe use of organic materials such as livestock excreta, slurries, abattoir wastes, sewage sludge as well as municipal and industrial waste treatment residuals as soil amendments is widespread Goss et al., 2013) . Although these serve as a costeffective source of nutrients for agricultural purposes, research demonstrates that raw manure as well as contaminated (or improperly treated) manure constitute a significant risk of pathogenic contamination for produce (James, 2006; Manyi-Loh et al., 2016) . Public health relevant bacteria, viruses and parasites such as E. coli O157:H7, Salmonella spp., L. monocytogenes, Campylobacter spp., porcine enteroviruses, bovine coronavirus, bovine virus diarrhoea Cryptosporidium parvum and Giardia have been isolated from raw/poorly treated manure (Derbyshire, 1973; Derbyshire and Brown, 1978; Sellers, 1981; Strauch, 1991; Pell, 1997; Grewal et al., 2006) . Pathogens may be spread through direct interaction of vegetable surfaces with manure, or by splashing of (contaminated) soil/manure particles from the soil on vegetables via rainfall and/ overhead irrigation or by vectors. Additionally, manure piles stored next to growing areas may constitute contamination risk due to run-off (James, 2006; Warriner et al., 2009) . Manure application could be by broadcasting as a solid, semisolid or liquid throughout the field or by the introduction of livestock or wildlife feces at distinct locations (Jung et al., 2014) . In many parts of the world, organic cultivation systems use more manure than conventional growers, and chemical treatment against pathogens is prohibited in organic farming. There have thus been some assertions that organic produce represents a more significant safety risk than its non-organic counterpart, although, there is no unequivocal research evidence supporting this claim Loncarevic et al., 2005; Warriner et al., 2009; Ivey, 2011; Maffei et al., 2016) .\n\nThe survival of pathogens in manure and biosolids depends on factors such as the manure source, production process, and characteristics, treatment technique applied, physicochemical factors like pH and relative humidity, incidence of antagonists or predators, weather conditions, desiccation, aeration, soil type, degree of manure incorporation, amongst others (Ingham et al., 2004; Wood, 2013) (Table 3) . The manure composition, which is determined in large part, by the feed formulation, dictates the profile of pathogens occurring in manure as well as their ability to persist even posttreatment (Franz et al., 2005) . Certain workers have proposed that cattle diet may influence the incidence of representative bacterial species; E. coli O157:H7 and Salmonella in manure. These pathogens have been reported to persist longer in manure obtained from cattle fed diets rich in energy but low in fiber content such as high digestible grass silage and maize silage compared to animals that received diets with low energy and higher fiber content such as straw . It has also been suggested that feeding cattle with hay may significantly reduce shedding of acid-resistant E. coli (Diez-Gonzalez et al., 1998; . How effective these strategies are in reducing pathogen load in (animal-derived) manure, is however not clear. Manure treatment techniques such as composting, aerobic and anaerobic digestion, pelleting, alkaline stabilization, conditioning, dewatering and heat drying have been used to treat manure before application as fertilizer for a long time. While many of them are reasonably efficient, concerns have been raised about their ability to satisfactorily eliminate pathogenic bacteria (Day and Funk, 2002; Lu et al., 2012; Lorin et al., 2016) . Tailing of pathogen inactivation curves, as well as apparent regrowth or recontamination of bacteria after treatment, have been reported. Many pathogens have been shown to be capable of withstanding manure treatment processes, thereby, constituting a major risk of contamination (Brackett, 1999) . Composting is a popular manure treatment and composting temperatures that exceed 55 C for three days are considered sufficient to kill most pathogens (Grewal et al., 2006) . However, few studies have demonstrated that the heat-induced death of bacteria in composted materials is a complex phenomenon (Ingham et al., 2004; Gupta, 2012) . Bacterial regrowth and recontamination in cooled compost have been reported (Hassen et al., 2001; Ingham et al., 2004) . Pelletizing is another common treatment available and is commonly applied to chicken manure (chicken manure pellets). Pelletizing the manure reduces the off-odor and facilitates transport and storage. Although the process usually involves a thermal procedure, more studies are required to validate whether the process efficiently inactivates clinically relevant pathogens (Chen and Jiang, 2014; Jung et al., 2014) . The use of a fish emulsion as fertilizer has raised similar concerns; although most preparation methods available include a thermal process, the ability of this to inactivate enteric bacteria and viruses needs to be rigorously validated (Jung et al., 2014) .\n\nDue to the diverse range of variables associated with manure composition, treatment, pre-application storage, application and incorporation, regulatory bodies have stipulated minimum manureto-harvest time intervals necessary to ensure microbiological safety. The United States Department of Agriculture (USDA) 'Organic production and handling' specifies that unless composted, raw animal manure must be incorporated into the soil not less than 120 days prior to harvest of a product whose edible portion has direct contact with the soil surface or soil particles, or 90 days if there is no direct contact (USDA, 2015) . Canadian authorities specify 3, 15 and 12 months for tree fruits and grapes, small fruits and vegetables respectively as the minimum time delay between manure application and harvest for these crops (Olaimat and Holley, 2012) .\n\nIrrigation water has been identified as a potential source of produce contamination (Benjamin et al., 2013; Uyttendaele et al., 2015; Faour-Klingbeil et al., 2016) . Being a common and essential requirement for crop production, water must be supplied to plants when necessary, and irrigation water sources are used to supplement limited rainfall in many areas (Kirby et al., 2003) . Epidemiological investigations of food poisoning outbreaks, experimental studies examining pathogen contamination of fruits and vegetables as well as observations of increased incidence of disease in areas practicing wastewater irrigation with little or no wastewater treatment indicate that contaminated irrigation water might indeed be a source of foodborne pathogens on fresh produce (Norman and Kabler, 1953; Hern andez et al., 1997; Steele and Odumeru, 2004) . For example, Hepatitis A outbreaks associated with lettuce (Seymour and Appleton, 2001) and spring onions (Josefson, 2003) were linked to sewage-contaminated irrigation water (Heaton and Jones, 2008) .\n\nVarious factors including irrigation regime (method and timing of irrigation), irrigation water sources, type of crop and land use practices in the farm influence the extent and frequency of pathogenic contamination of produce ( such as pathogen concentration, pathogen strain, weather patterns, plant state, and physiology also have significant implications for produce safety (Marvasi et al., 2013; Uyttendaele et al., 2015; Decol et al., 2017) (Table 4) .\n\nThere are several types of irrigation systems available, each of which is typically complex and has its own drawbacks. Most irrigation systems create complicated ecological environments with multiple potential sources and routes of pathogenic contamination (Pachepsky et al., 2011) . Each irrigation subsystem: collection, replenishment, storage, conveyance, distribution off and on-farm, as well as on-farm application involve processes that have great potential to compromise the microbiological integrity of the irrigation water in unique ways. During transportation from the source to the field, water is susceptible to significant microbiological depreciation (Pachepsky et al., 2011) . The prevailing deterioration dynamic will depend on the transportation mode. For instance, irrigation water transport via irrigation ditches and canals involves interaction with microbial reservoirs of bottom sediments, bank soils, algae and periphyton, whereas water transport via pipes involves interactions with biofilms in the transport pipes Pachepsky et al., 2014) . This sort of contamination is particularly prominent in reclaimed water distribution systems . The method of storage for irrigation water can have a profound effect on pathogen transmission. For example, certain studies have demonstrated that water quality is rapidly degraded in storage ponds and tanks due to inputs from avian species or other wildlife (Field and Samadpour, 2007; McLain and Williams, 2008; Higgins et al., 2009) . Other storage systems such as check dams, impoundments, inter-basin transfer schemes, abstraction schemes and reservoirs have been identified as places where indicator and pathogenic microorganisms can survive and proliferate (Abbasi, 2001; Kirubel, 2015) . The mode of application also has significant impacts on the risk of microbiological contamination (Berger et al., 2010) . Compared with furrow and subsurface drip irrigation systems, sprinkler irrigation poses a higher risk of microbiological contamination (Kisluk and Yaron, 2012; Pachepsky et al., 2014) . Surface furrow and drip irrigation systems minimize contact between edible portions of certain plants (leafy vegetables provide larger surface area for contact and possible microbial attachment) and irrigation water (Directorate, 2002; Fonseca et al., 2011; Mei Soon et al., 2012; Uyttendaele et al., 2015) . Hydroponic growing systems also offer this advantage (Jung et al., 2014; Allende and Monaghan, 2015) .\n\nThe irrigation application method has been determined to influence the internalization of some pathogens in produce such as spinach plants. According to some studies, the likelihood of internalizing pathogens increases when the organisms are introduced by water sprinkling systems as opposed to when the water is directly applied to the soil (Solomon et al., 2002; Stine et al., 2005; Mitra et al., 2009; Warriner et al., 2009; Erickson et al., 2010a; Kisluk and Yaron, 2012; Zheng et al., 2013) . More details on pathogen internalization are provided in section 4 (below). Depending on the geographical location, the irrigation regime with respect to time of day, season and harvest time may influence the likelihood of pathogenic contamination. For example, Kisluk and Yaron (2012) in a study conducted in Haifa, Israel demonstrated that night-time irrigation and irrigation during the winter season is more likely to contaminate plants with enteric bacteria. Contaminated irrigation water poses the most significant risk when crops are irrigated close to harvest time, because harvesting of produce containing viable pathogens is more likely. Therefore, an adequate time interval between irrigation and harvest should be conscientiously followed. The microbial quality of irrigation water depends mostly on the source of the water. In order of increasing risk of microbial contamination hazard, irrigation water sources can be ranked as follows: potable or rainwater, deep groundwater, shallow groundwater, wells, surface water and raw or inadequately treated wastewater (James, 2006; Pachepsky et al., 2011) . The microbial quality of rainwater or rain-harvested water is relatively good. The quality and safety of use, however, depends largely on the collection, transportation and storage means. This can be illustrated with roof-harvested rainwater, which may become contaminated with pathogenic bacteria and protozoan parasites because of the occurrence of animal droppings on roofs, particularly immediately after relatively long periods of drought . Groundwater (or borehole water) is usually microbiologically safe, except if it has been contaminated with surface runoff or other sources of contamination close to the aquifer. Certain farm operations such as intensive dairying and border-strip irrigation (a type of surface irrigation, which is a hybrid of level basin and furrow irrigation) (Valipour et al., 2015) lead to leaching of pathogens such as E. coli and Campylobacter to shallow groundwater, thereby contaminating it (Close et al., 2008) . Water from wells that are free from leaks and have sound casing are expected to be microbiologically safe. Factors such as the design of wells, nature of the substrata, depth to groundwater and rainfall may affect the microbial quality of good water (James, 2006; Gerba, 2009) . Surface waters; which are the predominant source of irrigation waters in many countries, including open canals, ponds, lakes, rivers and streams are much more susceptible to pathogenic contamination compared to groundwater (Allende and Monaghan, 2015; Uyttendaele et al., 2015) . Sewage discharges, septic tank contamination, storm drains, wild and livestock defecation, run-off from contaminated fields, industrial and municipal effluents can all potentially contaminate surface waters (Steele and Odumeru, 2004; James, 2006) . Wastewater is usually of poor chemical and microbiological quality. Therefore, it requires extensive treatment before it can be safely used to irrigate crops. Water sources (other than rain) used to irrigate produce is usually only minimally treated or untreated in many cases (Steele and Odumeru, 2004; Jung et al., 2014) . It is expensive and time-consuming to treat irrigation water up to drinking water standards, which is the ideal recommendation (Crook and Surampalli, 1996; Forslund et al., 2010) .\n\nAlthough awareness of the potential dangers of using microbiologically compromised water for irrigation has increased in recent times, scarcity of water resources in certain regions has contributed enormously to the use of sub-optimal supplementary irrigation water sources. In such cases, irrigation water represents a greater microbiological risk to produce (Sundstr\u20ac om et al., 2014) . One of the most frequent pathogens implicated in water-related outbreaks is E. coli O157:H7 (CDC, 1999; Hilborn et al., 1999) . The organism can survive for a protracted period in water (even in deionized water) depending on temperature conditions (Chalmers et al., 2000; Islam et al., 2004a) . It also exhibits a remarkable ability to withstand extreme environmental conditions such as high acidity and extremely low-temperature conditions.\n\nThe ability of a pathogen to survive (or persist) in the environment (and on produce) is an essential determinant in the risk of human infection. The actual risks associated with pathogens occurring in irrigation water depend on numerous variables including environmental conditions such as temperature, pH and UV light (Sant'Ana et al., 2014) . Other factors such as the excreted load of the pathogen, its latency period before it becomes infectious, its ability to efficiently multiply outside a mammalian host, its infectious dose for humans, inhibitory competition from the indigenous microflora as well as host response also play a relevant role (Steele and Odumeru, 2004) . Bacteria and viruses survive for lengthier periods in groundwater compared to surface water because groundwater tends to be cooler, offers protection from sunlight, and has less biological activity (Steele and Odumeru, 2004) . These groups of microbes only typically last no longer than 45 and 15 days in surface water and sewage, respectively. Conversely, parasites (eggs/cysts) may survive for as long as 60 days or even several months in surface water and wastewater (Lefler and Kott, 1974; Sagik et al., 1978; Bihn, 2011) . This suggests that pathogenic microorganisms are capable of surviving for extended periods, which constitutes a profound threat to produce safety. Regardless of the source or route of exposure, one potentially fatal consequence of pathogen contamination of irrigation water is the repeated inoculation of plants with the pathogens. The fate and transport of these pathogens once introduced into the produce vary widely (Table 4 ). Some pathogens are capable of adhesion to surfaces of produce while some others can rapidly internalize into plant tissues under certain conditions, translocate and persist until consumed (Wariner et al., 2003; Bernstein et al., 2007a; Doyle and Erickson, 2008) . This has rendered many conventional processing and chemical sanitizing methods ineffectual (Hong and Moorman, 2005) and is a growing public health concern.\n\nAlthough the potential for produce contamination via irrigation water has been identified, it is difficult to estimate the magnitude of the problem (Groves et al., 2002; Antwi-Agyei et al., 2015) . Despite the fact that numerous studies have linked poor microbiological quality of irrigation water with the incidence of human pathogens on fruits and vegetables, direct evidence of irrigation water causing foodborne disease is relatively rare (Harris et al., 2012) . This is because a substantive \"cause-effect\" relationship is yet to be established as it is required that the same pathogenic strain is isolated from the patient, produce, and irrigation sources (Pachepsky et al., 2011) . Furthermore, there must be a clear sequence of events connecting patient, produce, and irrigation source (Steele and Odumeru, 2004) . This is difficult to achieve due to certain limitations such as an inability to promptly identify the locations associated with produce contamination and delays inherent in foodborne outbreak investigations (Pachepsky et al., 2011) . In the absence of direct confirmation, the \"cause-effect\" relationship can only be deduced based on circumstantial or subjective evidence (Pachepsky et al., 2011) . Also, it is apparent that there is no valid link between detected pathogen levels in irrigation waters and disease risk. Some studies have demonstrated a lack of correlation between pathogen prevalence in waters used for irrigation and disease incidence due to consumption of irrigated produce (Cooley et al., 2007; McEgan et al., 2013 McEgan et al., , 2014 . There is an abundance of laboratory studies elucidating potential mechanisms of produce contamination from waterborne pathogens. However, field studies showing the exact process of produce contamination via this medium are relatively scarce. It is thus expedient to generate more field data in this regard.\n\nSoils typically harbor an abundant consortium of microorganisms, some of which are human pathogens such as B. cereus, Clostridium botulinum, C. perfringens, Listeria monocytogenes and Aeromonas (Nicholson et al., 2005; Warriner et al., 2009; Jay, 2012) . They may, therefore, serve as a medium of plant contamination through seeds, roots or surfaces. Many soil resident pathogens have adapted to survival in soil with spores persisting indefinitely. However, since many agricultural soils are predisposed to point and nonpoint sources of pathogenic contamination, allochthonous pathogens may continuously be introduced into soil environments (Santamaria and Toranzos, 2003) . Some of the primary sources of pathogens into soil include the use of contaminated irrigation water and manure, animal grazing, municipal solid wastes and other effluents (Santamaria and Toranzos, 2003; Sant'Ana et al., 2014) .\n\nThe fate, survival and recalcitrance of pathogens in soil depend on factors such as soil type, soil moisture, pH, temperature, nutrient availability, agronomic practices, as well as soil biological interactions (Table 5) . Soil matric potential (moisture levels) is determined by soil properties and water inputs through precipitation and/irrigation and has been demonstrated to be one of the most critical factors influencing microbial transport and survival in soil . Cool, moist environments are favorable for the survival of bacteria and viruses. Under dry soil conditions, a reduction in bacterial and viral population densities are usually observed (Santamaria and Toranzos, 2003; Ghorbani et al., 2008) . Escherichia coli survival has been reported to be highest in organic soils under flooded conditions, and peak populations recorded after a rise in the water-table accompanying significant rainfall events (Tate, 1978; Rochelle-Newall et al., 2016) . Some pathogens such as Streptococcus faecalis have been proven to thrive poorly under low soil moisture conditions (Kibbey et al., 1978; Jamieson et al., 2002; Cabral, 2010) . Increased rates of virus inactivation at low soil moisture levels have been demonstrated (Yeager & O'Brien, 1979) . Also, decreased recovery of viral (poliovirus type 1 and coxsackievirus B1) infectivity in dried soils was attributed to evaporation of soil water in the same study by Yeager & O'Brien (1979) . In addition, experimentation by Hurst et al. (1978) correlated inactivation of enteroviruses [echovirus type 7 (strain Wallace), coxsackievirus B3 (strain Nancy) and poliovirus type 1 (strain LSc)] in sludge-amended soils with moisture loss in the sludge piles.\n\nSoil pH influences microbial diversity and the biogeochemical processes, which they mediate (Fierer and Jackson, 2006; Nicol et al., 2008) . Optimum pH for bacterial survival seems to be neutral, but fungi are known to be more tolerant of acidic conditions, compared to bacteria (Leahy and Colwell, 1990) . Amino acids (most viruses behave as proteins) have different pK values and so the ratio of positive to negative charges on proteins vary with pH (Yates et al., 1985) . In an experiment that lasted 170 days using poliovirus type 1, echovirus 7, echovirus 9 and coxsackie B3, viruses were detected up till the 110 th e 170 th day at pH 7.5 while at pH 5.0, the viruses died off between the 25th and 60 th day depending on virus type (Bagdasaryan, 1964) .\n\nSoil types vary depending on organic matter content, water release characteristics, particle size distribution and moisture retention capacity. These variations significantly influence the survival of enteric pathogens in soil Atkinson et al., 2010) . Clay soils support the adsorption of microorganisms onto soil particles, and this reduces microbial die-off rates (Reddy et al., 1981) . Clays protect bacterial cells, and possibly viral particles, by creating a barrier against microbial predators and parasites (Santamaria and Toranzos, 2003) . Viruses, which are mostly large proteins possessing various charges, are capable of forming numerous bonds with clay minerals (Stotzky, 1986) . For example, the survival of E. coli is prolonged in clay soils where adsorption of cells to the soil particles protects it against protozoa (Mosaddeghi et al., 2009) . Escherichia coli can persist for up to 25 weeks in clay and loam soils, but for much less (8 weeks) in sandy soils (Lang and Smith, 2007) . Results of a study that compared Rotavirus survival in three soil fractions (whole soil, sand and clay) at temperatures 4, 25 and 37 C for 18 days showed least survival in sand fractions (Davidson et al., 2013) . In the absence of soil particles, Rotavirus survived best at 4 C with survival decreasing, with an increase in temperature, except in whole soil, where it survived better over the entire temperature range and for more than a week at 37 C, indicating that whole soil offered some protective effect (Davidson et al., 2013) . Conversely, though, there is a report of shorter survival duration of enteroviruses (poliovirus type 1, echovirus 7, echovirus 9 and coxsackie B3) in loamy soil than in sandy soil (Bagdasaryan, 1964) .\n\nA link between higher organic matter content and enteric pathogen persistence has been established (Jamieson et al., 2005; Williamson et al., 2005; . There is overwhelming research evidence in this regard, seeing that many of the studies that compared the persistence of enteric pathogens in top and sub-soils recorded higher survival rates in topsoil (Zhai et al., 1995; Nyberg et al., 2010) . Research has also shown higher pathogen levels in organic soils after manure application compared to sandy soils (Tate, 1978; Jamieson et al., 2002) . Therefore, the rates of pathogen survival are lower in sandy soils, which have a low water-holding capacity (Mubiru et al., 2000; Erickson et al., 2014a) .\n\nLower temperatures are more suitable for bacterial and viral survival. The ultraviolet radiation from the sun inactivates viruses on the surface of the soil, but viruses in deeper soil strata are protected from this (Rodr\u00edguez-L azaro et al., 2012; Zablocki et al., 2016) . In loamy soil samples, at pH 7.5, poliovirus and echovirus were recovered after 110e130 days at 3e10 C compared to recovery 40e90 days at 18e23 C (Bagdasaryan, 1964) . Similarly, 10 6 cfu/g Amended with dairy manure Persisted for 25e41 d in fallow soils, manure did not seem to affect persistence, clay seemed to improve pathogen persistence and activity Gagliardi and Karns, 2002 (continued on next page) Poliovirus Type 1 and coxsackievirus B 1 pfu were recovered for up to 12 days at 37 C whereas pfu were recovered from soil for up to 180 days at 4 C (temperature profiles tested were 4, 22 and 37 C) (Yeager & O'Brien, 1979) . The persistence of poliovirus in sludgeamended soil was assessed in a field study where appropriately cultivated and irrigated plots were treated with virus-spiked effluents by flooding. This was done for 123 days spanning through spring, summer and winter seasons. Poliovirus survived best during winter (when it was detected after 96 days), but during summer, the longest survival period was 11 days (Tierney et al., 1977) . Parasites seem to prefer warm temperature conditions. Prevalence of hookworms have been correlated to warm temperatures, relatively high rainfall and low clay content (sandy soils with clay content of less than 15%) (Mabaso et al., 2003) . Nutrient availability is essential for the survival of microbes in the soil. The presence of organic matter promotes the survival, and in many cases, the regrowth of enteric bacteria Looney et al., 2010) . Organic matter improves nutrient retention, serves as carbon sources for bacterial species and enhances moisture retention (Gerba et al., 1975; Schoonover and Crim, 2015) .\n\nApart from environmental stress responses, foreign enteric bacteria must compete with the endogenous microflora to become established in the soil environment (Jiang et al., 2002) . Some autochthonous soil organisms have been shown to be resistant to newly introduced microorganisms in their environment (Ellis and McCalla, 1976) . Also, certain bacteriophage, some protozoa, nematodes and free-living soil organisms such as Bdellovibrio can parasitize non-indigenous pathogens, thereby limiting their survival (Klein and Cassida, 1967; Goss and Richards, 2008) . Additionally, increased pathogen survival, and regrowth in some instances, in sterile soils and soils with relatively low biological activity has been reported (Gerba et al., 1975; Tate, 1978) . There is some research evidence that alien enteric pathogens compete poorly for nutrients and are thereby susceptible to inhibition by soil-borne bacteria (Jiang et al., 2002) . The effects that this has on the persistence of pathogens (especially pathogens introduced via contamination) in soil is however not yet fully understood. The impacts that soil edaphic and biotic conditions have on the occurrence, fate and persistence of microorganisms in soils should not be underestimated. These factors can collectively or independently stifle or encourage foreign pathogens. For instance, members of Listeria possess advantageous intrinsic factors such as an extensive repertoire of transport systems (like phosphotransferase system and transcriptional regulators) which makes them capable of successfully persisting in the soil ecosystem (Newell et al., 2010) . However, these species are highly sensitive to extrinsic factors and this affects their ability to survive in soil environments (Newell et al., 2010; Locatelli et al., 2013) . Although studies have been conducted on the occurrence of L. monocytogenes in various ecological niches, including soil, more emphasis has been placed on the occurrence of Listeria spp. in fresh vegetables under storage conditions, food processing and packaging environments. The expression of genes and induction of proteins such as cold shock and cold acclimation Rapid decline in cell numbers with no difference identified for soil type. Increase in retention at both rainfall rates.\n\nCampylobacter jejuni Clay loam silt 10 5 cfu/g 10 6 cfu/g 10 7 cfu/g proteins, as well as tolerance for low pH and high salt concentration in these environments have received much research attention. There is however, need for more research to understand the dynamics of Listeria survival in soils.\n\nAgronomic practices such as soil improvement and manure application method influence the survival of pathogens in the soil (Table 5) . Soil improvement strategies (inorganic and organic fertilizer, compost, biosolids and other residuals application), significantly enhance the nutrient loads of soils (Diacono and Montemurro, 2010) . In varying degrees, these are important sources of primary nutrients such as N and P as well as secondary nutrients such as Ca, Mg and S to the soil. A ready supply of essential nutrients encourages the growth of pathogens. Compost application modifies the long-term soil conditions by increasing the pH steadily, this, therefore, affects pathogen survival in soil (Weller, 1988; Sharma and Reynnells, 2016) . Bacteria tend to decline more rapidly when manure is applied superficially as opposed to when incorporated into the soil immediately after application (Solomon et al., 2002; Islam et al., 2004a) . This is probably due to the elimination of drying conditions and exposure to UV at the soil surface (Schulze-Makuch and Irwin, 2006) or because incorporation of manure disrupts macropores and boosts soil-bacteria contact .\n\nAfter manure application on land, if applied manure is contaminated, it is probable that the pathogens will move through the soil matrix, either vertically or horizontally. Vertical movement of pathogens through the soil is influenced by the amount and intensity of rainfall, climatic conditions as well as the season of application. Horizontal movement is known to be influenced by soil type, moisture levels, temperature, microbial activity, transport through plant roots, rainfall patterns, soil pH amongst other biophysical factors. It is, however, apparent that water flow is the most important dispersal factor for percolation of manure-derived pathogens in soils, regardless of type and structure although more quantitative information regarding this is desirable (Mawdsley et al., 1995; Jiang et al., 2002; Jamieson et al., 2002; Islam et al., 2004b; You et al., 2006; Semenov et al., 2009) .\n\nThe extent of movement will affect the distribution and eventual fate of the pathogens. Some will spread in soil and attach to roots. Others may be washed off to surface waters or percolate to aquifers, potentially contaminating irrigation water sources (Fig. 2) Vinten et al., 2002; Avery et al., 2004 a, b; Islam et al., 2004b) . Pathogens occurring in contaminated manure, therefore, can be rapidly transported within soil systems (Gagliardi and Karns, 2000; Kisluk and Yaron, 2012) . The success of conveyance and distribution, however, further depends on inherent survival capabilities of the pathogen as well as the presence and structure of plant root systems (Fig. 2) (Kemp et al., 1992; Mubiru et al., 2000; Avery et al., 2004a; .\n\nThere is some evidence that pathogens may indeed survive longer in manure-amended soils than actual manure samples, and this has been illustrated for enteric species such as S. Typhimurium and E. coli O157:H7. Salmonella Typhimurium, has, however, exhibited superior persistence capabilities compared to E. coli O157:H7 in manure-amended soils (Islam et al., 2004b; You et al., 2006; Fremaux et al., 2008; Pornsukarom and Thakur, 2016) . There is a paucity of data on the persistence of pathogens in manure amended-soils in the tropics (Ongeng et al., 2015) . One interesting study provides an insight into the survival of E. coli O157:H7 and Salmonella Typhimurium under tropical climatic conditions . The study showed that survival periods were mostly shorter than the observed record in temperate regions indicating that biophysical conditions in the tropics may be more injurious to these pathogens. It is, therefore, not prudent to predict the survival of E. coli and S. Typhimurium in tropical soils from data obtained in temperate locations.\n\nThe soil is the most important cultivation medium and represents a relevant risk for produce contamination. A myriad of studies regarding the behavior of pathogens in various kinds of soil ecosystems is available. However, validated consensus protocols for conducting and interpreting experimental studies as well as for evaluating the effects of environmental and soil characteristics on fate of pathogens in soils are not yet available. It is important to further understand the effects of soil types, environmental factors, biological processes and interactions, cultivation and management practices on the behavior of (indigenous and foreign) enteric pathogens in agricultural soils.\n\nApart from farm animals, whose roles as reservoirs of enteric pathogens has been established, wild animals such as birds, reptiles, rodents, amphibians, some helminths, and insects like flies and beetles can also serve as vehicles of pathogens to contaminate cultivation media and produce (Beuchat, 2006; Lim et al., 2014) . Livestock and wild animals may gain access to cultivation areas either because of adjacent land use (livestock rearing) or by intrusion (Jay-Russel, 2013) . Birds such as gulls, pigeons, chickens, starlings, Canada geese, migratory ducks and sandhill cranes (Pacha et al., 1988; Hald et al., 2004; Ekdahl et al., 2005; Humphrey et al., 2007) have been determined to be carriers of pathogens such as E. coli, Salmonella and Campylobacter (Wallace et al., 1997; Schmidt et al., 2000; Wani et al., 2004) . Insects are typically ubiquitous in cultivation fields, and hence, have unrestricted access to produce. They are usually found in manure piles, feedlots and other habitats near cultivation fields, and so farms practicing mixed farming represent a more significant risk (Mart\u00ednez-Vaz et al., 2014) . Many bacterial species have evolved to exploit insects as hosts or vectors. Filth flies, fruit flies, cockroaches and other insects act as mechanical and biological vectors to contaminate fruits and vegetables on the field (Sasaki et al., 2000; Mpuchane et al., 2004; Alam and Zurek, 2004; Humphrey et al., 2007) . Many pathogens use flies as vectors for cross-transmission. For example, the transient survival of Pectobacterium carotovorum subsp. carotovorum in the gut of the fruit fly Drosophila and subsequent transmission to other plants has been observed (Nadarasah and Stavrinides, 2011; Lim et al., 2014) .\n\nUnder laboratory environment, direct bacterial transfer from contaminated flies to fruits or plant leaves was shown to occur (Sela et al., 2005; Talley et al., 2009; Lim et al., 2014) . Members of Muscidae and Calliphoride which are usually abundant in production fields adjacent to cattle rearing lots have been associated with the transmission of E. coli O157:H7 . Insects that feed on plants also play significant roles in produce contamination by providing direct routes for internalizing pathogens from manure to plants in the field . Insect deterioration creates openings that aid the ingress of pathogens into inner plant tissues, thereby enhancing colonization of spoilage and pathogenic bacteria on produce (Warriner and Namvar, 2010; Lim et al., 2014) . A seasonal trend to contamination by insects has been identified. There is increased insect and animal activity during the warmer months of the year. Moreover, peak incidences of pathogens have been reported during the warmer months (Liang et al., 2015) .\n\nReptiles including snakes, lizards, chameleons, turtles, as well as other ophidians, saurians and chelonians have been found harboring enteric bacteria like Salmonella (Corrente et al., 2004; Beuchat, 2006) . Many wild rodents are asymptomatic carriers of pathogens like Salmonella and Campylobacter. The occurrence of rodents on farms are often associated with infrastructural impairment, and although their destructive tendencies have been widely recognized, their zoonotic risks are often primarily underestimated. They are capable of amplifying the number of pathogens in the environment and transferring them to other farm animals and produce (Meerburg and Kijlstra, 2007) . Commensal rodents (house mice and rats) pose a particular threat because of their ecology (they live close to livestock) and high fecundity (Brooks and Jackson, 1973; Witmer et al., 2014) .\n\nFoodborne illness resulting from the consumption of contaminated produce is dependent on specific factors. First, the produce must be contaminated with a pathogen, which must survive until the time of consumption at levels sufficient to induce illness (Harris et al., 2003) . The dose required to cause illness in many cases, is very low, which indicates that the microorganism needs only to contaminate the food to survive without necessarily reproducing. For instance, pathogenic parasites and viruses are not capable of multiplying outside a human or animal host and only need to survive in sufficient numbers to cause illness (Harris et al., 2003) . The survival and or growth of pathogens is influenced by the kind of organism, produce type, on-field environmental conditions, as well as the physiological state of the plant and pathogen. The possible routes of entry into plant tissues include: natural apertures (such as stomata, lenticels, sites of lateral root emergence), wounds caused by biotic or abiotic circumstances and following the flow of water from roots to leaves, where pathogens can efficiently survive and multiply (Steele and Odumeru, 2004; Deering et al., 2012; Hirneisen et al., 2012) . The popular opinion is that pathogens will survive but not thrive on intact (uninjured) outer surfaces of produce, primarily due to the protective effects of natural plant barriers (such as cell walls and wax layers) (Mathews, 2006; Heaton and Jones, 2008) . Survival and proliferation of enteric pathogens on produce is significantly enhanced if the protective barrier becomes compromised either by physical or biological damage (such as punctures or bruising), insect ruination or through degradation by plant pathogens. It is vital to understand the microbe-microbe and plant-microbe interactions that occur in the phyllosphere and rhizosphere which influence the adaptation, colonization, \n\nAttachment is pre-requisite for the colonization and subsequent transmission of enteric pathogens throughout plants including the edible portions (Berger et al., 2010) . It is important to note that attachment onto the surface of intact produce is limited in contrast to the attachment on other food commodities such as processed meat tissues . However, the attachment does indeed occur and is facilitated by stomata, lenticels, broken trichomes, as well as bruises and cracks occurring on produce surfaces. The incidence of scars and cracks (which may set in late in the growing season while the fleshy portion is enlarging rapidly) in certain fruits also aids pathogen attachment (Bhagat et al., 2010) . Cracks tend to occur in or on the weak areas on plant surfaces such as around lenticels and trichomes, and hence, these areas are more susceptible to invasion by pathogens. Cavities within the epidermis may also develop from cuticular cracks as the fruit develops, thereby entrapping pathogens and shielding them from desiccation and disinfection. The initial phase of bacterial attachment is a rapid process initiated once the bacteria establishes contact with the plant surface (phyllosphere) (Sant'Ana et al., 2014) . The phyllosphere, also known as the aerial parts of plants pose challenges for microbial survival. Exposure to high UV doses, temperature and relative humidity fluctuations sabotage viability (Brandl et al., 2004; Heaton and Jones, 2008) . Epiphytes that exist within the phyllosphere have, however, evolved specialized mechanisms to improve stress tolerance and nutrient acquisition. For instance, Pseudomonas spp. produce pigments to insulate against UV and pectolytic enzymes to gain nutrients (Heaton and Jones, 2008) . The ability of the pathogen to persist on the phyllosphere improves the chances of a viable or infectious dose remaining post cultivation (Heaton and Jones, 2008) . The successful attachment on the phyllosphere also depends on the crop and pathogen type. A classic illustration is Salmonella invasion of lettuce and tomatoes. Salmonella contamination of lettuce and tomatoes via soil is usually quite low, implying that Salmonella does not readily attach to or grow in the phyllosphere of these crops (Critzer and Doyle, 2010) . Also, attachment of Salmonella and E. coli O157:H7 is observed more frequently with Brassicaceae compared to lettuce, carrots, and tomatoes, which has generated the theory of selective attachment, suggesting that certain produce types are more prone to contamination than others (Warriner and Namvar, 2010). Specific pathogens such as Salmonella have surface epitopes that can bind to plant structures such as stomata to aid attachment (Warriner and Namvar, 2010). Some also have higher capabilities to metabolize nutrients contained within the apoplastic fluid of plants (Warriner and Namvar, 2010) . These traits significantly enhance their attachment abilities. Finally, hydrophobic interactions between a plants' epidermal layer and microbial cells are believed to play a major role in facilitating this initial phase of attachment (Burnett and Beuchat, 2001) .\n\nSurface colonization is the final phase of attachment during which biofilms may be formed. Biofilms are microbial colonies, which form when single microorganisms attach and aggregate on a hydrated surface and undergo a \"lifestyle switch,\" giving up life as a single cell to live on a surface in an adhesive cell matrix with other microorganisms (Lemon et al., 2007) . Cells in a biofilm have a better chance of adaptation and survival (especially during periods of stress) as they are protected within the matrix (Decho, 2000) and are usually resistant to antimicrobial agents (Lemon et al., 2007) . Naturally occurring biofilms are present in many fruits and vegetables, but the ability of foodborne pathogens to associate with them and persist is not yet fully understood (Brackett, 1999; Ferreira et al., 2014; Larsen et al., 2014) . Pathogen serovars that are strong biofilm producers have been shown to attach better to both intact and injured produce surface compared to strains that are weak biofilm producers (Lindow and Brandl, 2003; Kroupitski et al., 2009) . The occurrence of biofilms improves the chances of transient occupants of leaf surfaces such as enteropathogens of becoming effectively incorporated into phyllosphere biofilms (Heaton and Jones, 2008) . Bacterial appendages such as curli, pili, fimbriae, and flagella, as well as proteins in outer membranes and genes, may also facilitate the surface colonization by pathogens. Increases in the expression of fliC, flagellin-encoding gene have been observed in certain produce contamination studies. After attachment, it becomes very difficult to remove the pathogens from produce by surface washing (Beuchat and Scoutten, 2002) . Overall, enteric soil pathogens may reach the edible portions of fruits and vegetables via numerous mechanisms and routes and these have been elucidated by several studies (Natvig et al., 2002; Johannessen et al., 2005; Barak and Liang, 2008; Tyler and Triplett, 2008) . Some of these routes include germination of seeds in contaminated soils, which leads to bacterial colonization of roots and edible parts, direct transfer of pathogens within the soil to crops when heavy rain or water gun irrigation causes leaf splash, bacterial infiltration through roots, amongst others.\n\nAttached pathogens can reach the interior of fruits and vegetables via a variety of pathways. The extent of internalization depends on factors such as the route and mechanism of entry, the type and age of the plant, the aerial and/ or root morphology and exudates, the soil type and biology and the strain and/serovar of bacteria (Hirneisen, 2012; Brandl et al., 2013; Lim et al., 2014) . The mechanism could be either passive or active (Sant'Ana et al., 2014) . Passive internalization involves the uptake of bacteria mainly through roots and seeds. Mechanistically though, enteric pathogens may be internalized via the root system and transported to edible tissues, but the risk of contamination by this route is likely low . This is because in the environment, particularly areas that are not prone to contamination events, the levels of enteric pathogens are likely to be extremely low (Cooley et al., 2007; Matthews et al., 2014) . In contaminated zones, however, human pathogens may indeed invade root tissues and subsequently translocate to edible portions (Solomon et al., 2002; Solomon and Matthews, 2005) . Depending on the age of the plant, pathogens may invade external root surfaces (main and side roots, as well as root hairs) and subsequently internalize. The developmental stage of plant root systems when contamination occurs influences the capability of pathogens to interact with, penetrate plant roots and migrate to other tissues (Mootian et al., 2009) . The physiological characteristics of the roots may also determine the success of internalization; for example, some root vegetables possess antimicrobial properties, which limits the growth and internalization of enteric bacteria . Pathogens like E. coli O157:H7 have been demonstrated to survive longer in the soil in the presence of rye and alfalfa roots (Gagliardi and Karns, 2002) .\n\nOther work has demonstrated that pathogens enter root tissues at sites of lateral root emergence or through damaged roots (Mendes et al., 2013) . Salmonella and E. coli O157:H7 have penetrated Arabidopisis and lettuce plants' roots, while Klebsiella pneumoniae have been detected on numerous plants' roots (Tyler and Triplett, 2008) . Other examples include the invasion as well as (endophytic and systemic) colonization of barley roots by S. Typhimurium, the shoots of black pepper stem cuttings by Pseudomonas aeruginosa, as well as roots and shoots of tomato seedlings by P. aeruginosa (Kutter et al., 2006; Kumar et al., 2013) . It is, however, important to note that successful invasion of the root and shoot system may not guarantee translocation to the edible or foliar portions of produce. In some surveys, bacterial pathogens were detected in roots but not leaves of crops examined (Watchel et al., 2002; Warriner et al., 2003; Bernstein et al., 2007a; Mitra et al., 2009; Sharma et al., 2009) .\n\nA growing body of evidence suggests that seeds may serve as primary inoculum source in produce contamination. In the case of vegetables, seed sprouts have been implicated as the initial inoculum source, severally (Warriner et al., 2003; Deering et al., 2012; Kumar et al., 2013) . In recent time, seeds have been recognized as a significant source of inoculum for foodborne illnesses associated with sprout consumption (Mahon et al., 1997; National Advisory Committee on Microbiological Criteria for Foods, 1999; Buck et al., 2003; Yang et al., 2013) . It is possible that enteric bacteria may be transmitted from contaminated seeds to sprout to mature plants, throughout entire plant life cycle up to consumption. The contamination may be transferred from seed again, thus persisting in produce cultivation cycles, for a long time. There is a record of E. coli 0157:H7 adherence to outer surfaces and subsequent successful internalization of radish sprouts produced from contaminated seed during sprout growth (Itoh et al., 1998) .\n\nRate and efficiency of uptake also depends on the type of produce, and the level of internalization varies widely among plants and even among different species of the same crop due to variations in intrinsic factors, which affect pathogen survival and proliferation (Golberg et al., 2011) . For instance, certain produce items, like fully ripe tomatoes are typically in the pH range (3.9e4.5) which conventionally impedes growth of most enteric bacteria, whereas, the pH of numerous vegetables, melons, and soft fruits is usually 4.6 or higher, which is conducive for bacterial growth Gagliardi et al., 2003) . Therefore, Gram-negative bacteria are more commonly associated with vegetables while molds and certain yeasts mostly occur on fruits, due to the differences in pH requirements of the respective groups of microbes (Jay, 2012) . Members of the Brassicaceae family (radish, turnip and broccoli) were demonstrated to have a higher prevalence of Salmonella contamination than lettuce, tomatoes and carrots when grown in contaminated soil (Barak et al., 2008) . Among leafy greens, radicchio and endive may be more likely to be contaminated than lettuce, spinach, parsley or cilantro (Barak et al., 2008) . Salmonella Typhimurium has been demonstrated to internalize more efficiently in iceberg lettuce and arugula leaves compared to romaine, red lettuce, fresh basil, parsley and tomato leaves, which displayed only marginal internalization. Listeria monocytogenes seems to exhibit a selective preference for certain vegetables like radishes and potatoes, as certain studies reported that although L. monocytogenes successfully invaded tissues of a wide variety of vegetables, radishes and potatoes appeared to be more often and severely contaminated (Beuchat, 1996) . It is also apparent that L. monocytogenes does not survive and internalize satisfactorily on fresh carrot, in fact, low doses of raw carrot juice have been demonstrated to inhibit the growth of the pathogen (Beuchat et al., 1990; Farber and Peterkin, 1991; Oh, 1993; Benkerroum, 2013) .\n\nInternalization is believed to be a plant-pathogen specific interaction, and therefore, internalization success varies from pathogen to pathogen. A comparison of the internalization of L. monocytogenes to S. Typhimurium on inoculated seeds of cress, radish, spinach, lettuce, mustard, carrots, and tomatoes showed significant variations in the rate and efficiency of internalization by the pathogens. Under identical experimental conditions, S. Typhimurium internalized into the roots of the vegetables, whereas, L. monocytogenes did not (Jablasone et al., 2005) . Similarly, while S. Typhimurium was found to be associated with the internal portions of barley sprouts, L. monocytogenes, L. ivanovii and L. innocua were not (Kutter et al., 2006) . Furthermore, the degree of internalization is contingent on the serovar/strain (Larsen et al., 2014) . Gene expression, metabolic and antimicrobial capacities vary among strains. Certain strains manifest up-regulation of peculiar genes like the pdu, cob-cbi, and out which improve carbon source utilization and may confer a competitive edge, thereby enhancing the survival and persistence of these strains (Fox et al., 2011) . Some E. coli 0157 strains possess metabolic capacities, which foster their survival in certain agroecosystems such as soils (Franz et al., 2011) . In a bid to explain the strain-specific internalization dynamics, a five serovar Salmonella cocktail (Montevideo, Michigan, Poona, Hartford and Enteritidis) was inoculated into hydroponic growth substrates. Serotypes Montevideo and Michigan were most prevalent, while Enteritidis, Hartford and Poona were not detected in any of the tomato tissue samples (Guo et al., 2001) . This is a quintessential illustration of internalization variation among serovars. Likewise, Salmonella serovars; Cubana, Infantis and Typhimurium exhibited varying capabilities to internalize and colonize alfalfa sprouts when seeds were inoculated under identical environmental conditions (Dong et al., 2003) .\n\nSome scholars have endeavored to compare the survival of two arguably most prominent foodborne pathogens: E. coli and Salmonella. Serovars of both can proficiently adapt to environmental stress; -numerous strains are known to become habituated to low pH conditions and subsequently manifest remarkable tolerance to stress conditions. Escherichia coli can perpetually evolve new varieties that have neither been previously reported nor characterized and which are capable of exploring and inhabiting previously unrecognized niches (Newell et al., 2010) . Both seem to be capable of long-term survival in the agricultural environment and on produce, but it is quite apparent that Salmonella survives better than E. coli (Brandl, 2006; Mandrell, 2009; Newell et al., 2010; Schikora et al., 2012; Ongeng et al., 2015) . Many Salmonella serovars bind to plants significantly better than E. coli strains. Escherichia coli's inability to lower its metabolic rate to suit the low availability of accessible organic carbon and to competently cope with low nutrient conditions contributes significantly to its die-out in soils and on produce, and therefore, lowers its competitiveness (survival) compared to Salmonella Franz et al., , 2011 .\n\nInternalization has been correlated with motility and chemotaxis. Flagella mutants (fliGHI:Tn10, cheY) deficient in motility and chemotaxis respectively have exhibited reduced attachment and penetration of lettuce leaves (Kroupitski et al., 2009; Lim et al., 2014) . It has also been hypothesized that products of photosynthesis serve as nutrients to aid internalization of pathogens (Lim et al., 2014) .\n\nActive internalization typically involves the penetration of bacteria through natural openings. The ability of foodborne pathogens to internalize in produce represents a significant public health risk because internalized pathogens are protected against optimized disinfection modes (Meireles et al., 2016) except irradiation which seems capable of reasonably eradicating internalized pathogens in produce. The technique penetrates produce tissues to eliminate internalized pathogens, and Gram-negative bacteria are very susceptible to even low doses (Saroj et al., 2007; O'Bryan et al., 2008) . However, treatment with irradiation may produce off flavors, colors and odors and may inactivate some of the nutrients (Fan and Sokorai, 2008) . It is, therefore, not accepted and endorsed for produce treatment. There are other relatively new technologies such as modified atmosphere packaging, ozone, ultrasound and ultraviolet treatments, which seem promising in ensuring the microbiological safety of fresh fruits and vegetable products (Shayanfar and Pillai, 2014) . However, limited commercial applications have been described for most of these new technologies. Electron beam technology is another up-and-coming treatment option, which according to experts, can play a pivotal role in mitigating some of the contemporary microbiological risks facing the produce industry (Shayanfar and Pillai, 2014; Lung et al., 2015) . It is an environment friendly, cost and time effective decontamination strategy that uses low-dose ionizing radiation to treat crops (-as well as other food items), to eliminate microbial contamination. It is capable of inhibiting the germination of crops and controls the rate of ripening of fruits and vegetables, thereby extending their shelf life (Lung et al., 2015) . It inhibits a variety of enteric pathogens without compromising food sensory and nutritional qualities and can be used in combination with other traditional or non-traditional food processing technologies (Lung et al., 2015) . Regulatory authorities such as the US Food and Drug Administration have approved it, but the full import of the safety of use is not yet conclusive.\n\nGiven the amount of evidence indicating that enteric pathogens (that are not plant pathogens) can invade and be internalized into plants, it is important to understand how such microbes establish access to plant tissues, as this may facilitate the development of strategies to reduce internalization. For successful colonization, major interactions take place between pathogens and their plant hosts that determine the success of the pathogenic attack (Warriner and Namvar, 2010). Many enteric pathogens have devised mechanisms to overcome plants' basal defense mechanisms and innate immune responses (Lim et al., 2014) . Plants first line of response to foreign invasion is by the innate immune system. This consists of two main branches: PAMP-triggered immunity (PTI) and effector-triggered immunity (ETI). In the first stage, microorganism associated molecular patterns (PAMPs or MAMPs such as flagellin, peptidoglycan, lipopolysaccharide) are identified by plant host receptors popularly known as Pattern Recognition Receptors (PRRs) (Deering et al., 2012) . These batteries of receptors deployed by the host are designed to curb the growth and spread of the pathogen (Ausubel, 2005) . PTI response is broad-spectrum; sensitive to molecules familiar to many classes of microorganisms including non-pathogens. Upon recognition, plant defense signal pathways are activated among which, jasmonate, salicylic acid and ethylene play essential roles.\n\nVirulent plant pathogens may through diverse strategies, such as the production and secretion of effectors, efficiently override PTI, for example, there are some 'effectors' that can overcome PTI by interfering with MAMP detection and subsequent defense signaling (Kazan and Lyons, 2014) . This results in effector-triggered susceptibility (ETS). For susceptible interactions, effectors produced and released by the microorganism are transferred into the plant cell through the TTSS (Type III Secretion System). Specific nucleotidebinding leucine-rich-repeat (NB-LRR) proteins encoded by resistance genes, resulting in ETI and limitation of pathogen transmission to other tissues, recognize these effectors. While PTI is considered the first line of defense against pathogenic infection, ETI is an accelerated and amplified response, the outcome of which is often a hyper-sensitive response (HR) (Spoel and Dong, 2012) .\n\nThe ability of pathogenic bacteria to colonize a plant may also be influenced by their interactions with other microorganisms either positively or negatively (Deering et al., 2012) . If other microorganisms supply carbon sources (via degradation of cell wall polymers or induced secretion of sugars), or sequester antimicrobials, this can enhance pathogen colonization (Bais et al., 2006; Warriner et al., 2009; Augimeri et al., 2015) . Alternatively, plant pathogens that wound or destroy living tissue may create a microenvironment that is suitable for the survival and/replication of human pathogens (Rashid et al., 2016) . Pathogens are often associated more with plants whose tissues have been damaged by soft-rot pathogens compared to those with healthy tissues (Brandl, 2008) . Before pathogenic bacteria can colonize the surface or interior of a plant host, they have to contend with the naturally occurring microflora that is already established (Deering et al., 2012) . The ability of the indigenous bacterial community to inhibit the growth of introduced enteric pathogens has been demonstrated by numerous studies (Liao and Fett, 2001; Matos and Garland, 2005; Schuenzel and Harrison, 2002; Cooley et al., 2003; Johnston et al., 2009) .\n\nThere is direct evidence that the stomata play essential roles in internalization, host immunity and pathogen virulence of pathogens (Kroupitski et al., 2009; Zeng et al., 2010) . Some researchers have reported that plant stomata close in response to plant pathogens and some human pathogens (Melotto et al., 2008; Roy et al., 2013) . Escherichia coli O157:H7 has been reported to trigger stomatal closure even under high relative humidity, a stressful environmental condition that generally weakens plant defenses against bacteria in field and laboratory conditions (Roy et al., 2013) .\n\nStomata closure could be triggered by certain peptides such as flg22 produced by bacterial flagellin and lipopolysaccharides which are recognized by PAMPs or MAMPs in a salicylic acid-dependent manner. In the case of some plant pathogens such as Xanthomonas spp. and Pseudomonas syringae, virulence factors produced are capable of overcoming this innate immunity and counter stomata defense. For example, Pst DC3000 and several other pathovars of Pseudomonas syringae, produce coronatine (COR), a phytotoxin which can reverse stomatal closure induced by bacteria or MAMPs (Zeng et al., 2010) . Stomatal immunity can diminish the penetration of human pathogens through the leaf epidermis, resulting in low bacterial titers in the plant apoplast (Roy et al., 2013) . However, plant defense responses induced by pathogens vary and plants may recognize and respond to some human pathogens more effectively than others (Roy et al., 2013) . For example, comparison of plant defense responses induced by E. coli O157:H7 and S. Typhimurium SL1344 in Arabidopsis thaliana and lettuce (Lactuca sativa) revealed some variations. While E. coli O157:H7 triggered stomatal closure, SL1344 only induced a transient stomatal immunity. Also, PR1 gene expression was significantly higher in Arabidopsis leaves infected with E. coli O157:H7 compared with SL1344 (Roy et al., 2013) .\n\nAlthough, numerous studies have examined the intricacies of internalization in fresh produce, many of these are laboratory based. The few available field studies, which have mostly studied E. coli, indicate that internalization of pathogens may be not be very common in field settings Erickson et al., 2010b Erickson et al., , 2013 Erickson et al., , 2014b . More field studies are therefore, required to properly understand the potential/likelihood of enteric pathogens to internalize in fresh produce as well as the actual factors that influence the success of internalization.\n\nTo successfully achieve an acceptable level of microbiological safety for fresh produce, it is essential to control environmental contamination in the field by taking appropriate pre-harvest precautions. One fundamental factor to consider is the state or quality of the growing fields. Fields on which wild or domestic animals have been recently grazed that have been subjected to flooding or may have been previously contaminated with manure constitute an unacceptable microbiological risk (Turb e et al., 2010) . Therefore, growers need to scrupulously investigate land history when selecting a location for produce cultivation (Islam et al., 2004a, b) .\n\nCultivation areas should be safeguarded from flooding, and fecal contamination and manure should be adequately treated (using popular methods like composting and aging) before application as fertilizer. Also, suitable buffer zones (physical barriers) such as mounds, diversion berms, vegetative buffers as well as ditches should be erected between animal grazing regions and produce cultivation areas (James, 2006; Olaimat and Holley, 2012) . Appropriate livestock waste disposal and farm general waste management should be adopted to ensure safety.\n\nNumerous experts have highlighted the need for monitoring, regulation and control of the microbiological quality of irrigation water. Several regional and international standards exist for irrigation water use and practices to prevent incidence of bacterial contamination. The use of potable water for irrigation (and other cultivation operations) is highly recommended. Certainly, this is not economical in many instances and may increase production costs, which will raise prices; it is however, pertinent to public health safety. In developing countries, a myriad of safety regulations exists such as cessation of irrigation prior to harvesting, lowering of watering cans to reduce splashes from (contaminated) soil, adoption of furrow irrigation system over the use of sprayers which expose edible portions of leafy vegetables directly to irrigation water, and so on (Keraita et al., 2007; Amoah et al., 2011; Uyttendaele et al., 2015) . In cases where surface water is the irrigation water source, drainage of contaminated water into the surface water reservoir may be prevented by constructing ditches, buffer strips, as well as retention and drainage systems. Potential overflow points should be identified and eliminated. It is also important to determine (potential) points of contamination because control measures are bound to be more effective if focused on eliminating contamination at the source (Madramootoo et al., 1997; Pachepsky et al., 2011) . Irrigation wells, functional septic, water and sewage systems should be installed and properly maintained especially during periods of excessive rainfall to prevent pathogen contamination (Buck et al., 2003; Olaimat and Holley, 2012) . Surface and groundwater resources should be protected from any potential sources of contamination including wildlife, animal waste, agricultural run-off, human activity, sewage, or industrial effluents. Other management practices like; removal of riparian areas, erection of fences, and treatment of irrigation water (for example, using UV treatment) can be considered to enhance safety assurance of irrigation water. These precautions will minimize contamination risks on produce farms and should be applicable not just to supposed high-risk crops (such as leafy greens) but all produce (squash, and others) (Strawn et al., 2013b) . Implementing some of these may, however, be costly and have negative impacts on landscape health. Irrigation water sources should be routinely monitored to ensure microbiological safety (Brackett, 1999; Islam et al., 2004b) . Ideally, there should be more regular reporting on the microbiological quality of irrigation waters in different world regions. Such surveys should reflect the true levels of actual pathogens rather than indicators, and bias should be avoided towards contaminated samples by intensively monitoring every irrigation source possible, and not just sites where extensive contamination has been known to occur (Stoeckel, 2009) .\n\nAs part of a total package of hygiene measures to prevent the transfer of foodborne pathogens, wild animals, birds, flies and rodents should be controlled in cultivation areas. Interventions to mitigate wildlife intrusion of a farm may be costly and not entirely effective, especially if not done properly, thereby allowing certain animals direct access to crops. In many cases, it is not economical to fence large farms, but small farms can be fenced to restrict wild animals (Jung et al., 2014) . Other mechanical/biological control methods include the use of scarecrows, reflective strips, monitoring of animal tracks and field intrusion as well as gunshots to ward off pests and animals. Mechanical traps and baits can be used to control mice and rodents. Overall, practical, cost-effective methods should be adopted to mitigate wild sources and routes of produce contamination.\n\nConsidering that, in many important outbreaks, vegetable seed sprouts have been implicated as the initial inoculum source, the elimination of bacteria from seeds before planting has become crucial (Buck et al., 2003) . Chemical or physical treatment methods are usually used to decontaminate seeds, in a bid to reduce the risks of sprout borne disease outbreaks. However, this poses some challenges for growers, as the chosen decontamination method has to fulfill certain conditions. One important consideration is the preservation of seed viability. Selected treatment dosage should be able to inactivate pathogens without adversely affecting seed viability (Buck et al., 2003) . Also, the treatment must be able to penetrate and access bacteria that may be residing in protected seed tissues, and finally, certain treatments may be inactivated by seeds, rendering them less effective (Buck et al., 2003) . Nevertheless, the efficacy of chemical seed treatments for sprout seed including chlorine compounds (commonly calcium and sodium hypochlorite), ethanol, hydrogen peroxide, calcium EDTA, 4-hydroxybenzoic acid, ozonated water and other commercial disinfectants have been extensively documented. It is also possible to use gaseous chemicals and thermotherapy (e.g., hot water treatment), although excessively high temperatures may affect sprout vigor. Another potential issue with hot water treatment is that when treating large batches of seed, it is practically impossible to achieve temperature uniformity throughout the water bath. Therefore, while a portion of the seeds receives the appropriate temperature-time exposure, some will still contain viable bacteria after 'treatment.' Also, there is a potent risk of cross-contamination with this technique. Other viable options include seed treatment with bacteriophage, combinations of thermotherapy with chlorine and the use of ionizing radiation. Radiation is particularly appealing because it can penetrate seed tissues and possibly eliminate bacteria localized within protected tissues (Buck et al., 2003) . However, it has been postulated that high levels of irradiation may distort the physiology and organoleptic properties of seedlings, more research is therefore, needed to evaluate the prospects and risks of this approach. Other precautionary measures include testing seed lots for purity and germination rate prior to marketing, proper warehouse storage (in metal bins) until bagged, as well as ensuring general facility sanitation and employee hygiene (National Advisory Committee on Microbiological Criteria for Foods, 1999).\n\nSafety criteria and regulations are mostly region specific, it is however, critical to enforce these regulations, ensure that growers adhere to such and there is a need to constantly improve standards; if new information becomes available, regulations should immediately be updated (K\u20ac opke et al., 2007) . Most of the available data is from the developed world mainly from the US and certain parts of Europe. It is necessary to develop surveillance and tracking systems and generate robust databases for other regions as well. More studies should be conducted under field conditions, rather than laboratory or greenhouse simulations, as this will provide a better understanding of how enteric pathogens behave in agricultural production environments.\n\nFinally, and more importantly, it is necessary to ensure producers are mindful of their roles in assuring food safety. Growers should be encouraged to adopt the best possible agricultural practices to ensure produce safety. It is also important to enlighten consumers about possible risks and appropriate mitigation strategies. There are wrong notions and misconceptions, which have to be corrected promptly, for example, many people believe it is not necessary to wash organically grown fruits and vegetables (Leifert et al., 2008).\n\nIt is evident that epidemiologic investigations are worthwhile as public health directives and policies based on investigation output have averted impending foodborne disease crises in many cases. The relevance of epidemiological surveys globally and regionally, therefore, cannot be overemphasized. This means that epidemiological investigation tools and systems need to be objective, updated, precise, flexible and timely. While significant progress has been achieved in the area of epidemiology, there are still certain cracks that need to be addressed. The use of routine, optimized clinical pathogen identification techniques may mean that new pathogens may likely be missed. This is a potentially grave issue, because periodically, since the development of foodborne disease surveillance, the list of foodborne pathogens has continued to expand. Care should, therefore, be taken to avoid research bias since it is likely that produce items that have been previously associated with foodborne illness outbreaks and product recalls may receive particular scrutiny. New pathogens emerge due in part, to evolving ecology and technology while already recognized strains continue to evolve, potentially becoming smarter, evading and subverting detection, sanitization and plant host defenses. It is important to further understand the evolution dynamics and emergence of new pathogens, as well as develop and optimize methods to meet the emerging challenges.\n\nAwareness and surveillance of viral and parasitic enteric pathogens need to be more robustly developed. Although Noroviruses, Hepatitis A, Rotaviruses as well as certain emerging viruses such as SARS are well known, they are rarely routinely screened for in fresh produce in most countries. Also, their ecology in fresh produce is poorly understood, for instance, the knowledge of the stability and persistence of human Norovirus in foods has been garnered mostly from the study of surrogate viruses. More importantly, their significance in foodborne disease incidence remains undetermined. Parasitic pathogens like Ascaris, Giardia, Entamoeba, Cyclospora, Cryptosporidia and Trichinella are recognized (Newell et al., 2010; Robertson et al., 2014) , but not all are routinely monitored in produce.\n\nThe roles that livestock and wildlife play in pathogenic contamination of fruits and vegetables as well as their epidemiology through the food chain is poorly understood. It is difficult to compare the available studies because some have used naturally contaminated animals, while others used experimentally inoculated animals. The exact transport/transfer mechanisms of pathogens from animal fecal material or contaminated manure/soil to fruits and vegetables via splash are not yet properly understood. For example, it will be helpful to understand the specific spatial factors that influence the transfer of pathogens from fecal pellets to fruits and vegetables. The survival times for pathogens in fecal contaminants, manure, and manure-amended soils are inconsistent, reflecting the varying conditions under which many of the available studies have been conducted (These variations are demonstrated in Tables 3e5). The fate of pathogens on the soil surface, the relationship between manure-derived pathogens and soil particles, as well as the states in which pathogens occur in soil slurry or manure mixtures, should be further explored. The exact mechanisms of uptake or (transmission) of pathogens from contaminated manure or manure amended soils to plants, particularly in field settings should be studied. This will facilitate the design of scientifically sound produce safety standards. The majority of studies available on pathogen transport in soils have been conducted using homogenized natural soils in laboratory designed soil columns. These may not be a true representation of field conditions and diversifying the experimental conditions will aid the development of efficient, grower-level interventions that will effectively reduce the likelihood of on-field contamination of produce.\n\nThere are dissenting opinions among experts on a variety of issues pertinent to produce safety. With regards to the factors, mechanisms as well as principles that aid competent internalization and persistence of pathogens on produce, there are many variations. The available studies are difficult to compare largely because they have been conducted under varying physicochemical circumstances, types of microcosms, experimental conditions and used distinct strains (Shown in Tables 3e5). Most studies were conducted under disparate environmental conditions, and accurate weather data necessary to interpret results from the varying sources is lacking. Study results for one crop variety may indeed not hold true for other varieties, for instance, data for apples may not necessarily apply to all pome fruit and data for romaine lettuce may not apply to all leafy greens. When possible, varieties exhibiting greater potential for pathogen survival should be selected for experimental investigations. Another relevant consideration for crop selection is preference for varieties that are indigenous to the region in question. Some other seemingly trivial controversial issues include whether outer leaves are significantly more likely than inner leaves to become contaminated via splash and whether or not the potential for survival on the abaxial side of leaves is higher than on the adaxial side. The implications of dormant, non-dividing 'persister' cells occurring in certain plant pathogens on the ability to withstand environmental stresses and extensive survival as well as the issues surrounding linked resistance is still an important research debate. Also, even though atmospheric deposition seems to be an uncommon route of pathogenic contamination for produce, it has been documented as a potentially important route (Beuchat and Ryu, 1997; Harris et al., 2003; Mei Soon et al., 2012) . It will be worthwhile exploring how relevant this is for produce safety. While many of the available studies have made stringent efforts to simulate produce cultivation circumstances, it is extremely challenging to create precise/accurate environmental conditions in a laboratory setting. Most studies are conducted under controlled laboratory conditions. Factors like the biological activity of the soil, manure, water and crops, soil and water chemistries as well as meteorological elements such as wind, UV intensity, temperature, rainfall are simply impossible to replicate under laboratory conditions. Laboratory scale model systems may provide important details about the roles of environmental variables on pathogen growth and survival in agricultural environments, but the slightest tweaks in experimental protocols can affect pathogen survival in agroecosystems. Unfortunately, actual field-based studies are subject to disruption from unforeseen environmental events such as weather extremes and damage triggered by biological agents including insects or onset of plant diseases.\n\nMore field studies (where typical agricultural practices and conditions are closely simulated) are therefore, highly desirable to further understand the persistence phenomenon. Safety and ethical issues however restrict the use of pathogens in the greenhouse and field-based research. Strategies to improve existing biocontainment and decontamination processes should be developed and optimized as soon as possible. Another possible solution is to develop and optimize strategies that will cater for the experimental variations in model system development. An assessment and identification of environmental variables that influence the fate of test organisms should be included in experimental designs. Despite meticulous planning however, a field trial may fail to yield serviceable results due to factors that are out of the researcher's control. Consequently, more replicate trials may need to be conducted. Furthermore, agronomic and farm management practices are not uniform in all regions, and production practices significantly differ from region to region depending on seasons and weather patterns within the same region. These often depend on operation scale, type of farming practices et cetera. The risks associated with conventional cropping systems are bound to differ from those of systems that combine intensive livestock farming with arable farming. In addition to general studies, a case-by-case approach should be considered where possible (if financial and technical resources, as well as other circumstances, permit) because farming operations vary widely from farm to farm and this influences the potential for pathogen occurrence, survival, proliferation and dissemination.\n\nThe potential of fresh produce to harbor pathogens is now well recognized, and fresh produce has been established as a vehicle of foodborne disease. The diverse and complex sources and routes of enteric pathogens to fruits and vegetables have been widely researched. The interplay of land use, water management, weather patterns and specific pathogen properties and sources have been illustrated to have significant consequences for the microbiological safety of fresh fruits and vegetables. Attempts have been made to understand the general microbial profile of fresh produce, the behavior, fate and transport of pathogens, as well as their location in and on plant parts. The facts gleaned from these studies have been the subject of many extensive reviews. There is abundant information about the factors that affect the contamination and persistence of pathogens on fresh produce. In light of the available evidence, significant effort must be made to efficiently monitor and illustrate recent trends in the occurrence of foodborne diseases associated with the consumption of fruits and vegetables. Partnerships and collaboration among all relevant stakeholders; commercial growers, public health practitioners, veterinary and food safety experts and field biologists is necessary in order to ensure the safety of fruits and vegetables delivered to consumers.\n\nOn a final note, the need to control all potential pathogen entry pathways has been established and is being continuously stretched by regulators and other specialists. There are numerous other factors along the food production chain that may predispose produce to microbial contamination. However, it is of utmost importance to avoid and control microbial contamination of produce at the preharvest stage. This is because contaminated manure, water and soil have been shown to indeed contaminate produce, and decontamination of produce, polluted arable soil and groundwater has proven to be a very challenging and expensive endeavour.\n\nThe authors declare no conflicts of interest."}