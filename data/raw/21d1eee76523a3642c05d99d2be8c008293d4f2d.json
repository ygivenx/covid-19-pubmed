{"title": "Animal and Plant Health Inspection Service (APHIS)-Veterinary Services (VS) Center for Epidemiology and Animal Health, National Animal Health Monitoring System, Fort Collins, CO 80526-8117 URIE ET AL", "body": "The preweaning phase of dairy heifer calves represents a critical period in the life of replacement females, as they are very susceptible to disease and death. Calves are at the greatest risk of dying during the first 21 d of life (Waltner-Toews et al., 1986; USDA, 1994; Wells et al., 1996 Wells et al., , 1997 . Overall, the health status of a preweaned dairy heifer can greatly affect lifelong production, including growth, reproductive efficiency, and milk production. An association between calfhood bovine respiratory disease complex and decreased milk production, reproduction, and early culling has been demonstrated (Stanton et al., 2012) . Furthermore, morbidity, disease treatment, and mortality in preweaned calves result in a forfeiture of genetic potential for future herd improvement (Wells et al., 1997; Swali and Wathes, 2006) . The economic loss of replacing a dairy heifer due to disease and death is significant. Although raising a dairy heifer is expensive, this cost generally does not surpass the cost of purchasing a springing heifer (McGuirk, 2008) . Therefore, understanding relationships between management practices and calf health is essential for minimizing morbidity and mortality and enhancing future production.\n\nFew recently published studies have examined risk factors for morbidity and mortality in preweaned dairy heifer calves. The most recent estimates of national morbidity and mortality in preweaned dairy heifers were reported in 1992 and 2007 (USDA, 1994 (USDA, , 2010 . The percentage morbidity in the 1992 National Dairy Heifer Evaluation project was 36.1% and the percentage mortality was 8.4% (USDA, 1994) . These numbers stayed relatively consistent with the USDA NAHMS Dairy 2007 study, which reported a morbidity of 38.5% and a mortality of 7.8%. Both studies had morbidity and mortality percentages above the Dairy Calf and Heifer Association (2010) target rates for preweaned dairy heifers, which are 25 and 5% for morbidity and mortality, respectively.\n\nThe objectives of this prospective study were to (1) estimate current national morbidity and mortality rates, (2) describe common clinical signs and treatment methods currently used in preweaned dairy heifers, and (3) examine risk factors via inferential modeling for morbidity and mortality.\n\nThe USDA's National Animal Health Monitoring System (NAHMS) conducts national surveys to collect information on the health, management, and productivity of domestic livestock species . In 2014, a nationwide survey was conducted to collect information about the US dairy industry and included an 18-mo longitudinal preweaned heifer calf study. All participation in NAHMS studies is voluntary. However, to participate in the longitudinal calf study, operations had to have completed both NAHMS Dairy 2014 surveys, and agreed to provide information on enrolled calves, whether raised onsite or offsite.\n\nThe calf component was part of the NAHMS Dairy 2014 study, and it consisted of a convenience sample of 104 dairy operations, including both conventional and organic operations. These operations were located in 13 states, including California, Colorado, and Washington in the West region, and Iowa, Michigan, Minnesota, Missouri, New York, Ohio, Pennsylvania, Vermont, Virginia, and Wisconsin in the East region. Dairy operations were categorized based on the number of mature cows, as small (30 to 99 cows), medium-sized (100 to 499 cows), and large (500 or more cows). Figures describing the study sample and operations can be found in Urie et al. (2018a) .\n\nData collection for the calf component of the study occurred from March 2014 to September 2015. Data collectors were trained on data and sample collection. Each operation was instructed to enroll 24 heifer calves over a 1-yr period, or an average of 2 calves/mo. Farm personnel selected which calves to enroll in the study. However, a calf must have been alive at 24 h of age to be enrolled. Because fewer operations participated than originally planned, the target number enrolled per operation was increased to 48 calves. Additionally, because enrollment of farms did not occur as quickly as anticipated, the study encompassed 18 mo instead of the 12-mo period that was planned.\n\nEach calf enrolled in the study had a Heifer Calf Health Card (\"Calf Card\") filled out to record information on events that occurred from birth to weaning (https:// www .aphis .usda .gov/ animal _health/ nahms/ dairy/ downloads/ dairy14ques/ CalfHealth .pdf).\n\nThe Calf Card contained questions in both English and Spanish and was filled out by farm personnel, a veterinary medical officer, extension personnel, veterinarians, or a combination of people involved with calf raising. The Calf Card included birth data (e.g., birth date, weight, and calving ease), colostrum feeding data (including timing, amount, and method of colostrum feeding), preweaning housing and procedures data (e.g., housing, ventilation, bedding, navel disinfection, and dehorning), milk feeding (including type of liquid diet fed, any additives, and method of feeding), milk consumption record (volume and frequency of feedings), preweaning growth record (hip height and heart girth recordings every 2 wk), biologic sampling record (including serum collection and fecal sampling dates), vaccinations, disease incidence and treatment, weaning data (weaning date, primary weaning criteria), and any additional notes. Starter feed labels and milk replacer labels, if applicable, were also collected.\n\nProducers were provided with the Calf Health Scoring Chart (https:// www .vetmed .wisc .edu/ dms/ fapm/ apps/ chs .htm) to guide the reporting of clinical signs. The goal of the study was to collect recognized clinical signs and causes of death, not to standardize clinical signs and causes of death across operations. All clinical signs and causes of death were reviewed, edited, and categorized, as described in the Statistical Analysis section.\n\nBecause of the variation observed in the quantity and quality of the liquid diet fed per day, the amounts (kg) of protein and fat fed per day were calculated for each calf. The amount of liquid diet fed per feeding and the frequency of feedings per day were recorded on the Calf Card and used to calculate the total amount of liquid diet fed. For calves fed milk replacer, the percent protein and percent fat were obtained from the producer-reported values or the milk replacer label. For calves fed whole or waste milk, percent protein was set at 3.1% and percent fat as 3.8% (as-fed basis; Chester-Jones and Hoffman, 2003) . The total amounts of protein and fat were calculated and then divided by the number of days fed a liquid diet to determine the average kilograms of protein and fat fed per day in the liquid diet.\n\nAdditives in the liquid diet were categorized as antibiotics, direct-fed microbials, fly control, acids/preservatives, coccidiostats, vitamins and minerals, or other additives. Information regarding calf starter included the percent protein and the age starter was first offered, but starter intake was not recorded.\n\nBefore enrollment in the study, calves were screened for persistent infection with bovine viral diarrhea virus (BVDV). V-Cut ear notchers (Nasco, Fort Atkinson, WI) were used to collect ear notch samples from all calves, which were tested on-farm for BVDV using the Idexx SNAP BVDV Antigen Test (Idexx, Westbrook, ME). Calves that tested positive for BVDV were excluded from the study. Colostrum samples (40-50 mL) from the first feeding of colostrum administered to each calf were collected in conical screw-top tubes by farm personnel and frozen until shipping. Blood samples (5 mL) from calves between 1 and 7 d of age were collected in serum separator tubes by veterinary medical officers or animal health technicians, and samples were centrifuged if possible before shipping. Colostrum and blood samples were shipped together on ice to USDA's National Veterinary Services Laboratories (Ames, IA). Blood samples were centrifuged at the National Veterinary Services Laboratories and serum was separated. Serum and colostrum samples were then accumulated and shipped in bulk to the Saskatoon Colostrum Company (Saskatoon, SK, Canada) for testing. Colostrum samples were tested for IgG concentration using radial immunodiffusion and Brix score using a digital Brix refractometer. Serum samples were tested for IgG concentration using radial immunodiffusion and for total protein and Brix score using a digital Brix refractometer. Blood samples collected within 24 h of birth or after 7 d of age were excluded from this analysis. A complete description of the methods used for Brix scoring and IgG determination can be found in Shivley et al. (2018) .\n\nFor evaluation of enteric parasites, veterinary medical officers or animal health technicians collected approximately 50 g of feces directly from the rectum of calves between 2 and 4 wk of age. Fecal samples were placed in cylindrical screw-top containers and shipped on ice to the USDA Agricultural Research Service's Environmental Microbial Food Safety Laboratory (Beltsville, MD), as quickly as possible after sample collection. Fecal samples were tested by immunofluorescence microscopy for Cryptosporidium parvum and Giardia duodenalis. A complete description of the methods used for fecal parasite testing can be found in Urie et al. (2018b) .\n\nCalves were measured approximately every 2 wk during the preweaning period to measure growth. Height/ weight tapes from Coburn (Nasco, Fort Atkinson, WI) were provided for measuring calves. For consistency, it was recommended that 1 trained veterinary medical officer or animal health technician complete the measurements on all calves enrolled on an operation. Birth weights were estimated using a scale, hoof circumference, or heart girth circumference and reported in pounds or kilograms. The method operations used to estimate birth weight were not captured. Hip height and heart girth circumference were measured approximately every 2 wk and recorded in centimeters. Heart girth circumference in centimeters was converted to kilograms using the following equation (Heinrichs et al., 1992) :\n\nPreweaning weight gain was calculated by subtracting the birth weight from the final weight. Average daily gain was calculated by taking the weight gain during the preweaning period divided by the number of days between birth weight and final weight (approximately the preweaning period). Final weights did not always occur at the same time as weaning. However, only calves with final weight measurements within 14 d of weaning were included in the analysis.\n\nTemperature-humidity index (THI) data were obtained monthly by county from the National Oceanic and Atmospheric Administration (NOAA, 2016) . The THI accounts for the effects of temperature and relative humidity, and the equation uses dry bulb temperature (T, \u00b0F) and relative humidity (RH). The equation used for this analysis was THI = T -[0.55 -(0.55 \u00d7 RH/100)] \u00d7 (T -58). Each calf was assigned a THI for each month during the preweaning period and averaged over the months of their preweaning period (pTHI).\n\nWhen each calf was weaned, the Calf Card was mailed to USDA NAHMS (Fort Collins, CO). Initial ongoing validation was performed on every calf card, followed by data entry into SAS (version 9.4; SAS Institute Inc., Cary, NC). Once all Calf Cards were entered, the data were validated again by NAHMS staff and merged with the results from the colostrum, serum, and fecal testing obtained from the laboratories.\n\nAfter data validation was complete, primary morbidity categories, monthly calf enrollment, and incidence rates were calculated. Primary morbidity categories for digestive, respiratory, concurrent digestive and respiratory, and other clinical signs were created to allow for single classification of each case (i.e., each case was only listed under the primary reported clinical sign). These categories excluded nondescript signs, such as dull and dehydrated. For example, if a calf was listed as having diarrhea and being dull and dehydrated, it was classified under the primary morbidity category of \"digestive.\" If a calf was listed as being only dull or only dehydrated, it was classified under \"other\" clinical signs. Although each case was assigned a primary morbidity category, an individual calf could have been reported as displaying digestive signs, respiratory signs, concurrent digestive and respiratory signs, and other signs at different times throughout the preweaning period. If multiple cases for a single calf were recorded, digestive events and other events were considered independent cases if a minimum of 3 d had elapsed between each episode. Respiratory events and concurrent digestive and respiratory events were considered independent cases if at least 7 d had elapsed between each event.\n\nStudy morbidity incidence rates for any cases and independent classification of cases were calculated using calf-days at risk. Calf-days at risk were calculated by multiplying the total calves enrolled in the study by the average age at weaning. Monthly calf enrollment was calculated for each month of the study to estimate monthly morbidity incidence rates. A calf was included in every month it was enrolled in the study from birth until weaning or death, regardless of the exact number of days the calf may have spent in the month. Monthly morbidity incidence rates were calculated as a ratio of the number of cases observed in each month divided by the number of calves at risk in that month. Weekly morbidity rates were calculated as a ratio of the number of cases observed in each week divided by the number of calves at risk in that week of age. Write-in data for cause of death were reviewed for all calves and compared with disease data reported at the time of death. If a calf had concurrent respiratory and digestive signs, but the cause of death was only reported as one or the other, the cause of death was reclassified into concurrent respiratory and digestive.\n\nDescriptive data were analyzed using the FRE-QUENCY and MEANS procedures in SAS for categorical and continuous variables, respectively. To evaluate the time in days to morbidity and mortality events for preweaned heifer calves, Kaplan-Meier survival graphs were constructed using the LIFETEST procedure in SAS. The STRATA statement was used to visualize differences based on serum IgG level, serum total protein level, and serum Brix percent. PROC GLM was used to model serum total protein level and serum Brix score by serum IgG level to develop comparable categories. The log-rank test was used to evaluate differences of the survival curves throughout the preweaning period (Hosmer and Lemeshow, 1999) for each serum measurement. Censored calves were calves that were weaned before 60 d of age.\n\nTwo separate logistic regression models were constructed to determine the factors associated with morbidity and mortality. Operation was included in the model as a random variable to account for clustering of calves within operations. Predictor variables were considered for each model based on biological plausibility. To be included in the analysis, calves had to have data for all variables initially included in each model. Univariate screening was used to determine which variables to include in each of the initial models and variables with P < 0.20 were selected for inclusion in the multivariate models. Stepwise backward elimination model selection in PROC GENMOD in SAS was used to construct each final model, with P < 0.05 considered significant and thus included in the model. All first-order interactions of the final model variables were evaluated for significance. Model fit was evaluated using the Akaike information criteria.\n\nFigures to demonstrate the relationship of continuous variables in the morbidity and mortality model were created using PROC GPLOT. The log means of morbidity and mortality were taken and graphed against the model variable. The points were combined into 12 different groups and a line of best fit was constructed based on the 12 groups. These graphs were created for all final continuous variables in the morbidity and mortality models.\n\nOverall, 33.8% of enrolled calves had at least one morbidity case, with 6% of calves experiencing more than one case. Therefore, 859 of the 2,545 enrolled calves were reported ill, resulting in 1,103 reported cases (Table 1 ). The majority of calves reported ill had digestive signs (56.0%), appeared dull (38.4%), or displayed respiratory signs (33.4%). Primary categories of \"digestive,\" \"respiratory,\" \"concurrent digestive and respiratory,\" and \"other\" clinical signs were developed to eliminate nondescript signs of illness, such as dull and dehydrated, and to allow for classification of each individual case. Classification of these clinical categories is described under Statistical Analysis. Once primary clinical categories were developed, digestive signs remained the most prominent, with 17.2% of all calves reported having digestive signs, representing 50.9% of all ill calves and 43.6% of all cases (Table 2) . Respiratory, concurrent digestive and respiratory, and other signs were lower, with 9.5, 1.8, and 8.0% of all calves, respectively, representing 27.0, 4.6, and 24.8% of all cases, respectively. Of the 24.8% \"other\" cases, 75.5% were reported as only dull, dehydrated, or febrile, and 24.5% were reported as having infections, injuries, lameness, or neurological signs.\n\nThe overall morbidity incidence rate for the study was 0.450 cases per 65.7 calf-days at risk. Incidence rates for digestive cases, respiratory cases, concurrent digestive and respiratory cases, and other clinical cases were 0.196, 0.122, 0.021, and 0.111 cases/65.7 calf-days at risk for the study (Supplemental Table S1 ; https:// doi .org/ 10 .3168/ jds .2017 -14019).\n\nThe number of calves enrolled each month was relatively evenly distributed, with the highest enrollment in August (689 calves) and the lowest in February (556 calves). Digestive morbidity incidence rates were highest during December (0.086 cases/calf-month), February (0.076 cases/calf-month), and April (0.081 cases/ calf-month). Respiratory morbidity incidence rates were elevated during the months of January (0.045 cases/calf-month), March (0.062 cases/calf-month), and April (0.044 cases/calf-month). Concurrent digestive and respiratory signs were consistently low throughout the months, with a slight increase in morbidity incidence rate during the month of October (0.012 cases/ calf-month). \"Other\" clinical signs were highest during January, February, and April with morbidity rates of 0.053, 0.051, and 0.051 cases/calf-month, respectively. Ultimately, any morbidity was highest during the spring, with total morbidity incidence rates of 0.163 cases/calf-month in February, 0.174 cases/calf-month in April, and 0.184 cases/calf-month in May (Figure Other included calves reported with infections or injuries. Table S3 ; https:// doi .org/ 10 .3168/ jds .2017 -14019). The weekly incidence rates showed a peak in digestive disease at approximately 2 wk of age (0.0727 cases/calf-week). The incidence rate of digestive disease quickly declined by wk 4 to 0.0090 cases/calf-week and continued to decline throughout the rest of the preweaning period. \"Other\" disease followed a similar trend as digestive disease with a maximum incidence rate of 0.0306 cases/calf-week at 1 wk of age. Respiratory disease cases had a maximum incidence rate of 0.0169 cases/calf-week at wk 5 of age. Concurrent respiratory and digestive disease had low incidence rates throughout the preweaning period, with a maximum incidence rate of 0.0039 during wk 2 of age.\n\nMost ill calves (90.2%) received treatment. The majority (73.8%) of ill calves received antibiotics, 40.5% received fluids (oral or injectable), 27.6% were given a nonsteroidal anti-inflammatory (NSAID), and 8.0% of calves received a decreased liquid diet (Table 3) .\n\nAntibiotics were administered to over one-fourth (26.8%) of all enrolled calves (for treatment and prevention) and 73.8% of all ill calves. The most prominent antibiotic classes administered to calves were fluoroquinolones (29.1%), sulfonamides (23.5%), and penicillins (26.8%). Sulfonamides were the most common antibiotics used for cases with digestive signs (30.2%), whereas fluoroquinolones were administered to 17.6% of digestive cases. The primary antibiotics used for respiratory cases were macrolides (32.3%) and florfenicols (24.1%; Table 4 ). Unlike digestive cases, for which only 68.8% received antibiotics, 88.3% of respiratory cases received antibiotics, and 90.2% of concurrent respiratory and digestive cases were treated with antibiotics. Additionally, 50 calves received antibiotics as a preventative treatment, with the most common antibiotic classes being penicillin (64.0%) and fluoroquinolone (32.0%).\n\nThe overall study mortality was 5.0%; 128 of the 2,545 calves died during the preweaning period (Table 5 ). Recorded causes of death were digestive (32.0% of deaths), respiratory (14.1% of deaths), digestive and respiratory (7.0% of deaths), and other (13.3% of deaths), which included injuries, infections, and calves that were sold for unreported reasons. One-fourth of deceased calves (25.0%) had an unknown cause of death and 8.6% had no reported cause of death. The mean age at death was 24.4 d. The mean age for digestive causes of death was 18.3 d, the mean age for respiratory causes of death was 37.5 d, and the mean age for concurrent respiratory and digestive causes of death was 18.6 d. Unknown causes of death had a mean age of 25.7 d. The case-fatality rates were 8.5% of digestive cases, 6.0% of respiratory cases, 17.7% of concurrent digestive and respiratory cases, and 6.2% of other cases. However, 2.5% of calves that were never reported ill died during the preweaning period (Table 6 ). \n\nMorbidity. Kaplan-Meier survival curves through 60 d of age were created for overall morbidity stratified by serum IgG concentration with corresponding serum total protein levels and serum Brix scores listed ( Figure  3 ). The morbidity survival curve illustrates that the majority of reported clinical signs occurred in the first 20 d of life and then slowly tapered off throughout the rest of the preweaning period. Serum total protein level and serum Brix scores were modeled with serum IgG to develop comparable categories (R 2 = 0.803 and R 2 = 0.797, respectively). The morbidity survival curve stratified by serum IgG level with corresponding serum total protein levels and serum Brix scores had a significant log-rank (P < 0.0001, P < 0.0001) when comparing serum IgG \u226515.0 g/L to <15.0 g/L and \u226510.0 g/L to <10.0 g/L, respectively. These suggest that the survival curve for each serum test differed at the beginning and toward the end of the 60-d period and that there was a dose-dependent response regarding serum IgG, serum total protein, and serum Brix, and morbidity.\n\nMortality. Kaplan-Meier survival curves through 60 d of age were created for overall mortality stratified by serum IgG concentration with corresponding serum total protein levels and serum Brix scores listed (Figure 4) . The mortality survival curve is similar to the morbidity curve, in that most mortality events occurred in the first 20 d of life. The mortality survival curve stratified by serum IgG level with corresponding serum total protein levels and serum Brix scores had a significant log-rank when comparing serum IgG levels of \u226515.0 g/L to <15.0 g/L and \u226510.0 g/L to <10 g/L (P = 0.0019 and P < 0.0001, respectively). Once again, these results illustrate a dose-dependent response with serum IgG level, serum total protein, and serum Brix and mortality, suggesting that calves should have a serum IgG concentration of at least 15.0 g/L, a serum total protein \u22655.4 g/L, or a serum Brix score \u22658.6% to help prevent both morbidity and mortality.\n\nOf the 2,545 calves in the study, 2,374 calves had complete data and were included in the morbidity model for analysis. Initial variables included in the multivariable model after univariate screening for morbidity included sex of the primary caretaker during the preweaning period, primary housing ventilation type, a categorized housing number based on group size, vaccination status during the preweaning period, any preventative antibiotic treatments given during the preweaning period, liquid diet type, direct-fed microbial additives in the liquid diet, birth weight (kg), serum IgG concentration (g/L), amount of protein in the liquid diet per day (kg), and pTHI (Supplemental Tables S4 and S5; https:// doi .org/ 10 .3168/ jds .2017 -14019). The final multivariable model for morbidity in calves included birth weight (P = 0.004), serum IgG (P = 0.001), ventilation type (P = 0.015), and pTHI (P = 0.008) ( Table 7) .\n\nAfter controlling for other independent variables in the model, calves born at higher birth weights had a lower risk of morbidity than calves with lower birth weights [odds ratio (OR) = 0.962]. Selecting a low birth weight of 35.0 kg for this continuous variable resulted in a predicted morbidity risk of 40.0%, whereas a birth weight of 45.0 kg had a predicted morbidity risk of 31.2% (Supplemental Figure S1a ; https:// doi .org/ 10 .3168/ jds .2017 -14019). Similarly, an increase in se- rum IgG concentration decreased the risk of morbidity (OR = 0.977). A serum IgG of 8.0 g/L (consistent with failure of passive transfer) resulted in a predicted morbidity risk of 40.3%, whereas a serum IgG of 30.0 g/L (consistent with excellent passive transfer) had a predictive morbidity risk of 29.3% (Supplemental Figure  S1b ). Ventilation type was correlated with morbidity.\n\nCalves not housed in natural ventilation systems had 2.218 times higher odds of developing disease compared with calves housed in natural ventilation systems. Finally, pTHI was inversely correlated with morbidity (OR = 0.991). A pTHI of 20, below the reported calf thermoneutral zone, had a predicted morbidity risk of 39.5%, whereas a pTHI of 70 had a predictive morbidity risk of 29.1% (Supplemental Figure S1c ).\n\nOf the 2,545 calves in the study, 2,272 calves had complete data and were included in the mortality model for analysis. Initial variables included in the multivariable model after univariate screening for mortality included vaccination status during the preweaning period, morbidity during the preweaning period, amount of fat in the liquid diet per day, direct-fed microbial additives in the liquid diet, any additives in the liquid diet, antibiotic additives in the liquid diet, pasteurization of milk, birth weight, serum IgG concentration, amount of protein in the liquid diet per day, and pTHI (Supplemental Tables S6 and S7; https:// doi .org/ 10 .3168/ jds .2017 -14019).\n\nThe final multivariable model for mortality in calves included birth weight (P = 0.011), serum IgG (P = 0.004), amount (kg) of fat/d in the liquid diet (P = 0.011), and morbidity during the preweaning period (P < 0.001) ( Table 8) .\n\nAfter controlling for other independent variables in the model, calves born at a higher birth weight had a lower risk of mortality than calves with a lower birth weight (OR = 0.946). Selecting 2 arbitrary birth weights resulted in a relative mortality risk of 4.7% at a birth weight of 35.0 kg and a predicted mortality risk of 2.3% at a birth weight of 45.0 kg (Supplemental Figure S2a ; https:// doi .org/ 10 .3168/ jds .2017 -14019). Similarly, an increase in serum IgG decreased the risk of mortality (0.963). A serum IgG of 8.0 g/L (consistent with failure of passive transfer) resulted in a predicted mortality risk of 5.2%, whereas a serum IgG of 30.0 g/L (consistent with excellent passive transfer) had a predicted mortality risk of 2.0%. Fat per day in the liquid diet was also correlated with mortality (Supplemental Figure S2b ). The odds of mortality was 3.010 times higher for calves fed \u22640.15 kg of fat/d compared with calves fed \u22650.22 kg of fat/d. Mortality did not differ Calves that experienced any disease throughout the preweaning period had a 4.567 times higher odds of mortality than calves that did not experience any disease.\n\nThe reported morbidity rate of this study was 33.8% compared with a morbidity rate of 36.1% reported in the 1992 National Dairy Heifer Evaluation project (USDA, 1994) . The Dairy Calf and Heifer Association report that the target morbidity rate for calves 24 to 60 d of age is less than 25%. Thus, there is still room to improve overall morbidity in preweaned dairy heifer calves (Dairy Calf and Heifer Association, 2010). Similar to the 1992 and 2007 USDA studies, digestive and respiratory signs were the most commonly identified problems, with digestive signs peaking at 2 wk of age and respiratory signs being more prevalent later in the preweaning period (USDA, 1994 (USDA, , 2010 .\n\nCases of disease, including digestive and respiratory signs, were highest during the winter and spring months. This is likely due to changing temperatures and possible inclement weather, which results in a calf spending more time outside its thermoneutral zone. Wiseman et al. (1976) showed that the incidence of pneumonia increased during inclement weather events. Additionally, harsh weather events that include rain, snow, and high levels of moisture have been shown to increase the susceptibility of calves to diarrhea (Perez et al., 1990; Larson and Tyler, 2005; Carroll and Forsberg, 2007) . Calculated incidence rates demonstrated that most calves developed digestive signs at approximately 2 wk of age. Diarrhea at 2 wk of age is often seen in at-risk calves, given that the infectious causes of diarrhea, such as bovine rotavirus and coronavirus, have a short incubation period and tend to cause diarrhea in calves to peak at 1 to 2 wk of age (Steele et al., 2004; Dhama et al., 2009) . Additionally, calves developed respiratory signs that peaked at approximately 5 wk of age. Respiratory disease seen at this age is common in at-risk calves, because viral and bacterial agents, such as respiratory syncytial virus and parainfluenza type 3, associated with pneumonia in calves are normally found in calves over 4 wk of age (Lorenz et al., 2011) .\n\nAlmost all (90.6%) ill calves reported in this study received treatment. The most common treatments were antibiotics and oral electrolytes. The use of antibiotics for clinical signs has stayed relatively constant since 1992, with 24.5 to 29.8% of all calves receiving antibiotics compared with 26.6% of all calves in the current study (USDA, 1994) . The most commonly used antibiotic class to treat calves was fluoroquinolone, accounting for 29.1% of all calves administered an antibiotic, 17.6% of digestive cases, 11.9% of respiratory cases, 17.6% of concurrent digestive and respiratory cases, and 31% of preventative antibiotic administration. However, the use of fluoroquinolones is only approved for the treatment of respiratory disease in food-producing animals in the United States (Constable, 2004) .\n\nDecreasing milk or milk replacer during a diarrheic event is a practice that is no longer recommended Other included infection, injury, or calves that were sold without a recorded reason. (Garthwaite et al., 1994) . Decreasing the amount of liquid diet fed as treatment during a disease event has diminished over the years; currently 2.7% of all calves received this treatment method compared with 15.9% of all calves in 1992 (USDA, 1994 (USDA, , 2010 . It is recommended that calves be fed through the course of the diarrheic event to help maintain caloric intake and to add fluid volume and electrolytes to supplemental fluid administration (McGuirk, 2008) . The mortality rate of 5.0% was lower than mortality rates reported in previous US studies that ranged from 8 to 11% (USDA, 1994 (USDA, , 2002 (USDA, , 2010 . The target mortality rate for calves 24 to 60 d of age is less than 5% (Dairy Calf and Heifer Association, 2010). The reported cause of death in this study was consistent with previous studies, highlighting digestive and respiratory disease as the most common contributors to calf death (USDA, 1994 (USDA, , 2010 Sivula et al., 1996; Tyler et al., 1999; Svensson et al., 2003) . Consistent with previously reported data, the mean age of death due to digestive signs was 18.3 d, and the mean age of death due to respiratory signs was 37.5 d of age (USDA, 1994 (USDA, , 2010 . Diarrhea is the most common cause of death in calves less than 30 d old, and pneumonia is the most important disease in calves greater than 30 d of age (USDA, 1994 (USDA, , 2010 Agerholm et al., 1993; Virtala et al., 1996; Svensson et al., 2003) . However, approximately one-fourth of calves died from unknown causes. This highlights the importance of working with calf caretakers to identify and classify the cause of death for individual calves through necropsy and diagnostics. Case-fatality rates for digestive (8.5%) and respiratory (6.0%) cases decreased compared with those in the Sivula et al. (1996) study, in which the case-fatality rate for enteritis was 17.9% and that for . Mortality survival percentage for preweaned heifer calves by days of age and serum IgG concentration categories. Corresponding to serum IgG levels of \u226515.0 mg/dL, 10 to 14.9 mg/dL, and <10.0 mg/dL were serum total protein categories of \u22655.4 g/dL, 5.1-5.3 g/dL, and <5.1 g/dL, and Brix score categories of \u22658.6%, 8.1-8.5%, and <8.1%, respectively. Color version available online. Temperature-humidity index (THI) accounts for the effects of temperature and relative humidity, and the equation uses the dry bulb temperature (T, \u00b0F) and the relative humidity (RH). The equation used for this analysis was THI = T -[0.55 -(0.55 \u00d7 RH/100)] \u00d7 (T -58).\n\npneumonia was 9.4%. A decrease in case-fatality rates is most likely due to routine management changes, the development of better treatment protocols, and the implementation of vaccinations for specific digestive and respiratory diseases (McGuirk, 2008) . Multivariable evaluation of management and health factors showed that low birth weight and serum IgG concentration were associated with disease and death. Additionally, housing ventilation and average THI during the preweaning period were associated with increased morbidity. Moreover, disease and the amount (kg) of fat per day in liquid were associated with increased mortality.\n\nCalves born with higher birth weights were less likely to have clinical signs and less likely to die during the preweaning period than calves with lower birth weights. Previous studies reported that calves with a birth weight that deviated from the mean (i.e., both heavy and light calves) were more likely to have digestive disease (Par\u00e9 et al., 1993; Fallon et al., 1987) . In the current study, calves were required to live beyond 24 h to be enrolled in the study; therefore, it is possible that calves with severe dystocia due to heavy birth weights that could be at a higher risk for morbidity and mortality were excluded from the study. Nonetheless, for low-birthweight calves, the majority of fetal growth is determined by the maternal uterine environment (Penrose, 1952) . Thus, low birth weight could be due to a restricted supply of nutrients available to the developing fetus, which may occur due to maternal age and body condition at the time of conception and maternal nutrient supply during the gestation period (McCrabb et al., 1991; Bauer et al., 1998; McMillen et al., 2001; Osgerby et al., 2003a,b) . Wolfenson et al. (1988) demonstrated that heat stress in dams resulted in calves with lower birth weight. Additionally, the size of the placenta may inhibit fetal growth, especially in late gestation, when maximal fetal growth occurs (McMillen et al., 2001; Mellor and Stafford, 2004) . Low birth weight due to intrauterine restrictions of nutrients and space may be a potentiating factor to increase morbidity and mortality in smaller calves. A post-uterine factor associated with low-birth-weight calves that may increase the likelihood of morbidity and mortality is a greater body surface-area-to-mass ratio. This greater ratio allows increased heat dissipation and increases energy requirements directly after birth, resulting in a compromised ability to acquire passive transfer (Par\u00e9 et al., 1993; Mellor and Stafford, 2004; Monteiro et al., 2014) . However, management interventions, such as drying off all calves at birth, providing dry housing, and immediately providing high-quality colostrum, may mitigate most environmental risk factors associated with morbidity and mortality in low-birth-weight calves. Nonetheless, ensuring that cows are healthy throughout the gestation period and providing high-quality nutrients will promote healthy newborns with decreased morbidity and mortality and increased lifelong production measures, such as fertility and milk production (Linden et al., 2009) .\n\nAlthough birth weight is a difficult variable to control on the farm, serum IgG, which was directly associated with morbidity and mortality, can be controlled through the implementation of a quality colostrum feeding program (DeNise et al., 1989; Donovan et al., 1998; Furman-Fratczak et al., 2011) . Because calves are born agammaglobulinemic, they rely on consumption and absorption of maternal immunoglobulins via colostrum to fight infections, especially during the first few weeks of life (Gulliksen et al., 2008) . Absorption of colostral immunoglobulins can be measured via serum IgG concentration. In this study, as serum IgG concentration increased, the likelihood of disease and death decreased. Several other studies support this result, demonstrating that calves with a serum IgG concentration \u226510g/L had decreased morbidity and mortality during the preweaning period (Arthington et al., 2000; Gulliksen et al., 2008; Godden et al., 2009) . Calves with a serum IgG concentration <10 g/L are considered to have failure of passive transfer and are at a higher risk of disease and death. However, survival analyses of morbidity and mortality by serum IgG concentration in this study demonstrated that serum IgG concentration has a dose-dependent effect. Therefore, increasing the minimum serum IgG concentration target to 15 g/L may reduce morbidity and mortality, compared with the current 10 g/L industry standard. Additionally, serum total protein level and serum Brix score, which act as on-farm proxies for serum IgG concentration, also demonstrated a dose-dependent effect in the survival analyses for morbidity and mortality. Thus, if the industry standard for serum IgG concentration is increased, it is recommended to increase the target of serum total protein to 5.4 g/dL and serum Brix score to 8.6%. Housing ventilation was another significant predictor of morbidity. Calves housed in ventilation systems other than natural ventilation were more likely to develop disease. Previous research associated enzootic pneumonia with poorly ventilated housing conditions, because poor ventilation can increase pathogen density (Nardell et al., 1991; Callan and Garry, 2002) . Additionally, Lago et al. (2006) demonstrated that factors related to ventilation, including air changes per hour, were associated with the prevalence of calf respiratory disease, and decreased ventilation was associated with increased alleyway bacterial counts. Ultimately, increasing natural ventilation rates and providing a hygienic environment with decreased pathogen loads, regardless of the provided ventilation type, will help reduce morbidity, especially in regard to respiratory disease.\n\nAn increased average THI during the preweaning period was associated with decreased morbidity. The thermoneutral zone for a calf less than 1 mo of age is 10.0\u00b0 to 26.6\u00b0C (Scibilia et al., 1987) . Averaging THI throughout the calf's preweaning period allowed us to better account for seasonal variability across the United States. Because the THI was an average per month throughout each calf's preweaning period, extremely high or low temperatures outside the thermoneutral zone (and likely resulting in heat or cold stress) were not observed. However, past research demonstrated that both cold stress and heat stress affect the immune and stress responses of the body, specifically elevating pro-inflammatory cytokines, tumor necrosis factor-\u03b1, ACTH, and cortisol (Frank et al., 2003; Hangalapura et al., 2006; Dahl et al., 2016) . Additionally, Godden et al. (2005) demonstrated that calves were more likely to receive treatment for clinical signs in the winter and summer months. Nonnecke et al. (2009) reported that respiratory disease and antibiotic costs were higher for calves in cold environments. Therefore, minimizing cold stress and, to a degree, heat stress is extremely important in raising healthy calves throughout the preweaning period.\n\nIn this study, an increase in the amount (kg) of fat per day in the liquid diet resulted in a decrease in calf mortality. Research has demonstrated the importance of adipose tissue in mammalian survival-it provides energy, a source of heat and water, and thermal insulation (Young, 1976) . Furthermore, early research regarding milk replacers in dairy calves demonstrated that higher fat replacers produced a lower incidence of scours and improved rates of gain (Lister, 1971; Leaver and Yarrow, 1972; Appleman and Owen, 1975) . Thus, it would be expected that calves with limited fat intake would have lower energy levels available to fight infections, such as scours, and be at greater risk for cold and heat stress, and more likely to die during the preweaning period.\n\nLast, morbidity status was highly correlated with mortality. Keeping calves healthy throughout the preweaning period is essential to decreasing overall mortality rates. However, it is important to recall that morbidity is affected by many other factors, such as birth weight, passive transfer, housing ventilation, and THI. Therefore, management techniques to decrease morbidity rates will ultimately help decrease mortality rates.\n\nAlthough calving ease did not remain in the final morbidity and mortality models, dystocia is known to have a negative influence on calf health. Because calves were required to live beyond 24 h to be enrolled in the study, it is probable that calves born with any dystocia were less likely to be enrolled due to decreased thrift and risk of dying before 24 h of age. Additionally, most calves were tested for Cryptosporidium and Giardia; however, there was no association between the enteric protozoa and morbidity. For more information regarding Cryptosporidium and Giardia in preweaned dairy heifers, please see Urie et al. (2018b) .\n\nVarious management factors can help minimize morbidity and mortality on US dairy operations. The morbidity rate of 33.9% and mortality rate of 5.0% in this study suggest that US dairy operations have improved in terms of overall calf care. However, areas for improvement remain, including properly treating calves with antibiotics, feeding calves through scours events, and classifying causes of death. To continue reducing morbidity and mortality, the minimum serum IgG concentration target should be increased to 15 g/L compared with the current industry standard of 10 g/L. Additionally, more research should be conducted on factors that were shown to affect morbidity and mortality in this study, especially birth weight, housing ventilation, THI, and amount of fat fed per day in the liquid diet."}