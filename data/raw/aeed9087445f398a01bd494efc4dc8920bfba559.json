{"title": "Forecasting smog-related health hazard based on social media and physical sensor", "body": "Smog disasters are becoming more and more frequent and may cause severe consequences on the environment and public health, especially in urban areas. Social media as a real-time urban data source has become an increasingly effective channel to observe people's reactions on smog-related health hazard. It can be used to capture possible smogrelated public health disasters in its early stage. We then propose a predictive analytic approach that utilizes both social media and physical sensor data to forecast the next day smog-related health hazard. First, we model smog-related health hazards and smog severity through mining raw microblogging text and network information diffusion data. Second, we developed an artificial neural network (ANN)-based model to forecast smogrelated health hazard with the current health hazard and smog severity observations. We evaluate the performance of the approach with other alternative machine learning methods. To the best of our knowledge, we are the first to integrate social media and physical sensor data for smog-related health hazard forecasting. The empirical findings can help researchers to better understand the non-linear relationships between the current smog observations and the next day health hazard. In addition, this forecasting approach can provide decision support for smog-related health hazard management through functions like early warning.\n\n& 2016 Elsevier Ltd. All rights reserved.\n\nSmog disasters are becoming more and more frequent and may cause severe consequences on the environment and public health in China. For example, in January 2013, smog had covered the capital of China, Beijing, for over 20 days. According to recent statistics [1] , smog affects more than a quarter of the land and over 600 million people in China.\n\nAccording to Virginia Hughes [2] , smog is a health hazard that may adversely affect people's health. Sometimes it causes extreme and immediate public health emergency, like the one in 1952 London [3] . Therefore, it is necessary to develop a systematic approach to analyze, monitor and forecast smogrelated health hazards in a timely manner.\n\nOn the other hand, social media as a real-time urban data source has become an increasingly important channel to observe events, trends and sentiment [4, 5] . Negative comments on smog or complaints about smog-related health conditions from a small group of environment sensitive individuals can diffuse really fast on social media and cause much large scale of discussions and reactions. Therefore, social media with its network effects can be used to capture possible smog-related public health disasters in its early stage and provide warnings.\n\nIn the big data era, various technologies are developed to extract, process and analyze population-level social media Contents lists available at ScienceDirect journal homepage: www.elsevier.com/locate/infosys data, but few with the purpose of forecasting. Previous research [6] usually collected and analyzed such social media data for monitoring the impacts of nature environment on public health, but there is a lack of systematic approaches for forecasting smog-related health hazards with social media data.\n\nMoreover, a variety of physical sensor platforms for monitoring smog status, including air quality stations, weather stations and earth observation satellites, are also widely deployed across China for both big cities and small towns [7] , generating a huge amount of observational data about smog severity.\n\nAs Fig. 1 shows, we propose a predictive analytics approach that utilizes both social media and physical sensors for smog-related health hazard forecasting. It contains two major components: (1) modeling smog-related health hazards and smog severity with raw microblogging text and network information diffusion records and (2) forecasting the next day smog-related health hazards using an artificial neural network-based model.\n\nTo the best of our knowledge, our research is the first study to systematically model and analyze real-world social media and physical sensor data for smog-related health hazard forecasting. Firstly, this study can help researchers to better understand the non-linear relationships between current smog observations and the next day health hazard, in which physical sensors alone often fail to capture. Secondly, the proposed predictive analytics framework aims to provide decision support for smog-related health hazard management through functions like early warning for the coming smogrelated public health emergency.\n\nMoreover, we investigate the strengths of social media in smog-related health hazard forecasting. It can contribute more than physical sensors in forecasting the smog-related health hazards when the smog disasters are severe.\n\nMeanwhile, data about social observations' diffusion in social networks can further improve the forecasting accuracy.\n\nOn one hand, predictive analytics that are related to smog disasters or other kinds of air pollutions usually investigates the natural observations themselves without considering their related health hazard. Here are some examples. Merz et al. [8] conducted a time-series analysis of air monitoring data for the downtown Los Angeles station to detect the air pollution trends. Casado et al. [9] applied a series of geostatistics and visualization procedures to analyze hourly ozone measurements collected from 29 stations in the southeastern United States, which clearly confirmed the diurnal pattern of ozone fluctuations. Van et al. [10] investigated smog prediction problem in perspective of computational steering techniques which allow an optimal trade off between computation speed and prediction accuracy.\n\nOn the other hand, most studies that involve smog-related public health problems usually analyzed the impacts of smog on public health, but largely ignored real-time health hazard monitoring and forecasting. They mainly used objective indicators from physical sensors or statistics from hospitals. Pope and Dockery [11] conducted an extensive review on the research about health effects of particulate matter (PM)the most harmful component in smog. They focused on the shortterm and long-term PM exposure and its effects on mortality and some diseases. Recently, Hughes et al. [2] compared annual case numbers of chronic obstructive pulmonary disease (COPD) with smog trends in some cities to investigate the health effects of smog in past years. Meanwhile, current studies investigating smog-related public health often adopt small data sets with limited population and time coverage. Motley et al. [12] measured only 66 volunteers' health status to acquire public health information during a smog disaster in Los Angeles. Chen et al. [13] used hospital visits to analyze health hazard trends under smog disasters, but the records only covered one hospital in Beijing during one high-smog period. Schwartz [14] investigated mortality caused by smog disasters, but the research was based on data records from some typical big smog disasters in some big cities.\n\nRecently, social media including microblogging and social network services provided us real-time and large scale data sets related to public health. Lee et al. [6] and Culotta et al. [15] studied public health issues concerning with flu and cancer by analyzing Twitter messages. Paul et al. [16, 17] mined topics of various ailments, symptoms and treatments from tweets with the Ailment Topic Aspect Model. Greene et al. [18] investigated several disease-specific information that was shared and exchanged on Facebook. Gardy et al. [19] acquired epidemiologic and genomic data through a social network for the research of tuberculosis.\n\nSocial media has also been applied to track the impacts of natural disasters as it will provide detailed information for situation awareness. Sakaki et al. [20, 21] adopted Twitter data to detect and monitor disaster events including earthquake and typhoon in Japan with high probability and timeliness. Kongthon et al. [22] obtained up-to-date information about the disaster damage and the needs of the populace in 2011 Thai flood. Yin et al. [4] built an information system that utilized Twitter messages to enhance situation awareness during various crises and events, including natural disasters.\n\nAlthough social media has been widely applied in investigating natural disasters and public health problems, there are very few studies that use social media to investigate smog disasters, not to mention smog-related health hazard. Mei et al. [23] was one of the earliest studies for smog disaster analysis with social media, but it aimed to infer the smog severity in the cities where no air quality stations were deployed, which was not related to smog-related health hazard. Our previous work [24] utilized Chinese tweets on Weibo to analyze the correlation between smog disasters and public health statuses, but did not study smog-related health hazard forecasting. Its prediction model simply approximated the health hazard with physical observations, which aimed at quantitatively analyzing the relationship and generating a standard for rating smog disasters' health hazard. The work presented in this paper actually extends our previous work [24] from historical relationship analysis to real-time forecasting. Our another work [25] in progress aims at forecasting smog disaster directly with different kinds of data including social media data. It will reinforce our study for decision making under smog disasters, but is quite different from the health hazard forecasting topic presented in this paper.\n\nArtificial neural networks (ANNs) are computational models inspired by an animal's central nervous systems, and have been widely used in predictive analytics research. They are capable of learning complex non-linear discriminant functions [26] , and can help solve public health problems like predicting active pulmonary tuberculosis [27] and Severe Acute Respiratory Syndromes (SARS) epidemic [28] . In our ongoing study [25] , which applies different social observations and physical sensor observations to smog disaster forecasting, ANNs with single hidden layer or two hidden layers achieve a little higher performance than random forest and support vector machine. In recent years, ANNs are further extended with deep architectures. They have been proven to work very well for many complex prediction problems with big data in the fields like computer version, nature language processing and so on [29] . These theoretical properties and real world applications indicate that ANNs are able to approximate the relationship between the smog disaster and its health hazard.\n\nThere have been some state-of-the-art algorithms, such as back propagation (BP) [30] , to train multiple layers ANNs for various regression problems. The recently proposed learning algorithm named extreme learning machine (ELM) [31] can train a single hidden layer feed-forward ANN at a high speed with high generalization performance. It can universally approximate any continuous target function and effectively solve many real-world regression problems, such as sales forecasting in fashion retailing [32] . According to some experiments [31] , the single hidden layer ANN trained by ELM can achieve higher testing accuracy than some typical methods like support vector machine on many regression and classification benchmarks.\n\nHowever, there is a lack of research which utilized prediction methods, such as ANNs, to fuse both social media data and physical sensor data for the forecasting of smog-related health hazard. This is mainly due to the lack of (1) systematic approaches for collecting, modeling and analyzing such information and (2) efficient prediction framework which can combine features from both social media and physical sensors.\n\nThis research work addresses two challenges:\n\n(1) Smog-related health hazard and smog severity modeling with social media. (2) Smog-related health hazard forecasting using social media and physical sensor data.\n\nWe propose a predictive analytics framework, as shown in Fig. 2 . It has three main parts. First, smog-related health hazard and smog severity are measured using raw social observations and social network diffusion data. Second, a health hazard prediction model is built using records of public health index, smog severity index, social network diffusion factor and physical observation, and is further utilized to forecast smog-related health hazards. Third, the forecasted and measured smog-related health hazards are applied to support decision making in smog-related health hazard management including real-time monitoring and emergency warning.\n\nIn our study, smog-related health hazards and smog severity are modeled as two indexes using social media information.\n\nDefinition 1. Public Health Index (PHI) is the sum of total relative frequencies of smog-related health hazard phrases in the current tweets. D-PHI is an enhanced public health index that includes consideration of diffusion in social networks.\n\nSmog Severity Index (SSI) is the weighted sum of total relative frequencies of smog severity phrases in the current tweets. D-SSI is an enhanced smog severity index that includes consideration of diffusion in social networks.\n\nCalculation of the two indexes includes five steps. First, both smog-related health hazard phrases and smog severity phrases are extracted. Smog-related health hazard phrases are those that are commonly used in Weibo (a Chinese social media site that is similar to Twitter) to complain about health problems that may be caused by smog disasters. According to some smog-related medical studies, smog disasters usually cause nose, eyes and throat irritation as well as heart and respiratory diseases in the short term [2, 13] . We collect 200 Chinese phrases that are commonly used to complain about these health problems. Table 1 (a) presents their English counterpartssome may represent multiple Chinese phrases with the same meaning.\n\nSmog severity phrases are those that are commonly used in tweets to describe the current condition of a smog disaster. Based on the study of smog-related tweets, we collect 160 common Chinese phrases and define a severity order for each of them, as shown in Table 1 \n\nSecond, raw tweets with tags of time and location are gathered from Weibo. We partition a city into small grids, and then collect the current tweets of each grid area continuously. In detail, the program calls some APIs that enable us to acquire raw tweets (not forwarded) posted in a specified circular area defined by one position and one radius. The program ensures that the grid area is totally covered by the circular area.\n\nThird, daily relative frequency rf of each phrase is calculated:\n\nwhere T H and T C represent historical and current tweet sets respectively, p represents a phrase, d represents a tweet, f \u00f0p; d\u00de represents the frequency of phrase p in tweet d, af \u00f0p; T C \u00de represents the average frequency of phrase p in the current tweet set T C , idf \u00f0p; T H \u00de represents the inversed document frequency of the tweets with phrase p in the historical tweet set T H . The logarithm function is to scale up the fraction of rare tweets. The above algorithm is derived from the typical tf-idf algorithm [33] . The difference lies in the replacement of the largest word frequency in current tweet set with the size of current tweet set, which aims at eliminating the influence of other heat phrases on Weibo. Fourth, PHI and SSI are calculated with the relative frequencies of all the phrases:\n\nwhere P 1 stands for the set of smog-related health hazard phrases shown in Table 1 (a), P 2 stands for the set of smog severity phrases shown in Table 1 (b) and order(p) stands for a phrase's severity order. The calculation of SSI is weighted, because most Chinese tweets about smog itself come from experts such as the local environment agency or people who have a good knowledge of smog disaster. Usually, they use a fixed word set to describe the severity, and subjective severity level that the words' reflect is unified. In contrast, the health status tweets are mostly posted by common people. The severity level of one description word may vary from people to people. Using severity words for weighting may not reflect the peoples' subjective idea about severity, which is what we really want in this study. Fifth, social network diffusion is considered to calculate D-PHI and D-SSI. On Weibo, any of people's actions, including retweet and like, indicates an agreement to the original tweet. Therefore, one record of such action is regarded as a duplicated raw tweet, based on which we calculate the network diffusion-based average frequency:\n\nwhere g(d) represents a tweet's total number of retweet and like. Once daf is calculated, we use it to replace the average frequency af in Formula 1 to calculate the relative frequency rf, and further compute the value of D-PHI and D-SSI according to Formula (2).\n\nAs Fig. 3 shows, we develop an ANN-based prediction model to forecast the next day smog-related health hazard (PHI record) with the inputs including the current and the past air quality observations, meteorology observations and social observations.\n\nThe inputs of the prediction model contain four kinds of features. The first kind of features (F a ) is extracted from air quality observations, including both air pollution concentrations (CO, NO 2 , SO 2 , O 3 , PM 2:5 and PM 10 ) and air quality index (AQI) which comprehensively evaluates the air quality. The second kind of features (F m ) is extracted from records of various meteorological elements, including humidity, cloud value, pressure, temperature and wind speed, all of which have been proven to affect smog disasters greatly. For example, high wind speed and low cloud value usually make smog pollution to decrease in the next day. The third kind of features (F s ) comes from records of smog severity index (SSI) and social network diffusion incorporated smog severity index (D-SSI), both of them represent people's opinions and observations on the current and future smog disasters. The fourth kind of features (F h ) uses the current and recent PHI and D-PHI records, which enables the model to take the autocorrelation factor of the time-series data into consideration.\n\nWith all these features, we need to conduct feature selection. On one hand, we should filter out some unimportant kinds of observations such as some specific air pollutants and meteorology elements, as they may be not very predictive for the next day PHI or may be quite correlated with some other inputs. For example, we find that O 3 does not improve the prediction much when it is inputted with the other air pollutions. On the other hand, we should find out important records in time line for each observation. For example, we find that only the current records instead of all the recent records are important for wind speed and wind direction. In contrary, for D-PHI and PHI, both the current records and the records in the past 6 h are useful. In selecting features, a view independent subset searching method is adopted. It does not search all the subsets of the whole feature set \u00f0F a \u00fe F m \u00fe F s \u00fe F h \u00de, but finds out proper features from each kind of features individually, which reduce the complexity from 2 jFaj \u00fe jFmj \u00fe jFsj \u00fe jF h j to 2 jFaj \u00fe 2 jFmj \u00fe 2 jFsj \u00fe 2 jF h j . It is reasonable because in our application one kind of features represents one independent view to observe the forecasting target, which means two features from different views will not be highly correlated. Meanwhile, we further decrease the search spacing through testing records of each observation in time line from the current to the past. If we find the record at time t \u00c0 i is not important, the records before that time point will also be regarded as unimportant.\n\nThe ANN-based prediction model is built with a method that searches for the optimum feed-forward ANN structure. It contains three components: parameters adjusting, training and testing. We use both ELM algorithm [31] and BP algorithm [30] for training, and adopt a typical cross-validation strategy which partitions the whole sample set into m complementary subsets in testing. We present the model building procedure with the case of ELM which only uses one hidden node layer. First, initial activation function G, hidden node number L and regular parameter c are set. Second, input weights a i and bias b i of each hidden node are randomly generated. Namely, the input x A R d are mapped into a random feature space in each hidden node:\n\nThird, output weights \u03b2 of all the hidden nodes are calculated through regular linear solution. Namely, the algorithm minimizes the following objection function with small residual error and output weight norm.\n\nwhere J U J denotes the Frobenius norm, H is hidden layer output matrix: Have a little smog, a little bad air quality, air is slightly polluted, a little dusty sky 2\n\nHave smog, bad air quality, air is polluted, dusty sky, high AQI, high PM 2:5 3\n\nHave a severe smog, smog outbreak, air is severely polluted, extremely bad air quality, very dusty sky, extremely high AQI, extremely high PM 2:5 \u00c0 1\n\nThere is no smog, air quality is good, sky is clear, the smog has gone \u00c0 2\n\nAir quality is very good, sky is very clear and T is training data target matrix:\n\nwhere N is the number of training samples and m is the output dimension. Fourth, the trained model is tested. Fifth, another setting of parameters G, L and c is adopted and goes to the second step, or the program stops if all the parameter settings have been traversed. The ANN that achieves the highest testing accuracy is adopted. The optimum hidden node number L is found by incrementally searching with a stopping condition when the testing accuracy begins to decrease. The case for BP algorithm is quite similar with an additional parameternumber of hidden node layers, but without regular parameter c.\n\nIn evaluation, we analyze the advantages of incorporating both social media features and physical sensor features, display the improvement brought by utilizing network information diffusion and compare the forecasting accuracies of ANNs and other regression methods. We also present some forecasting results for Beijing and Shanghai when they are attacked by big smog disasters.\n\nWe use physical sensor data and social media data in 8 cities (Beijing, Shanghai, Shijiazhuang, Tianjin, Nanjing, Hangzhou, Guangzhou and Wuhan) for experiments. Both data cover more than one year from May 2013 to November 2014. The former contains about 592 million hourly records about air quality and weather, while the latter contains about 315 million tweets with their retweet and like records. Meanwhile, the tweet number exceeds 10,000 for most days in both Beijing and Shanghai.\n\nIn evaluation, the records observed in the current and previous days are used as inputs. The next daily day PHI record which quantifies the smog-related health hazard is forecasted and compared against the observed records. We use PHI instead of D-PHI because information diffusion on the social network will non-uniformly magnify the observation thus making the index less objective and harder to forecast. For social media features, PHI/D-PHI and SSI/D-SSI records are calculated daily and the records of the current and past days are used, while for physical sensor features, the records are observed hourly and the records before 24 o'clock are used. Days without enough tweets, which are caused by Weibo API limitations and network failures in data collecting, are discarded to ensure that PHI/D-PHI and SSI/D-SSI records are calculated based on a high population. Finally, 3240 samples are generated according to the data processing described in the paper. They are partitioned into a training set and a testing set with a cutting time.\n\nThrough the model building method described in Section 3.3.2, we get optimized ANN structures for different evaluations with different features. For ELM, which is a training algorithm for single hidden layer ANN, the optimized hidden node number ranges from 35 to 45, while for BP, the optimized ANN structure contains 2 hidden layers with 12-15 nodes in each layer. We evaluate the effect of using more hidden node layers, but find that it does not improve the generalization performance as our sample set is not very large. Meanwhile, two classic SVM regression methods, nu-SVR and epsilon-SVR provided by LIBSVM [34] , as well as random forest regression method provided by sklearn, are also applied for comparison. Each experiment is conducted with multiple pairs of training and testing sample sets (partitioned with different cutting times) and each test is repeated multiple times. The average of the results is finally adopted for evaluation.\n\nWe conduct some data analysis to evaluate the correlations between our features and the forecasting targetthe next day PHI. It includes two parts: visual comparisons and statistical tests. Fig. 4 displays the records of PHI and some of the considered features during two big smog disasters in Shanghai and Beijing. In the figure, the trends of SSI, D-SSI and D-PHI are relatively consistent with that of PHI, and the latest PM 2:5 record is usually consistent with the next day PHI records. The latest AQI record may either be consistent with the current day PHI or the next day PHI. Meanwhile, we calculate the correlation coefficients between the next day PHI and the current day SSI, D-SSI, D-PHI, PM 2:5 and AQI with the data in all 8 cities (3240 samples). The coefficients are 0.434, 0.452, 0.481, 0.387 and 0.399, and their corresponding p values are all less than 0.042 in two-sided confidence testing. The above analysis indicates that SSI, D-SSI, D-PHI, AQI and PM 2:5 are quite correlative with PHI, and are indicators to forecast the next day PHI.\n\nThe results in our previous study [35] can further confirm the above conclusion about the correlation. According to [35] , both meteorology elements like wind speed and air pollutants like O 3 highly correlate with PM 2:5 and are important short-term factors of smog disasters. They can indirectly influence the next day smog-related health hazard. Briefly, the next day PHI is correlated with our social media and physical sensor features, and it is reasonable to utilize them for smog-related health hazard forecasting.\n\nFirst, we compare the testing accuracies of the health hazard prediction model using different kinds of features. As shown in Table 2 , the model using both physical sensor and social media features (P \u00feS) has much lower rootmean-square error (RMSE) than that using either physical sensor features (P) or social media features (S). On average, the RMSE of P \u00feS is about 20% lower than P and about 25% lower than S when single hidden layer ANNs and ELM are adopted for training. The comparison result is similar when multiple hidden layers ANNs and BP are adopted.\n\nSecond, we investigate the advantages of physical sensor features and social media features, which helps explain why their integration can predict more accurately. All the tested samples are classified into four categories according to smog severity which is evaluated by AQI here. Average RMSEs and relative errors are recalculated for each category, as shown in Figs. 5 and 6.\n\nFrom the figures, we can find out that for the days that are not seriously polluted \u00f0AQI o 200\u00de, physical sensor features can achieve lower RMSE and relative error than social media features, while for the days that are severely polluted \u00f0AQI Z 300\u00de, social media features perform better. This result is consistent with our common sense, as people post much more tweets in those severely polluted days, which provide more positive samples. Actually, according to the statistics, Table 2 Testing accuracies (RMSEs) of the health hazard prediction model using different training methods and features. ELM-ANN and BP-ANN represent one hidden layer ANNs with ELM and multiple hidden layers ANNs with BP. P and S represent physical sensor features and social media features, while the prefix D-means considering network diffusion simultaneously. people posted 14.5% more tweets about smog disasters or smog-related health hazard in days when AQI exceeds 200 than in days when AQI is less than 200. Additionally, we can also find that social network diffusion can further improve the accuracy of social media features for polluted days. It will be discussed in details in the next subsection.\n\nIn our predictive analytics framework, social network diffusion is considered to further improve prediction accuracy. As shown in Table 2 , the RMSEs with the diffusion factor considered (P\u00fe DS) are smaller than those without the diffusion factor considered (P\u00feS) for most cities. Actually, the decrement of average RMSE brought by the diffusion factor ranges from 7.6% to 10.5% when different training approaches are adopted.\n\nFigs. 5 and 6 help explain why social network diffusion can improve prediction accuracy. First, the RMSEs of DS are less than those of S in the AQI ranges of 0-50, 200-300 and Z300, and the relative errors of DS are less than those of S in all the four AQI ranges. Second, in both figures, when compared with P, DS outperforms P in two AQI ranges (200-300 and Z300), while S outperforms P in only one AQI range \u00f0 Z300\u00de. This may be because considering retweets and likes enlarges the signal of severe pollution and health hazard, thus reducing the noise in learning the nonlinear relationships. Especially, the number of retweets and likes to smog-related tweets becomes larger in extreme weather days when it is either severely polluted or has good air quality. It is confirmed by our statistical analysis to the retweet and like records. When compared with the \u00fe when AQI ranges from 50 to 200, the days when AQI exceeds 300 have 38% more highly retweeted or liked tweets (more than 40 retweets or likes), and the days when AQI is less than 50 have 12% more highly retweeted or liked tweets.\n\nThe accuracies of the health hazard prediction models using ANNs, nu-SVR, epsilon-SVR and random forest, are shown in Table 2 . We can find that two ANNs' methods outperform the SVM regression methods and the random forest regression method in forecasting the next day PHI, and the single hidden layer ANNs trained by ELM achieve slightly higher prediction accuracy than the multiple hidden layers ANNs trained by BP.\n\nIn detail, when only physical sensor features or social sensor features are inputted (P columns and S columns in Table 2 ), ANNs (ELM-ANN and BP-ANN) achieve very similar performance as the other three methods, especially the random forest regression method. However, when both kinds of features are jointly inputted (P\u00feS columns in Table 2 ), ELM-ANN outperforms nu-SVR and epsilon-SVR for all eight cities, and outperforms Random Forest for six cities. The average RMSE is about 10.5% smaller than that of nu-SVR, 12.5% smaller than that of epsilon-SVR and 5.2% smaller than that of Random Forest. Meanwhile, when the network diffusion is considered (P\u00feDS columns in Table 2 ), we can get similar comparison results. On the other hand, we can find that another ANN model trained by BP achieves similar accuracy as that trained by ELM, which confirms that ANNs are suitable to in our application of forecasting smog-related health hazard.\n\nAccording to the above evaluation, our forecasting approach with single hidden layer ANNs, ELM algorithm and both types of features achieves the highest forecasting accuracy. We adopt such settings for the evaluation of our approach in real world cases. In this part, the forecasting performance during two big smog disasters in Beijing and Shanghai are presented. The forecasted PHI records and the measured PHI records during the two big smog disasters are shown in Fig. 7 . The trends of the forecasted PHI (P\u00fe S) is basically consistent with that of the measured value (Target), and the consistency becomes even higher when the network diffusion is considered (P \u00feDS). The results indicate that this method can indeed work for realworld situations.\n\nIn this study, we propose a predictive analytics framework for smog-related health hazard forecasting using information from both social media and physical sensors, which is helpful for smog analysis but not investigated. In this framework, we first propose a new method for smog-related health hazard measurement based on individuals' smog and health related comments, as well as their diffusions on social media. Next, we develop a prediction model that utilizes ANNs to learn the non-linear relationships between the current physical and social smog observations and the next day smog-related health hazard. Fig. 7 . Results of smog-related health hazard (PHI) forecasting for Beijing and Shanghai.\n\nThe evaluation results indicate that the performance of ANN together with both social media and physical sensor features is the best among all candidates that we used in the experiments. We also find that social media features provide more predictive information than physical sensor features under the situations when the smog disaster is severe. Moreover, such benefit from social media data will be enlarged if we further consider information diffusion on social network.\n\nThe study also contains some limitations which should be studied on in the future work. One major limitation lies in using social media data for health hazard observations as each of its steps may bring in some errors. For example, we use keywords to find health-related or smog-related tweets, but actually people may use the same keywords in different contexts to mean different things. The percentage of correct tweets after filtering by a keyword mostly ranges from 85% to 95% [24] . We can find some other public health information such as hospital visit records as supplements.\n\nMeanwhile, the adaptivity of the approach for real world circumstances will also be considered in our future work. On one hand, some visual analytics [36] functions will be added into our on going demo system. Through presenting some similar historical circumstances or forecasting results by different features, the system can provide more information for flexible decision making. On the other hand, a new prediction model that utilizes the data from consistent historical circumstances by understanding the underlying semantic of the data is being investigated."}