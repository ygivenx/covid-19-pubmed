{"title": "Fitting dynamic models to epidemic outbreaks with quantified uncertainty: A primer for parameter uncertainty, identifiability, and forecasts", "body": "Emerging and re-emerging infectious diseases are undoubtedly one of humankind\u2019s most important health and security risks (Fauci & Morens, 2016). As epidemic threats increase so is the potential impact of mathematical and statistical inference and simulation approaches to guide prevention and mitigation plans. As the recent 2013\u20132016 Ebola epidemic exemplified, an infectious disease outbreak often forces public health officials to make key decisions to mitigate the outbreak in a changing environment where multiple factors positively or negatively impact local disease transmission (Chowell et al., 2017). Hence, public health officials are often interested in practical yet mathematically rigorous and computationally efficient approaches that comprehensively assimilate data and model uncertainty to 1) generate estimates of key transmission parameters, 2) assess the impact of control interventions (vaccination campaigns, behavior changes), 3) test hypotheses, 4) evaluate how behavior changes affect transmission dynamics, 5) gain insight to the contribution of different transmission pathways, 6) optimize the impact of control strategies, and 7) generate short and long-term forecasts, just to name a few.\n\nMathematical models provide a quantitative framework with which scientists can assess hypotheses on the potential underlying mechanisms that explain patterns in the observed data at different spatial and temporal scales. Models vary in their complexity in terms of the number of variables and parameters that characterize the dynamic states of the system, in their spatial and temporal resolution (e.g., discrete vs. continuous time), and in their design (e.g., deterministic or stochastic). While agent-based models, formulated in terms of characteristics and interactions among individual agents, have become increasingly used to model detailed processes often occurring at multiple scales (e.g., within host vs. population level), models based on systems of ordinary differential equations are widely used in the biological and social sciences. These dynamic models are specified by a set of equations and their parameters that together quantify the spatial-temporal states of the system via a set of interrelated dynamic quantities (e.g, viral load, susceptibility levels, disease prevalence) (Banks et al., 2009).\n\nIn this paper we review and illustrate a simple data assimilation framework for connecting ordinary differential equation models to time series data describing the temporal progression of case counts relating to population growth or infectious disease transmission dynamics (e.g, daily incident cases). This frequentist approach relies on modeling the error structure in the data unlike Bayesian approaches which always raise the question of how to set priors for the parameters. We present examples based on phenomenological and mechanistic models of disease transmission dynamics together with simulated and real datasets. We discuss issues related to parameter identifiability, uncertainty quantification and propagation as well as model performance and forecasts.\n\nThis is a phenomenological model that has proved useful to characterize and forecast early epidemic growth patterns (Chowell and Viboud, 2016, Viboud et al., 2016). In particular, previous analyses highlighted the presence of early sub-exponential growth patterns in infectious disease data across a diversity of disease outbreaks (Viboud et al., 2016). The generalized-growth model allows relaxing the assumption of exponential growth via a \u201cscaling of growth\u201d parameter, p. The model is given by the following differential equation:(1)C\u2032(t)=rCp(t)where C\u2032(t) describes the incidence growth phase over time t, the solution C(t) describes the cumulative number of cases at time t, r is a positive parameter denoting the growth rate, and p, the \u201cdeceleration of growth\u201d parameter varied between 0 and 1. If p=0, this equation describes constant incidence over time and the cumulative number of cases grows linearly while p=1 leads to the well-known exponential growth model (EXPM). Intermediate values of p between 0 and 1 describe sub-exponential (e.g. polynomial) growth patterns. In semi-logarithmic scale, exponential growth patterns are visually evident when a straight line fits well several consecutive generations in the growth pattern, whereas a downward curvature in semi-logarithmic scale indicates early sub-exponential growth dynamics.\n\nThe GRM is an extension of the original Richards growth model (Richards, 1959) with three free parameters, which has been fitted to a range of logistic-type epidemic curves (Dinh et al, 2016, Hsieh and Cheng, 2006, Ma et al, 2014, Turner et al, 1976, Wang et al., 2012). When C\u2032(t) represents the number of new infected cases at time t, the Richards model is given by the following differential equation:C'=rC[1\u2212(CK)a]where r represents the intrinsic growth rate in the absence of any limitation to disease spread, K is the size of the epidemic, and a is a parameter that measures the extent of deviation from the S-shaped dynamics of the classical logistic growth model (Turner et al., 1976). During the early stages of disease propagation when C(t) is significantly smaller than K, this model assumes an initial exponential growth phase. To account for initial sub-exponential growth dynamics (Viboud et al., 2016), we can modify the Richards model replacing the growth term rC by rCp (Viboud et al., 2016), incorporating a \u2018deceleration of growth\u2019 parameter (p). Hence, the GRM has the form:(2)C'=rCp[1\u2212(CK)a]where 0\u2264p\u22641. At the early stages of the epidemic, this model enables us to capture different growth profiles ranging from constant incidence (p=0), polynomial growth (0<p<1), to exponential growth (p=1) (Viboud et al., 2016). This model has been useful to generate post-peak forecasts of Zika and Ebola epidemics (Chowell et al., 2016b, Pell et al, 2016).\n\nThe simplest and most popular mechanistic compartmental model for describing the spread of an infectious agent in a well-mixed population is the SEIR (susceptible-exposed-infectious-removed) model (Anderson & May 1991). In this model, the infection rate is often defined as the product of three quantities: a constant transmission rate (\u03b2), the number of susceptible individuals in the population (S(t)), and the probability that a susceptible individual encounters an infectious individual (I(t)N). Moreover, infected individuals experience a mean latent and a mean infectious period given by 1/k and 1/\u03b3, respectively. The model is based on a system of ordinary differential equations that keep track of the temporal progression in the number of susceptible (S), exposed (E), infectious (I), and removed (R) individuals as follows:{S\u02d9=\u2212\u03b2S(t)I(t)NE\u02d9=\u03b2S(t)I(t)N\u2212\u03baE(t)I\u02d9=\u03baE(t)\u2212\u03b3I(t)R\u02d9=\u03b3I(t)C\u02d9=\u03baE(t)\n\nIn the above system, C(t) is an auxiliary variable that keeps track of the cumulative number of infectious individuals, and C\u02d9(t) keeps track of the curve of new cases (incidence).\n\nIn a completely susceptible population, e.g., S(0)\u2248N, the number of infectious individuals grows following an exponential function during the early epidemic growth phase, e.g., I(t)\u2248I0e(\u03b2\u2212\u03b3)t where the average number of secondary cases generated per primary case, R0, is simply given by the product of the mean transmission rate (\u03b2) and the mean infectious period (1\u03b3) as follows: R0=\u03b2\u03b3. However, as the number of susceptible individuals in the population declines due to a growing number of infections, the effective reproduction number over time, Rt, is given by the product of R0 and the proportion of susceptible individuals in the population:Rt=S(t)N\u03b2\u03b3\n\nIn order to calibrate mathematical models, researchers require time series data that describe the temporal changes in one or more states of the system. The temporal resolution of the data typically varies according to the time scale at which the relevant processes occur (e.g, daily, weekly, yearly) and the frequency at which the state of the system is measured. How well the model can be constrained to a given situation depends in part on the amount and resolution of the time series data. In general, we denote the time series of n longitudinal observations byyti=yt1,yt1,\u2026,ytnwherei=1,2,\u2026,nwhere ti are the time points of the time series data and n is the number of observations.\n\nIn our examples below, we make use of simulated data as well as outbreak data, which has been used in previous studies.\n\nParameter estimates for a given dynamical system are subject to two major sources of uncertainty:1)Noise in the data which is typically addressed by assuming a particular error structure in the data, e.g., Poisson vs. negative binomial distribution; and2)The underlying assumptions in the model employed for inferring parameter estimates from data.\n\nOther sources of error could be associated with the algorithms employed to numerically solve the model in the absence of analytic or close-form solutions. Here we focus on quantifying parameter uncertainty arising from noise in the data.\n\nWhen each data point should not be given equal weight in the estimation of model parameters, weighted least squares can be useful to assign relative weights to each data point in our dataset. For instance, weights could reflect variable quality (e.g., precision of the measurements) of the time series so that less weight is given to those data points associated with inferior quality or precision. We define the nonnegative weights given to each data point as wti so that the objective function for weighted least squares fitting is given by\u0398\u02c6=argmin\u2211i=1nwti(f(ti,\u0398)\u2212yti)2\n\nIf we want to give more weight to smaller data points, the weights given to each data point can be modeled as follows:wti=1/yti\n\nIf the goal is to give more weight to the most recent data, simple exponential smoothing can be used to assign higher weights to more recent data points relative to older data points. Specifically, the weights assigned to observations decrease exponentially as we move from recent to older data in the time series. Thus, the corresponding weight for observation ti is given by:wtn\u2212i=\u03b1(1\u2212\u03b1)i\u22121wherei=0,1,2\u2026n\u22121where parameter 0<\u03b1<1 regulate the rate at which the weights decrease exponentially so that the higher the value of alpha, the more weight is given to recent data relative to older data (Fig. 1).\n\nAfter parameters have been estimated, we can assess the quality of the model fit to the data by analyzing the temporal variation of the residuals, e.g., the difference between the best fit of the model and the time series data as a function of time:res(ti)=f(ti,\u0398\u02c6)\u2212yti\n\nA random pattern in the temporal variation of the residuals suggests a good fit of the model to the data. Conversely, systematic deviations of the model to the data (e.g., temporal autocorrelation) indicate that the model deviates systematically from the data, which prompts modelers to reassess the current version of the model. If the model is used for forecasting purposes, it is particularly important that the residuals are uncorrelated and the variance of the residuals is approximately constant.\n\nExample #1: Model fitting to time series data\n\nThe models:\n\nThe generalized-growth model (GGM)\n\nThe exponential growth model (EXPM)\n\nThe data:\n\nWe employ the weekly series of the number of reported Ebola cases in Sierra Leone during the 2014-16 Ebola epidemic in West Africa.\n\nParameter estimation:\n\nWe can fit the GGM to the first 15 weeks of the Ebola epidemic in Sierra Leone via least-square fitting using the Matlab built-in function lsqcurvefit.m. The initial number of cases C(0) is fixed according to the first observation week in the data (i.e.,C(0)=3). The parameter estimates are as follows:r=0.81p=0.48\n\nThe best fit of the GGM model and the corresponding residuals using the first 15 weeks of data of the Ebola epidemic in Sierra Leone is shown in Fig. 2. Our estimate for the scaling of growth parameter p indicates that the early growth pattern of the epidemic in Sierra Leone followed polynomial growth dynamics (Chowell et al., 2015). However, we still need to assess parameter uncertainty to construct confidence intervals and diagnose any potential parameter identifiability issues (see Section 9). While the GGM gives a good fit to the data, the EXPM model deviates systematically from the early growth phase, which is evident from the temporal autocorrelation in the set of residuals (Fig. 3).\n\nExample #2: Weighted least squares fitting to time series data\n\nThe model:\n\nThe generalized-growth model (GGM)\n\nData weighting scheme:\n\nSimple exponential smoothing.\n\nData:\n\nWe employ the weekly series of the number of reported Ebola cases in Sierra Leone during the 2014-16 Ebola epidemic in West Africa.\n\nBest fits of the GGM to the first 15 weeks of the Ebola epidemic in Sierra Leone using weighted least square nonlinear fitting where the weights of the data points are assigned according to simple exponential smoothing are shown in Fig. 4.\n\nIn the next section we present a computational approach for generating parameter uncertainty.\n\nWe rely on the general bootstrap method (Efron & Tibshirani, 1994) and describe a parametric bootstrapping approach which we have previously used in several publications to quantify parameter uncertainty and construct confidence intervals in mathematical modeling studies (e.g., (Chowell et al., 2006a, Chowell et al., 2006b)). In this method, multiple observations are repeatedly sampled from the best-fit model in order to quantify parameter uncertainty by assuming that the time series follow a Poisson distribution centered on the mean at the time points ti. However, it is also possible to consider overdispersion in the data (see next section). This computational method requires generating simulated data from f(ti,\u0398\u02c6), which is the best fit of the model to the data. The step-by-step algorithm to quantify parameter uncertainty follows (Fig. 5):1.Derive the parameter estimates \u0398\u02c6=(\u03b8\u02c61,\u03b8\u02c62,\u2026,\u03b8\u02c6m) through least-square fitting the model f(ti,\u0398) to the time series data yti=yt1,yt1,\u2026,ytnto obtain the best-fit model, f(ti,\u0398\u02c6).2.Using the best-fit model f(ti,\u0398\u02c6), we then generate S-times replicated simulated datasets, which we denote by f1\u2217(tj,\u0398\u02c6),f2\u2217(tj,\u0398\u02c6),\u2026,fS\u2217(tj,\u0398\u02c6).3.To generate the simulated datasets, we first use the best-fit model f(ti,\u0398\u02c6) to calculate the corresponding cumulative curve function, F\u2217(tj,\u0398\u02c6), as follows (see Fig. 6):F(tj,\u0398\u02c6)=\u2211l=1jf(tl,\u0398\u02c6)wherej=1,2,\u2026,n4.Each simulated dataset fk\u2217(tj,\u0398\u02c6) is generated by assuming a Poisson error structure as follows (Fig. 6):fk\u2217(tj,\u0398\u02c6)=Po(F(tj,\u0398\u02c6)\u2212F(tj\u22121,\u0398\u02c6))wherej=2,3,\u2026,nandk=1,2,\u2026,S\n\nMoreover, fk\u2217(t1,\u0398\u02c6)=f(t1,\u0398\u02c6)fork=1,2,\u2026,S. Thus, each new observation for each simulated dataset is sampled from a Poisson distribution (denoted by Po(.)) with mean F(tj,\u0398\u02c6)\u2212F(tj\u22121,\u0398\u02c6).5.Re-estimate parameters for each of the S-simulated realizations, which are given by \u0398\u02c6i where i=1,2,\u2026,S.6.Using the set of re-estimated parameters (\u0398\u02c6i where i=1,2,\u2026,S), it is possible to characterize their empirical distribution, correlations, and construct confidence intervals. The resulting uncertainty around the model fit is given by f(t,\u0398\u02c61),f(t,\u0398\u02c62),\u2026,f(t,\u0398\u02c6S) (Fig. 7).\n\nExample #2: Quantifying parameter uncertainty (see also Example #1)\n\nEstimate the uncertainty of the randp parameters of the GGM calibrated to the early growth phase of the 2014-16 Ebola epidemic in Sierra Leone.\n\nAssuming a Poisson error structure, Fig. 8 displays 1) the uncertainty of parameters \u201cr\u201d and \u201cp\u201d associated with the fit of the GGM model to the early phase of the Ebola epidemic in Sierra Leone and 2) the uncertainty in our parameter estimates translates into the 95% confidence bounds around the best fit of the model to the data.\n\nIn the previous section we assumed a Poisson error structure to quantify parameter uncertainty. The Poisson distribution only requires one parameter and is suitable to model count data where the mean of the distribution equals the variance. In situations where the time series data shows overdispersion, we can employ a negative binomial distribution instead. The negative binomial distribution requires two parameters to model the mean and overdispersion in the data. Thus, it is possible to model variance levels in the data that are relatively higher than the mean.\n\nExample #3: Quantifying parameter uncertainty with a negative binomial error structure (see also Example #2)\n\nAssuming a negative binomial error structure where the variance is 5 times higher than the mean, we estimate the uncertainty of the randpparameters of the GGM calibrated to the to the early growth phase of the 2014-16 Ebola epidemic in Sierra Leone. Results are shown in Fig. 9: 1) the uncertainty of parameters \u201cr\u201d and \u201cp\u201d associated with the fit of the GGM model to the early phase of the Ebola epidemic in Sierra Leone and 2) the 95% confidence bands around the best fit of the model to the data.\n\nA key question in model parameterization is whether the model parameters are identifiable from the available data. As a general rule, a parameter is identifiable when its confidence interval lies in a finite range of values (Cobelli and Romanin-Jacur, 1976, Jacquez, 1996, Raue et al, 2009). Conversely, lack of parameter identifiability can be recognized when large perturbations in the model parameters generate small changes in the model output (Capaldi et al, 2012, Chowell et al., 2006b, Pillonetto et al., 2003). Multiple factors can give rise to lack of parameter identifiability. For instance, structural parameter non-identifiability (Cobelli & Romanin-Jacur, 1976) results from the particular structure of the model independently of the characteristics of the observed time series data used to estimate parameters. However, even when structural identifiability is not an issue, a parameter may still be non-identifiable in practice due to other factors including: 1) the amount and quality of the data available and/or 2) the number of parameters that are jointly estimated from the available data. This type of parameter non-identifiability is commonly referred to as practical non-identifiability (Raue et al., 2009).\n\nStructural parameter non-identifiability is often the most difficult to remedy as it requires appropriately modifying the model to eliminate the structural non-identifiability issue. On the other hand, practical parameter non-identifiability issues could be fixed by 1) employing an alternative model of lower complexity when possible, 2) collecting more data about other states in the system to better characterize the system dynamical features, 3) increasing the spatial-temporal resolution of the data to better constrain the model parameters and/or 4) reducing the number of parameters that are jointly estimated, perhaps by constraining a subset of the unknown parameters based on estimates previously reported in similar studies and conducting extensive sensitivity analyses on those parameters (Arriola et al., 2009). Finally, specific approaches have been adapted to address parameter identifiability including regularization techniques that aim for stable parameter reconstruction (Smirnova & Chowell, 2017).\n\nExample #4: Parameter non-identifiability arises from the limited amount of data available to quantify parameter uncertainty\n\nFor this example, we first generate simulated data from the generalized-Richards model (GRM) using the parameter values: r=0.2,p=0.8,a=1,andK=1000. Next, we use the simulated data to attempt to estimate parameters randp using the GGM from an increasing length of the early growth phase of the daily incidence curve simulated using the GRM. Fig. 10 shows the resulting empirical distributions of the parameters using an increasing length of the growth phase: 10, 20, \u2026, 80 days. Importantly, Fig. 10 shows that using only 10 days of data, it is not possible to reliably estimate the deceleration of growth parameter, p, because its confidence interval ranges widely from 0.5 to 1.0. Indeed, we conclude that it is not possible to discriminate between sub-exponential and exponential-growth dynamics based on data of only the first 10 days. In fact, the corresponding confidence interval of p include the values of 0.5 and 1.0, which indicate that both linear and exponential growth dynamics cannot be ruled out with the data at hand. As more data of the early growth phase is employed to estimate parameters of the GGM, the uncertainty in parameter estimates is not only reduced, but the parameter estimates are better constrained around their true values (Fig. 10).\n\nWe can quantify the parameter correlations using our joint empirical distributions of the parameters (denoted by \u0398\u02c6iwhere i=1,2,\u2026,S) which are derived from our bootstrap approach (described in Section 7)), For instance, for our Example # 2 based on fitting the GGM to the first 15 weeks of the Ebola epidemic in Sierra Leone, parameters r\u02c6i and p\u02c6i were significantly correlated as shown in Fig. 11. Despite this, the confidence intervals of these parameters display reasonable uncertainty to reliably characterize the parameters.\n\nExample #5: Evaluate the correlation of the\nr\u02c6,p\u02c6\nparameters derived from fitting the GGM to the first 15 weeks of the Ebola epidemic in Sierra Leone (See also Example #2;\nFig. 11). These parameters are significantly correlated (Spearman rho = -0.99; P-value < 0.001).\n\nWhile we can inspect the residuals for any systematic deviations of the model fit to the data, it is also possible to quantify the error of the model fit to the data using performance metrics (Kuhn & Johnson, 2013). These metrics are also useful to quantify the error associated with forecasts. A widely used performance metric is the root-mean- squared error (RMSE), which is given byRMSE=1n\u2211i=1n(f(ti,\u0398\u02c6)\u2212yti)2\n\nAnother performance metric is the mean absolute error (MAE), which is given byMAE=1n\u2211i=1n|f(ti,\u0398\u02c6)\u2212yti|\n\nSimilarly, the mean absolute percentage error (MAPE) is given by:MAPE=1n\u2211i=1n|(f(ti,\u0398\u02c6)\u2212yti)/yti|\n\nExample #6: Compare performance metrics for both the GGM and EXPM models when calibrated to the first 15 weeks of the 2014-16 Ebola epidemic in Sierra Leone (See also Example #1).\n\nThe RMSE, MAE, and MAPE of the fits provided by the GGM and EXPM models to the first 15 weeks of the Ebola epidemic in Sierra Leone (See also Fig. 2, Fig. 3) are as follows:\n\nWe are frequently interested in calibrating a model not only to understand and characterize the current state of the system, but also to aim to predict its behavior in the near or long terms. The particular time horizon of forecast depends on the purpose of the forecast. For instance, a long-term forecast (e.g., several years) could be useful to make strategic decisions regarding the construction of facilities to respond to natural disasters such as epidemics and hurricanes whereas a short-term forecast (e.g., days to weeks) are useful to plan for scheduling resources (e.g., number of face masks, hospital beds). However, it is important to keep in mind that forecasts are often inaccurate as these are mostly based on the current values and uncertainty of the parameters of the system, which are likely to change over time. Moreover, the further out the forecast is made, the more wrong it is expected to be.\n\nA properly calibrated model to data can be used to generate short-term or long-term forecasts of the system. Generating a forecast based on the model uncertainty given by f(t,\u0398\u02c61),f(t,\u0398\u02c62),\u2026,f(t,\u0398\u02c6S) is a relatively straightforward computational task that requires propagating the uncertainty of the current state of the system in time by a time horizon of h time units as follows (see Fig. 12):f(t+h,\u0398\u02c61),f(t+h,\u0398\u02c62),\u2026,f(t+h,\u0398\u02c6S)\n\nThat is, we forecast the entire uncertainty of the system using the uncertainty associated with the parameter estimates, which were previously derived from our uncertainty quantification procedure described in Section 7.\n\nTo assess forecasting performance, we can use one of the performance metrics (e.g., RMSE, MAE) previously described in Section 11.\n\nExample #7: Forecast the early growth phase from daily synthetic data obtained from the GRM model.\n\nThe model:\n\nThe generalized-growth model (GGM)\n\nThe data:\n\nSimulated daily incidence data using the generalized-Richards model (GRM) with parameters r=0.2,p=0.8,a=1,andK=1000.\n\nForecasts:\n\n30-day ahead forecasts using the GGM by estimating parameters r and p with quantified uncertainty when the model is fitted to an increasing length of the epidemic growth phase (10, 20, \u2026, 80 days) (Fig. 13).\n\nWe can observe that the uncertainty of the forecasts narrows down as more data of the early growth phase is employed to estimate parameters of the GGM. That is, the uncertainty in parameter estimates is not only reduced, but the parameter estimates are also increasingly constrained around their true values (Fig. 13). Importantly, using only 10 days of data, it is not possible to reliably estimate discriminate between sub-exponential and exponential-growth dynamics. The corresponding performance of the GGM during the calibration and forecasting periods is shown in Fig. 14.\n\nExample #8: Forecast the early growth phase of the Zika epidemic in Antioquia, Colombia\n\nThe model:\n\nThe generalized-growth model (GGM)\n\nThe data:\n\nThe daily number of new Zika cases by date of symptoms onset in Antioquia, Colombia.\n\nForecasts:\n\n10-day ahead forecasts using the GGM by estimating parametersr and p with quantified uncertainty when the model is fitted to an increasing length of the epidemic growth phase (20, 25 30, 35 days) (Fig. 15).\n\nWe can observe that the uncertainty of the forecasts narrows down as more data of the early growth phase is employed to estimate parameters of the GGM. Importantly, using less than 10 days of data, it is not possible to reliably estimate discriminate between sub-exponential and exponential-growth dynamics. The corresponding performance of the GGM during the calibration and forecasting periods is shown in Fig. 16 Matlab code for 1) fitting the GGM, 2) derive parameter uncertainty, and 3) generate short-term forecasts using incidence data of the Zika outbreak is provided in the supplement.\n\nExample #9: How much data is needed to refit a model to itself?\n\nThe model:\n\nThe generalized Richards model (GRM)\n\nThe data:\n\nSimulated daily incidence data using the generalized-Richards model (GRM) with parameters r=0.2,p=0.8,a=1,andK=1000.\n\nForecasts:\n\nIt is of interest to understand how much data is needed to faithfully calibrate a model to synthetic data derived from the same model. For this purpose, we conducted long-term forecasts based on the GRM, the same model that was employed to generate the data, by estimating parameters r,p and K using an increasing amount of epidemic data (40, 60, \u2026, 140 days) (Fig. 17).\n\nUsing only data of the early epidemic growth phase (before the inflection point occurring around day 50), the model is underdetermined and significantly underestimates the incidence curve. Forecasts are gradually improved particularly when the model is calibrated using data past the epidemic's inflection point (Fig. 17).\n\nIn previous Examples #2 and #3, we measured the uncertainty of model parameters estimated from data by constructing confidence intervals using the empirical distribution of the parameters. However, it is possible to use the empirical distributions of the parameters to assess the uncertainty associated with composite parameters whose values depend on several existing model parameters and are often useful to gauge the behavior of the modeled system. For instance, a key epidemiological parameter to measure the transmissibility of a pathogen is the basic reproduction number, R0 (Anderson and May, 1991, Diekmann et al., 1990, van den Driessche and Watmough, 2002). This parameter is a function of several parameters of the epidemic model including transmission rates and infectious periods of the epidemiological classes that contribute to new infections. This is an important parameter as it often serves as a threshold parameter for SEIR-type compartmental models. If R0>1 then an epidemic is expected to occur whereas values of R0<1cannot sustain disease transmission. For instance, for the simple SEIR model, the basic reproduction number is given by:R0=\u03b2\u03b3\n\nUsing the empirical distributions of the transmission rate (\u03b2\u02c6i where i=1,2,\u2026,S) and the recovery rate (\u03b3\u02c6i where i=1,2,\u2026,S), we can directly estimate the empirical distribution of the basic reproduction number as follows (Chowell et al., 2006a, Chowell et al., 2009):R\u02c60i=\u03b2\u02c6i\u03b3\u02c6iwherei=1,2,\u2026,S\n\nUsing the empirical distribution of R\u02c60i, we have full control of the uncertainty allowing us to not only construct confidence intervals, but also assess the probability that R\u02c60i lies above the epidemic threshold at 1.0.\n\nExample #9: Estimating\nR0\nby fitting the SEIR model to the early epidemic growth phase (adapted from ref. (Chowell, Nishiura, & Bettencourt, 2007)).\n\nHere we provide an example of estimating R0 of the 1918 influenza pandemic in San Francisco, California, by estimating the transmission rate \u03b2 while fixing the latent period at 2 days (e.g., \u03ba=1/2) and the infectious period at 2 or 4 days (e.g., \u03b3=1/2 or \u03b3=1/4) based on the epidemiology of influenza.\n\nThe model:\n\nSEIR model Chowell et al., 2007 described in Section 2.2.1\n\nThe data:\n\nWe employ the daily series of influenza case notifications during the fall wave of the 1918 influenza pandemic in San Francisco.\n\nBaseline parameter values:\n\nLatent period of 2 days (e.g., \u03ba=1/2) while the infectious period was assumed to be 2 or 4 days (e.g., \u03b3=1/2 or \u03b3=1/4). At the time of the 1918 pandemic, San Francisco had a population size of approximately 550,000.\n\nParameter estimation:\n\nFor simplicity, we only estimate one parameter from the time series data of the early epidemic growth phase: the transmission rate, \u03b2. We can fit the SEIR to the first 16\u201320 days of the influenza pandemic in San Francsico via least-square fitting using the Matlab built-in function lsqcurvefit.m. The initial number of cases I(0) is fixed according to the first observation day in the data (i.e., C(0) = 4). For instance, fitting the model to the first 16 epidemic days, we estimate the transmission rate at:\u03b2=1.1(95%CI:1.1,1.2)\n\nUncertainty in\nR0:\n\nThe best fit of the SEIR model to the first 16, 18, and 20 days of the influenza pandemic in San Francisco along the corresponding empirical distribution of R0 is shown in Fig. 18. We can observe that the distribution of R0 is stable when using 16, 18 or 20 epidemic days of data. R0 was estimated at 2.3 (95% CI: 2.2, 2.3) using 16 epidemic days, 2.3 (95% CI: 2.2, 2.3) using 18 epidemic days, and 2.3 (95% CI: 2.3, 2.3) using 20 epidemic days.\n\nWhile the basic reproduction number, commonly denoted by R0, gauges the transmission potential of an infectious disease epidemic in a fully susceptible population during the early epidemic take off (Anderson & May 1982), the effective reproduction number Rt captures changes in transmission potential over time (Chowell et al., 2016c, Nishiura et al., 2009). We can characterize the effective reproduction number and its uncertainty during the early epidemic exponential growth phase (Wallinga & Lipsitch, 2007). When the early dynamics follow sub-exponential growth, another method relies on the generalized-growth model (GGM) to characterize the profile of growth from early incidence data (Chowell et al., 2016c). In particular, the GGM can reproduce a range of growth dynamics from constant incidence (p=0) to exponential growth (p=1) (Viboud et al., 2016). We can generate the uncertainty associated with the effective reproduction number during the study period directly from the uncertainty associated with the parameter estimates (r\u02c6i,p\u02c6i) where i=1,2,\u2026,S.. That is, Rtj(r\u02c6i,p\u02c6i) provides a curve of the effective reproduction number for each value of the parameters r\u02c6i,p\u02c6iwhere i=1,2,\u2026,S. Then, we can compute the curves Rtj(r\u02c6i,p\u02c6i) based on the incidence at calendar time tj denoted by I(tj,r\u02c6i,p\u02c6i), and the discretized probability distribution of the generation interval denoted by \u03c1tj. The effective reproduction number Rtj(r\u02c6i,p\u02c6i)can be estimated using the renewal equation (Chowell et al., 2016c, Nishiura et al., 2009):Rtj(r\u02c6i,p\u02c6i)=Itj\u2211k=0jItj\u2212k\u03c1tkwhere the denominator represents the total number of cases that contribute (as primary cases) to generating the number of new cases Itj (as secondary cases) at calendar time tj (Nishiura et al., 2009).\n\nExample #10: Estimating the effective reproduction number from the early epidemic growth phase using the GGM method (Chowell et al., 2016c).\n\nThe data:\n\nWe employ the same data as in Example #8 describing the daily series of influenza case notifications during the fall wave of the 1918 influenza pandemic in San Francisco.\n\nWe assumed an exponential distribution for the generation interval of influenza with a mean of 4 days and variance of 16. Using the early growth phase in the number of new case notifications during the first 20 epidemic days, we estimated the deceleration of growth parameter at 0.99 (95% CI: 0.95, 1.0), an epidemic growth profile with uncertainty bounds that includes exponential growth dynamics (i.e., p = 1) (Fig. 19). Based on the generalized-growth method, we estimated the effective reproduction number at 2.1 (95% CI: 2.0, 2.1) (Fig. 20).\n\nIn this article we have described and illustrated a relatively simple computational approach to quantify parameter uncertainty, evaluate parameter identifiability, assess model performance, and generate forecasts with quantified uncertainty. In the process we have employed simple phenomenological and mechanistic models to characterize epidemic patterns as well as estimate key transmission parameters such as the basic reproduction number R0 and the effective reproduction number Rt. This uncertainty quantification approach is computationally intensive and relies solely on case incidence series from an unfolding outbreak and allows considerations of different error structures in the case series data (e.g., Poisson vs. negative binomial).\n\nIn future research we will build on this framework to address issues related to model uncertainty (Lloyd Chowellet al, 2009). In particular, researchers often focus on a given model and data to characterize the state of the system, but less on how different sets of assumptions influence parameter estimates, their uncertainty and impact on forecasts. Instead of relying on a single model, the information provided by multiple contending models can be integrated into ensemble models (e.g., model weighting schemes (Burnham & Anderson, 2002)), analogous to weather prediction systems (Raftery et al., 2005)."}