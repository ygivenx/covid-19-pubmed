{"title": "Individual-Based Models for Public Health", "body": "Today, infectious diseases represent a threatening concern for human health. In addition to the main pathogens affecting human populations, such as Plasmodium falciparum causing malaria or influenza viruses that together kill hundreds of thousands of human lives throughout the world (Murray et al., 2012) , an increasing number of pathogens are emerging throughout the world, such as the MERS Coronavirus in Saudi Arabia or the Ebola virus in West Africa to cite a few (Jones et al., 2008) . Understanding their transmission, and possibly forecasting the dynamics of these pathogens, represents both a scientific and sanitary emergency.\n\nTo this goal, modeling has been a widely used tool (Hufnagel et al., 2004; Najera, 1974) . First, mathematical models gave crucial insights for fundamental infectious diseases management (Anderson and May, 1991) . For instance, the probability of epidemics and its expected magnitude can be easily calculated through mathematical models by deriving the basic reproductive ratio (R0), which quantifies the number of secondary cases that a single infectious individuals will produce when introduced within a population entirely na\u00efve from an immunological point of view (Heesterbeek, 2002) . These models have also allowed understanding that imperfect vaccination could lead to an increase of the mean age of infection, which is problematic for many childhood infections because of the higher probability of severe cases when these diseases occur after childhood (Keeling and Rohani, 2008) .\n\nIn addition to these fundamental estimations, mathematical models have also been used in emergency situation. For instance, the 2001 outbreak of foot-and-mouth disease in the United Kingdom has seen a team of mathematical modelers being in charge of shaping the public health interventions, and especially the number and locations of animals to kill, to mitigate the explosiveness of the epidemics (Keeling et al., 2001) . Despite many criticisms regarding the huge number of slaughtered animals (Kitching et al., 2006) , partly justified regarding the lack of model validation due to our poor knowledge on the pathogen biology at that time, epidemics has been successfully controlled.\n\nNevertheless, the mathematical models have important limitations to explicitly model the mechanisms involved in the infectious processes at the individual level, such as evolution within large strain assemblages, complex spatial configuration shaping contacts, or heterogeneous individual behaviors facing an epidemic. Considering the huge amount of computational power we have access today, we have the opportunity to explore mechanisms of diseases spread, surveillance, and control at very fine scale (individuals), and how these mechanisms are related with what is observed at the population level. This improves our understanding of fundamental processes involved in diseases spread, and may improve as well forecasting potential of these models.\n\nBroadly, two main alternatives can be invoked. First, a forecasting goal can be satisfied with big data approach. Based on the extensive exploration of large datasets (such as requests made on Google website or message contents from digital social networks) combined with cutting-edge statistical modeling, it is possible to forecast the dynamics of an epidemics reasonably well in advance (Ginsberg et al., 2009; Salath e et al., 2013) . However, such approach is similar to a \"black box\" because it does not rely on a mechanistic formulation of the problem. Therefore, it hardly participates to the understanding of pathogen transmission; neither forecast the dynamics of rare events.\n\nAs we said, computational models such as individual-based models (IBMs) are very relevant for understanding the complexity of mechanisms at the individual level that can be involved in disease outbreaks. Their computational formalism allows a large flexibility, while they rely on the same philosophy than current models in mathematical epidemiology that have proved their relevance.\n\nIn this chapter, we review the main qualities of IBMs, what kind of new knowledge they can bring and they have already produced in epidemiological modeling. Then, we highlight their caveats and what could be developed during the future years to make IBMs a more reliable and useful approach.\n\nBy definition, an IBM focuses at the individual scale. It is worth pointing out that we will use the term \"individual-based models\" in a broad sense throughout this chapter, i.e., encapsulating a large gradient from basic models with few individuals interacting in simple ways like cellular automata until very complex situations where individuals can interact with other individuals directly and/or through modifications of their environment, and where individuals can exhibit complex behaviors, such as what is generally assumed for agent-based models (ABMs). It is worth to notice here that ABMs are developed in many disciplines, from distributed artificial intelligence in computer sciences, to sociology, geography, or ecology. In this chapter, we use the term IBM because in epidemiology, IBMs clearly refer to the modeling of individuals within a population.\n\nIn software engineering, we talk about objects, which represent individuals that are instance of a class, which are abstract pattern of an object (such as human, vector, or pathogen in our case), and include methods (i.e., functions that an object can execute). Then, each individual has a set of attributes and functions. For epidemiological systems, classic attributes are the infectious status of the individual, such as Susceptible (who can be infected), Infectious (who can transmit the pathogen), or Recovered (who cannot be infected anymore) to frame it within the SIR philosophy (Keeling and Rohani, 2008) . Obviously, any kind of status could be considered and other attributes than infectious status can be included, such as time of infection, spatial location, age, etc.\n\nThe methods of each individual characterize the processes that could be involved at this individual scale. In epidemiological systems, transmission and recovery are obviously naturally present. Nevertheless, these processes could rely on the individual itself (such as for recovery that will only depend of host attributes) or on the population, as it is the case for transmission that will depend on the number of individuals in contact with the focal host. Each of these methods is used to characterize the change of host's attributes at the next time step.\n\nIndeed, IBMs are generally discrete time models. To avoid important edge effects, it is required to properly schedule the updating of the individuals states. Two main methods exist. The first one consists in updating every individuals states at time t considering the individuals states at time tdt. This method respects the causality principle and considers that individuals instantaneously change their states at the same time t. The corresponding scheduling algorithm is as follow: The second method consists, for each time step, to randomly draw an individual from the list of individuals, compute its new state, and then repeat the drawing for all individuals. This method considers that individuals randomly interact between the time t and t + dt. Here, interactions are asynchronous and continuous within one time step. The corresponding scheduling algorithm is as follow: \n\nThe specification corresponds to the description of a model using a particular formalism, being mathematical, graphical, textual, or hybrid. Knowing reproducibility to be a key process in the scientific activity, the specification of IBMs appears to be fundamental. It enables to communicate the model to pairs for validation and verification. Without unambiguous specification, pairs cannot reproduce the model, and then the simulation results. They are many different types of specification for IBMs: from pseudo-code, like in the previous section, to graphical modeling language such as UML (Unified Modeling Language, see below). Nevertheless, most of the specifications remain ambiguous and need to be complemented by a documentation. Grimm et al. have proposed the \"Overview Design Details\" (ODDs) protocol to communicate IBMs (Grimm et al., 2006) . Using ODD can improve significantly the replication of IBM simulations. The major drawback of ODD remains in the ambiguity of its terminology and the absence of a rigorous description of the simulator, i.e., the software that supports the implementation of the IBM . In a recent paper, Donkin et al. (2017) show that using ODD does not fully support replication by implementing the same IBM on two different simulation platforms (Repast and NetLogo). They found that the results not only differ in magnitude but also in trends leading to different conclusions regarding the question the model is supposed to address. This work highlights the tremendous importance to be able to specify IBMs rigorously. Despite the fundamental importance of replication in science, few works have been done regarding IBMs (Axtell et al., 1996; Bajracharya and Duboz, 2013; Donkin et al., 2017; Railsback et al., 2006; Wilenksy and Rand, 2007) . Of course, replication is time consuming and perceived as nonrewarding. It is nevertheless mandatory if we want IBM to be considered as a valid scientific tool.\n\nThe main language used to model IBM is the UML. Developed during the 1980s to improve software design with the increasing use of object-oriented languages, this language allows a graphical representation of the interactions between the different classes involved in the software (class diagram) as well as the behavior of the most important methods (Fig. 1 ).\n\nA possible answer to the specification problem in IBMs is to ground the specification into existing abstract formalisms that have been developed by simulation practitioners in the field of \"automatic.\" In particular, states machines and computation theory are both very inspiring. The most promising formalism for IBMs specification is probably the Discrete Events Specification System (DEVS), an operational formalism a to specify discrete events simulation models. a An operational formalism is a specification having a strict equivalence with the algorithms that implement it. It is independent from the programming language used for its implementation. DEVS is derived from discrete mathematics and relies on systems theory. It involves enough generalism to formalize discrete and continuous time models and can also merge continuous and discrete time dynamics together. (A comprehensive introduction to DEVS is available in Zeigler et al. (2000) .) From DEVS, several extensions have been proposed for specific purposes. For instance, spatially explicit systems can be formalized using Cell-DEVS, differential equations systems by using QSS (a quantized version of DEVS to solve differential equations), etc. An interesting property of DEVS extensions models is that they are all formally equivalent to DEVS models. Therefore, any models specified using a particular extension can be formally and computationally coupled with other models in a DEVS framework. This property ensures the scalability of models and promotes models reuse. Furthermore, DEVS proposes a compositional and hierarchical approach to modeling. Therefore, elementary models (called atomic models) can be coupled together to build a coupled model that can be view as an atomic model at upper hierarchical level.\n\nSince the beginning of this century, DEVS is used for the specification of IBM (Kim and Kim, 2001; Zhang et al., 2011) . One specific feature of IBM is that individuals can continuously appear and disappear, change their behaviors, and change the way they interact with others. Then, the structure of such a model is dynamic, conversely to differential equations models that possess a static structure. It is now recognized that the dynamic structure of IBMs can be specified with DS-DEVS (Dynamic Structure DEVS (Duboz et al., 2006; Zeigler et al., 2000) ), and recent works still improve IBM specification within this DEVS framework (Won Bae and Il-Chul, 2015). \n\nAs said previously, many mathematical models have been developed on numerous diseases at a population scale, and it will be a pity to not rely on these models when we add details at the individual scale. Conversely, building an aggregated model from an IBM can enlighten how properties emerge at the population level (Huet and Deffuant, 2008) . Nevertheless, going up and down in scales is not that trivial. Indeed, parameters in population mathematical models (also called mean-field models, MFMs, because they are continuous states models assuming well-mixed populations and the ergodic hypothesis) are generally expressed as rates at a population scale, while parameters in IBMs have generally to be expressed as a probability at an individual scale (deterministic IBMs are not very common and will therefore not be addressed in this chapter). If we assume that time between state transitions are exponentially distributed, the individual probability can be calculated from the corresponding rate used in MFMs (Keeling and Rohani, 2008) as following:\n\nBut it underlines that the duration t of a time step is dramatically important. While a large time step will allow faster model execution, it will also dangerously overaggregate the event distribution and can therefore bias completely the dynamics. While a time step of 1 day was generally considered in epidemiology (Keeling and Rohani, 2008) , it has been shown that a finer time resolution is required, even with very simple dynamics (Roche et al., 2011) .\n\nBy comparing an IBM with its equivalent MFM, we can learn what are the approximations made by the MFM that can lead to significant differences with the IBM simulation results. In their paper, Bont e et al. (2012) built two epidemiological models of the same system. The first one is an IBM and the second one is the corresponding MFM derived using the moments approximation technique. This work highlighted the importance for the models to capture the displacement behaviors and the contact processes at the individual level in the study of disease spread.\n\nIn their recent paper, \u20ac Ozmen et al. (2016) built several MFMs and IBMs of the well-documented Spanish Flu epidemic. They show clearly that the underlying modeling hypothesis in MFM and IBM can lead to significantly different simulation results for the simulation of the same phenomenon. This result is extremely important as both models can be used for decision-making.\n\nAnother interesting use of both approaches is to couple them to form a single model of the considered system at different space and timescales. Doing this, it is possible to simulate a scale transfer between two hierarchical levels, the individual level (IBM) and the population level (MFM) (Duboz et al., 2003) . This coupling enables to study how individual behavior (foraging in the mentioned study) impacts the population dynamic. In this approach, the IBM is considered as a virtual experiment to compute the parameters of the MFM. This technique has the potential to greatly reduce computation time, which represents one of the main limitations of complex IBMs. Indeed, the MFM can deal with any size of population and the IBM is considered as a population sample that is coupled through individual statistics computed from simulation outputs.\n\nMathematical models have proved that they can provide crucial insights into our understanding and our ability to forecast epidemiological dynamics. Therefore, it is important to identify what kind of improvements IBMs can bring. We will here review nonexhaustively some of the most important opportunities offered by this modeling approach, which can be also combined together.\n\nClassic compartmental mathematical models are able to consider a spatial dimension, through a collection of populations connected between them (Xia et al., 2004) or through a diffusion equation involved within a partial differential equations systems (Tran and Raffy, 2006) . However, such models have generally a simplistic representation of space at fine scale.\n\nTherefore, the most involved improvement of IBM has been clearly to consider a spatially explicit dimension at this fine scale. Use of IBM in this context has started very early with the development of cellular automata (Wolfram, 2002) and remains one of the main reasons to develop IBM in epidemiology.\n\nConsidering an explicit spatial dimension yields to the fact that the different elements of an epidemiological system (humans, animal reservoir, and/or vectors among others) have less or more contact than with others. As a result, this explicit spatial dimension can be summarized as a network of interactions, which can fluctuate through time.\n\nWhen individual movements are not considered explicitly, epidemiological studies have generally considered spatial dimension through a network of interactions among individuals. These networks could be derived from complex socioeconomic data, with a clear aim of forecasting possible future disease outbreaks (Eubank et al., 2004) . These networks could be also a result of well-known algorithms, which can generate a large gradient of network properties such as random networks with the Erdos-R enyi algorithm (Erdos and R enyi, 1959) until realistic network patterns that can mimic accurately documented network with the preferential attachment that generates scale-free networks (Barabasi et al., 1999) . Generally, epidemiological studies that have relied on such networks have investigated the role of connectivity between these epidemiological elements on the different metric of disease transmission (Keeling, 1999; Roche et al., 2013) or on pathogen evolution (Read and Keeling, 2003) .\n\nThese interactions networks can be the result of an explicit consideration of individual movement from one place to another. Generally, when the spatial dimension is discrete, we consider that individuals can interact with their neighborhood, i.e., with the individuals that are located closely. Thanks to an inheritance from the cellular automata era, we can consider a \"Von Neumann\" neighborhood (assuming that an individual can interact only with the adjacent individuals) or a \"Moore\" neighborhood (assuming that individuals can interact with all the eight individuals surrounding the considered one). Today, even when the spatial dimension can be still discrete, it is also common to consider a spatial radius where focal individual can interact with other elements of the model (Fig. 2) . In epidemiology, these models that not relying on field data have been mostly used to identify fundamental properties of pathogen propagation with physical constraints or different kind of habitats (Roche et al., 2008) .\n\nBased on this explicit consideration of the spatial dimension, many sophistications have been made possible thanks to software design and computing resources. The recent developments of IBM platforms that allowed the integration of very complex heterogeneous data with multiple layers of spatial dimensions from Geographical Information Systems (GISs). These models are allowing to explore the spatial propagation of epidemics in a realistic context (Brown et al., 2005; Muller et al., 2004) , despite their validation on epidemiological data remains extremely challenging. \n\nThe second main reason to use IBMs is the potential to consider complex behaviors of the different epidemiological elements. For instance, the decision of an individual to adopt a preventive measure depends on her/his perception of risks associated with her/his actions. This perception depends on individual experiences and personal beliefs and can evolve in time. The set of possible choices is discrete and finite, and the rules selecting actions often refer to qualitative data.\n\nDifferential equations hardly represent discrete choices and qualitative information, whereas computer programming is extremely flexible and allows considering behaviors relying on many different things, such as the surrounding individuals, the environment status, etc. Therefore, integrating complex behavior has always been a key element of IBMs, mainly in ecological studies (Grimm et al., 2005) .\n\nThis aspect is critical for IBMs. Indeed, human behavior is increasingly recognized as a key element in pathogen transmission (Funk et al., 2010) . Nevertheless, integration of behaviors in IBMs is still burgeoning in epidemiology despite the numerous models that have been developed in the field of artificial intelligence, and more specifically for ABMs. The main reason is the quasi-absence of data regarding human behavior being explicitly collected for a modeling purpose in epidemiology. It appears necessary here to collaborate with sociologists or behavioral economists for instance, to benefit from their methodologies. More generally, participatory modeling methods are needed to integrate the different disciplines and stakeholders concern with public health issues (Binot et al., 2015) . Moreover, the possibility to track simple parts of human behavior in simple mathematical model relying on the classic SIR framework has been very insightful (Andrews and Bauch, 2016) , but has also delayed the consideration of more complex behaviors.\n\nAnother possibility is also to expand the possibility of classic mathematical models. To this extent, considering large assemblages of pathogen strains represent a crucial example. Many pathogens show an antigenic variability that constrains vaccine development and therefore disease control, such as in the case of influenza or Dengue viruses. In populational mathematical models, the strain space has to be fixed, which could also generate some edge effects around the first or the last strains (Gog and Grenfell, 2002) . This constraint is removed thanks to the computational formulation because this strain space can be extended dynamically, while it can nevertheless rely on the same compartmental formulation (Roche et al., 2011) .\n\nIBM calibration or optimization is an important issue. For IBM, as for any kind of models, it may happen that some of the parameters cannot be estimated experimentally. In such case, the modeler could fit model parameters to reproduce collected data and then calibrate the model. The main difficulty is the stochastic nature of IBM. We have therefore to use Monte Carlo techniques to fit parameters, which are difficult to implement and for which the convergence toward an optimum cannot be assert. Moreover, this problem is particularly important with IBMs when one single simulation may require minutes or hours to be achieved. Heuristic techniques, such as evolutionary algorithms, can reduce the computation time for parameter exploration (Fig. 3) (Duboz et al., 2010) . Choice of some individuals that will become parents based on the fitness. Crossover/mutation: Use of parents to create new individuals, the offspring. Several parents' genotypes can be mixed (crossover) and/or modified (mutation). Evaluation: Computation of the phenotype and the fitness of the offspring. Replacement: Choice of the individuals (among parents and offspring) which will constitute the next generation (based on the fitness). Nevertheless, convergence is still an issue and IBMs calibration remains difficult when the number of parameters is large. Some methodological developments in Approximate Bayesian Computation (ABC) have started addressing this problem. ABC has the potential to optimize an IBM's structure and parameters within an established statistical framework. In their paper, Van der Vaart et al. (2015) showed that ABC can potentially represent uncertainty in IBM parameters, structure, and predictions. However, work remains to be done in developing ABC for IBM with a large number of parameters.\n\nSensitivity analysis focuses on studying uncertainties in model outputs because of uncertainty in model inputs. In other words, the aim of sensitivity analysis is to assess the influence of model's parameter values and model structure on its output. Sensitivity analysis is divided into two main approaches: local and global. The local approach looks at the variation in the output of the model when parameters vary around their reference values. The global approach considers the whole values intervals for each parameter.\n\nWhen using sensitivity analysis with IBM, we consider model-free methods, for which the explicit mathematical formulation of the model is not necessary (Saltelli et al., 2008) . With sensitivity analysis, we can identify the parameters that we have to know precisely (conversely roughly) to increase the confidence we have in simulation results. The method is particularly useful in the design phase of the model, when the modeler wants to identify what are the important processes to consider when trying fitting the simulation output to data. Then, the modeler increases his knowledge on the mechanisms responsible for generating the model output.\n\nOne of the most interesting methods for IBM is the variance decomposition method . It consists in using the analysis of variance method (ANOVA) to study the part of variance in the output originating in the variance of the input parameters values. It is possible to study linear and nonlinear direct effects and the interaction effects between parameters. Ginot et al. also showed how local sensitivity analysis can be done for all the parameters, showing how their influence can vary over time. In epidemiology, Nsoesie et al. (2015) developed an IBM for influenza spread. They considered public health-relevant outcomes of the model: time to epidemics peak, proportion of infected people at the peak, and total attack rate. Their results suggested that small changes in some of the model parameters could significantly influence the characteristics of an epidemic.\n\nThe first objective of IBMs was clearly to consider an explicit spatial dimension at a low scale combined with the need to represent individual heterogeneity. Therefore, many different scenarios have been tested to simulate the propagation of numerous emerging pathogens, such as Ebola disease for instance (Merler et al., 2015) . For other pathogens like influenza viruses potentially pandemic, the simulation of complex spatiotemporal dynamics (Ferguson et al., 2005) may have also been used to quantify how different kind of interventions may impact this spatiotemporal dynamics (Longini et al., 2005) . Such approach has been even used in decision-making, in the case of the foot-and-mouth epidemics in the United Kingdom (Keeling et al., 2001 .\n\nAs said previously, IBMs can be used to offer a more flexible formalism than the mathematical one in the case where mathematical model could yield to combinatory explosion. It was the case for the genetic diversity of avian influenza viruses, where the environmental transmission route (i.e., infected ducks are excreting viral particles in their feces that can persist during several weeks in the environment) was suspected to play an important role. Nevertheless, the current multistrains mathematical models were involving mathematical assumptions in order to reduce the complexity of this problem that consequently underestimate transmission. While these assumptions are not problematic with human influenza viruses where direct transmission is short and intense, relying on these assumptions for environmental transmission, which operates on a longer term, would yield to a disappearance of this transmission route. Through the development of an IBM validated to reproduce the same dynamics than an equivalent mathematical one (Roche et al., 2011) , its investigation has been able to show the importance of this transmission in avian influenza genetic diversity (Roche et al., 2014) .\n\nThe Pattern-Oriented Modeling (POM) method introduced by Grimm et al. (2005) is a pragmatic method to develop IBM. It is well known in ecology. It consists in selecting observational data characterizing the processes under study, the patterns (for instance the evolution of total biomass in time if we are interested in population tree growth), and measuring the distance between observations (selecting several independent patterns) and the model output. The idea is to increment the complexity of the model until the fit with the patterns is considered acceptable (the distance between the patterns and simulation output is small). An important contribution to this method is the use of an information criterion similar to AIC to select the best model (Piou et al., 2009 ). As we said previously regarding IBM calibration, ABC can be used to optimize the model ( Van der Vaart et al., 2015) and could be combined with an information criterion to select the best model. When using the POM, the modeler selects the minimum set of processes and their formulation necessary to reproduce the field data. The use of a parsimony criterion increases the confidence we have on the ability of the model to effectively represent the processes that occur in the real system, therefore increasing the knowledge we have on it.\n\nDespite their flexibility and the increase of computing resources, there are some relevant reasons that mathematical models are still more used than IBMs. The first one is a problem of reproducibility that is inherent to the complexity of these models. For mathematical models, everything is encapsulated within a set of equations or table of rates associated with the different events. IBMs rely on complex computational code, and that is rarely made available to the whole scientific community. As a consequence, most of the results produced by these IBMs are not reproducible, which casts doubts on their validity because epidemiological outcomes could be the result of coding errors or even hidden process not explained in model description. This is even more detrimental because some specification norms (DEVS, see previously) already exist, but are not well disseminated in the epidemiological literature. This concern leads to a lack of confidence in this approach, which is extremely detrimental for the use of IBMs in many fields and not only for epidemiological studies. The effort of computational coding, which has obviously to be rewarded through scientific valorization, should nevertheless not constrain its reproducibility.\n\nThe second caveat to this approach is the rare confrontation of models simulations to real data. This is also inherent to the complexity of this model, because such comparison is challenging to conduct. It is worth pointing out that the initial goal of these models was clearly to evaluate, or at least to visualize, different scenarios, mostly of spatial propagation of some diseases. Nevertheless, the lack of confrontation with real data makes these models equivalent as some kind of thought experiments, or toy models that are impossible to validate, casting doubts on the predictions produced by them. Because the computing resources are now available to confront these simulations to real data, it would extremely relevant to develop relevant statistical framework to ensure a clear and deep evaluation of their accuracy.\n\nThe last important caveat to these IBMs relies on the difficulty to explore the whole range of parameters included in these models. Because they generally involve a high complexity, doing a sensitivity analysis could be extremely challenging and time consuming. Nevertheless, algorithms such as the Latin Hypercube Sampling (Tang, 1993) can allow an efficient exploration of this multidimensional parameter space and be combined with a decomposition of variance, in order to quantify the fraction of the variance that can be attributed to each input parameters. Today, other approaches using Monte Carlo filtering exist with the goal to identify parameter space where models outputs are significantly different from other parameter space. Because the risk of overparameterization is high with IBMs and that the parsimony principle of the modeling approach is not always satisfied, sensitivity analyses should become a standard for these models.\n\nThese IBMs can be developed with any kind of computing language, especially object-oriented ones such as C ++ or Java. Nevertheless, in order to focus only on the concepts involved in the model and then facilitate its coding (especially when complex interactions exist between the different objects), numerous platforms exist. We review briefly here three of them that are representative to what could exist.\n\nSwarm is one of the first individual-based platforms that has been made available. Historically rooted in the field of artificial life, this platform has been originally developed at the Santa Fe Institute. As a consequence, it has been developed to offer a wide range of possibilities in order to be able to simulate a large range of systems, such as social, ecological, or epidemiological ones. Coded in Java today, this platform does not have a specific interface to develop these models, but rather offers numerous libraries to facilitate task scheduling, simulation visualization, or data interface among many other possibilities.\n\nNetlogo is a platform initiated by the Northern University's Center for Connected Learning and Computer-Based Modeling. At the opposite of Swarm, its use is extremely simple through a friendly integrated interface. While it is consequently less flexible than low-scale platforms such as Swarm, NetLogo is extremely used for educational purposes because it could illustrate rapidly the possibilities offered by IBMs.\n\nGAMA is one example of a sophisticated individual-based modeling platform, involving hybrid models (models combining simultaneously numerical resolution of integrated mathematical models and IBMs), GIS integration, and parallel simulations allowing simulations of huge ecosystems (up to millions of agents) among others. Developed by the French National Research Institute for Sustainable Development (IRD), this platform is an example of intermediate platforms between the gradient offered going NetLogo to Swarm. Models are developed through a high-level language (GAML) through a dedicated interface. Therefore, it offers more flexibility than NetLogo despite more complicated to use for educational purposes, but is nevertheless more constrained than Swarm and its Java libraries.\n\nIBMs represent an intriguing opportunity in epidemiological modeling. They allow addressing epidemiological systems that are not easily addressable with classic mathematical models, such as complex spatial landscape, complex behaviors, or large pathogen assemblages among others. Nevertheless, these models have not passed the threshold of \"being an opportunity\" to become a reliable and widely used formalism. This is mostly because of the lack of validation of these models, which is deeply rooted in their complexity but that could be overcome with the increasing computing resources and sophisticated statistical framework.\n\nIt is therefore crucial to highlight that IBMs and mathematical models are not competing with each other, but rather complementary. On one hand, mathematical modeling offers a compact and synthetic way to a given problem that should be favored for simple problem. On the other hand, IBMs can go further when mathematical formalism is not adequate anymore. The emergence of hybrid models (Bobashev et al., 2007) , involving numerical resolution of ordinary differential equation, and stochastic IBMs confirms the complementarity of these different approaches. Nevertheless, it also highlights the relevance of the paradigm \"one model for one question.\""}