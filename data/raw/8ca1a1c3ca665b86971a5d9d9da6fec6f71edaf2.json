{"title": "Automatically quantifying the scientific quality and sensationalism of news records mentioning pandemics: validating a maximum entropy machine-learning model", "body": "The news media is one of the most powerful societal influences and most important sources of publicly available health information. It can significantly influence people's health-related behaviors [1] , clinical practices [2] , and policymaking processes [3] . Yet, current news coverage of health issues is not optimal. Prior studies have identified instances of health information being distorted or misreported in the news, presumably resulting in gaps among what researchers know about health issues, how journalists convey this information, and, ultimately, the reports on which health professionals, policymakers, and the public act [4e7] .\n\nAccurate health news coverage is particularly important in the context of crises like pandemics, when events are rapidly unfolding, when facts are constantly changing, and when more credible sources may be unavailable or inaccessible [8] . But news coverage is probably no better during crises and may actually be worse. A recent systematic review, which integrated findings from 13 contentanalytic studies, concluded that the news media may have dramatized the A/H1N1 influenza (H1N1) pandemic of 2009e2010 through excessive coverage and overemphasis on the threat posed by the virus rather than available selfprotection measures [9] . Initial genomic studies of the H1N1 virus were reported sensationally and in isolation without being put in the context of the larger body of research to which they contributed. Worst-case scenarios for the H1N1 pandemic were sometimes laid out theatrically without caveating possible risks with any sense of What is new?\n\nA new automated method for quantitatively evaluating the relevance, scientific quality, and sensationalism of individual news records was developed and successfully modeled, applied, and validated on a huge corpus of news records mentioning two pandemics.\n\nWhat this adds to what was known? Even rudimentary machine-learning models can accurately classify text documents for complex attributes like scientific quality and sensationalism.\n\nWhat is the implication and what should change now? Automated text analysis and machine-learning modeling represent exciting frontiers in health research and news media analysis.\n\nWith further developments, these approaches should be able to help detect performance gaps, identify problems, develop solutions, evaluate interventions, and hold organizations accountable.\n\nthe likelihood (or unlikelihood) in which they may or may not be realized. When high quality, specific information was available, the journalistic imperative of balanced coverage too often resulted in trustworthy evidence from credible scientists reported alongside ill-informed opinions from the most popular celebrities and conflicted lobbyists [10] . Similar concerns were raised following the severe acute respiratory syndrome (SARS) outbreak in 2003 [11, 12] . Likewise, the 2014 Ebola outbreak was consistently front-page news around the world for weeksd drawing unprecedented public interest (see Fig. 1 )ddespite only a single Ebola death outside of West Africa [13] . To researchers, this ''research-to-reporting gap'' and the broader ''research-to-action gap'' that it perpetuates is frustrating. But to those people who rely on the media as a primary source of health informationdthe health professionals who provide treatment, the policymakers who direct government action, and the public who make personal health decisions every daydthis gap is potentially harmful. It means people may be routinely left to act on suboptimal information and unnecessary fear, and therefore cannot make informed decisions about how to respond to pandemics.\n\nAt the very least, suboptimal media coverage of pandemics reduces capacity to quickly access, assess, adapt, and apply emerging information as it is generated, disseminate public health guidance, and coordinate responses of health system stakeholders. More broadly, suboptimal coverage can diminish public discourse on policy issues, trust in science, and accountability for decisions, thereby affecting good governance, oversight and broader principles of civic engagement and democratic responsibility [3, 14] .\n\nThis study developed a systematic and comprehensive method for automatically quantifying the scientific quality and sensationalism of news media coverage which was then validated on a corpus of news records published during the SARS and H1N1 pandemic alert periods. Scientific quality is about accurate reporting that reflects truth and avoids bias [15] . Sensationalism is a discourse strategy of presenting news as more extraordinary, interesting, or relevant than is objectively warranted [16] . Analysis of vast quantities of qualitative data is aided by advances in automatic and computer-assisted methods of extracting, organizing, and consuming knowledge from unstructured text [17, 18] . These machine-learning methods allow classifications of text documents according to user-chosen categories by applying human classifications of a small subset of documents to the rest of the documents [19] .\n\nThe ultimate goal of this study is to advance methods for ''technoregulation'' of the news media, which represents the deliberate use of technology to regulate an industry that is mostly impervious to traditional law-based regulatory mechanisms, due to, in this case, constitutional freedoms of speech and the press [20] . For example, since in most countries it is impossible to pass a law mandating high-quality news coverage, interested parties could use automated methods to continuously publish evaluations of news media organizations with the goal of incentivizing top-notch work and shaming suboptimal sources. Hopefully the development of methods for assessing news media coverage can also facilitate evaluations of interventions to improve it.\n\nThe scientific quality and sensationalism of news media coverage mentioning pandemics were evaluated by optimizing the retrieval of relevant news records, developing tools for quantitatively measuring these qualities on a random sample of news records, using a maximum entropy model for automatic unstructured text classification, and validating the classification model by measuring its accuracy. See Appendix 1 for a detailed explanation of these steps which are only summarized in the following paragraphs.\n\nNews records were retrieved from the LexisNexis database using a search protocol that was developed in consultation with a social science librarian and continually optimized over 3 stages of pilot tests to maximize sensitivity (i.e., true positives) and specificity (i.e., true negatives). LexisNexis provides access to over 15,000 sources, including over 3,000 newspapers, 2,000 magazines, and many newswires, blogs, and television broadcasting transcripts from around the world [21] . The following search was implemented to retrieve English-language records published on SARS from \n\nThe scientific quality of individual news records was quantitatively measured using an adapted version of the Index of Scientific Quality outlined in [22] . This index was used because it was the only empirically validated tool for measuring scientific quality that was found after extensive literature searches, it was devised with input from 38 research methodologists and additional journalism scholars, and it was specifically developed for evaluating health news records. The index facilitates systematic scoring of news records by integrating human ratings on five-point Likerttype scales measured along seven dimensions: (1) applicability; (2) opinions versus facts; (3) validity; (4) magnitude; (5) precision; (6) consistency; and (7) consequences. A score of ''1'' or ''2'' indicates the news record contains ''critical or extensive shortcomings,'' a score of ''3'' indicates ''potentially important but not critical shortcomings,'' and a score of ''5'' indicates ''minimal shortcomings'' [22] . Other approaches to measure scientific quality tend to rely on proxies, such as author affiliation [23] , sources of information [24] and referencing practices [25] .\n\nFor this study, the Index of Scientific Quality was slightly updated to improve clarity based on pretesting with three research assistants (RAs) and consultation with a professional copy editor. This included dropping the ''magnitude'' dimension and merging ''consistency'' and ''consequences'' into a single rating due to overlap and highly correlated responses in pretesting. Illustrative examples of statements that would militate toward different scores for the news records were added to boost interrater reliability; the unit of analysis remained the news record as a whole, which is the unit of analysis for which the Index of Scientific Quality was originally developed [22] .\n\nThe sensationalism of news records was measured using a new tool developed from a pragma-linguistic framework of five ''sensationalist illocutions''dexposing, speculating, generalizing, warning, and extollingdthat [16] identified as indicative of sensationalist reporting through surveys and focus groups. This framework was used because it facilitated direct measurement of sensationalism by conceptually identifying its facets and dividing it into discrete components. Other approaches were either: too specifically tailored for evaluating news about particular events like suicides [26, 27] and anthrax attacks [28] ; depended on elements not found in text-based databases like background music [29] and camera positions [30] ; incorporated consideration of the topic cover [31] ; used simple dictionary methods like counting intensifying adjectives [32] ; or relied on proxy indicators like newspaper page number [27] , article length [26] , and off-record attribution [28] . One previous study examined the 1918 influenza pandemic, but its approach assessed news media coverage broadly rather than measured the sensationalism of individual news records [33] .\n\nQuestions, examples, and corresponding five-point Likert-type scales were crafted to assess the five components of sensationalism identified in [16] . A score of ''1'' indicates the news record was ''not at all sensationalizing,'' a ''2'' indicates there was ''not too much sensationalizing,'' a ''3'' indicates the record was ''somewhat sensationalizing,'' a ''4'' corresponds to ''fairly sensationalizing,'' and a ''5'' means it was ''very sensationalizing.'' This means that a ''5'' is the worst score possible, unlike with scientific qualityda virtue, rather than a vicedwhere a ''5'' is the best score possible. Only minor word changes were made after pretesting. See Appendices 2 and 3 for the pretested and final tools used to measure the scientific quality and sensationalism of individual news records.\n\nThree RAs independently assessed a simple random sample of 500 retrieved LexisNexis news records, first for relevance based on whether they were actually focused on the SARS or H1N1 pandemics, and then, if so, to score them using the tools developed for measuring scientific quality and sensationalism. This sample size was chosen based on previous work that suggests the advantages of more human coding begins to experience diminishing returns at this point [14] . Disagreements on relevance were resolved by consensus. Three-rater Fleiss' kappa, intraclass correlation coefficients (ICCs), and Krippendorff's alphas were calculated to assess interrater reliability.\n\nThese 500 news records served as a ''training set'' for development of a maximum entropy model that probabilistically classifies text documents [34e36]. Maximum entropy modeling is equivalent to multinomial logistic regressiondboth using maximum likelihood estimationd albeit the two methods are derived differently. Specifically, logistic regression maximizes the log-likelihood of model parameters knowing the exponential form of posterior probability functions, which is equivalent to the dual problem of maximum entropy modeling's unconstrained optimization [37e40] .\n\nThis modeling involved a computationally intensive inductive machine-learning procedure that (1) processed the training set to remove punctuation, capitalization, non-English words, white spaces, symbols, and non-ASCII letters; (2) converted it into a document-term matrix for quantitative analysis; (3) identified relationships distinguishing the 500 news records by how the RAs assessed relevance; (4) combined these relationships as constraints into a multinomial logistic regression that best predicts records' relevance; (5) applied this regression to 10,000* randomly selected news records mentioning pandemics for determining relevance by the least biased maximum likelihood estimate on the available information; (6) repeated steps 3e5 using RA scores for a second analysis to evaluate each relevant record for scientific quality; and (7) repeated steps 3e5 using RA scores for a second analysis to evaluate sensationalism. See Fig. 2 for a flowchart of this modeling.\n\nMore simply, a statistical model was trained to predict whether news records mentioning pandemic-related terms were actually about pandemics (i.e., first application) and whether they deserved a ''1,'' ''2,'' ''3,'' ''4,'' or ''5'' score for scientific quality and for sensationalism (i.e., second application). The first application is important to boost specificity after it was sacrificed in the optimized search for greater sensitivity.\n\nMaximum entropy modeling was chosen from among the many machine-learning approaches that can be used for text analysis because it does not assume independence of terms; in future applications, this would allow the use of bigrams and phrases in modeling without the possibility of overlapping or double counting words that often appear together such as ''World Health Organization'' [34] .\n\nData processing, statistical analyses, and text classification were conducted using the MaxEnt package (v1.3.3.1) for R statistical software (v2.15.1). See Appendix 4 for R code implementing these procedures.\n\nTo assess the model's internal validity, a test set of 200 news records was randomly drawn from the corpus of retrieved news records (excluding those in the training set) and classified independently by two RAs for relevance, scientific quality, and sensationalism. The mean RA score for each relevant record in the test set was assumed to be ''correct'' and used as a benchmark against which the model's second-application classifications were judged. Two-rater Cohen's kappa, ICCs, Krippendorff's alphas, and two-way paired t-tests were calculated to assess the model's reliability. Accuracy was calculated based on the percentage of news records that the model classified the same as the two RAs.\n\nThe optimized search protocols conducted from 17 to 19 October 2013 on LexisNexis identified 89,846 news records mentioning SARS and 73,587 records mentioning H1N1, for a total of 163,433 records. RAs deemed 195 of the 500 training set records to be relevant. This means the Lex-isNexis searches yielded an estimated 63,739 news records that were actually focused on SARS or H1N1 for an estimated specificity of 39.0%.\n\nThe 195 relevant records in the training set had a mean overall scientific quality score of 3.17 and a mean overall sensationalism score of 1.81 with an overall Fleiss' kappa of 0.74, ICC of 0.98, and Krippendorff's alpha of 0.82, indicating substantial interrater reliability among RAs [41, 42] . Scores for specific dimensions of scientific quality ranged from 2.62 for validity (''not assessed or very misleading'') to 4.66 for applicability (''minimal ambiguity''). Scores for sensationalism ranged from 1.20 for extolling to 1.73 for speculating (both indicating ''minimal'' presence of these illocutions).\n\nMaximum entropy modeling of the 10,000 randomly selected news records provided revised aggregated estimates of scientific quality (total mean of overall Scores for overall sensa onalism Fig. 4 . Histograms of scores from the training set (500 records) and maximum entropy model (10,000 records). The histograms exclude those cases that were deemed irrelevant to either the SARS or H1N1 pandemics in the first screening. score 5 3.32, ranging from 2.54 for validity to 4.83 for applicability) and sensationalism (total mean of overall score 5 1.73, ranging from 1.09 for extolling to 1.88 for warning). This means the average news record had ''potentially important but not critical shortcomings'' in scientific quality with ''not too much sensationalizing.'' These records can also now be stratified at the individual record level for subgroup comparisons such as between records published about different pandemics; in this example, news coverage of the H1N1 pandemic was found to be statistically significantly better than the earlier SARS outbreak (two-sample t-test 95% confidence interval for overall scientific quality score 5 [0.0982, 0.2083], P ! 0.0001; for overall sensationalism score 5 [\u00c00.3707, \u00c00.2549], P ! 0.0001). See Figures 3 and 4 for a summary of these results.\n\nThe model performed well in the validation exercise. In the first application, the model determined relevance with 86% accuracydwhich means the model and RAs were in agreement 86% of the time on which of the 200 news records in the test set were about pandemics, and disagreed on 14% of the 200 records. In the second application, overall scientific quality was scored accurately 65% of the time (or 78% if allowing 61 deviations on the five-point scale). The model's overall sensationalism scoring was 73% accurate (or 82% if allowing 61 deviations). These statistics indicate substantial agreement between the human and computer scoring for scientific quality and sensationalism [42] . Notwithstanding errors, population-wide estimates from the model should be relatively unbiased given the histogram of misclassifications appear to be equally biased upward and downward. See Figures 5 and 6 for a summary of these results. \n\nA new method for quantitatively evaluating the relevance, scientific quality, and sensationalism of individual news records was developed and successfully modeled, applied, and validated on a huge corpus of news records mentioning two pandemics. Analyses confirmed that news media coverage of pandemics is not perfect, especially its scientific quality if not also its sensationalism. Slight improvements were observed between the SARS and H1N1 pandemics. Possible explanations for this improvement include the media learning from experience with the first pandemic and/or better crisis communications from public health authorities throughout the second pandemic. It could also be a reaction to the 2005 revision of the International Health Regulations [43\u00c045].\n\nThis study has several strengths compared to previous work in news media analysis on which it builds. First, it drew from over 15,000 sources of news records, making results more generalizable across a broader range of contexts. Second, it used pilot tests to optimize searches for maximal sensitivity. Third, it drew on existing toolsdan empirically validated index and rich pragma-linguistic frameworkd when developing new metrics for quantitatively measuring news records' scientific quality and sensationalism. Fourth, the study assessed 10,000 randomly sampled news records from a massive corpus of 163,433 records (instead of just a small sample feasible for human scoring) using recent advances in machine-learning methods and computing power. This means that population-wide estimates incorporate more information from more sources and that detailed subgroup analyses are theoretically possible given most-likely scores are available at the individual record level. Fifth, the study incorporated a relevance screening into the modeling procedures to boost specificity. Sixth, there were multiple RAs scoring the training and test set records to reduce human errors and biases. Seventh, the study validated the approach and measured its reliability and accuracy. Overall, the study showed that automated methods can quantify characteristics of news records faster (i.e., within seconds), cheaper (i.e., fewer human resources), and possibly better than humans (i.e., avoiding silly mistakes and rater drift). The specific procedure implemented in this study can at the very least identify subsets of news records that are far more likely to have particular scientific and discursive qualities.\n\nThis study also has several limitations. First, it has all the usual pitfalls of automated text classification, including its many assumptions and simplifications [12] . Second, its maximum entropy model leaves information on the table by relying on a multinomial regression that treats the scientific quality and sensationalism scores as nominal rather than ordinal data. Third, it required a substantial initial investment of human resources to score the training and test sets. Fourth, the final model was not perfectly accurate with classification errors compounded across two applications. Fifth, the performance of this particular machine-learning approach was not compared to others.\n\nThe suboptimal news media coverage of pandemics that was found in this study affirms the need to further research this problem and identify prospects for amelioration. Also needed are advances in the imperfect methods and metrics for making these assessments.\n\nSpecifically, the ability to automatically score individual news records for their scientific quality and sensationalism should be applied to track changes, make comparisons, identify outliers, find correlations, and evaluate interventions. This could include, for example, constructing day-by-day time series of these characteristics that could be stratified to compare countries, rank news media organizations, or even judge individual journalists (see Figs. 7 and 8) . Publishing rankings comparing news media organizations or journalists could encourage them to compete on quality and enhance their reporting practices. These data could also help find factors broadly associated with better news coverage (e.g., record length, readership, political affiliation) or predictive of rapid changes in scientific quality and sensationalism (e.g., new event, major announcement, public scolding of news media). Record-level data can also be used for rigorous impact evaluations such as quasi-experimental interrupted time-series analyses of interventions aimed at improving news media coverage. The feasibility of real-time analysis of news media coverage on emerging pandemics should also be explored.\n\nAnother opportunity for future research is to improve automated methods of quantitatively measuring the scientific quality and sensationalism of news records as well as other characteristics of other qualitative texts. All automated text classification methods use necessarily wrong models of text designed to help draw inferences from data; this means that diverse methods should be explored for assessing news records, and their merit evaluated according to how well they perform specific tasks, especially because more realistic or sophisticated models do not always offer better performance [19] . Methodological advances that improve models' accuracy and applicability to out-ofsample records would allow researchers to make more helpful inferences with fewer resources.\n\nNews media coverage of emerging pandemics is not as good as it should be. Developing new methods for automatically quantifying characteristics of news media coverage is an important step toward improving it. These methods represent an exciting frontier in public health research and news media analysis, because they can help detect performance gaps, identify problems, develop solutions, evaluate interventions, and hold news organizations accountable for their health reporting."}