{"title": "Epidemiology and Biostatistics 3.1 EPIDEMIOLOGY AND BIOSTATISTICS QUESTIONS", "body": "C. An investigator is trying to prove that cabbage consumption in pregnancy causes autism, so he interrogates mothers of autistic children about cabbage consumption, but not mothers of children without autism. D. A scale that records all of the patients 2500 kg more than they actually weigh E. Randomly assigning 1000 study participants to separate arms in a study 9. A new screening test is applying for FDA approval. The owners of the test say that they can increase the length of survival in patients with pancreatic cancer. Their studies show that by diagnosing it 6 months earlier than any other test, they can increase the length of survival with the disease. The FDA approval panel responds that this test has what type of bias? A. Confounding B. Hawthorne effect C. Misclassification D. Lead-time E. Length 10. A hospital administrator wants to conduct a study to investigate the benefits of a hand soap that promises to reduce iatrogenic hospital infections. She decides to enroll 400 nurses in the study. The study is designed so that half of the nurses are instructed to use the new soap, while the other half are instructed to use the old soap. To ensure that the nurses are using soap, the preventive medicine physician on staff is to randomly give the nurses a \"sniff test\" to make sure their hands smell like soap. Because the preventive medicine physician only works on the first floor, he is only able to sniff nurses that work on the first floor. The nurses on the second floor are never subjected to the sniff test. When the study results are tabulated, it is found that there is no difference between iatrogenic infections between the new soap and old soap. However, it is found that there were more iatrogenic infections on the second floor than the first.\n\nWhich answer best explains why there were more infections in patients on the second floor? A. Regression towards the mean B. Healthy worker effect C. Placebo effect D. Hawthorne effect E. Random error 11. A biochemist at a pharmaceutical company created a new drug that lowers blood pressure. He instructs the trial investigators to only include study participants that are not taking an antihypertensive and have a systolic blood pressure greater than 180. After screening by the investigator, those that met the criteria were told to return the following month for the first dose of the investigational drug.\n\nOn the day that the participants are set to start receiving the new drug, the study investigator has to disqualify many of the study participants because they no longer meet the systolic blood pressure requirements.\n\nWhat is the best explanation for the participants no longer meeting study criteria? A. Hawthorne effect B. Information bias C. Neyman bias D. Recall bias E. Regression towards the mean 12. Which of the following is a control for confounding primarily used in the analysis stage? A. Randomization B. Restriction C. Matching D. Stratification E. Hypothesis testing 13. An epidemiology student conducted a study to find the strength of association between smoking and cardiomyopathy in his community. For ease of conducting his research, the student surveyed subjects at his local bar. To his surprise, he found a higher correlation between smoking and cardiomyopathy than other published studies. Which factor is a cofounder? A. Alcohol B. Lung cancer C. Smoking D. Bias between those that took the survey and refused the survey E. There are no cofounders 14. Bayes' theorem requires which of the following to calculate?\n\nA. Sensitivity, specificity, and negative predictive value B. Specificity, false-positive rate, and incidence C. Sensitivity, specificity, and prevalence D. NPV, incidence, and prevalence E. NPV, false-positive rate, and sensitivity 15. Which of the following is the denominator of incidence density?\n\nA. Geographic region B. Person-time C. Population at risk D. Prevalence E. None of the above 16. A group of scientists believe they have found a single dose universal influenza vaccine that is resistant to antigenic drift and antigenic shift. The FDA grants them a phase 1 trial to test the safety of this new vaccination. The incidence of influenza is shown in the following What is the incidence density of Influenza after receiving the experimental influenza vaccine? A. 0.13 cases/person-year B. 0.25 cases/person-year C. 0.5 D. 3 E. 4 17. A large hospital chain plans to assess the pain scales of 100 patients in the orthopedic units of their 40 hospitals. The same 1\u00c010 scale was used for all of the patients on the day after surgery. Each patient's score was only recorded one time and there were no repeat patients that were recorded on separate visits. Which of the following hypotheses states that the hospital chain may assume their data approaches normal distribution? A. Hawthorne effect B. Central limit theorem C. Inferential statistics D. Binomial distribution E. Kaplan\u00c0Meier function 18. A mother is contacted by her son's teacher to discuss his poor academic performance. Because of this discussion, the mother hires a professional to evaluate her son's intelligence quotient (IQ). His IQ is found to be two standard deviations below the mean. You are the director of four local for-profit urgent care clinics, currently employing 11 salaried physicians. At the end of the month, you decide that you would like to recognize the physicians that have seen the most patients. Arbitrarily, you decide that the top 50th percentile of physicians deserve your acknowledgment. The physician counts are as follows:\n\nD. Numbers within confidence intervals all have equal clinical importance E. p-Values are not comparable to confidence intervals 27. 300 Medical Students are forced to take an art history course because the school administration believes that it will lead to well-rounded physicians. The final grade is based off of one test. All of those scoring higher than two deviations below the mean receive a passing score. The scores were normally distributed. The average test score for the class was 50%. The standard deviation was 10% and the highest score was 90%. Approximately what percentage of students passed the exam? A. 99% B. 98% C. 95% D. 90% E. 68% 28. A psychiatrist at a drug rehabilitation facility has noticed that 60% of her patients are recovering from alcohol abuse and 50% are recovering from opiate abuse. Some patients are recovering from both. The rest of her patients abuse a plethora of other recreational drugs.\n\nWhat is the probability that her next patient will be recovering from either alcohol or drug abuse? A young epidemiologist wishes to calculate the standardized mortality ratio (SMR) of drowning deaths in his landlocked state. Which of the following is the best way to calculate this number?\n\nA. Henry's law B. Interquartile range C. Stratification D. Direct adjustment E. Indirect adjustment 31. A preventive medicine resident contemplated reading a book full of practice questions and answers to help him prepare for the board exam. He hypothesized that reading the book would help increase his test score. He asked former residents that have taken the test about whether or not they had read the book and how well they scored. Which of the following statements represents his null hypothesis? A. Reading the book would increase his test score B. Reading the book is not associated with his test score C. Reading the book would decrease his test score D. The null hypothesis may not be developed until the p-value is available E. None of the above 32. Which of the following statements is true regarding a null hypothesis, alternative hypothesis and p-value? A. Accept the alternative hypothesis when there is a high p-value B. Accept the null hypothesis when there is a high p-value C. Accept the alternative hypothesis when there is a low p-value D. Accept the null hypothesis when there is a low p-value E. There is no relationship between the null hypothesis, alternative hypothesis and p-value 33. Which of the following accurately describes a Type I error?\n\nA. False-positive B. False null is rejected C. True null is accepted D. True null is rejected E. More than one of the above 34. Which is true of a purified protein derivative (PPD) tuberculosis test in a patient that has tuberculosis and poorly controlled HIV?\n\nA. This is a type I error because the result is likely positive when the disease is not present B. This is a type I error because the result is likely negative when the disease is present C. This is a type II error because the result is likely positive when the disease is not present D. This is a type II error because the result is likely negative when the disease is present E. There is no error, as the test will correctly identify when the disease is present 35. A professor wants to analyze test scores of his 225 students taking a biostatistics exam. If the average score of the exam is 80 A. There is a negative correlation between sunset and traffic B. There is no correlation between sunset and traffic C. There is a positive correlation between sunset and traffic D. Either sunset or traffic is causative of the other E. None of the above 41. A medical student would like to learn the relationship between heart rate and blood pressure. While in a clinic, he creates the following scatterplot of the heart rate and blood pressure of his 10 patients. According to this scatterplot, what is the relationship between systolic blood pressure and heart rate? A. Negative exponential B. Negative linear C. No correlation D. Positive exponential E. Positive linear 42. A teenager believes that the severity of his acne vulgaris is related to the number of hours he works at a fast food restaurant. Every Sunday for 5 months, he meticulously counts the number of comedones on his face and records the number of hours he worked the week prior. His older sister, an epidemiology student, helps him calculate the Pearson correlation coefficient of 0.5 for the correlation of hours worked and number of comedones. Which answer best describes the relation of this teenager's number of hours worked and severity of acne? A. In looking at the favorite ice cream by gender, the professor feels that there are distinct differences between men and women in his class. He decides to conduct a chi-squared (\u03c7 2 \u00de analysis to test his hypothesis. How many degrees of freedom (df ) are there in this \u03c7 2 analysis?\n\nA county health department has recently begun a new fitness initiative by installing outdoor gyms complete with public exercise equipment in local parks. Several months after constructing the newest outdoor gym, an employee of the health department returned to the site to reevaluate the project and research improvements for the next project. To create objective data, the health department employee kept records of which genders exclusively used one of the two types of exercise equipment: Aerobic (outdoor treadmills and exercise bicycles) and anaerobic (weight resistance). People that used both aerobic and anaerobic equipment were not recorded.\n\nThe results are as follows: After looking at the data, the health department employee hypothesizes that different genders prefer to use different types of gym equipment. He decided to use chi-squared (\u03c7 2 ) analysis to determine if this difference is due to chance alone. What is the approximate test statistic that the employee will use to compare to the critical value to accept or reject the null hypothesis? Week 3\n\nWeek 4\n\nWeek 5\n\nWeek 6\n\nWeek 7\n\nWeek 8\n\nWeek 9\n\nMonday Better Worse Better Better Better Better Worse Better Better Friday Worse Better Worse Worse Worse Worse Better Worse Worse Assuming that the two classes are identical, outside of the date they go to class and take tests, which test would be most appropriate to analyze this data? Which food is most likely the cause of the diarrheal illness? A. Chicken tender B. Burger C. Hotdog D. Watermelon E. Egg roll 91. Jane is a registered nurse that traveled to Africa to assist in the containment of a suspected smallpox outbreak. At the end of her second day in the field, Jane noticed a gaping hole in her personal protective equipment. Earlier that day, she was directly exposed to a young man with symptoms that appeared to be due to smallpox. When she reports the exposure to another investigator the following morning, the National Institute of Occupational Safety and Health (NIOSH) wishes to conduct an analysis between the link of Chemical X and brain cancer. A retrospective cohort study is conducted in 300 former employees. 100 of them were exposed to Chemical X. 15% of those exposed to Chemical X had received a diagnosis of brain cancer. Meanwhile, 10% of those that were not exposed to Chemical X had received a diagnosis of brain cancer. What is the relative risk (RR) of diagnosis of brain cancer in those exposed to Chemical X, compared to those unexposed to Chemical X?\n\nA case-control study is analyzed with which of the following tools?\n\nA. Attack rate B. Cox regression analysis C. Odds ratio D. Meta-analysis E. Risk ratio 127. What is the name of the program conducted by the CDC in conjunction with state health departments to monitor trends in women and infant care by surveying pregnant women and women that have just given birth? In which case is the odds ratio a poor estimate of the risk ratio?\n\nA. A case-control study is used to yield the odds ratio B. If the prevalence is .10% C. The total number of subjects is .100 D. The outcome is rare E. When causation has been established 129. A body builder believes that the trendy unregulated supplement he has been taking is responsible for his constipation. He asks his friend, a biostatistician, to look into this claim. The biostatistician decides to hold a study to quantify the relationship between this supplement and constipation. He randomly samples 120 gym members with similar demographics. Fifty-five of those surveyed were also on the supplement. Of those on the supplement, seven had constipation. Of the gym members not on the supplement, 13 had constipation.\n\nThe data can be filled into the following 2 3 2 A. Necessary and sufficient B. Necessary and not sufficient C. Not necessary and not sufficient D. Not necessary and sufficient E. None of the above 131. Healthcare is typically _____ for improved health status when one has an infectious health ailment such as HIV. Which of the following options best answers this question? A. Efficient B. Sufficient C. Necessary D. Sufficient and necessary E. None of the above 132. Which of the following conditions is classified as nondifferential error? A. A case-control study where exposed subjects are misclassified as unexposed and a similar number of unexposed are classified as exposed B. A case-control study where exposed subjects are misclassified as unexposed, but no unexposed subjects are misclassified C. Interviewing mothers of children with birth defects about chemical exposures in pregnancy D. Study participants receiving the experimental drug dropping out of a study due to adverse effects, while subjects in the placebo group remain in the study E. None of the above 133. A study selects 1800 men aged 57\u00c060 for a trial of a new type of cholesterol drug. 89% of the subjects on the drug are found to have decreased cholesterol with an alpha value of 0.01, averaging a 30 mg/dL decrease from those on the placebo. Over time, the risk of cardiovascular events was reduced 15% compared to those in the placebo group. When considering whether to approve this drug for all people at risk of dyslipidemia, which of the following might the US Food and Drug Administration (FDA) say about this study? A. The drug has poor clinical significance B. The trial lacks internal validity C. The trial lacks external validity D. The trial is not statistically significant E. There is no need for new cholesterol medications 134. Which of the following contributes to the difference between vaccine effectiveness and vaccine efficacy? A. Antivaccination public sentiment B. Limited access to vaccinations C. Improper storage of a vaccine D. Prohibitive cost of the vaccine E. All of the above 135. A researcher has proposed a formula to predict the number of soldiers that develop post-traumatic stress disorder (PTSD) based on their wartime duties. Prior to initiating the study, the researcher wishes to see how accurate the formula is if the expected parameters were to vary. What is the name of the process used to accomplish this? A. Attributable risk B. Data organization C. Sensitivity analysis D. Standard deviation E. None of the above\n\nA. Infectivity B. Immunogenicity C. Pathogenicity D. Secondary attack rate E. Virulence 137. As the director of a local health department, the director of the emergency preparedness division submits a report to you about an abrupt increase in the number of influenza-like-illness (ILI) cases reported by local clinics and health departments. Additionally, the report cites increased purchasing of over the counter cold remedy medications from local pharmacies. Local clinicians report an increase in the number of confirmed influenza cases. After speaking with a representative at the CDC, it appears that the influenza virus has mutated to a form that is not covered by the annual vaccination. Furthermore, it has turned into a strain that only those older than 45 years old demonstrate any immunity.\n\nWhat is the best explanation for this influenza epidemic? A. Antigenic drift B. Antigenic shift C. Malaria coinfection D. Recall bias E. Resistance to neuraminidase inhibitor 138. A geographic territory experiences an outbreak of laboratory confirmed influenza in chickens. The prevalence of humans diagnosed with influenza is unchanged. Prevalence of influenza in chickens had been stable for the prior two decades. The surrounding region has not noticed a change in influenza incidence in humans or animals. How is this outbreak in chickens best categorized?\n\nWhat is the first step in an epidemiologic investigation?\n\nA. What is a potential concern of the IRB?\n\nA. The test may significantly alter the way that clinicians screen for this disease B. The company that manufactures the treatment may suffer financial loss C. It would be unethical to not screen patients for a curable disease D. Harms exceeding benefits is not a legitimate concern to investigate E. \n\nCross-sectional studies look at a snapshot of the population being studied. Extrapolating the population findings to an individual level may lead to ecological fallacy, in which an association at the population level is not necessarily true at the individual level. This is especially true when there is a larger population (constituting a cross-sectional ecological study). For example, a cross-sectional ecological study showing that City B has a higher rate of mesothelioma than City C may falsely lead someone to believe that all residents of City B are more likely than residents of City C to get mesothelioma, regardless of asbestos exposure.\n\nA RCT allows the investigator control over the exposure. Although an RCT would yield the most robust results, it would also be considered unethical to withhold a screening tool that has been shown by multiple previous studies to be effective. A cohort study would involve enrolling study participants based off their exposure, in this case whether or not he/she had a screening colonoscopy. Information gained from a cohort study can be used to compare case fatality ratios and determine how effective an intervention is.\n\nA case-control study categorizes study participants based off of their disease status rather than their exposure status and is thus less appropriate for this example.\n\nThe vital statistics sector of each state's department of health records birth, death, marriage, and divorce. Cancer is typically reported through registries recorded at health facilities. Hospital cancer registries send their records to the central cancer registry in its state. The state cancer registry will then submit it to the CDC's National Cancer Institute's Surveillance, Epidemiology, and End Results (SEER) Program.\n\nThe NCHS compiles statistical information in numerous categories from numerous sources (states, municipalities, private organizations, etc.). These statistics are used to guide public health decision making and create goals, such as the Healthy People program. In addition to storing health statistics, NCHS also collects health data. The NHANES is hosted within NCHS.\n\nWhen a study has subjective outcomes, such as wrinkle reduction, blinding the parties is used to eliminate bias. Single-blinded studies blind the study participants. Double-blinded studies blind the study participants and the study investigators. Triple-blinded studies blind the study participants, study investigators, and the statisticians. Blinding is less common in objective studies, such as those recording lab results, but it still may be beneficial. The IRB is the group that approves studies, mainly based on how ethical and feasible they are. There is no need to blind this group. Shareholders should not have influence over the internal workings of the study, so there is no need to blind this group either.\n\nThe main difference between an observational study and a controlled study is that controlled studies will manipulate the risk factor. In a randomized controlled trial (RCT), exposure to the risk factor is determined by those conducting the study; thus, it is an experimental study and not an observational one.\n\nLength bias occurs when a less aggressive disease appears to have a higher incidence. This is because slower moving diseases are more likely to be detected since the subject is alive for longer. To the contrary, diseases that cause mortality sooner are less likely to be detected. Length bias is often confused with lead-time bias. Lead-time bias occurs when the diagnosis is made earlier and creates the illusion that the subject lived longer than if the diagnosis were made later. If a subject with a terminal disease is diagnosed 1 month earlier, he will still die at the same time. However, the records will indicate that he lived one month later with the earlier diagnosis. 8. E. Randomly assigning 1000 study participants to separate arms in a study Nondifferential error/bias is also called random error, or chance error. If a sample is has equal amounts of error on both sides of the true value, the error will cancel out and the overall value will closely approximate the true value. Differential error/bias produces deviation in one direction from the true value, either above or below. Answers A, B, C, and D are all examples of differential bias. All of these answers target only one of the two populations that should be interrogated equally. In answer \"A,\" only the men and not the women are questioned about marital happiness. In answer \"B,\" the CHF patients that have been discharged will not be questioned regarding their experience. The patients that have been discharged sooner may have a different perception of their hospital stay. In answer \"C,\" the researcher contributes to recall bias by pressuring mothers of autistic children, but not the nonautistic. Mothers of children with mental or physical disabilities are more likely to reflect more heavily on their exposures and activities during pregnancy. In answer \"D,\" the scale produces a differential bias by pushing the observed value in one direction away from the true value.\n\nOnly answer E is an example of nondifferential bias. Participants are randomized equally, without bias.\n\nLead-time bias is the appearance that early diagnosis of a disease prolongs survival with that disease. In this case, the FDA panel should be concerned that the early diagnosis of pancreatic cancer is not actually increasing the length of time that the patients live.\n\nConfounding is not a type of bias. Confounding occurs when there is a variable interacting with the independent variable (exposure) and dependent variable (outcome).\n\nThe Hawthorne effect is also known as observer bias. It is the theory that people (including study participants) will change their behavior if they believe that they are being observed.\n\nMisclassification bias occurs where there are errors in recording disease or exposure.\n\nLength bias occurs when the prevalence of a disease with a longer lasting disease appears higher than the prevalence of shorter lasting diseases. Consider \"Disease X,\" which lasts 1 month and \"Disease Y,\" which lasts for 6 months. There are 5 months of extra opportunity for \"Disease Y\" to be discovered, leading to the appearance of a higher prevalence compared to \"Disease X.\"\n\nThe Hawthorne effect states that individual behavior is changes when a person is aware they are being observed. In this case, the nurses are more likely to use the soap because they are being observed. However, the nurses on the first floor are even more likely to use soap because there is a higher risk of being discovered to not be in compliance with study parameters. Regression towards the mean states that the further a value is from the mean, the more likely future recordings are closer to the mean. For example, if an otherwise healthy patient presents to your clinic and is found to have high blood pressure, future blood pressure readings are expected to be closer to the true blood pressure.\n\nThe healthy worker effect states that workers are typically healthier than the general population because they are different from the general population, as ill and disabled people are typically unemployed.\n\nThe placebo effect, occurs when a person believes they are healthier because they are receiving treatment, even if the treatment is not scientifically effective.\n\nRandom error is an accepted discrepancy in clinical studies. It may be controlled for in all phases of study design. There is no reason to believe that random error is the source of the findings in this vignette.\n\nRegression towards the mean states that the further a value is from the mean, the more likely future recordings are closer to the mean. Systolic blood pressure is not a static measurement: It varies daily, and those recorded at outlier measures are more likely to be at the extreme for their norm. Over time, an individuals recorded blood pressures will average out to more accurately show their mean. The Hawthorne effect is the observation that individual behavior changes once that individual is aware they are being observed.\n\nInformation bias is the use of erroneous study data and may result from imprecise and invalid study measures.\n\nNeyman bias (also known as selective survival bias), occurs when cases in a study that survive have different exposures than those that die.\n\nRecall bias occurs when those that suffer an adverse event recall their exposure history differently than those that did not suffer an event. A common example is that mothers of children with birth anomalies may recall their pregnancy differently than mothers of healthy children.\n\nStratification is the only technique listed that reduces confounding during the analysis stage. It involves breaking the data into strata that can be more descriptive. For example, stratification of elementary school students would reveal that 3 rd graders have higher understanding of math than 1st graders, but less than 5th graders. Randomization, restriction, and matching are all techniques to reduce confounding during the design stage of a study. Hypothesis testing is not a method used to control for confounding.\n\nConfounding is a type of bias that occurs when a third variable influences (confounds with) the factor of interest and skews the observed result between exposure and disease. A confounding variable by definition must be associated with both the outcome and exposure. In this example, alcohol use is a confounder, as it is a risk factor for cardiovascular disease and is associated with cigarette smoking. Confounding may be accounted for and controlled in studies. Controls to reduce confounding may be built into the design stage, or analysis stage of a study. Ways to control for confounding in the design stage include randomization, restriction, and matching. Ways to control for confounding in the analysis stage include standardization, stratification, and statistical modeling.\n\nBayes' theorem is a mathematical tool for figuring out the probability of an event. It can be calculated every time new information is received that may alter the probability of the event.\n\nOnce a prior probability is known the next sequential event will yield a posterior probability. When more information is learned, the former posterior probability becomes the prior probability and a new posterior probability is calculated. Prevalence is equivalent to a prior probability. Bayes' theorem can be considered an alternative method of calculating the PPV.\n\nThe following formula may be used in place of Bayes' theorem:\n\nThis formula may be difficult to remember, but the prevalence, specificity, and sensitivity can be used to fill out the cells of a 2 3 2 table, which will yield the PPV.\n\nIncidence density is a tool used to describe the number of new cases of a disease (incidence) per summation of time that each person is at risk of disease in a specified time and place. It is useful for observing dynamic populations (including clinical trials), where people are entering the leaving the risk pool. Additionally, it allows for each subject to be counted in the numerator more than once. This is important in cases where a subject experiences the disease of interest more than once.\n\nFor example, a researcher may be interested in observing a daycare to see how frequently children develop conjunctivitis over a 3-year period. In this setting, not all children are in the daycare for the same amount of time. Moreover, some children contract conjunctivitis more than once. If one child attends the daycare daily for 36 months and another is only there for 6 months, the overall person-time would be 42 months.\n\nThe downside of person-years is that a few amount of subjects may substantially influence the incidence density. This happens when a small number of people are observed over a long period of time are calculated along with a large number of people observed for a short duration.\n\nRefer to Answer #15 (directly above) for an explanation of incidence density.\n\nThe time period of observation in this vignette is measured in years. Altogether, the six patients combined for 16 years of observation. During that time period, there were four cases of influenza. The central limit theorem states that when there are a large amount of mutually independent random variables, the mean population will approach normal distribution. A general rule of thumb is that for the central limit theorem to hold true, N $ 30. The Hawthorne effect is the phenomenon where subjects change their behavior because they are aware that they are being observed.\n\nInferential statistics methods allow one to make a statement about the general population by studying a smaller part of that population.\n\nBinomial distribution describes data that has two discrete outcomes, typically success or failure.\n\nKaplan\u00c0Meier function is a tool used to examine survival analysis.\n\nThe IQ is a tool for gauging intellectual abilities. It is determined through standardized testing to calculate the mental age, which is then divided by the chronological age and multiplied by 100. IQ was designed to follow a normal distribution with the central mean at 100 and the standard deviation of 15. As always, 68% of people fall within one standard deviation of the mean (85\u00c0115), 95% fall within two standard deviations of the norm (70\u00c0130), and 99% fall within three standard deviations of the norm (55\u00c0155). Observations more than two standard deviations from the mean are typically considered to be abnormal. Two standard deviations below 100 is 70. An IQ of 70 nearly meets criteria for intellectual disability from the International Classification of Diseases (ICD) and the Diagnostic and Statistical Manual of Mental Disorders (DSM) categorize intellectual disability as follows: Mild (50\u00c069), moderate (35\u00c049), severe (20\u00c034), and profound ( . 20). Individuals with intellectual disability experience reduced intellect and impaired adaptive functioning.\n\nThe Z-score describes how many standard deviations are between an observed value and the mean. Observed values that are larger than the mean will have positive Z-scores, while observed values that are less than the mean will have negative Z-scores. It can be calculated through the following formula:\n\nZ-score 5 x 2 \u03bc \u03c3 x is the observed value \u03bc is the mean of the distribution \u03c3 is the standard deviation To solve this problem, it is important to understand that the IQ scale was designed to follow a normal distribution with the central mean at 100 and the standard deviation of 15.\n\nThe Z-score for an IQ of 105 is calculated as follows:\n\nZ-score IQ 105 5 105 2 100 15 5 5 15 5 0:334\n\nIn this set, 3000 is the outlier. This outlier would greatly skew the mean. With the full set, the mean is 281.8. Without the value of 3000 contributing to the mean, the overall mean drops to 10. The geometric mean is typically used for extremely large numbers. It is most often used in logarithmic fashion. The geometric mean is found by multiplying all of the numbers (n) in the set and then taking the nth root of the product. Another method of calculating the geometric mean involves converting all of the numbers in a set into a logarithmic scale. The geometric mean cannot be used on negative numbers, or the number zero.\n\nTo answer this question, it is important to understand the definition of obesity. Obesity is determined based on the body mass index (BMI), which is calculated as a person's weight in kilograms (kg) divided by the person's height in meters squared (m 2 ). Five of the ten (50%) people filing claims are obese.\n\nIf a distribution is skewed to one side, there are fewer events on that side of the curve. This means that there are more events on the other side of the curve. An easy way to remember this is that skew rhymes with few.\n\nIn a skewed distribution, relative positions of the measures of central tendency is constant. From the tail (side with few) to the peak, the order is mean, median, and mode, as is shown in the diagram.\n\nTo solve this problem, you must first put the physicians in order of the number of patients they saw: 150; 220; 250; 250; 300; 315; 335; 360; 390; 400; 410 After placing the numbers in ascending order, you must then multiply the number of subjects by the percentage sought. This leads to 0.50 3 11 5 5.5. After rounding this number up, you get the 6th number. The 6th number in the ascending series is 315. Therefore, 315 represents the 50th percentile. The 50th percentile is also called the median.\n\nThe same method of calculation can be used for all percentiles, not just the 50th. As there is no universally correct consensus on how to measure percentiles and quantiles, some numbers may slightly vary by mathematician.\n\nTo calculate the interquartile range (the middle 50%), subtract the number found at the 25th percentile from the 75th percentile.\n\nAs the number of subjects increases, the confidence interval becomes smaller. This is because the confidence interval is calculated by the formula x 6 Z-score \u00c0 S:D: ffiffi n p \u00c1 , where n represent the number of trials. S:D: ffiffi n p represents the standard error of the mean (S.E.M), which describes how far a population mean varies from the true mean. Since n is in the denominator, the S.E.M. will decrease as n increases. As the S.E.M. decreases, the confidence interval decreases.\n\nThe NNT is a function of the attributable risk and is unrelated to the number of overall subjects.\n\nAs the number of subjects increases, the power increases. Clinical relevance of a study is unrelated to the number of subjects.\n\nAs the prevalence of a disease increases, the PPV increases. Thus, the PPV will only increase if the number of subjects with the disease of interest increases.\n\nA. If the drug is tested on 1000 similar sample populations, 950 would lose a mean between 1 and 39 lbs.\n\nThe true value of this weight loss medication may never be known. However, confidence intervals provide a range of values that are believed to include this value. Confidence intervals are often preferred to p-values because they convey more information. Using the information presented in the vignette, the reader is not only able to tell that the average weight loss is 20 lbs, the reader is also able to know that if this subject is repeated in a similar group 95% of the time, the average weight loss will fall between 1 and 39 lbs. Confidence intervals are used to describe parameters of the study population, not the individual subjects. If the confidence interval characterizing a risk difference does not include 0, the findings are said to be statistically significant. This is because if 0 is not included in the confidence interval, less than 5% of similar studies would have a mean of 0. A risk difference of 0 means that there is no change. Similarly, if a confidence interval describing an odds ratio (OR) or risk ratio (RR) includes 1, the results are not statistically significant. This vignette describes the risk difference, not the OR or RR.\n\nClinical significance depends on the situation and the person interpreting the data. A 1 lb weight loss may not be considered clinically significant in a 250 lbs person, but a 39 lbs weight loss in a 250 lbs person is significant.\n\nThe true value of an intervention within a population may never be known. Confidence intervals provide a range of values that are believed to include this value. They are used to describe parameters of the study population, not the individual subjects. A confidence interval gives the likelihood of future studies to yield a range of results. A 95% confidence interval means that 95 out of 100 studies in similar groups will yield a population statistic that falls within the confidence interval. Unlike p-values, confidence intervals are expressed in units. Confidence intervals are often preferred to p-values because they convey more information. Both confidence intervals and p-values express a degree of certainty, which may be converted from one approach to the other. For example, a 95% confidence interval is the same as a p-value of 0.05.\n\nIf the confidence interval characterizing a risk difference does not include 0, the findings are said to be statistically significant. This is because if 0 is not included in the confidence interval, less than 5% of similar studies would have a mean of 0. A risk difference of 0 means that there is no change. Similarly, if a confidence interval describing an odds ratio (OR) or risk ratio (RR) includes 1, the results are not statistically significant.\n\nConfidence intervals measure precision around a point estimate. Larger studies are more precise and yield more narrow intervals. In homogenous confidence intervals, all values carry equal importance. However, in heterogeneous confidence intervals, certain values hold more significance than others.\n\n97.5% of the medical students passed the exam. To better understand this question, one must understand standard deviation within the normal distribution. When the count is normally distributed, 68% will fall within one standard deviation. Half of these (34%) will be greater than the mean and the other half (34%) wall fall below the mean. Roughly 96% of the count will fall between two standard deviations. Half of the 96% (48%) will fall above the mean and half below. The remaining 4% falls outside of the two standard deviations, with 2% above and 2% below the cutoff.\n\nThe following illustration demonstrates the distribution of the test scores. All of the students that received a score greater than two standard deviations below the mean have been shaded in gray. Adding together all of the gray segments, one can find that 98% of the medical students passed their art history course. They are now able to continue their medical education and take a course in geography. This question requires the understanding of probabilities. For any event, the highest possible probability is 1.0, while the lowest possible probability is 0. If the number of alcohol (0.6) and opiate (0.5) abusers are added together, the number would be 1.10, which is not a possible percentage. This addition does not account for overlap in the group that abuse both alcohol and opiates. If alcohol and opiate abuse were mutually exclusive, adding these two numbers would yield the correct answer.\n\nTo find the probability that the next patient abuses either alcohol or opiates, the Rule of Addition must be used. This requires the addition of the events of interest (alcohol and opiate abuse), minus the common overlap between events. Therefore the answer to this question would be calculated by: 0:6 1 0:5 2 0:3 5 0:8 There is a 0.8 probability that the next patient will abuse either alcohol or opiates. 29 Calculating the standardized mortality ratio (SMR) is a method dividing the total number of observed deaths in a population to the total number of expected deaths in a population. The number is usually multiplied by 100, leaving the standard population with a value of 100. If the observed number of deaths is greater than the expected number of deaths, the SMR will be .1.\n\nAdjustment produces fictional numbers that can be used to compare populations with different variables. There are two forms of adjustment: Direct adjustment and indirect adjustment. Direct adjustment requires a second population from the original, which is used to extrapolate rates that create a less biased comparison. Meanwhile, indirect rates are performed when there is no comparison population, so a standard population must be used to accomplish the same goal. The standardized mortality ratio (SMR) is a form of indirect adjustment used to evaluate the actual versus the expected ratio of deaths and compare this metric between populations.\n\nObserved Number of Deaths Expected Number of Deaths 3 100\n\nAn SMR value of 1 indicates that the number of observed deaths is what is expected. Meanwhile, an SMR greater than 1 indicates there have been more observed deaths than expected. Finally, an SMR less than 1 indicates that there have been less observed deaths than expected.\n\nThe SMR of each population can be compared to others, while holding the variable of concern constant, in order to determine if the outcome of interest (death) is different between populations.\n\nNone of the other options are directly related to the SMR.\n\nDeveloping a hypothesis is the first step to answering a statistical question. For any given investigation, there is either an association between the variables, or there is not. A null hypothesis (H 0 ) assumes that there is no difference between the variables being tested. To the contrary, an alternative hypothesis (H A ) assumes that there is a difference between the variables. The alternative hypothesis may be considered the opposite of the null hypothesis. The null hypothesis is assumed to be true unless stated otherwise. The purpose of a hypothesis test is to determine if the sample results of a study provide enough evidence against the null that it is likely the null would be false in the target population. Once the null is rejected, the alternate hypothesis is accepted as true. If the null cannot be rejected, it does not mean that the null is accepted as true. This is because data that is insufficient to show that a difference between variables is zero does not prove that the difference is zero.\n\nIn this case, the null hypothesis would assume that there is no association between reading the book and test score. Meanwhile, alternative hypothesis states that there is an association between reading the book and test score. 32. C. Accept the alternative hypothesis when there is a low p-value\n\nThe answer builds upon the explanation of hypothesis testing from Answer #31 (directly above). The strength of evidence to accept or reject the null hypothesis is calculated as the p-value. It estimates the probability of finding an association in the target population as large as the association found in the sample, while assuming that the null hypothesis is true. A small p-value means that the association found in the sample is unlikely to be due to chance. Furthermore, the null and alternative hypothesis are differentiated by an artificial cut point, known as the significance level. If the p-value is less than the significance level, the null hypothesis is rejected and the alternative hypothesis is accepted. Because it is considered to be a measure of strength of evidence against the null, the p-value should not be used to infer whether or not the null is true or false. 33. E. More than one of the above A type I error occurs when a null hypothesis is rejected when it is actually true. It is frequently called a false-positive. If the true null is rejected, that means the alternative hypothesis may falsely accepted when the association may be due to chance. The probability of making a type 1 error is represented by \u03b1.\n\nA type II error occurs when a false null hypothesis is not rejected, while the alternative hypothesis is true. This is known as a falsenegative. The probability of making this type of error is represented by \u03b2. The power (1 2 \u03b2) of a study is the likelihood of committing a type II error.\n\nThis question requires understanding the concept of anergy. Anergy refers to an inadequate immune response and may be a result of several variables including: The age of the patient, the overall health of the patient, immunosuppression, and many other factors. In this problem, the poorly controlled HIV status implies that the patient is immunosuppressed and incapable of demonstrating a response to the PPD. Therefore, the PPD will be negative, despite the patient actually having tuberculosis. This question is an example of a Type II error. Also known as false-negative error and beta error, type II error occurs when something is declared as false, when it is actually true. This type of error may occur in cases of anergy and testing within the window period, amongst many other examples.\n\nA type I error occurs when something is declared as true, when it is actually false. This type of error is also known as false-positive error and alpha error. An example of a type I error would be a positive PPD due to nontuberculosis mycobacterium exposure. In this instance, a positive PPD would indicate that a patient has been exposed to tuberculosis, when they have not. Another example of a type I error would be a positive RPR while testing for syphilis in a patient that has Lyme disease, lupus, malaria, or is pregnant.\n\nThe confidence interval is derived by using the formula:\n\nConfidence Interval \u00f0Z-score\u00de 5 x 1 Z\u00c0score SD ffiffi ffi n p\n\nWhere: -x is the mean -SD ffiffi \n\nAs the Prevalence of the disease in the population increases, the PPV increases. Alternatively, when the prevalence increases, the NPV decreases. 37. D. Increasing the null from 65th to 75th percentile\n\nThe power of a test is the probability of correctly rejecting the null hypothesis. A study with insufficient power may not detect and accurately identify an important causative effect. Increasing the number of study participants always increases the power. Power can be represented by the equation: Power 5 1 2 beta, where beta represents the probability of rejecting the null when the null is actually true. Therefore, as beta increases, power decreases (answer C). Increasing the threshold of the null hypothesis (answer D) means that the null is more likely to be rejected, thus increasing power. As the difference between the alternative and mean hypothesis increases, the power will also increase. A higher alpha level will result in rejecting the null more often, thus increasing power (answer B). 38. E. All of the above are considerations when calculating sample size Determining minimum sample size for a study is an important stage in the planning process. Having a large enough sample size is important for obtaining power and detecting a clinically meaningful differences with statistical assurance. On the other end, having a large sample size is expensive and exhausts resources. A common goal of clinical researchers is to find the minimum number of study participants necessary to yield meaningful results that are valid, accurate, reliable, and have integrity.\n\nEach study has special considerations when determining the sample size. The type of study and the hypothesis being tested are primary considerations. Other important variables include the degree of precision desired, expected attrition (dropout) rates, size of population under investigation, and the method of sampling adopted.\n\nThe larger the sample size, the greater the accuracy and precision. Specific types of studies may anticipate larger attrition. There should be enough study participants to counteract the expected attrition. Larger populations call for larger sample sizes to represent enough of the population to produce accurate study conclusions. Finally, sample size depends on the type of sampling being used. For example, randomly drawn studies will require a larger sample size than a stratified sampling plan.\n\nSample size calculation should be deliberated with consideration to precision analysis, power analysis, and probability assessment. A large part of calculations are based upon criteria for controlling type I and type II errors. Sample size calculation is typically performed in stages: Size estimation/determination, sample size justification, sample size adjustment, and sample size re-estimation. Each stage carried specific considerations. For example, the adjustment stage must consider factors such as expected attrition rate and covariates.\n\nThis equation is the key to performing multivariable analysis, which is used to understand the relationship that different independent variables interact upon a dependent variable. This type of analysis is useful for showing the change in a dependent variable when one or more independent variable changes. In y 5 a 1 b 1 x 1 1 b 2 x 2 1 e; y 5 dependent variable a 5 regression constant, the starting point where independent variables begin to act on the dependent variable b 5 adjustment coefficient that weighs different independent variables according to importance\n\nx 5 independent variable e 5 error An example of multivariate analysis would be the understanding of how grade point average (GPA), physics test scores, biology test scores, and chemistry test scores correlate with medical school entrance exam scores. If the entrance exam emphasizes biology most of all, the biology test scores will hold a higher adjustment coefficient. \n\nThe variable (r) describes the linear correlation between two quantitative variables. Potential values of r span from 21 to 11. If r , 0, the correlation is negative. If r 5 0, there is no correlation. If r . 0, the correlation is positive. Answer \"D\" is not correct because correlation does not equal causation. In this example, it is unlikely that the sun setting causes more traffic, or that an increase in traffic causes the sun to set. It is more likely that rush hour occurs during the time the sun is setting.\n\nA line of best fit can be used to help identify a linear relationship between variables. When inserted into the scatterplot from the vignette, it reveals a positive linear relationship. This question is an example of correlation analysis. When the investigator has control over the independent variable, it is known as regression analysis.\n\nPearson correlation coefficient is a tool used to estimate the strength of a linear relationship between two normally distributed variables. It is represented by the r value, which varies from 21 to 11. Values closer to 21 have a negative association. That is, when one value decreases, the other increases. In contrast, values closer to 11 have a positive association, meaning both variables increase together. When r is 0, there is no association between the variables.\n\nMultiple regression is a method for examining how a normally distributed dependent variable is influenced by two or more continuous independent variables. If performed correctly, it allows researchers to assess the impact of one variable while controlling others. Multiple regression may be viewed as an extension of simple linear regression.\n\nSurvival analysis is used to determine the outcome of dichotomous variables, including live/die and success/failure. The actuarial method of survival analysis is used to determine the number of survivors in fixed time intervals, such as years and months. A new line of the table is created for every fixed time period. Because of the set time periods, this method as not as good as the Kaplan\u00c0Meier method for accounting for censorship and loss to follow-up. This method is useful medical research and the insurance industries. It may be easier to apply this method if the sample size is large.\n\nThe Kaplan\u00c0Meier method of survival analysis does not have fixed time intervals. A new line of the life table is calculated for every new death. During death free intervals, study participants may be removed from the denominator if these participants are censored or lost to follow-up. This allows for more accurate computation of survival rates. It is easier to apply this method if the sample size is small.\n\nA funnel plot is a tool used to evaluate for publication bias. Publication bias occurs when the results of published studies are different than the results of unpublished studies. There is a tendency of studies demonstrating certain opinions to be published, while studies demonstrating the contrary opinion go unpublished. Consider an example of a study that demonstrates colon cancer screening actually increases the incidence of colon cancer. The researcher may more apprehensive to publish such a study that bucks conventional understanding and may provide a danger to the public. Funnel plots populate a \"funnel\" of expectations around the mean. Gaps in the funnel may suggest publication bias.\n\nMeta-analysis studies are composed of numerous studies and have an inherent risk of publication bias.\n\nThis question is asking about the NNT, the number of people who would need to be treated to benefit one person. It is calculated by the following equation: \n\nThis question is intended to test understanding of statistical inference and validity. Statistical inference is the practice of making general characterizations after analyzing a sample. Part of exercising statistical inference is the assessment of validity. Internal validity describes how well a study represents true associations present within the study. It is dependent on how well the design, data collection, and analysis is performed. Bias and random variation can reduce internal validity.\n\nThere is nothing to suggest that the study in the vignette lacks internal validity. External validity describes how well results of one study are generalizable to a different population. Due to the small number of subjects, this study lacks external validity and should not be used to infer that patients worldwide will experience similar results. A study may be statistically significant with or without biological or scientific significance. If this study is found to be internally valid, the weight loss medication not promoting weight loss would be considered clinically significant.\n\nDegrees of freedom (df ) is calculated by the following formula: Degrees of Freedom 5 \u00f0Rows 2 1\u00de\u00f0Columns 2 1\u00de In this problem, there are two rows and three columns.\n\nDegrees of Freedom 5 2 2 1 \u00f0 \u00de 3 2 1 \u00f0 \u00de5 2 51. D. 57 Chi-squared is the most commonly used nonparametric test and is best for hypothesis testing between categorical variables. When used appropriately, it tells an investigator whether observations are correlated, or if they are due to chance. After rounding, the test statistic is 57. Because of rounding, other people calculating this question may reach a slightly different test statistic.\n\nChi-squared (\u03c7 2 ) is calculated by using the following formula: P \u00f0Observed Data2Expected Data\u00de 2\n\nThe observed data is the number in the original table that was plotted by the health department employee.\n\nThe expected data is calculated though the following formula for each box: When these four numbers are added together (20 1 16.3 1 11.3 1 9.5), the test statistic is 57.1. The test statistic is then compared with the \u03c7 2 critical value using the \u03c7 2 table. To use the table, you must choose a significance level and select the degree of freedom (df):\n\nDegrees of Freedom 5 \u00f0Rows 2 1\u00de\u00f0Columns 2 1\u00de\n\nIn this example, df is 1. Following the table, using df 1 and \u03b1 0.05, the critical value (aka significance value) is found to be 3. 84 . Because the test statistic is higher than the critical value, the null hypothesis is rejected and the alternative hypothesis is accepted. This means that the worker's observations show that men and women have different preferences between aerobic and anaerobic exercise machines.\n\nThe Kappa ratio is a measurement of the agreement between two parties when accounting for random chance. It is necessary to account for chance because random agreements will happen by chance. Kappa ratios are valued from 21 to 11. The further negative the ratio, the more disagreement, while the further positive, the further the agreement. A Kappa ratio of 0 indicates that the agreement is occurring due to chance alone. The true or false answers represent a dichotomous variable that can be placed on a 2 3 2 table. The table is set up by the number of true and number of false responses by each resident. Boxes a and d represent agreement, while boxes b and c represent disagreement.\n\nThe table below has been filled out with the information extracted from the above vignette. \n\nNo matter how well a study is matched, it is not possible to find better comparisons than the subjects to match themselves. A paired ttest allows researchers to compare the significance of an intervention on a normally distributed group before and after they experience the intervention. The null hypothesis is that the intervention produces little to no difference from before to after. Meanwhile, the alternative hypothesis is true if the before and after measurements are further apart.\n\nAnalysis of variance (ANOVA) uses the f-test. This test compares the dispersion within individual variable groups to dispersion between variable groups. An f-test is used when one is comparing three or more variables. It may indicate that one group is statistically different from the others, but it does not exhibit which group is different.\n\nWhile t-tests are used to directly compare two groups to each other, analysis of variance (ANOVA) is used to compare multiple groups simultaneously. The null hypothesis when using ANOVA is that there is no difference between groups. To the contrary, the alternative is that the groups are different. ANOVA does not tell how they are different, only that they are different. After a significant effect has been found through ANOVA, post-hoc analysis is used to tell how the variables differ. In this analysis, post-hoc analysis would determine which coffee is best (and worst). While t-tests could be used to compare the data directly, it is not preferred. This is because it would require each combination of variables to be compared directly. In this case, each coffee would have to be compared against one another. If each t-test has 0.05 risk of error, the overall error rate compounds with each t-test.\n\nANOVA uses the f-test. This test compares the dispersion within individual variable groups to dispersion between variable groups.\n\nThe coefficient of determination describes the proportion of variation of a dependent variable that can be explained by an independent variable. It can be calculated by squaring the Pearson correlation coefficient. If the Pearson correlation coefficient (r) is 0.8, the coefficient of determination (r 2 ) is 0.64, or 64%. This means that hours of sleep (independent variable) is responsible for 64% of point production (dependent variable). Because only 64% of point production is attributed to sleep, 36% (100%\u00c064%) is caused by other factors. Multiple R squared (R 2 ) is analogous to r 2 , but is used in multiple regression.\n\nThe ideal test to calculate this problem should be able to compare the means of categorical independent variables (type of insecticide) with a continuous dependent variable (incidence of mosquito-borne illness). Analysis of variance (ANOVA), analysis of covariance (ANCOVA), and t-test are all capable of performing this calculation. Because there are more than 2 independent variables (3 insecticides), it is less preferable to use the t-test. This is because multiple t-tests would have to be performed, a test comparing each independent variable to one other. Not only does this route take longer, but it also leaves more room for error. ANOVA compares the means of $ 2 independent variables on one dependent variable to investigate if there is a significant difference between the independent variables. ANOVA can only tell that a difference exists, but it cannot tell where the difference is. To find where the difference exists, post-hoc tests should be performed. ANCOVA is similar to ANOVA, except that group means are adjusted by a covariate to adjust for confounding in ANCOVA. Confounding occurs when there is an association between the exposure and outcome that is distorted by another variable, such as age.\n\nCohort studies, epidemic curves, and longitudinal data collection are epidemiologic tools used in time-series analysis. A time-series is a sequence of measurements and observations made at successive points in time. Time-series analysis interprets this data by recognizing time as the independent variable. The effect is measured at various times, including before and after suspected cause, but is not necessarily used to demonstrate causation of correlation. Another example of timeseries analysis is the multiple time-series study, in which a suspected risk factor is introduced to several groups at different times.\n\nDegrees on the Fahrenheit or Celsius scales are examples of interval data. When an interval scale is used, the exact difference between each number is known. However, because there is no true zero, one number that is twice the other number does not have twice the difference. For example, 60 F is not twice as hot as 30 F. To the contrary ratio numbers have a true zero and exact difference between numbers. For example, 60 meters is exactly twice as long as 30 meters.\n\nA nominal scale does not rank the variables, it merely categorizes them, such as the hair color of people a room.\n\nAn ordinal scale categorizes variables by the order they are placed in, even if there is no constant value between the variables, such as the satisfaction scale of patients in a hospital.\n\nThe possible grouping of variables in this question is: Underweight (BMI , 18.5%), ideal weight (BMI between 18.5 and 25), overweight (BMI . 25%), and obese (BMI . 30%). This grouping of variables are considered ordinal. If the question asked for the type of variable according to each individual weight, the correct answer would be ratio.\n\nOrdinal variables have an order, but not necessarily equal values between the variables. Consider the pain scale in a hospital, where the difference between one and two may not be the same as six and seven. Similarly, with regards to BMI classification: Obesity . overweight . ideal weight . underweight.\n\nInterval variables are ordered according to value. The difference between ordinal variables and interval variables is that interval variables have set values between the variables. The difference between 67 and 68 is the same as between 97 and 98 .\n\nNominal variables do not have an order and are grouped in name only. An example of nominal variables are colors: Red, blue, green, etc.\n\nRatio variables have a true zero, where zero value actually means there is zero of the variable. In ratio variables, two times of a specific value is actually twice as high. For example, 20K is twice as warm as 10K. To the contrary, 20 C is not twice as warm as 10 C\n\nThere is no such thing as numerical variables.\n\nThe Kelvin temperature scale is an example of a ratio variable. Other examples include measurements in height (meters, feet) and weight (pounds). For further explanation of nominal, ordinal, interval, and ratio variables, refer to Answers #59 and #60 (directly above).\n\nThe binomial distribution curve graphs probabilities of dichotomous, binary variables (not continuous). There is a different binomial distribution for every combination of numbers (n) and probability of success (p). The larger the number of observations (n) and the closer the probability of success (p) is to 0.5, the closer the binomial curve appears to the normal curve. If n $ 30, many statisticians feel comfortable using the normal distribution in place of the binomial distribution.\n\nWhen the smaller the p, the further the distribution is skewed to the right. Likewise, the larger the p, the further the distribution is skewed to the left. Even with extreme p, a larger n will approximate the normal distribution.\n\nWhen the binomial distribution approaches the normal distribution, the following tendency measures apply: Chi-square uses a hypergeometric probability distribution, where larger numbers accurately follow the distribution. For this reason, chi-square only provides approximate p-values. If a sample size is sufficiently small, the sample size will not follow a chi-square distribution.\n\nFisher's exact test shows exact p-values. When larger chi-square numbers are used, the two tests approximate each other. If less than 20% of the cells in a chi-square table have an expected count of ,5, or any one cell has an expected count of ,1, it is recommended to use Fisher's exact test.\n\nMcNemar's test may be viewed as a special type of chi-squared, in which the variables are not completely independent (variables in chisquare are independent). McNemar's test is used to analyze matched pairs or calculate before and after changes in the same variable.\n\nWhile the t-test analyzes continuous variables, McNemar's test checks for an association between binary/dichotomous variables\n\nThe Mann\u00c0Whitney U test is a nonparametric test comparable to the two sample t-test. It is used to test the median between two groups. The null hypothesis is that both groups are similar. The alternative hypothesis is that the two populations are different.\n\nThe interquartile range is the difference between the 25th and 75th percentiles of the observations. To calculate the interquartile range, the observations should be placed in ascending order. 1 The 25th percentile is then subtracted from the 75th percentile to get the interquartile range: 71 2 64 5 7.\n\nIf the interquartile range were to fall between two numbers, an average of these numbers may be chosen to represent a quartile.\n\nAs you can see, the interquartile range provides insight to the spread of data, but it ignores a large amount of data.\n\nDue to the small sample population available, normal distribution may not be assumed and a nonparametric test must be used. This immediately eliminates the paired t-test and student's t-test, which are parametric studies. By comparing two otherwise identical populations, a t-test would have been appropriate if there were the assumption of normalcy. Another important observation of the available data is that there are no specific numbers available. The data only denotes that there is a difference between the two groups, but does not describe how much the difference is. Both Wilcoxon signed rank test and chi-squared test (as do both t-tests) require specific number to perform. The sign test is a nonparametric test that compares dichotomous differences (better/worse or 1/2 ) in data from matched otherwise identical pairs and ignores the magnitude of difference. Related to the Wilcoxon signed rank test, the sign test is an analog to the paired t-test. The null hypothesis in a sign test is that the difference between two groups is zero. In this question, the null hypothesis is that each group tested better than the other group five times.\n\nPlease refer to the following Chi-square (\u03c7 2 ) is a nonparametric test.\n\nSpearman rank correlation coefficient (r s ) is the nonparametric alternative test to the Pearson correlation coefficient (used to measure linear strength of association between two variables). It works by ranking the X and Y variables according to value and inserting these rankings into the formula used for the Pearson correlation coefficient. In addition to being used for nonnormal continuous data, it can also be used for ordinal data. Analysis of variance (ANOVA) is used to compare multiple groups simultaneously. The null hypothesis when using ANOVA is that there is no difference between groups. To the contrary, the alternative is that the groups are different. ANOVA does not tell how they are different, only that they are different. After a significant effect has been found through ANOVA, post-hoc analysis is used to tell how the variables differ. In this analysis, post-hoc analysis would determine which coffee is best (and worst).\n\nChi-squared is the most commonly used nonparametric test and is best for hypothesis testing between categorical variables. When used appropriately, it tells an investigator whether observations are correlated, or if they are due to chance.\n\nThe Mann\u00c0Whitney U test is a nonparametric test comparable to the two sample t-test. It is used to test the median between two groups. The null hypothesis is that both groups are similar. The alternative hypothesis is that the two populations are different.\n\nThe sign test is a nonparametric test that compares dichotomous differences (better/worse or 1/2 ) in data from matched otherwise identical pairs and ignores the magnitude of difference. Related to the Wilcoxon signed rank test, the sign test is an analog to the paired t-test. The null hypothesis in a sign test is that the difference between two groups is zero.\n\nRefer to the table in Question #68 (directly above) to see nonparametric alternatives to parametric tests.\n\nRefer to the table in Question #68 (two questions above) to see nonparametric alternatives to parametric tests.\n\nThe survival table for this question is shown in the following graph:\n\nTo solve this problem, the survival rate from the end of the first three time periods (years) of interest should be multiplied by each other.\n\nLogistic regression is used to find the likelihood of an outcome when the outcome is dichotomous. Dichotomous variables are commonly represented in the form of success/failure, improved/unimproved, or alive/dead.\n\nThis question asks to find the overall effect that one independent variable (new medical office) has on two dependent variables (physician satisfaction and patient satisfaction). When there is more than one dependent variable, the situation is said to be multivariate. Furthermore, multivariate tests involve more than one dependent variable. Multivariate analysis of variance (MANOVA) is a tool used to evaluate multivariate tests and determine significance between groups. MANOVA is an extension of analysis of variance (ANOVA). When there are multiple dependent variables, they are often times related to one another. In this case, the contentment of each party in the physi-cian\u00c0patient relationship likely depends on the contentment of the other party. If one were to perform separate t-tests or ANOVA tests to solve for each dependent variable, this relationship is not properly addressed.\n\nMANOVA considers the correlation between dependent variables, reducing distortion from relationships amongst them.\n\nPublication bias refers to the tendency of investigators to publish results that have the desired outcome. Studies that achieve desired outcome are more likely to be published. If an investigator performing a meta-analysis only includes studies that have achieved the investigators' preconceived notions, the meta-analysis will have a differential error.\n\nFetal death prior to 20 weeks gestation is defined as an early fetal death, commonly called a miscarriage. Fetal death between 20 and 28 weeks gestation is defined as intermediate fetal death.\n\nFetal death after 28 weeks gestation is defined as a late fetal death, commonly referred to as stillbirth.\n\nMaternal Mortality Rate 5\n\nNumber of Pregnancy Related Deaths Number of Live Births 3 100; 000\n\nAlthough the denominator of this equation should technically be the number of pregnancies, that statistic is not as readily available. For ease of calculation, the number of live births is used. This figure includes pregnancies with more than one child.\n\nThe demographic gap is the difference between birth and death rates.\n\nLife expectancy may be calculated at birth or any age afterwards. The overall life expectancy in the United States has been steadily increasing due to public health and medical advancements. Differences of life expectancy between sexes, ethnic groups, and races have been narrowing. Women are expected to live roughly 81.2 years, while men succumb roughly 5 years earlier, at age 76.4. Of course these numbers fluctuate with a plethora of variables accrued over a lifespan.\n\nIn the United States overall, the non-Hispanic White population has a higher life expectancy than the non-Hispanic Black population. However, the Hispanic White population has a higher life expectancy than the non-Hispanic White population and likewise for the Hispanic Black population. The difference with Hispanic ethnicity adds on over two years of extended life expectancy to both White and Black populations.\n\nThe exact reason for this Hispanic epidemiological paradox is under debate. Although extended life expectancy is typically tied to wealth and education, Hispanics buck this trend. The healthy migrant effect reasons that Hispanic immigrants are generally healthier than those that do not immigrate. Other arguments state that unhealthy Hispanic immigrants to the United States may return to their country of origin prior to death. Other theories suggest that cultural effects may confer a protective risk factor.\n\nThe Asian-American population enjoys the longest lifeexpectancy in the United States.\n\npital to ask about confirmed HIV cases Active surveillance occurs when the health department takes action to seek out cases of illness. A health department calling healthcare providers to inquire about cases of illness is an example of active surveillance. Passive surveillance is where a health facility or laboratory notifies the health department of a reportable disease.\n\nThe NNDSS is a collaboration between local, state, and federal public health agencies to combat notifiable diseases through surveillance, data collection, data analysis, and sharing of public health data. It does this through numerous media, including the maintenance of the National Electronic Disease Surveillance Syndrome (NEDSS). NNDSS is supported through the Center of Disease Control and Prevention's Division of Health Informatics and Surveillance (DHIS).\n\nThe CSTE is an organization composed of epidemiologists, representing epidemiologists from all of the states and territories of the United States. Together, these epidemiologists collaborate and provide assistance to each other and other public health agencies, such as the CDC. CTSE maintains a list of notifiable diseases that the states modify and adopt into law. States submit this data to the CDC to help track local trends in infectious diseases.\n\nThe CSTE is an organization that represents public health epidemiologists from states and territories. It maintains a list of notifiable diseases that the states modify and adopt into law. In addition to practitioners and laboratories being mandated to report notifiable diseases to the state in a set time period, the state is also asked to submit their notifications to the CDC in a set time period of either 4 hours, 24 hours, or 7 days depending on the type of disease.\n\nThe appropriate screening test to identify those at risk of the fatal adverse event would have a high sensitivity [true-positive/(true-positive 1 false-negative)]. Sensitivity shows the proportion of those that have a disease that are accurately identified as those really having it. In a 2 3 2 tablet, sensitivity is calculated by A A 1 C . A highly sensitive test helps to rule out a disease because it indicates that a negative test is likely not to have the disease. This can be remembered by the acronym snout (sensitivity 1 rule out).\n\nTo the contrary, specificity is defined by true-negative/(true-negative 1 false-positive) and calculated on the 2 3 2 table from B B 1 D . It represents the proportion of those without a disease that are accurately identified as not having it. A highly specific test helps rule in a disease because it indicates that a positive test is less likely to be a false-positive. Specificity can be remembered by the acronym spin (specificity 1 rule in).\n\nThe PPV identifies the probability of those who test positive for the disease to those that actually have the disease.\n\nThe NPV identifies the probability of those that test negative for the disease to those that do not have the disease.\n\nPrevalence is the proportion of those that have the disease in the population. 84. E. All of the above are considerations to make when creating screening recommendations Implementation of a properly conducted screening test is a complex effort. Prior to initiating a screening test, the test itself must be fully scrutinized. Factors that should be evaluated include ethical consequences, psychological consequences, stigmatization, predictive value of results, test validity (and reliability once validity is established), treatment options, economics, and the risk of false-positives or falsenegatives. It is also important to figure out how to properly notify the public about the availability of the screening test so that only those in the target population come forward for screening. After results are available, it must be determined how to appropriately disseminate the results. Ethical and psychological considerations when considering a screening test come in a variety of forms, depending on the test being deliberated. Consider for example, a test that has no available treatment. What type of mental anguish would it cause for someone to know that they have an ailment that cannot be reversed? On the other hand, what if one sibling tests positive for a genetic trait that leads to cancer in everyone that has that trait. Is it ethical to perform a genetic test on one sibling, while the other would rather not have the test performed? Often times there is no correct answer in bioethics. However, it is important to consider the main bioethics principles of autonomy, beneficence, nonmaleficence, and justice.\n\nThere are always benefits and drawbacks in medicine. One common drawback is stress and angst related to undergoing the screening process. Those found to be at risk of disease suffer from psychological stress and identify themselves as weak and vulnerable. Meanwhile, self-perceived health is a predictor of future health status.\n\nAnother common drawback is false-positive results. The rate of false-positives depends on the test being conducted. A test yielding a large amount of false-positives may be considered acceptable if the benefit of discovering a serious disease is present. This is currently up for debate with prostate cancer, where it is estimated that for every man whose life is saved from prostate cancer by PSA screening methods, 47 men are over-diagnosed and treated, even though they do not need the treatment. 85. E. Cases of Norovirus in Tulsa by Data of Onset, Tulsa, Oklahoma, July, 2014 When creating any type of graph in epidemiology, it is important to be as descriptive as possible. It is ideal if a reader can understand the context of the graph by looking at the graph alone. The title should include the type of illness, place of outbreak, and when the outbreak occurred. In addition, the table should be labeled with the dates of incidence on the x-axis and the number of cases on the y-axis.\n\nWhen charting the annual incidence of a disease, the incidence pattern is the primary focus. This is best displayed by the epidemiologic year, which spans from the month of lowest incidence from one year to another. By placing the lowest incidence at both sides of the graph, it allows the person viewing the graph to appreciate the time leading up to and after the highest incidence. If the highest incidence is plotted in the beginning or end of the curve, the peak incidence is likely to be broken up between the beginning and end of the curve.\n\nAn epidemic curve is an investigative tool that describes the patterns of an outbreak. These patterns help to identify the source of the outbreak and potentially how to address it. To compose an epidemic curve, the number of new cases should be plotted against a unit of time, most often days.\n\nEpidemic curves were defined in Answer #87 (directly above). There are typical outbreak patterns found on epidemic curves: Common source-A group of people become ill after being exposed to a point source contaminant. All affected persons become ill within one incubation period. There are no secondary waves, in which people fall ill outside of the first incubation period. Example: Radiation toxicity after a nuclear power plant radiation leak.\n\nContinuous common source-A common source continuously affects those with contact. Example: Soft serve ice cream machine contaminated with Listeria.\n\nPropagated-Infection is transmitted from one person to another. May be direct or indirect contact. Often include waves of secondary or tertiary spread outside of the first incubation period. Rate of propagation depends on herd immunity, opportunities for exposure, and secondary attack rate. Example: Influenza outbreak.\n\nMixed-Occurs when a common source outbreak is complicated by person-to-person spread. Example: A bacterial conjunctivitis outbreak from a telescope that spreads amongst children at daycare.\n\nDifferent patterns of disease outbreaks were explained in Answers #87 and #88 (directly above).\n\nTo solve this question, it is necessary to build upon the numbers presented in the table. Variables of interest include the attack rate % (number of ill/total number), the attack rate difference (attack rate in exposed 2 attack rate in unexposed), and the attack rate ratio (attack rate in exposed/attack rate in unexposed). Burgers have the highest attack rate %, attack rate difference and attack rate ratio, making them the most likely source of the diarrheal illness. \n\nSmallpox is a viral infection that can be transmitted by respiratory droplets and fomites, such as blankets. Although there is no antiviral agent that has been proven to be effective against smallpox, there is a very effective vaccination. This vaccination has been used to eradicate the smallpox virus worldwide. The last naturally occurring case of smallpox was in 1977. Because of this, worldwide immunization ceased in 1980. The vaccination is not used to prevent acute infection.\n\nThe key to answering this question correctly is understanding the difference between isolation and quarantine. Isolation insulates people with an infectious illness from those without the illness. Meanwhile, quarantine insulates people who have been exposed to a contagious illness from those without the illness. There are several different types of personal and property quarantine measures. In the United States, government entities have the ability to enforce quarantine and isolation measures.\n\nThe incubation period for smallpox is typically 10\u00c014 days. Quarantine should last this long at a minimum. If possible, use of a negative pressure room would be advised.\n\nAlthough individuals have rights, their liberties may be trumped by the rights of society as a whole to be protected from health threats. For this reason, governments may impose isolation for people showing signs of contagious illness and quarantine for those exposed (and asymptomatic) to the illness. These public health practices protect the public by reducing exposure to infectious disease.\n\nIn the United States, legal authority to isolate and quarantine is divided between the states and federal government. If a communicable disease is suspected or present in someone entering the United States, the CDC may issue a federal order to isolate or quarantine. Furthermore, the CDC may issue orders to isolate or quarantine in order to limit spread of disease from one state to another. Each state has their own isolation and quarantine statutes. States may isolate, quarantine, and trace persons with infections disease within their borders. This is commonly performed for tuberculosis.\n\nPublic health officials have the legal authority to react swiftly to infectious disease threats. An order to quarantine or isolate does not need advance approval from courts and violation of these orders may result in arrest. Detainees may legally challenge public health orders, but these orders take time and judges have limited jurisdiction and typically defer to medical experts.\n\nFor the sake of the rights of society to be protected from health threats, public health officers have the authority to reveal a patient's condition to those exposed. Similarly, hospitals are obligated to inform health departments of names and contacts of those with specific contagious disease.\n\nThe WHO governs disease globally and maintains the International Health Regulations (IHR). These voluntary regulations attempt to limit spread of contagious disease by addressing by influencing political, diplomatic, and trade relationships amongst all WHO member states. IHR is not directly enforceable, but insubordinate nations may face economic and social disruptions from other participating nations.\n\nNearly every event on Earth can be spatially referenced into geographic data. The GIS are interactive computer-based applications that map geographic data. It describes the way we study the environment and produces spatial data that is used in a variety of industries, including healthcare and public health. Many aspects of health and wellbeing have spatial dimensions, including health disorders, disease risk factors, health interventions, and health outcomes. GIS is an important tool in epidemiology, health administration, and health marketing. It may be used to map out any combination of factors to reveal potential correlations between health events. For example, one may use it to look at a cluster of increased disease incidence and compare it to individual demographic characteristics. Equally interesting, it could identify the exposed and unexposed groups to known risk factors for disease. With this information, GIS may help find appropriate places to institute a health intervention.\n\nEpidemic curves are charts that plot the number of people with an infection versus the time at which they get the infection. It is used to identify the origin of an infection and the speed at which it travels through the population.\n\nGantt charts are an administrative tool to help create a project schedule. On a Gantt chart, each member is assigned a task and a time period to complete that task.\n\nIshikawa diagrams are also commonly known as cause-and-effect diagrams and fishbone diagrams. An Ishikawa diagram reads from right to left. At the far right of the diagram is the problem to be addressed. Moving to the left, the diagram identifies root causes of the problem(s). These root causes are further broken into sub-causes. Once the diagram has been drawn out, it takes the shape of fish bones.\n\nTo calculate the answer, it is easiest to use a 2 3 2 table to determine the unknown values from the known values: Prevalence is the percentage of a population that has a condition. It is calculated by a 1 c.\n\nSensitivity represents the proportion of those that have a disease that are accurately identified as those really having it. In a 2 3 2 tablet, sensitivity is calculated by A A 1 C : Specificity represents the proportion of those without a disease that are accurately identified as not having it. In a 2 3 2 tablet, specificity is calculated by D B 1 D . \n\nSpecificity is represented by the formula D \u00f0B 1 D\u00de . Meanwhile, falsepositive is represented as B \u00f0B 1 D\u00de . When added together, the specificity and false-positive error rate is equal to 1 or 100%. The false-positive rate is often calculated as (1 \u00c0 specificity).\n\nSensitivity is represented by the formula A \u00f0A 1 C\u00de . Meanwhile, falsenegative error rate is represented as C \u00f0A 1 C\u00de . When added together, the sensitivity and the false-negative error rate is 1.\n\nSensitivity ( A \u00f0A 1 C\u00de \u00de is the proportion of those with a disease that test positive. As stated in the vignette, nine out of every ten people with the disease test positive. Meanwhile, specificity ( D \u00f0B 1 D\u00de ) is the proportion of those without a disease that test negative for it. These two metrics are not to be confused with the PPV and NPV. The PPV ( A \u00f0A 1 B\u00de ) is the proportion of those with a positive test result that actually have the disease. Finally, the NPV ( C C 1 D ) is the proportion of those with a negative result that do not have the disease.\n\nThe receiver operating characteristic (ROC) curve can be considered a graph of positive likelihood ratios. The y-axis is the sensitivity, while the x-axis is 1 2 specificity, which is the proportion of false-positive results. The closer the cutoff point is to the upper left corner of an ROC Curve, the higher is the sensitivity and the lower is the falsepositive error rate.\n\nWhen measuring a continuous variable, such as the amount of potassium in a serum metabolic panel, setting the cutoff point between abnormal and normal limits can be a challenge. If the cutoff is too high, there are a lot of false-negative results. If the cutoff is too low, there will be more false-positive results. The ROC curve is a tool used to determine the best cutoff point for a continuous variable. It is a graph composed of the sensitivity along the y-axis and the false-positive error rate along the x-axis.\n\nThe false-positive error rate is equal to (1 2 specificity). The sensitivity divided by the false-positive error rate is the likelihood ratio 1 (LR1). Therefore, the ROC curve graphs the LR1.\n\nRecording vital statistics (birth, marriage, divorce, and death) is a responsibility of the states, two select cities (Washington D.C. and New York City) and United States territories. These entities share their vital statistics with the National Vital Statistics System (within the NCHS).\n\nThe US Outpatient Influenza-like Illness Surveillance Network (ILINet) receives information regarding the number of patients seen overall and the number of Influenza-Like Illness (ILI) seen from roughly 2000 outpatient healthcare providers weekly. This information is stratified by age group and evaluated for changes in trends. ILINet case definition includes the following: Fever . 100 F (37.8 C), cough, and/or sore throat. If a patient has these signs/ symptoms and is found to have a noninfluenza illness, it is not reported to be ILI. If a flu test is positive, it will be reported as influenza-like illness.\n\nThe NHANES is a program designed to assess the health and nutritional status of residents of the United States. It is a survey conducted by the NCHS, which is a part of the CDC. The data gathered from NHANES helps determine risk factors and prevalence of disease seen in the United States. This input has been used to influence health policy affecting individuals and the general public. Examples of changes influenced by NHANES include removing lead from gasoline, establishing baseline estimates for serum cholesterol and use of height/weight percentiles. Information gathered for NHANES is exclusively through home interviews and standardized physical exams in mobile exam centers. Physical exams vary depending on age, gender, and medical history.\n\nNHANES participants are chosen at random, based on their community, which is further divided into neighborhoods. After being screened for eligibility, there are nearly 5000 people surveyed each year. This means each participant represents approximately 50,000 US residents.\n\nThe BRFSS is performed exclusively through a telephone survey.\n\nThe best option listed for this student is to access a pregnancy medication exposure registry. With restricted resources, the student will be limited in conducting his own study. Furthermore, conducting an RCT to find medication adverse events is controversial in pregnant women. Pregnant women are usually excluded from pharmaceutical clinical trials, so information on medication adverse events is limited. Therefore, the Food and Drug Administration supports pregnancy exposure registries for reporting of adverse events discovered after taking specific drugs. Departments of vital statistics are often times mandated to only cover a combination of statistics related to birth, death, fetal deaths, marriages, and separations. This data would not be adequate to evaluate the risk of adverse events related to pharmaceuticals taken during pregnancy.\n\nThe International Classification of Diseases (ICD) is published by the WHO. This classification outlines an international catalog of diseases, disorders, and injuries into a universal common language. It is used by epidemiologist, health administrators, and clinicians in over 100 countries around the world to study population-wide disease patterns and healthcare outcomes. This data is used to adjust the way healthcare is provisioned and practiced. Health-related variables recorded include vital records, reason for physician encounters, morbidity, and mortality. ICD is also widely used as a basis for resource allocation, including reimbursements in the United States.\n\nIn the United States, Black individuals experience the highest IMR, while the lowest IMR is experienced by Asians and Pacific Islanders.\n\nObstetrical-related data are important metrics used to compare health status among different populations. The most widely compared obstetric metric is the IMR, which widely serves as a surrogate marker for the overall status of a health system. The neonatal mortality rate takes into consideration deaths that occur within the first 28 days of life. This is an important metric, as the majority of infant deaths typically occur shortly after birth.\n\nA list of obstetric oriented rates has been listed below: \u2022 Fetal Death Rate 108. E. 5\u00c07 per 1000\n\nThe IMR is an important health statistic internationally. Although it includes many variables, it is used as a key measure to compare health systems between countries, regions, and even cities. IMR is recorded as the number of deaths occurring per thousand live births that occur before the first birthday. The IMR in the United States typically hovers around 6.0. This number has been declining over the decades. The WHO estimates that the global IMR is roughly 32 deaths per 1000 live births. This number is a vast improvement from 30 years ago, when the IMR was double today's numbers.\n\nWorldwide, nearly 75% of all deaths under five years of age occur within the first year of life.\n\nThe US Census Bureau is a government organization mandated by the constitution with the mission to serve as the leading source of quality data about the country and economy. The Census Bureau conducts a population and housing census once every ten years. It also conducts an economic and government census every five years. In addition, it conducts a community survey annually. The information is used to distribute resources adequately to fund programs for public health, education, neighborhood improvements, etc.\n\nThe Youth Risk Behavior Surveillance System (YRBSS) is a nationwide high and middle school-based survey conducted by the CDC in conjunction with all other levels of government. It monitors behaviors that contribute to morbidity and mortality in America's youth.\n\nClassifications of behavior surveyed include actions leading to violence and injury, sexual decision making, alcohol use, tobacco use, drug use, diet, physical activity, and prevalence of asthma.\n\nThe The difference between experimental studies and nonexperimental studies is that investigators have control over the exposure in experimental studies. For this reason, experimental studies are considered to be the gold standard, secondary to meta-analysis studies, which analyze data from numerous peer-reviewed studies. It is not always possible to control the exposure. Quasiexperimental studies are utilized when an investigator only has partial control over the study. For example, an investigator may wish to investigate a hypothesis by creating an exposure to a cohort that experienced of a nonreproducible natural disaster and compare it to a control that was not in the disaster.\n\nTo answer this question, it is important to understand the definitions of both risk and rate. Risk is defined as the number of subject achieving the qualifying event (death in this example) during the defined time period. Assuming there is no attrition, the denominator (subjects at risk of qualifying event) does not change.\n\nTo the contrary, the rate is the number of qualifying events that occurring during the defined time period divided by the average number of subjects at risk. The average subjects at risk used in this equation is typically the number of subjects at risk during the halfway mark of the time period. In this question, the rats exposed to Drug A died early in the study. Therefore, there were fewer rats susceptible to death at the halfway point (because they had already died.) When more deaths take place in the early period, the number of subjects at risk during the halfway point is lower. When the denominator is lower, the rate will increase, and vice versa. 115. B. Number of bicycle injuries in Florida per the state population in a year A rate must have a numerator and a denominator. The numerator typically displays the event of interest, while the denominator will establish the population at risk. Because there is no denominator in answer A, this statistic is not considered to be a rate. Rates are either crude or specific. Crude rates use the entire number of events without breaking it down into subgroups. Meanwhile, specific rates are created by using subgroups such as hair color and age. 116. B. Life expectancy is higher at age 60 Life expectancy can be described as the average number of years of life one can expect to live based on mortality rates. It can be calculated from any stage of life. The life expectancy at birth is considerably lower than life expectancy of the same person at a later age. The reason for this is that those still living at the later age will have avoided mortality at a younger age, while others in the same cohort will have died. The life expectancy at birth takes account of all of those that will suffer premature death.\n\nBetween the ages 1 and 44, unintentional injuries are the leading cause of death. Within this group, motor vehicle accidents are the most common specific cause. Heart disease is the leading cause of death in the United States, but typically affects the elderly.\n\nYears of potential life lost (YPLL) is a measure of premature mortality. It statistic emphasizes deaths that occur at an earlier age and deemphasizes deaths that occur later in life. The YPLL is calculated as follows:\n\nWhen there are less than 20 deaths, it is easiest to subtract the age of death from the endpoint. In the United States, the endpoint is usually determined to be 75 years of age. The difference between age of death and the endpoint represents the YPLL from one person. YPLLs from all individuals in consideration are then added together to get the aggregate YPLL. If a 17-year-old and 20-yearold die, the total YPLL would be 113. This is calculated as follows: When there are more than 20 people in consideration, it is easier to calculate YPLL through frequency tables. It is calculated by dividing ages into the following groups: Under 1 year, 1\u00c014 years, 15\u00c024 years, 25\u00c034 years, 45\u00c054 years, 55\u00c064 years, and 65\u00c074 years. Each age group is then identified by the midpoint. That midpoint is then subtracted from the endpoint (usually 75). The difference between the midpoint and endpoint is then multiplied by each person that falls within the group. For example, if a 17-year-old and 20-year-old were to die in a motor vehicle accident, this would tally into the 15\u00c024 group. The midpoint of this group is 19.5, which is subtracted from 75 to get 55.5. Finally, this number is multiplied by two, because there are two people in this age group being calculated. Therefore, the YPLL from this group could be calculated as 110.\n\nThe leading causes of YPLLs in descending order are cancer, unintentional injuries, heart disease, and respiratory infections. Note that although heart disease is the leading overall cause of death, most deaths caused by heart disease are older and do not contribute on an individual basis as greatly to YPLL.\n\nYears of potential life lost (YPLL) is explained in Question #118 (directly above). YPLLs for the five options are listed below: \n\nThe population growth of the population pyramid pictured in the question represents a declining population. As is often seen in developed nations, there is a larger population of older individuals than younger ones. A smaller group of younger individuals signifies that the birth rate has slowed down. When the birthrate equals the death rate, the population pyramid exhibits parallel vertical lines on both sides. When a population is growing, as seen in developing nations, the pyramid appears as a triangle with the base at the bottom wider than the peak at the top.\n\nSick building syndrome (SBS) is a constellation of symptoms that occurs in workers continuously exposed to indoor environments. Symptoms include dry skin/mucous membranes, pruritus, mental fatigue, headache, and airway infections that are more pronounced during exposure to the indoor workplace. These symptoms lead to a large economic impact due to absenteeism, presenteeism, litigation, and workers' compensation claims. There are numerous risk factors contributing to SBS, including overly populated buildings, presence of carpet within the building, presence of mold/dust, and psychiatric stress. Due to this varied etiology and lack of specific biological markers, the prevalence of SBS is hard to determine. This question asks to find the risk difference (RD): Risk Difference 5 Risk in exposed 2 Risk in unexposed RD may be performed with point prevalence, cumulative incidence, or incidence rates.\n\nRisk difference is positive when the risk in the exposed population is greater than the risk in the unexposed. It is negative when the risk in the exposed group is less than the risk in the unexposed group. In this problem, the new AC system is considered to be the exposure.\n\nTo calculate the RD for this equation, you must first calculate the risk in the exposed and unexposed groups. Knowing that there are 1000 employees, there are 225 employees in the unexposed group. This means that the point prevalence rate of SBS is lence dropped to 125 1000 5 0:125: These numbers are the plugged into the risk difference equation:\n\nRisk Difference 5 Risk in Exposed 2 Risk in Unexposed 5 0:125 2 0:225 5 2 0:1\n\nThe risk difference is negative when the exposed population is less than those without the exposure.\n\nTo solve this problem, one must use the following equation: Vaccine efficacy is a useful calculations for ideal conditions, while Vaccine effectiveness better demonstrates how vaccinations work in the real world. Vaccine effectiveness accounts for variables such as vaccine storage, vaccine administration, access to care, and cultural barriers to vaccination.\n\nThe Attributable risk (AR) is also known as the risk difference. It is the risk in the exposed, minus the risk in the unexposed. It does not give the percentage, as asked for in the question. Attributable risk percentage (AR%) is able to give the percentage of risk attributed to a specific risk factor, in those that are exposed to the risk factor, as asked for in the question. Meanwhile, the population attributable risk percentage (PAR%) is able to give the percentage of risk attributed to a specific risk factor in the general population, not just the group exposed to the risk factor. Note that in the second PAR% equation below (fourth equation), the variable \"effective proportion\" is added to the second AR% equation to specify the population at risk, in this case red meat consumers. The attributable risk percent is represented by the following two equations;\n\nIncidence Exposed2Incidence Unexposed Incidence Exposed 3100\n\nAttributable Risk Percent \u00f0AR%\u00de 5 Relative Risk 2 1 Relative Risk 3 100\n\nThe population attributable risk percent is represented by the following two equations; 125. C. 1.5\n\nThe relative risk (also known as the risk ratio) represents the risk amongst those exposed to a risk factor, compared to those unexposed. The vignette explains that 15% of those exposed had brain cancer, while 10% of the unexposed had brain cancer. The RR for this problem could be solved as follows:\n\nRelative Risk RR \u00f0 \u00de5 Risk Among Exposed Risk Among Unexposed 5 0:15 0:1 5 1:5\n\nThe relative risk may also be completed by competition of a 2 3 2 The relative risk (risk ratio) shows that those exposed to Chemical X are 1.5 times more likely to be diagnosed with brain cancer than those not exposed to Chemical X. Because the RR is .1, there is said to be a positive association between Chemical X and brain cancer. When RR is equal to 1, there is no relationship between the two variables. Finally, when RR ,1, there is a negative association.\n\nWhen possible, the risk ratio (RR) is the preferred method of analysis of risk. Cohort studies are best calculated with RR. However, the risk ratio cannot be calculated directly from a case-control study. The odds ratio (OR) is the best tool for risk analysis of casecontrol studies. It is important to remember that as the OR is only a reliable estimator of the RR if the prevalence of disease is less than 5%. The OR is also used in other methods of analysis, including calculating the logistic regression and Cox regression analysis.\n\nThe PRAMS is operated by the CDC in conjunction with state health departments in order to identify healthcare trends in pregnant women and women that have just given birth. It provides state-specific, population-based data on maternal attitudes and experiences prior-to, during, and after pregnancy. This data is used by health professionals to monitor and track healthcare goals in addition to formulate opportunities for improvements in care. 128. B. If the prevalence is .10%\n\nThe odds of an event considers the chances that one event will occur compared to another. The odds of square A to square B would be compared as A:B. This is opposed to the risk, which includes the denominator and would be written as a a 1 b . When the prevalence (A 1 C) is small, the Odds Ratio . As prevalence grows, the denominator of the risk ratio also grows (while odds remain the same) and the odds ratio will no longer approximate the risk ratio.\n\nIn summary, the odds ratio is a good estimate of the risk ratio when the prevalence of a disease is low. Some investigators are willing to accept a 10% prevalence, while others feel that the odds ratio does not approximate the risk ratio if the prevalence is greater than 5%.\n\nTo answer this question correctly, the table must be completed, as shown below. To better depict all that is truly occurring in a 2 3 2 table, the borders have been added. \n\nIn a sufficient cause, the disease will always occur if the cause is present. In a necessary Cause, the cause must be present for the disease to occur. Cirrhosis is not always a result of alcohol and not everyone that consumes alcohol will develop cirrhosis. Therefore, alcohol is not sufficient or necessary for cirrhosis.\n\nHealthcare is necessary, but not sufficient for improved healthcare. This was the conclusion of \"Health care: Necessary but not sufficient,\" a brief from the Center of Society and Health in conjunction with the Robert Wood Johnson Foundation. Necessary and sufficient are terms used to describe correlation between two variables, once causality has been established. For something to be necessary, it must be present to produce change. The change does not occur unless the necessary factor is present. For something to be sufficient, the presence of that factor will always bring change. Consider Mycobacterium tuberculosis: It is both necessary and sufficient for miliary tuberculosis. 132. A. A case-control study where exposed subjects are misclassified as unexposed and a similar number of unexposed are classified as exposed Differential and nondifferential error are classified as the two types of misclassification bias. Nondifferential misclassification occurs when the frequency of errors are the same in both populations being compared. This coincides with option A, where both the exposed and unexposed subjects are misclassified. Nondifferential error typically reduces the effect of the association and brings the measured association back to the null. An example of nondifferential bias would be analysis of incomplete medical records. More specifically, if a dichotomous variable such as cigarette smoking were left blank, the bias would be nondifferential.\n\nMeanwhile, differential misclassification occurs when the misclassification occurs more in one of the groups being compared than the other. Differential misclassification may influence the association either towards or away from the null. Going back to the previous example of incomplete medical records, if each patient's chart was indiscriminately checked off for being a nonsmoker to save time, this would be an example of differential bias. Option C is an example of recall bias. People that have had a memorable event (such as mothers of children with birth defects) will typically think harder to recall that event than those that did not experience the event. Because the ill and nonill groups may provide different recollection of their experience recall bias is a type of differential misclassification.\n\nThe sample group in this study represents a small spectrum of the population. In a drug that is potentially marketed to men and women of age that places them at increased risk of cardiovascular disease resulting from dyslipidemia, this study only evaluated men between the ages 57 and 60. External validity (also known as generalizability) occurs when results of an observation hold true in different situation. The sample population in clinical studies should represent the target population that is to be treated. Due to the limited variation in this study sample, the trial lacks external validity.\n\nWith the information provided in the vignette, there is every reason to believe that the conclusion of the study accurately represents the population that was studied. When the conclusions accurately represent the population being studied, the study is said to be internally valid. Internal validity depends on the study design, data collection, and analysis of data.\n\nThere is great clinical significance in controlling dyslipidemia and reducing cardiovascular events.\n\nStatistical significance is demonstrated via the alpha value.\n\nVaccine efficacy is obtained through studies, while vaccine effectiveness is how vaccinations perform in the real world. All of the above options are real-life variables that work to decrease the efficacy of vaccines.\n\nSensitivity analysis is the process of examining how expected outcomes change when they are placed under different assumptions. Attributable risk is used to determine what effect an exposure to a risk factor has on the effect of the population.\n\nData organization is a general term that is not specific to sensitivity analysis.\n\nStandard deviation is a measure of dispersion, which biostatisticians use to see how far spread out the numbers in a population are.\n\n\u2022 Immunogenicity-The ability to produce an immune response and protection from reinfection of a pathogen. \u2022 Infectivity-Ability to cause an infection. Measured by the number of infectious particles required to cause infection. \u2022 Pathogenicity-Ability of microbial agent to induce disease.\n\n\u2022 Secondary attack rate-The proportion of susceptible people that contract a disease after exposure from an infected person. It is a measure of the infectivity. \u2022 Virulence-Severity of the infection after the disease occurs.\n\nMeasured by case fatality or severe morbidity.\n\nInfluenza is an RNA virus in the Orthomyxoviridae family. It is composed of hemagglutinin and neuraminidase proteins that are directly involved in the infectivity of the influenza virus. The two component proteins are continuously undergoing changes. When these changes are abrupt the genetic changes generate a new set of amino acids, leading to different proteins and a new influenza strain that may differ greatly from immunity provided from vaccination or previous exposure. Dramatic changes to the influenza virus are known as antigenic shift and are more often responsible for particularly severe influenza seasons. Less dramatic changes are called antigenic drift. In this case, the population typically carries a greater immunity from vaccinations and prior exposure of influenza strains that were similar to the currently circulating one.\n\nThe current outbreak of influenza in chickens can best be described as epizootic. An epizootic is an increase in the usual prevalence of a disease in an animal population, an animal disease outbreak. An endemic is when the prevalence level of a disease in a human population is regular and constant.\n\nAn enzootic is when the prevalence level of a disease in an animal population is regular and constant.\n\nAn epidemic is an unusual increase in the occurrence of a disease. Even one case of a disease may be considered epidemic, such as would be the case if there was a confirmed polio diagnosis in the United States.\n\nA pandemic is when a disease affects more people than usual and affects many regions and nations.\n\n\u2022 Identify vaccinations with high adverse event rate that were manufactured in the same batch \u2022 Assess and monitor the safety of newly licensed vaccines 142. C. Review the ethical considerations of a research study\n\nOrganizations that conduct research on human subjects are required to create an IRB to evaluate the potential risks and benefits of human experimental research. In order to approve human research experiments, the IRB must evaluate the full proposed study. The IRB members must have an understanding of the science behind the research and the legislation regarding human research. The IRBs also assure that human subjects receive appropriate informed consent and fully understand their involvement in the research process.\n\nInformed consent is an educational process by which a person makes an educated decision to participate or not participate in a procedure. To adequately obtain informed consent, several factors must be in place. The individual must be legally allowed to make a decision. For example, minors are typically unable to make a decisions that lead to informed consent.\n\nInformed consent requires presumption of competence. This presumption implies that a person can comprehend information, understand risks and benefits, exercise judgement, and make a decision based off the information.\n\nInformed consent requires voluntary decision making that is free of coercion. If there are external factors influencing the decision making process, the decision may not be independent and cannot be considered appropriate for informed consent.\n\nFinally, informed consent requires the full disclosure of all relevant information. An informed decision cannot be made if there is missing information. 144. C. It would be unethical to not screen patients for a curable disease It is considered unethical not to use a screening test that has been shown to save lives. Consider mammography as an analogous example to the screening test in this question. Because it is known to be effective at detecting breast cancer, many groups would consider it unethical to not use screening mammography to screen breast cancer for research purposes. On the other hand, mammography often leads to harms in the form of stress to the patient and unnecessary surgical procedures.\n\nThe NSDUH is housed within the Substance Abuse and Mental Health Administration (SAMHSA). It is the nation's primary source of information on patterns, prevalence, and consequence of drug use and mental disorders in the noninstitutionalized population, age 12 and older. NSDUH questions include alcohol, marijuana, tobacco, and all other illicit drugs. The study gathers data through face-to-face interviews at the place of residence and does not include incarcerated prisoners, homeless people not living in shelters, or military personnel on active duty. The NSDUH is a key source of information to provide complimentary information to the BRFSS.\n\nAdministration SAMHSA is the operating division under the DHHS that aims to reduce the public health impact of mental illness and substance abuse in the United States. The DAWN records hospital emergency room information in order to provide surveillance of trends in drug use. Meanwhile, SAMHSA's National Study on Drug Use and Health (NSDUH) tracks patterns and consequences of alcohol, tobacco, illicit drugs, and mental illness in the United States through random interviews. The BRFSS is conducted by the CDC to monitor healthrelated risk behaviors, use of preventive health services and status of chronic health conditions. Like SAMHSA, CDC is one of the operating divisions under the DHHS.\n\nThe DOJ contains the Drug Enforcement Agency, which originally founded DAWN. However, DAWN has been fully transferred to SAMHSA. The DOJ used to maintain the National Drug Intelligence Center (NDIC), which predicted future drug use trends via a national drug threat assessment. The NDIC is no longer in operation.\n\nThe ONDCP is a component of the Executive Office of the President of the United States. ONDCP advises the president on drug issues and coordinates activities to control illicit drug use. Additionally, the ONDCP composes the annual National Drug Control Strategy, which describes efforts to reduce drug use, drug distribution, drug-related violence, and health problems related to illicit drugs.\n\nThe IMR in the United States hovers around six deaths per 1000 live births. Of these deaths, the largest percentage is due to congenital malformations. Low birth weight is the second leading contributor to IMR in the United States. After these top two causes of infant morbidity, there is a significant drop in infant specific causes of death. Other common contributors to IMR include maternal complications, SIDS, unintentional injuries, placental complications, sepsis, and respiratory distress."}