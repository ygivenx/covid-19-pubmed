{"title": "iPPBS-Opt: A Sequence-Based Ensemble Classifier for Identifying Protein-Protein Binding Sites by Optimizing Imbalanced Training Datasets", "body": "Individual proteins rarely function alone. Most proteins whose functions are essential to life are associated with protein-protein interactions [1]. Actually, these kinds of interactions affect the biological processes in a living cell. To really understand protein-protein interactions, however, it is indispensable to acquire the information of protein-protein binding site (PPBS). Despite many studies on the binding site of a protein or DNA with its ligand (small molecule) have been made [2,3,4,5,6,7,8], relatively much less studies have been conducted on PPBS, particularly based on the sequence information alone. It is both time-consuming and expensive to determine PPBS purely based on biochemical experiments. Facing the enormous number of protein sequences generated in the postgenomic era, it is highly desired to develop computational methods to identify PPBSs for uncharacterized proteins so that they can be timely used for both basic research and drug development, such as conducting mutagenesis studies [9] and prioritize drug targets.\n\nGiven a protein sequence, how can one identify which of its constituent amino acid residues are located in the binding sites? Actually, considerable efforts were made to address this problem [10,11]. Although the aforementioned works each have their own merits and did play a role in stimulating the development of this area, further work is needed due to the following shortcomings: (1) The datasets used by these authors to train their prediction methods were highly imbalanced or with a strong bias; i.e., the number of non-PPBS samples was significantly larger than that of PPBS samples; (2) None of their prediction methods has a publicly accessible web server, and hence their practical application value is quite limited, particularly for the majority of experimental scientists.\n\nThe present study is initiated in an attempt to develop a new PPBS predictor by addressing the aforementioned shortcomings. According to the Chou\u2019s 5-step rule [12] and the demonstrations in a series of recent publications [13,14,15,16,17,18,19,20], to establish a really useful sequence-based statistical predictor for a biological system, we should make the following five aspects crystal clear: (1) how to construct or select a valid benchmark dataset to train and test the predictor; (2) how to formulate the biological sequence samples with an effective mathematical expression that can truly reflect their intrinsic correlation with the target to be predicted; (3) how to introduce or develop a powerful algorithm (or engine) to operate the prediction; (4) how to properly perform cross-validation tests to objectively evaluate its anticipated accuracy; (5) how to establish a user-friendly web-server that is accessible to the public. Below, we are to address the five procedures one-by-one.\n\nTwo benchmark datasets were used for the current study. One is the \u201csurface-residue\u201d dataset and the other is \u201call-residue\u201d dataset, as described below. The protein-protein interfaces are usually formed by those residues, which are exposed to the solvent after the two counterparts are separated from each other [21]. Given a protein sample with L residues as expressed by:\n(1)P=R1R2R3R4R5R6R7\u22efRL\nwhere R1 represents the 1st amino acid residue of the protein P, R2 the 2nd residue, and so forth. The residue Ri (i=1,2,\u22ef,L) is deemed as a surface residue if it satisfies the following condition:\n(2)\u03d5(Ri)=ASA(Ri|P)ASA(Ri)>25%\nwhere ASA(Ri|P) is the accessible surface area of Ri when it is a part of protein P, ASA(Ri) is the accessible surface area of the free Ri that is actually its maximal accessible surface area as given in Table 1 [22], and \u03d5(Ri) is the ratio of the two.\n\nFurthermore, the surface residue Ri is deemed as interfacial residue [23] if:\n(3)ASA(Ri|P)\u2212ASA(Ri|PP)>1\u212b2\nwhere ASA(Ri|PP) is the accessible surface area of Ri when it is a part of protein-protein complex.\n\nFor a given protein, we can use DSSP program [24] to find out all its surface residues based on Equation (2), and use PSAIA program [25] to find all its interfacial residues based on Equation (3).\n\nIf only considering the surface residues as done in [26] for the 99 polypeptide chains extracted by Deng et al. [10] from the 54 heterocomplexes in the Protein Data Bank, we have obtained the results that can be formulated as follows:\n(4)Ssurf=Ssurf+\u222a\u200bSsurf\u2212\nwhere Ssurf  is called the \u201csurface-residue dataset\u201d that contains a total of 13,771 surfaces residues, of which 2828 are interfacial residues belonging to the positive subset Ssurf+ while 10,943 are non-interfacial residues belonging the negative subset Ssurf\u2212, and \u222a\u200b is the symbol of union in the set theory.\n\nIf considering all the residues as done in [11], however, the corresponding benchmark dataset can be expressed by:\n(5)Sall=Sall+\u222a\u200b Sall\u2212\nwhere Sall is called the \u201call-residue dataset\u201d that contains a total of 27,442 residues, of which 2828 are interfacial residues belonging to the positive subset Sall+ while 24,614 are non-interfacial residues belonging the negative subset Sall\u2212.\n\nFor readers\u2019 convenience, given in S1 Dataset (List of the 99 proteins and their residues\u2019 attributions associated with the protein-protein binding sites is in Supplementary Materials) is a combination of the two benchmark datasets, where those labeled in column 3 are all the residues determined by experiments, those in column 4 are of surface and non-surface residues, and those in column 5 are of interface and non-interface residues.\n\nAs pointed out in a comprehensive review [27] there is no need to separate a benchmark dataset into a training dataset and a testing dataset for examining the quality of a prediction method if it is tested by the jackknife test or subsampling (K-fold) cross-validation test because the outcome obtained via this kind of approach is actually from a combination of many different independent dataset tests.\n\nGiven a protein chain as expressed in Equation (1), the sliding window approach [28] and flexible sliding window approach [29] are often used to investigate its various posttranslational modification (PTM) sites [16,30,31,32,33,34] and HIV (human immunodeficiency virus) protease cleavage sites [35]. Here, we also use it to study protein-protein binding sites. In the sliding window approach, a scaled window is denoted by [\u2013\u03be, +\u03be] [28], and its width is 2\u03be+1, where \u03be is an integer. When sliding it along a protein chain P, one can see through the window a series of consecutive peptide segments as formulated by:\n(6)P\u03be(\u211d0)=R\u2212\u03beR\u2212(\u03be\u22121)\u22efR\u22122R\u22121\u211d0R+1R+2\u22efR+(\u03be\u22121)R+\u03be\nwhere R\u2212\u03be represents the \u03be-th upstream amino acid residue from the center, R+\u03be the \u03be-th downstream amino acid residue, and so forth. The amino acid residue \u211d0 at the center is the targeted residue. When its sequence position in P (cf. Equation (1) is less than \u03be or greater L\u2212\u03be, the corresponding P\u03be(\u211d0)  is defined, rather than by P of Equation (1), but by the following dummy protein chain:\n(7)P(dummy)=\u211d\u03be\u22ef\u211d2\u211d1\u21d5R1R2\u22efR\u03be\u22efRi\u22efRL\u2212\u03be+1\u22efRL\u22121RL \u21d5\u211dL\u211dL\u22121\u22ef\u211dL\u2212\u03be+1\nwhere the symbol \u21d5 stands for a mirror, the dummy segment \u211d\u03be\u22ef\u211d2\u211d1 stands for the image of R1R2\u22efR\u03be reflected by the mirror, and the dummy segment \u211dL\u211dL\u22121\u22ef\u211dL\u2212\u03be+1 for the mirror image of RL\u2212\u03be+1\u22efRL\u22121RL (Figure 1). Accordingly, P(dummy) of Equation (7) is also called the mirror-extended chain of protein P.\n\nThus, for each of the L amino acid residues in protein P, we have a working protein segment as defined by Equation (6). In the current study, the (2\u03be+1)-tuple peptide P\u03be(\u211d0) can be further classified into the following categories:\n(8)P\u03be(\u211d0){P\u03be+(\u211d0), if its center is a PPBSP\u03be\u2212(\u211d0),                    otherwise \nwhere \u2208 represents \u201ca member of\u201d in the set theory.\n\nDifferent types of amino acid in the above equation may have different physicochemical properties. In this study, we considered the following seven physicochemical properties: (1) hydrophobicity [66] or \u03a6(1); (2) hydrophicility [67] or  \u03a6(2); (3) side-chain volume [68] or \u03a6(3); (4) polarity [69] or \u03a6(4); (5) polarizability [70] or \u03a6(5); (6) solvent-accessible surface area (SASA) [71] or \u03a6(6); and (7) side-chain net charge index (NCI) [72] or \u03a6(7). Their numerical values are given in Table 2.\n\nThus, the peptide segment P\u03be of Equation (10) can be encoded into seven different numerical series, as formulated by:\n(11)P\u03be={\u03a61(1)\u03a62(1)\u03a63(1)\u03a64(1)\u03a65(1)\u03a66(1)\u03a67(1)\u22ef\u03a62\u03be+1(1)\u03a61(2)\u03a62(2)\u03a63(2)\u03a64(2)\u03a65(2)\u03a66(2)\u03a67(2)\u22ef\u03a62\u03be+1(2)\u03a61(3)\u03a62(3)\u03a63(3)\u03a64(3)\u03a65(3)\u03a66(3)\u03a67(3)\u22ef\u03a62\u03be+1(3)\u03a61(4)\u03a62(4)\u03a63(4)\u03a64(4)\u03a65(4)\u03a66(4)\u03a67(4)\u22ef\u03a62\u03be+1(4)\u03a61(5)\u03a62(5)\u03a63(5)\u03a64(5)\u03a65(5)\u03a66(5)\u03a67(5)\u22ef\u03a62\u03be+1(5)\u03a61(6)\u03a62(6)\u03a63(6)\u03a64(6)\u03a65(6)\u03a66(6)\u03a67(6)\u22ef\u03a62\u03be+1(6)\u03a61(7)\u03a62(7)\u03a63(7)\u03a64(7)\u03a65(7)\u03a66(7)\u03a67(7)\u22ef\u03a62\u03be+1(7)\nwhere \u03a61(1) is the hydrophobicity value of R1 in Equation (10), \u03a62(2) the hydrophilicity value of R2, and so forth. Note that before substituting the physicochemical values of Table 2 into Equation (10), they all are subjected to the following standard conversion:\n(12)\u03a6i(\u03be)\u21d0 \u03a6i\u03c6\u2212\u2329\u03a6i\u03c6\u232aSD(\u03a6i\u03c6) (\u03c6=1, 2, \u22ef,7; i=1, 2, \u22ef,2\u03be+1)\nwhere the symbol \u2329 \u232a means taking the average for the quantity therein over the 20 amino acid types, and SD means the corresponding standard deviation. The converted values via Equation (12) will have zero mean value over the 20 amino acid types, and will remain unchanged if they go through the same standard conversion procedure again.\n\nThe low-frequency internal motion is a very important feature of biomacromolecules (see, e.g., [73,74,75]. Many marvelous biological functions in proteins and DNA and their profound dynamic mechanisms, such as switch between active and inactive states [76,77], cooperative effects [78], allosteric transition [79,80,81], intercalation of drugs into DNA [82], and assembly of microtubules [83], can be revealed by studying their low-frequency internal motions as summarized in a comprehensive review [84]. Low frequency Fourier spectrum was also used by Liu et al. [85] to develop a sequence-based method for predicting membrane protein types. In view of this, it would be intriguing to introduce the stationary wavelet transform into the current study.\n\nThe stationary wavelet transform (SWT) [86] is a wavelet transform algorithm designed to overcome the lack of shift-invariance of the discrete wavelet transform (DWT) [87]. Shift-invariance is achieved by removing the downsamplers and upsamplers in the DWT and upsampling (insert zero) the filter coefficients by a factor of 2j\u22121 in the j-th level of the algorithm. The SWT is an inherently redundant scheme as the output of each level of SWT contains the same number of samples as the input-so for a decomposition of N levels there is a redundancy of N in the wavelet coefficients. Shown in Figure 2 is the block diagram depicting the digital implementation of SWT. As we can see from the figure, the input peptide segment is decomposed recursively in the low-frequency part.\n\nThe concrete procedure of using the SWT to denote the (2\u03be+1)-tuple peptides is as follows. For each of the (2\u03be+1)-tuple peptides generated by sliding the scaled window[\u2212\u03be, +\u03be] along the protein chain concerned, the SWT was used to decompose it based on the amino acid values encoded by the seven physicochemical properties as given in Equation (11). Daubechies of number 1 (Db1) wavelet was selected because its wavelet possesses a lower vanish moment and easily generates non-zero coefficients for the ensemble learning framework that will be introduced later.\n\nPreliminary tests indicated that, when \u03be=7,\ni.e., the working segments are 15-tuple peptides, the outcomes thus obtained were most promising. Accordingly, we only consider the case of \u03be=7 hereafter.\n\nUsing the SWT approach, we have generated five sub-bands (Figure 2), each of which has four coefficients: (1) \u03b1i, the maximum of the wavelet coefficients in the sub-band i (1,2, \u22ef5); (2) \u03b2i, the corresponding mean of the wavelet coefficients; (3) \u03b3i, the corresponding minimum of the wavelet coefficients; (4) \u03b4i, the corresponding standard deviation of the wavelet coefficients. Therefore, for each working segment, we can get a feature vector that contains \u03a9=5\u00d74=20 components by using each of the seven physicochemical properties of Equation (11). In other words, we have seven different modes of PseAAC as given below:\n(13)P(k)=[\u03a81(k) \u03a82(k) \u03a83(k) \u22ef \u03a8u(k) \u22ef \u03a820(k)]T (k=1, 2, \u22ef, 7)\nwhere:\n(14)\u03a8\u03bc(k)={\u03b1\u03bc(k)when 1\u2264\u03bc\u22645\u03b2\u03bc\u22125(k)when 6\u2264\u03bc\u226410\u03bb\u03bc\u221210(k)when 11\u2264\u03bc\u226415\u03b4\u03bc\u221215(k)when 11\u2264\u03bc\u226420\n\nIn the current benchmark dataset Ssurf or Sall , the negative subset Sall\u2212 or Ssurf\u2212  is much larger than the corresponding positive subset Sall+ or Ssurf+ as can be seen by the following equation:\n(15){Ssurf(13771)=Ssurf+(2828)\u222a\u200bSsurf\u2212(10943)for surface residuces Sall (27442)=Sall+(2828)\u222a\u200bSall\u2212(24614)for all residues\nwhere the figures in the parentheses denote the sample numbers taken from Section 2.1. As we can see from Equation (15), the numbers of the negative samples are nearly nine and four times the sizes of the corresponding positive samples for the all-residue and surface-residue benchmark datasets, respectively.\n\nAlthough this might reflect the real world in which the non-binding sites are always the majority compared with the binding ones, a predictor trained by such a highly skewed benchmark dataset would inevitably have the bias consequence that many binding sites might be mispredicted as non-binding ones [88]. Actually, what is really the most intriguing information for us is the information about the binding sites. Therefore, it is important to find an effective approach to optimize the unbalanced training dataset and minimize this kind of bias consequence. To realize this, we took the following procedures.\n\nFirst, we used the K-Nearest Neighbors Cleaning (KNNC) treatment to remove some redundant negative samples from the negative subset so as to reduce its statistical noise. The detailed process can be described below: (i) for each of the samples in the negative subset S\u2212 find its K nearest neighbors, where K may be any integer (such as 3 or 8), and its final value will be discussed later; (ii) if one of its K nearest neighbors belongs to the positive subset S+, remove the negative sample from S\u2212. A similar method, called the Neighborhood Cleaning Rule (NCR), was also been used by Laurikkala et al. [89], Xiao et al. [90], and Liu et al. [91] although their details are different with the current practice. Also, the current KNNC approach is more flexible because it contains a variable K and hence can be used to deal with various different training datasets.\n\nSecond, we used the Inserting Hypothetical Training Samples (IHTS) treatment to add some hypothetical positive samples into the positive subset so as to enhance the ability in identifying the interactive pairs. For the details of how to generate the hypothetical training samples, see the Monte Calo samples expanding approach in [92,93], or seed-propagation approach in [94], or the SMOTE (synthetic minority over-sampling technique) approach in [95].\n\nAfter the above two treatments, we can change an original highly skewed training dataset to a balanced training dataset with its positive subset and negative subset having exactly the same size.\n\nIt is instructive to point out that the hypothetical samples generated via the IHTS treatment can only be expressed by their feature vectors as defined in Equation (13), but not the real peptide segment samples as given by Equations (6) or (10). Nevertheless, it would be perfectly reasonable to do so because the data directly used to train a predictor were actually the samples\u2019 feature vectors but not their sequence codes. This is the key to optimize an imbalanced benchmark dataset in the current study, and the rationale of such an interesting approach will be further elucidated later.\n\nThe random forest (RF) algorithm is a powerful algorithm, which has been used in many areas of computational biology (see, e.g., [44,96,97]). The detailed procedures and formulation of RF have been very clearly described in [98], and hence there is no need to repeat here.\n\nAs shown in Equations (11)\u2013(13), a peptide segment concerned in the current study can be formulated with seven different PseAAC modes, each of which can be used to train the random forest predictor after the KNNC and IHTS procedures. Accordingly, we have a total of seven individual predictors for identifying PPBS, as formulated by:\n(16)PPBS individual predictor=\u211dF(k) (k=1, 2, \u22ef, 7)\nwhere \u211dF(k) represents the random forest predictor based on the k-th physicochemical property (cf. Equation (13)).\n\nNow, the problem is how to combine the results from the seven individual predictors to maximize the prediction quality. As indicated by a series of previous studies, using the ensemble classifier formed by fusing many individual classifiers can remarkably enhance the success rates in predicting protein subcellular localization [99,100] and protein quaternary structural attribute [101]. Encouraged by the previous investigators\u2019 studies, here we are also developing an ensemble classifier by fusing the seven individual predictors \u211dF(k) (k=1,2,\u22ef,7) through a voting system, as formulated by:\n(17)\u211dFE=\u211dF(1)\u2200\u22ef\u2200\u211dF(7)=\u2200k=17\u211dF(k)\nwhere \u211dFE stands for the ensemble classifier, and the symbol \u2200 for the fusing operator. For the detailed procedures of how to fuse the results from the seven individual predictors to reach a final outcome via the voting system, see Equations (30)\u2013(35) in [27], where a crystal clear and elegant derivation was elaborated and hence there is no need to repeat here. To provide an intuitive picture, a flowchart is given in Figure 3 to illustrate how the seven individual RF predictors are fused into the ensemble classifier.\n\nThe final predictor thus obtained is called \u201ciPPBS-Opt\u201d, where \u201ci\u201d stands for \u201cidentify\u201d, \u201cPPBS\u201d for \u201cprotein-protein binding site\u201d, and \u201cOpt\u201d for \u201coptimizing\u201d training datasets. Note that the iPPBS-Opt predictor contains a parameter K, reflecting how many nearest neighbors should be considered in removing the redundant negative samples from the training dataset during the KNNC treatment (cf.\nSection 2.4). Its final value is determined by maximizing the overall success rate via cross-validation, as will be described later.\n\nFor measuring the success rates in identifying PPBS, a set of four metrics are usually used in literature. They are: (1) overall accuracy or Acc; (2) Mathew\u2019s correlation coefficient or MCC; (3) sensitivity or Sn; and (4) specificity or Sp (see, e.g., [102]). Unfortunately, the conventional formulations for the four metrics are not quite intuitive for most experimental scientists, particularly the one for MCC. Interestingly, by using the symbols and derivation as used in [103] for studying signal peptides, the aforementioned four metrics can be formulated by a set of equations given below [14,30,60,61,104]:\n(18){Sn=1\u2212N\u2212+N+0\u2264Sn\u22641Sp=1\u2212N+\u2212N\u22120\u2264Sp\u22641Acc=\u039b=1\u2212N\u2212++N+\u2212N++N\u22120\u2264Acc\u22641Mcc=1\u2212(N\u2212++N+\u2212N++N\u2212)(1+N+\u2212\u2212N\u2212+N+)(1+N\u2212+\u2212N+\u2212N\u2212)\u22121\u2264Mcc\u22641\nwhere N+ represents the total number of PPBSs investigated whereas N\u2212+ the number of true PPBSs incorrectly predicted to be of non-PPBS; N\u2212 the total number of the non-PPBSs investigated whereas N+\u2212 the number of non-PPBSs incorrectly predicted to be of PPBS.\n\nAccording to Equation (18), it is crystal clear to see the following. When N\u2212+=0 meaning none of the true PPBSs are incorrectly predicted to be of non-PPBS, we have the sensitivity Sn=1. When N\u2212+=N+ meaning that all the PPBSs are incorrectly predicted to be of non-PPBS, we have the sensitivity Sn=0. Likewise, when N+\u2212 = 0 meaning none of the non-PPBSs are incorrectly predicted to be of PPBS, we have the specificity Sp=1; whereas N+\u2212 = N\u2212 meaning that all the non-PPBSs are incorrectly predicted to be of PPBS, we have the specificity Sp=0. When N\u2212+=N+\u2212=0 meaning that none of PPBSs in the positive dataset and none of the non-PPBSs in the negative dataset are incorrectly predicted, we have the overall accuracy Acc=1 and MCC=1; when N\u2212+=N+ and N+\u2212 = N\u2212 meaning that all the PPBSs in the positive dataset and all the non-PPBSs in the negative dataset are incorrectly predicted, we have the overall accuracy Acc=0 and MCC=\u22121; whereas when N\u2212+=N+/2 and N+\u2212 = N\u2212/2 we have Acc=0.5 and MCC=0 meaning no better than random guess. As we can see from the above discussion, it would make the meanings of sensitivity, specificity, overall accuracy, and Mathew\u2019s correlation coefficient much more intuitive and easier-to-understand by using Equation (18), particularly for the meaning of MCC.\n\nIt should be pointed out, however, the set of metrics as defined in Equation (18) is valid only for the single-label systems. For the multi-label systems whose emergence has become more frequent in system biology [46,105,106] and system medicine [107], a completely different set of metrics as defined in [108] is needed.\n\nOnce established the evaluation metrics, the next issue is the selection of the most appropriate validation method should be used to derive the values of these metrics. Three cross-validation methods are often used to derive metrics values in statistical prediction: the independent dataset test, subsampling (or K-fold cross-validation) test, and jackknife test [109]. Of the three the jackknife test is deemed the least arbitrary as it can always yield a unique outcome for a given benchmark dataset, as elucidated in [12] and demonstrated by Equations (28)\u2013(32) therein. Accordingly, the jackknife test has been widely recognized and increasingly used by investigators to examine the quality of various predictors (see, e.g., [46,53,54,110,111,112,113,114,115]. However, to reduce the computational time, in this study we adopted the 10-fold cross-validation, as done by most investigators with SVM and random forests algorithms as the prediction engine.\n\nWhen conducting the 10-fold cross-validation for the current predictor iPPBS-Opt, however, some special consideration is needed. This is because a dataset, after optimized by the KNNC and ITHTS treatments, may miss many experimental negative samples and contain some hypothetical positive samples. It would be fine to use such a dataset to train a predictor, but not for validation. Since the validation should be conducted based on all the experimental data in the benchmark dataset but not on the added hypothetical samples nor only on the data in the reduced negative subset, a special cross-validation, the so-called target cross-validation, has been introduced here. During the target cross-validation process for the positive samples, only the experiment-confirmed samples are singled out as the targets (or test samples) for validation; but during the target cross-validation process for the negative samples, even all the excluded experimental data are taken into account. The detailed procedures of the target 10-fold cross-validation are as follows:\n\nStep 1. Before optimizing the original benchmark dataset, both its positive and negative subsets were randomly divided into 10 parts with about the same size. For example, for the all-residue benchmark dataset Sall , after such evenly division we have:\n(19)Sall =Sall(1)\u222a\u200bSall(2)\u222a\u200b\u22ef\u222a\u200bSall(10)= \u222a\u200bi=110Sall(i)\nand:\n(20)Sall(1)\u225c Sall(2)\u225c \u22ef\u225c =Sall(10) \nwhere the symbol \u225c means that the divided 10 datasets are about the same in size, and so are their subsets.\n\nStep 2. One of the 10 sets, say Sall(1), was singled out as the testing dataset and the remaining nine sets as the training dataset.\n\nStep 3. The training set was optimized using the KNNC and IHTS treatments as described in Section 2.4. After such a process, the original imbalanced training dataset would become a balanced one; i.e., its positive subset and negative subset would contain a same number of samples. Note that although the starting value for K in the KNNC treatment could be arbitrary, the following empirical approach might be of help to reduce the time for finally finding its optimal value. Suppose the starting value for K is K(0), then we have according to our experience\n(21)K(0)=Int[N\u2212N+]\nwhere  N+ and N\u2212  are the numbers of the total positive and negative samples in the benchmark dataset, respectively, and Int is the \u201cinteger truncation operator\u201d meaning to take the integer part for the number in the brackets right after it [116]. Substituting the data of Equation (15) into Equation (21), we obtained K(0)=3 or 8 for the surface-residue case or of all-residue case, respectively.\n\nStep 4. Use the aforementioned balanced dataset to train the operation engine, followed by applying the iPPBS-Opt predictor to calculate the prediction scores for the testing dataset, which had been singled out in Step 2 before the optimized treatment and hence contained the experiment-confirmed samples only.\n\nStep 5. Repeat Steps 2\u20134 until all the 10 divided sets had been singled out one-by-one for testing validation.\n\nStep 6. Substituting the scores obtained from the above 10-round tests into Equation (18) to calculate Sn, Sp, Acc, and MCC. The metrics values thus obtained should be a function of K; for instance, the overall accuracy Acc can be expressed as Acc(K).\n\nStep 7. Repeat Steps 2\u20136 by increasing K with a gap of 1, we consecutively obtained Acc(3), Acc(4), \u2026., Acc(12) for the surface-residue case or Acc(8), Acc(9), \u2026., Acc(17) for the all-residue case, respectively (Figure 4). The value of K that maximized Acc would be taken for iPPBS-Opt in the current study, as given in the footnote c of Table 3.\n\nIt is instructive to emphasize again that it is absolutely fair to use the above 10-fold cross-validation steps to compare the current predictor with the existing ones. This is because all the predictors concerned were tested using exactly the same experiment-confirmed samples and that all the added hypothetical samples had been completely excluded from the testing datasets.\n\nListed in Table 3 are the values of the four metrics (cf. Equation (18)) obtained by the current iPPBS-Opt predictor using the target 10-fold cross-validation on the surface-residue benchmark dataset Ssurf (Equation (4)) and the all-residue benchmark dataset Sall (Equation (5)), respectively. See S1 Dataset for the details of the two benchmark datasets. For facilitating comparison, the corresponding results obtained by the existing methods [10,11] are also given there.\n\nAs we can see from the table, the new predictor iPPBS-Opt proposed in this paper remarkably outperformed its counterparts, particularly in Acc and MCC; the former stands for the overall accuracy, and the latter for the stability. At the first glance, although the value of Sn by Deng et al.\u2019s method [10] is higher than that of the current predictor when tested by the surface-residue benchmark dataset, its corresponding Sp value is more than 30% lower than that of the latter, indicating the method [10] is very unstable with extremely high noise.\n\nBecause graphic approaches can provide useful intuitive insights (see, e.g., [117,118,119,120,121,122]), here we also provide a graphic comparison of the current predictor with their counterparts via the Receiver Operating Characteristic (ROC) plot [123], as shown in (Figure 5). According to ROC [123], the larger the area under the curve (AUC), the better the corresponding predictor is. As we can see from the figure, the area under the ROC curve of the new predictor is remarkably greater than those of their counterparts fully consistent with the AUC values listed on Table 3, once again indicating a clear improvement of the new predictor in comparison with the existing ones.\n\nAll the above facts have shown that iPPBS-Opt is really a very promising predictor for identifying protein-protein binding sites. Or at the very least, it can play a complementary role to the existing prediction methods in this area. Particularly, none of the existing predictors has provided a web server. In contrast to this, a user-friendly and publically accessible web server has been established for iPPBS-Opt at http://www.jci-bioinfo.cn/iPPBS-Opt, which is no doubt very useful for the majority of experimental scientist in this or related areas without the need to follow the complicated mathematical equations.\n\nWhy could the proposed method be so powerful? The reasons are as follows: First, the KNNC and IHTS treatments have been introduced to optimize the training datasets, so as to avoid many misprediction events caused by the highly imbalanced training datasets used in previous studies. Second, the ensemble technique has been utilized in this study to select the most relevant one from seven classes of different physicochemical properties. Third, the wavelets transform technique has been applied to extract some important key features, which are deeply hidden in complicated protein sequences. This is just like the studies in dealing with the extremely complicated internal motions of proteins, it is the key to grasp the low-frequency collective motion [74,75] for in-depth understanding or revealing the dynamic mechanisms of their various important biological functions [84], such as cooperative effects [78], allosteric transition [80,81], assembly of microtubules [83], and switch between active and inactive states [76]. Fourth, the PseAAC approach has been introduced to formulate the statistical samples, which has been proved very useful not only in dealing with protein/peptide sequences, but also in dealing with DNA/RNA sequences, as elaborated in a recent review paper [124].\n\nTo enhance the value of its practical applications, a web-server for iPPBS-Opt has been established at http://www.jci-bioinfo.cn/iPPBS-Opt. Furthermore, to maximize the convenience for the majority of experimental scientists, a step-to-step guide is provided below:\n\nStep 1. Opening the web-server at http://www.jci-bioinfo.cn/iPPBS-Opt, you will see the top page of iPPBS-Opt on your computer screen, as shown in Figure 6. Click on the Read Me button to see a brief introduction about the iPPBS-Opt predictor.\n\nStep 2. Either type or copy/paste the query protein sequences into the input box at the center of Figure 6. The input sequence should be in the FASTA format. For the examples of sequences in FASTA format, click the Example button right above the input box.\n\nStep 3. Click on the Submit button to see the predicted result. For example, if you use the two query protein sequences in the Example window as the input, after 20 s or so, you will see the following on the screen of your computer: (1) Sequence-1 contains 109 amino acid residues, of which 11 are highlighted with red, meaning belonging to binding site; (2) Sequence-2 contains 275 residues, of which 25 are highlighted with red, belonging binding site. All these predicted results are fully consistent with experimental observations except for residues 53 in sequence-1 and residues 62 and 249 in sequence-2 that are overpredicted.\n\nStep 4. As shown on the lower panel of Figure 6, batch prediction can also be selected by entering an e-mail address and the desired batch input file (in FASTA format naturally) via the Browse button. To see the sample of batch input file, click on the button Batch-example.\n\nStep 5. Click on the Citation button to find the relevant papers that document the detailed development and algorithm of iPPBS-Opt.\n\nStep 6. Click the Supporting Information button to download the benchmark dataset used in this study.\n\nIt is a very effective approach to optimize the training dataset via the KNNC treatment and IHTS treatment to enhance the prediction quality in identifying the protein-protein binding sites. This is because the training datasets constructed in this area without undergoing such an optimization procedure are usually extremely skewed and unbalanced, with the negative subset being overwhelmingly larger than the positive one. It is anticipated that the iPPBS-Opt web server presented in this paper will become a very useful high throughput tool for identifying protein-protein binding sites, or at the very least, a complementary tool to the existing prediction methods in this area."}