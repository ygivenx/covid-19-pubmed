{"title": "Structural Resilience in Sewer Reconstruction", "body": "Reviews of resilience theories and mathematical generalization 2.1 Resilience theories and practices in socioecological systems Abbreviations\n\nSocioecological system CAS Complex adaptive system CFCs Chlorofluorocarbons SARS Severe acute respiratory syndrome NGO Nongovernmental organization In this section, the resilience approach in socioecological systems (SESs) is introduced first, including its underlying rationales and assumptions. Then, the key principles for enhancing the capacity of SES to deal with change and disturbance are discussed, along with their social and political implications. For more detailed discussions, refer to the original publication (Biggs et al., 2015a ).\n\nSES research is a fast-growing interdisciplinary field of study, including the social, economic, political and ecological sciences. The resilience approach focuses specifically on the capacity of SES to deal with change, particularly unexpected change in these kinds of systems. Here, resilience of an SES is defined as the capacity of an SES to sustain human well-being in the face of change, both by buffering shocks but also through adapting or transforming in response to change.\n\nSince the beginning of the 20th century, mankind has experienced unparalleled rapid development due to huge advances in science and technology, manifested by a 15-fold increase in the global economy, large-scale conversion of land to agriculture and an increase in the global population from 1.6 billion in 1900 to over 7 billion in 2011 (MA, 2005; Steffen et al., 2007) . The rapid changes have brought huge benefits and dramatic improvements to people's lives. Consider the average life expectancy for example. For most of human history the figure stood at 20\u00c030 years, reflecting the combined effects of poor nutrition, disease, warfare, and especially the low infant survival rate (Lancaster, 1990) . In 2010, the average life expectancy had reached 67 years, which was an astonishing feat when considering the fact that in 1900 it was merely 31 years (CIA, 2013) . And it is predicted to continue increasing.\n\nAlthough the command of modern technology has enabled mankind to cope better with nature, it does not put human beings above nature. Ultimately people still depend largely on nature for a variety of essential needs, including fresh air, clean water and food, protection from hazards such as droughts and storms, and a wide variety of cultural, spiritual and recreational needs that play a key role in human well-being. Such benefits derived from the interaction of people with nature are known as ecosystem services (Ernstson, 2013; Reyers et al., 2013; Huntsinger and Oviedo, 2014) . Ecosystem services can be divided into three categories: provisioning services such as food and fuel, regulating services such as disease and flood control, and cultural services such as sports and education.\n\nIt has become increasingly evident that the massive scale and extent of human activities such as agriculture, transport, release of novel chemicals, and various environmental problems are degrading the capacity of nature to generate key ecosystem services on which we depend. Climate change provides a prime example. It is scientifically proven that rising atmospheric carbon dioxide levels resulting from anthropogenic fossil-fuel combustion and land clearing are changing rainfall and temperature patterns around the globe and leading to an increased incidence of extreme events such as droughts and storms (IPCC, 2014) . Other undermining effects on nature include biodiversity loss and changes in nutrient cycles. All these changes are impacting food security, disease prevalence and infrastructure, as well as affecting traditional lifestyles and cultural practices that shape people's identity.\n\nHuman-induced changes to the environment are also increasing the risk of crossing critical thresholds or tipping points in the ecosystem that could lead to large, nonlinear and potentially irreversible changes at local through global scales, such as the death of coral reefs, shifts in regional monsoon rainfall patterns or the collapse of the Greenland ice sheet (MA, 2005; Rockstr\u00f6m et al., 2009; Barnosky et al., 2012) . Beyond these somewhat known effects, our impacts on the environment are also leading to completely novel changes that are very difficult to anticipate, and could have dramatic impacts on a variety of ecosystem services. The formation of the ozone hole due to the use of chlorofluorocarbons in refrigerators (Farman et al., 1985) , the potential emergence and spread of new diseases such as severe acute respiratory syndrome, and the potential consequences of nuclear proliferation and massively increased global connectivity and trade on the environment are all grave concerns (Martin, 2007) .\n\nIn the face of these rapid, ongoing changes to the environment and human society, continued human well-being obviously requires breakthrough approaches to cope with change, especially unexpected change, as part of their normal functioning. In the words of Holling, the founder of resilience theory in ecology, It is that creation of something fundamentally novel that gives an evolutionary character to development of a region that might make sustainable development an achievable reality rather than an oxymoron (Holling, 1996) . As Holling showed us in his epochal review paper published in 1973, \"Resilience and stability of ecological systems\" (Holling, 1973) , nature's reaction to change teaches us the best lessons. Based on his shrewd observation, he offered two different interpretations of the behavior of ecological systems:\n\nThe whole sequence of environmental changes can be viewed as changes in parameters or driving variables, and the long persistence in the face of these major changes suggests that natural systems have a high capacity to absorb change without dramatically altering. . . .. . . It is useful to distinguish two kinds of behaviour. One can be termed stability, which represents the ability of a system to return to an equilibrium state after a temporary disturbance; the more rapidly it returns and the less it fluctuates, the more stable it would be. But there is another property, termed resilience, that is a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables.\n\nThe resilient property of nature best exemplifies system resilience to change and disturbance. Since the publication of Holling's resilience theory, the resilience approach has increasingly attracted researchers' attention, beginning with ecological studies and later expanding to socioecological studies, computer science, engineering and civil infrastructural systems, focusing on system resilience against change, expected or unexpected, incremental or abrupt. In general, resilience research is about studying system attributes that may promote or undermine the resilience of a system, identifying key principles for building resilience and establishing practical procedures for their applications in real-world settings.\n\nFundamental to the resilience approach of SES is the view that human beings are integrated into the biosphere, or the global ecological system at local to global scales and their activities help shape their environment, which they are fundamentally dependent on for various ecosystem services that underpin human well-being (Berkes and Folke, 1998; Berkes et al., 2003; Walker and Salt, 2006) . These intertwined SESs are assumed to be complex adaptive systems (CASs), i.e., they have the capacity to self-organize and adapt based on past experience and changing conditions, and are characterized by nonlinear dynamics that may generate uncertainties regarding system behavior (Norberg and Cumming, 2008) . Consequently, in SES the resilience approach to change involves not only recovery from unexpected shocks and avoiding undesirable thresholds or tipping points, but also the capacity to adapt to ongoing change and fundamentally transform SES if necessary (Walker et al., 2004; Folke et al., 2010) . The resilience approach is considered to lie within the broad emerging field of sustainability science, which studies the interactions between nature and society for long-lasting human well-being without compromising the social, economic and environmental foundations.\n\nAlthough resilience theories in SESs have not yet reached the stage of maturity, a certain common sense has been built on several theorems or principles that are considered important to building system resilience in SES. The following seven principles have been selected by a group of young resilience scholars from a wide range of disciplinary backgrounds, forming a tentative theoretical framework for developing the resilience theories in SES. These are: Principle 1-Maintain disturbance (response diversity) are considered to be particularly important for resilience (Elmqvist et al., 2003; Leslie and McCabe, 2013; Mori et al., 2013) .\n\nRedundancy represents the replication of elements or pathways in a system and is determined by the number of elements that perform a particular function similarly (Walker, 1992) . Redundancy provides potential means for replacing failed or lost elements, thus increasing system reliability. In general, elements that show different responses to disturbances provide response diversity, while elements that perform a particular function similarly provide redundancy for that function.\n\nEcosystem services are viewed as coproduced by the ecological and social components of an SES (Reyers et al., 2013) . Take fish or vegetables that we eat as an example of a provisioning ecosystem service, which are produced by a combination of ecological processes and the human knowledge and skills to catch or cultivate, store and transport the produce to the place where it is consumed. The resilience of an ecosystem service may be influenced by redundancy and diversity pertaining to the system elements involved in producing that ecosystem service.\n\nRedundancy due to the presence of multiple elements with similar functional roles allows for substitution among elements when the failure or loss of elements occurs, thus ensuring functional continuity in an SES. In the case of farming, to ensure a continuous food supply it is common practice for poor small-scale farmers to plant several different types of food crop to reduce the potential impact on food provision caused by failure of any one crop (Altieri, 2009) . The fact that several crops contribute to the provision of food and can be substituted for each other provides redundancy in food supply for these farmers. It is known that different crop types or species often respond differently to disturbances such as drought or disease (response diversity) and hence simultaneous failure of all crops is unlikely. Therefore, in this case the redundancy and diversity in food supply for these farmers enhance the resilience of this particular provisioning ecosystem service.\n\nTake the social governance system as another example. The existence of a variety of organizational forms such as government department, nongovernmental organization and community organization with overlapping domains of authority can provide both redundancy and response diversity in governing or regulating the usage of ecosystem services. In regulating the use of a particular ecosystem service, if different organizations all play a similar role, redundancy is present because the regulatory function will continue even if one or more of the organizations become dysfunctional. As organizations with different sizes, cultures, funding mechanisms and internal structures are likely to respond in different ways to various socioecological challenges, these differences between organizational forms and structures naturally lead to response diversity (Williamson, 1985; Ostrom, 2005) . Hence, if managed properly, the functional redundancy and response diversity in the social governance system are likely to enhance the system's adaptive capacity and resilience for maintaining ecosystem services and dealing with disturbance and change.\n\nSeveral issues concerning diversity and redundancy should be addressed. It is known that in some SESs, high redundancy and low response diversity may coexist. This happens when there are many system elements contributing to a particular ecosystem service, but all the elements have similar structures either by design (e.g., human institutions or activities) or due to environmental or historical constraints. This kind of system may function well when change and disturbance occur within the traditional range of variation experienced by the system, and thus resilience is expected for the provision of ecosystem services. However, when facing new types of disturbance, such as those caused by climate change, the system is likely to be vulnerable because of the lack of options available for responding to the new challenges (Janssen et al., 2007) . Also, a very high level of diversity and redundancy can sometimes have negative impacts on the resilience of ecosystem services, because it increases the possibility for inactivity within some aspects of SES. In management organizations, high redundancy may hinder governance functions for ecosystem services, because it tends to increase not only administrative costs, but also the potential for power struggles and contradictory regulations that are known to compromise the ability of governance systems to respond effectively to change (Jentoft et al., 2009 ). These problems should be noted when studying diversity and redundancy for enhancing the resilience of ecosystem services in SES. In ecological systems, there is less evidence of negative influences of high diversity and redundancy on system resilience to change and disturbance. For more detailed discussions, refer to Kotschy et al. (2015) .\n\nConnectivity is defined as the way in which parts of an SES (e.g., species, landscape patches) interact with each other (e.g., exchanging information, transferring material). If a system can be considered as being composed of individual components, then connectivity refers to the nature and the strength of the interactions between these components. From a network perspective, all individual components of a system are nodes integrated into a web of connections that form the links. Examples of links are species interactions (e.g., the mutualism interaction between flowering plants and their animal pollinators), vegetation corridors across habitats or communication channels between human communities. The structure of an SES is determined by how the links are arranged within the system, which includes not only the structure of connections between components (e.g., with or without links, one-way, or mutual links), but also the strength of these links.\n\nIn general, connectivity in SES provides the pathways for the flow of energy, material, or information necessary for building the resilience of ecosystem services. Depending on its structure and strength, connectivity may facilitate recovery or constrain locally the spread of a disturbance, hence safeguarding ecosystem services against change (Nystr\u00f6m and Folke, 2001) .\n\nThe recolonization of coral reefs is a good example that demonstrates the importance of connectivity for the recovery of disturbed SESs and thus for the maintenance of their ecosystem-service resilience. Studies show that the degree of connectivity between remnant coral patches influences the extent of reef recolonization, and this connectivity is determined by the prevailing currents that allow coral recruitment between neighboring reefs (Treml et al., 2007; Mumby and Hastings, 2008) . Similarly, in disturbance experiments, scientists found that recovery of macrobenthic communities was largely determined by the degree of connectivity across metacommunities (Thrush et al., 2008) . Well-connected habitats with no physical barriers facilitate the recolonization of nearby sites, making loss of species due to change and disturbance less likely than poorly-connected habitats. This is why in many conservation initiatives that aim at enhancing the resilience of SES, approaches that protect, maintain, and restore connectivity between habitats are given priority.\n\nIn some cases, connectivity may enhance the resilience of ecosystem services not by facilitating recovery from a disturbance, but by acting as a barrier to the spread of disturbances (e.g., the spread of fire or of disease vectors). For instance, when local losses of ecosystem services due to local disturbance become inevitable, limited connectivity with the outside world can reduce the possibility of large-scale global effects, i.e., containing the potential loss of ecosystem services to a local scale. This function of connectivity is frequently utilized in fighting the spread of disease, wildfire, or nuclear contamination by isolating the affected area.\n\nIn general, connectivity facilitates the maintenance of biodiversity in the landscape, which is vital to the production of many ecosystem services, hence indirectly enhancing the resilience of SES. For example, among well-connected habitat patches, extinctions of local species may be avoided by the inflow of species from their surroundings. On the other hand, reduced connectivity caused by humaninduced fragmentation (e.g., roads, dams) can have negative effects on population viability. However, the issue of connectivity and the maintenance of biodiversity should not be oversimplified. Under extreme conditions the probability of population survival for an overly connected system may be lower than that for a moderately connected system (Baggio et al., 2011; Salau et al., 2012) .\n\nIn human social networks, connectivity can facilitate the resilience of ecosystem services through enhanced governance opportunities and functions. When high levels of connectivity between different social groups exist, information-sharing becomes smooth and the trust and reciprocity necessary for collective action can develop (Brondizio et al., 2009) . However, just like in natural systems detrimental effects on the resilience of ecosystem services may develop when overconnectivity of social networks occurs.\n\nThough connectivity in general can facilitate recovery or constrain the spread of a disturbance, in certain situations it is also known to have detrimental effects on the resilience of ecosystem services. Depending on the nature and scale of disturbance, in a strongly connected SES the possibility of rapid propagation of disturbances through dense pathways between parts of the system exists, which may lead to widespread impacts on SES and associated ecosystem services (Van Nes and Scheffer, 2005; Ash and Newth, 2007) . Typical examples of the high risk of propagation of disturbances in strongly connected systems can be found not only in ecological systems such as pest outbreaks and invasion of alien species, but also in socioeconomic systems such as financial crises, like the global spread of the 2008 recession triggered by the collapse of the US housing market (Adger et al., 2009; Biggs et al., 2011) . In these cases, the resilience of ecosystem services, such as food production, pest regulation, and safety of the financial system, is negatively impacted by high connectivity. For more detailed discussions, refer to Dakos et al. (2015) .\n\nSESs are self-organizing, CASs, which consist of and are influenced by a great number of variables that change and interact on a range of timescales. These variables can be divided into slow variables and fast variables, as some of them change much more gradually than others. Products of provisioning ecosystem services such as crop production and freshwater are examples of fast variables, and they are affected by soil composition and phosphorus concentration in lake sediments, which are examples of slow variables. Note that the notions of slow and fast variables are relative, and based on the context of the system each SES has its own timescales to determine its slow and fast variables (Walker et al., 2012) .\n\nIn most SESs, it seems that a limited set of key variables and internal feedback processes interact to control the configuration of the system, and these key variables are usually slow variables. Thus, the underlying structure of SES is typically defined by slow variables, and the dynamics of the system arise from the interactions and feedbacks between fast variables under the conditions created by the slow variables (Gunderson and Holling, 2002; Norberg and Cumming, 2008) . Regulating ecosystem services are often linked to slow ecological variables, such as erosion control, nutrient retention, and flood regulation (MA, 2003) .\n\nFeedbacks refer to the reversion effects on a variable, process, or signal under the current system conditions that have been influenced by an original change in that same variable, process or signal in an SES. The effect of feedbacks can either be reinforcing if it assists more change of the same kind, or damping if it resists further similar changes. These two feedbacks are also known as positive feedback and negative feedback. An example of reinforcing feedback is the exponential population growth with a positive growth rate: Population increase leads to more births that increase the population. On the other hand, law enforcement measures against crime produce damping feedbacks by discouraging unlawful behavior.\n\nFor all SESs, change and disturbance are assumed to be continuous threats, such as droughts and floods, while they are also exposed to more gradual, ongoing changes, such as increasing global trade connectivity, which often influence slow, controlling variables. It is important to know that the feedbacks within an SES contain critical information that reveals how the SES responds to such shocks and ongoing changes. In general, the possible configurations of an SES are shaped within certain bounds set by the controlling variables, which constitute the basic conditions for the system to develop its underlying structure and processes. For instance, under high rainfall conditions it is typical to find forests, and under low to medium rainfall conditions savannas (tropical grassland with scattered trees) are normally found. As the internal processes of an SES have little or only a very gradual influence on controlling variables, they are often regarded as being external to the system. Much research has been focused on understanding the links between controlling variables and SES outcomes.\n\nAlthough in many situations, the configuration of an SES can be derived from the key controlling variables, this is not always the case. It is known that for a given set of conditions, it is sometimes possible for an SES to adopt two or more configurations with completely different sets of ecosystem services. For example, under intermediate rainfall conditions (around 1000\u00c02500 mm per year), landscapes can exist either as open savannas (20%\u00c040% tree cover) or as closed wooded landscapes (around 80% tree cover), with no substantial differences in soil and other factors (Sankaran et al., 2005; Hirota et al., 2011; Staver et al., 2011) . In these situations, whether a particular place has savanna or forest at a particular point in time depends on the past configuration of the system, or more specifically, depends on which feedback processes have been dominant. With some disturbance, such as a drought or a large fire, the landscape can even abruptly shift from a forest to a savanna (or from a savanna to a forest with different disturbances), as a result of the change in the feedback processes triggered by the disturbance. These cases highlight the important role of the feedback processes in influencing SESs and the ecosystem services they provide.\n\nIn the face of change and disturbance, the self-organizing internal feedbacks of an SES typically buffer and absorb the impact of such shocks. Dampening feedbacks in particular are important in helping counteract disturbance and keep the system functioning consistently, thereby maintaining the current system configuration. In the previous example of imposing law enforcement measures against crime, the dampening feedbacks help keep social safety and maintain the way of life for ordinary people.\n\nHowever, no SES can sustain infinite shocks or endure boundless changes without its basic framework and functions being fundamentally affected. All SESs have limits to how large a shock or how much change they can be exposed to and still recover and keep functioning as before. When critical limits in controlling slow variables are exceeded, the feedbacks that keep the system in a particular configuration are significantly weakened or totally disappear, and thus the system is unable to buffer or absorb the changes. In the example of the forest to savanna shift, a key slow variable is the ratio of grass biomass to woody biomass, which decreases under the pressure of heavy grazing by cattle. When this ratio drops to a critical level there is too little grass to sustain fires that are sufficiently hot to burn and kill shrubs. As the small shrubs grow into trees, the SES will then shift to a forest configuration (Aneries et al., 2002) . Note that in this case, fire is a key reinforcing feedback that maintains the grassy landscape of savannas.\n\nAs seen from this example, critical thresholds in controlling slow variables in an SES typically correspond to points at which previously dominant feedbacks disappear and new feedbacks suddenly come into operation, enforcing the SES to reorganize, often abruptly, into a different configuration. Consequently, the system is structured and functions in a different way, and produces a different set of ecosystem services. In ecological systems, such large, persistent and often abrupt system reorganizations are known as regime shifts. Regime shifts often occur suddenly, because the feedback mechanisms buffer change and leave little observable change to the system until the onset of regime shift. Similarly, changes in controlling slow variables, and thus the gradual erosion of resilience in the system, are frequently being ignored. Hence, a critical aspect of enhancing resilience in an SES is concerned with improving the management of slow variables and feedbacks to ensure that the system remains in a configuration that produces desired ecosystem services.\n\nA well-known issue about feedbacks is that they can lock a system into an undesirable configuration or reduce its ability to adapt or transform in the face of change and disturbance. A typical example is the cycle of poverty in economics, in which reinforcing feedbacks keep the poor trapped in impoverished conditions that are hard to change. Other issues are mainly concerned with mismanagement of slow variables and feedbacks, which reduces the resilience of ecosystem services in SES. Examples include management interventions that obscure, remove, or ignore stabilizing feedbacks that underlie the provision of desired ecosystem services, or lack of knowledge about which key slow variables and feedbacks support particular SES configurations, or simply lack of action for improvement in regulating SES for various reasons. For more detailed discussions, refer to Biggs et al. (2015) .\n\nHere, CAS thinking is defined as a mental model or worldview that considers SES as CAS and advocates taking relevant approaches to management accordingly. Based on this view, an SES is made up of many interacting components that are adaptive to change, and hence the system is capable of self-organizing and evolving with new properties emerging at different scales and at different stages of development. Furthermore, a regime shift may take place in such a system, leading to a completely new SES that produces different ecosystem services.\n\nAs such, CAS thinking causes uncertainty in an SES as a system characteristic, making aspects of SES highly uncertain and difficult to predict and control. While active research and practical experiments have helped to reduce some important aspects of system uncertainty (Lee, 1993) , CAS thinking regards some facets of uncertainty as irreducible due to unpredictability, incomplete knowledge or multiple knowledge frames (Levin, 2003; Brugnach et al., 2008) , and embraces uncertainties as opportunities for positive motivational change (Janssen, 2002; Cilliers et al., 2013) .\n\nNote that fostering CAS thinking does not directly address the issue of enhancing the resilience of ecosystem services. Instead, it changes and adapts the cognitive foundations and paradigms that underpin management processes and decisions. In other words, recognizing that SESs are based on a complex and unpredictable web of connections and interdependencies is the first step toward management actions that can foster resilience in SES.\n\nMuch of the theory that CAS thinking can help enhance the resilience of ecosystem services comes from the lessons learned from cases where conventional resource management with no CAS mindset has resulted in a loss of SES resilience. Based on many decades of widespread practices of ecosystem modification in the USA, Holling and Meffe (1996) described a \"pathology of resource management\" entailing practices such as river stabilization, fire suppression and monocultural farming to the point of system collapse. Such a pathology also characterizes mismanagement of agricultural activities (Allison and Hobbes, 2004) , fisheries (Mahon et al., 2008) , and forests (Agrawal, 2005) , where an SES was narrowly managed to maximize economic production in the short term but this management approach degraded the underlying capacity of the system to produce ecosystem services. As a consequence groundwater tables were drawn down, land was degraded, fisheries, and forests were degraded or even depleted, and rivers were transformed and polluted.\n\nThese lessons show that management practices based on linear, reductionist worldviews of ecosystems inadvertently undermine the ability of these systems to continue producing ecosystem services in the face of disturbance and change. By inference, an alternative management approach based on a mindset that recognizes CAS properties may result in more resilient ecosystem services in the long term, because it considers consequences at a system level, across time, space, and actors (the resource users, managers, and policy-makers).\n\nCAS thinking is not new, and cases exist where CAS thinking has contributed to improved socioecological outcomes through resilient ecosystem services. Recent examples of transformations in ecosystem management suggest that changes in underlying mental models that acknowledge the characteristics of SES as CAS can lead to improvements in the resilience of ecosystem services. The large-scale rezoning of Australia's Great Barrier Reef, one of the seven natural wonders of the world, is a case in point. This management change was driven by increased recognition of the importance of connectivity, nonlinear change and multiscale interactions in coral reef systems (Olsson et al., 2008) . By introducing spatial restrictions on fishing and other uses of the reef, the rezoning aimed at enhancing the resilience of ecosystem functions to a range of disturbances including temperature anomalies and cyclones. CAS thinking in this approach is evident in two aspects: maintaining connectivity within the reef system and increasing the system's capacity to absorb large disturbances; and importantly, recognizing the values and perspectives of different reef users. After decades of implementing this precautionary and adaptive management approach, improvements in the resilience of the reef's marine ecosystems to climate-change impacts were evident through ecological monitoring and experimentation, but challenges remain as the problem domain has become increasingly complex (Brodie and Waterhouse, 2012; Brodie, 2014) .\n\nDespite examples that demonstrate the positive influence of CAS thinking on resilience building, due to the lack of solid scientific proof, it is still difficult to attribute the enhanced resilience directly to CAS mental models. This is not only due to the indirect influence of CAS thinking on management and resilience building, but also due to the short history of CAS thinking as a science with its application in management taking place only recently. In many cases more time is needed to assess the extent to which CAS thinking is guiding management or catalyzing positive change.\n\nMain issues related to fostering CAS thinking in management approaches to SES mainly result from misunderstanding of complexity, ineffective operations of new management approaches, and rigid interpretations of CAS worldviews. For instance, when \"complexity\" is interpreted merely as the unknown aspects of an SES, it may encourage managers to invest heavily in monitoring and data collection, rather than encourage the employment of adaptive approaches that allow for experimentation and the probing of boundaries as a mechanism to address uncertainty (Walters and Holling, 1990) . Also, as a CAS approach implies a change in management paradigm from a focus on causality and control within short time-frames, to a focus on coping with change and uncertainty over longer timescales, ineffective operations in this management shift may occur. In addition, an incorrect attitude toward a CAS mental model may regard it as a static management form, rather than a dynamic management approach that pursues continued learning, experimentation and adaptation. As these problems may compromise resilience in SES to varying degrees, efforts should be made to avoid or solve them. For more detailed discussions, refer to Bohensky et al. (2015) .\n\nAccording to S\u00e4lj\u00f6 (1979) , learning means: (1) acquiring information and increasing knowledge; (2) memorizing; (3) acquiring facts, skills and methods; (4) making sense or abstracting meaning; and (5) interpreting and understanding reality in a different way by reinterpreting knowledge. Hence, learning can be understood as a multifaceted phenomenon. Two complementary approaches to learning are believed to enhance the resilience of ecosystem services, i.e., loop learning and social learning.\n\nThere are three types of loop learning: single-loop learning, double-loop learning, and triple-loop learning. Single-loop learning leads to a change in skills, practices or actions to meet existing goals and expectations, and it focuses on the question, \"are we doing things right?\" Double-loop learning involves questioning the assumptions that underlie action, and it focuses on the question, \"are we doing the right things?\" Triple-loop learning comprises a more in-depth questioning of values and norms that underlie institutions and actions, and it focuses on the question, \"how do we know what the right thing to do is?\" (Flood and Romm, 1996) . By its nature, triple-loop learning can lead to the restructuring of beliefs and values, underlies transformations in worldviews, and may prompt changes in ecosystem governance and management approaches (Pahl-Wostl, 2009; Biggs et al., 2010) .\n\nAccording to Reed et al. (2010) , social learning refers to \"a change in understanding that goes beyond the individual to become situated within wider social units or communities of practice through social interactions between actors within social networks.\" Social learning aims at sharing knowledge and perspectives, and takes place in two key ways, i.e., through deliberative processes of interactions between individuals, or through deliberate experimentation and reflection involving shared activities (Cundill and Rodela, 2012).\n\nAs complex dynamical systems, SESs are constantly evolving and knowledge of these systems is always partial and becoming outdated. Enhancing the resilience of ecosystem services, therefore, requires continuous learning about the SES that provides these services (Holling, 1978; Walker and Salt, 2006; Chapin et al., 2009 ). Experience has shown that learning can enhance the resilience of ecosystem services primarily through influencing decision-making processes and governance. Such learning can be achieved through a variety of both planned and unplanned processes, in the forms of loop learning and social learning that include active experimentation and monitoring, multiactor collaboration and through intergenerational interactions with the environment.\n\nInformation about changes in the availability of ecosystem services can be obtained through experimentation and monitoring (Bellamy et al., 2001; Boyle et al., 2001) , which can also be used to resolve uncertainty about how the SES works. Also, the use of monitoring and experiments to explore alternative management options is an important means to support learning and enhance the resilience of ecosystem services.\n\nThere is growing recognition of the importance of broader participation in SES management and its role in enhancing the resilience of ecosystem services through learning (Danielsen et al., 2005) . A study of five US community-based forestry organizations involved in collaborative ecological monitoring programs found that comonitoring activities led to single-loop learning, which identified changes in recommendations for optimal treatment of invasive weed species, and double-loop learning, which led to the realization of the significance and need for sustainable harvesting of mushrooms. These learning processes together altered social attitudes and assumptions toward adaptive management of SES. In the case of Australia's Great Barrier Reef, the change of perceptions among politicians and the public from viewing the reef as pristine to viewing it as a severely threatened ecosystem cleared obstacles for stronger protection of the reef and its associated ecosystem services (Olsson et al., 2008) . Both of these shifts in perceptions occurred through processes of learning. These experiences highlight the role of learning in potentially supporting the resilience of ecosystem services.\n\nThere is growing recognition that learning can be ineffective, or worse, maladaptive to enhancing resilience in SES, and that the design of the learning process is crucial. A number of factors have been identified as potential challenges for effective and legitimate social learning and adaptive governance, which include both management issues and technical issues. The former influences how learning takes place, and the latter affects how effective monitoring and experimentation can be conducted and applied at the right scale. In general, to be effective, the process of monitoring, experimentation and learning should be collaborative and long-term, at an appropriate scale for decision-making and the SES, as well as able to withstand the impact of short-term funding cycles, politics and objectives (Barthel et al., 2010) . Other aspects such as diverse participation, appropriate facilitation, sufficient financial and human resources and social networking are also considered to be important for effective learning. For more detailed discussions, refer to Cundill et al. (2015).\n\nAccording to Stringer et al. (2006) , participation refers to the active engagement of relevant stakeholders in the management and governance process. Participation covers a wide range of engaging activities: from simply informing stakeholders to complete devolution of management power. It can take place in all or some stages of an ecosystem services process: from identifying problems and goals to implementing policy and monitoring results, to assessing outcomes.\n\nAs the interaction of the human race and nature is fundamental to the concept of SES, the role of participation in ecosystem management is well accepted (Schreiber et al., 2004; Armitage et al., 2007) . Ample evidence suggests that, for ecosystem services to be resilient, a diversity of stakeholders should be involved in their management to improve legitimacy, to expand the depth and diversity of knowledge, and to help detect and assess system changes. Note that these elements are interdependent and interactive.\n\nFirst, participation can improve legitimacy of SES management through establishing processes that are deliberative and support the formation or development of relationships between different stakeholders. These relationships create the potential to build trust and consensus as a basis for collective action around approaching thresholds, inventing innovative solutions or facilitating learning or shared experiences (Lebel et al., 2006) .\n\nSecond, participation can also promote understanding of the system through enriched knowledge. Encouraging a variety of actors or stakeholders to participate can promote understanding of the SES dynamics by providing a range of ecological, social and political perspectives that may not be acquired through more traditional scientific processes (Armitage et al., 2009; Folke et al., 2005) .\n\nThird, participation can help to strengthen the link between informationgathering and decision-making, thus enabling responses to ecosystem change (Danielsen et al., 2005; Evans and Guariguata, 2008) . Acting together, these mechanisms can enhance the capacity of a management system to detect and interpret shocks and disturbances, which is central to facilitating the collective action required to respond to changes in SES.\n\nAgain, the large-scale rezoning of Australia's Great Barrier Reef is a case in point. An extensive public participation and consultation campaign was launched in an effort to raise awareness about threats to the reef, and to assist with new zoning plans to protect the reef (Olsson et al., 2008) . Through greater understanding of the threats facing the reef, the public participation process was able to achieve both broad public support for the management decision on rezoning as well as the alteration of marine park zoning plans, in order to incorporate the concerns of some groups. In the related water governance sector, public participation was also found to broaden the range of interests considered in ecosystem-service management (Lebel et al., 2006) .\n\nAlthough participation is essential to SES management, to actually enhance resilience in SES depends on factors such as the participants, the process and the social and institutional environment (Stringer et al., 2006) . These factors are interconnected and also context-dependent and, if not well prepared, executed, supported or resourced, participation can then undermine or compromise resilience.\n\nThe participants are crucial to the governance process because they determine or influence what ecosystem services are desired from a landscape and what should be the focus for resilience building. Resilience of ecosystem services can be compromised through participation that fails to engage the appropriate individuals and groups, or fails to consider the context-specific nature of the participation. Ineffective processes of participation can also be detrimental to ecosystem-service management, such as those that fail to have participatory strategies that successfully build social capital or fail to effectively link to natural systems. Participation processes may also compromise SES management if they fail to create a supportive social or institutional environment. Besides, attention should also be paid to balancing stakeholder interests, improving stakeholder relationships and choosing adequate scales of participation, because these factors can also influence the outcome of participation. For more detailed discussions, refer to Leitch et al. (2015).\n\nPolycentricity refers to a governance system in which there are multiple interacting governing bodies with autonomy to make and enforce rules within a specific policy arena and geography. In an ideal polycentric system, to achieve a balance of collaboration and autonomy each individual governing body needs to interact and link with other authorities both horizontally and vertically. Here, governance is defined as the exercise of deliberation and decision-making among groups of people in the act of self-ordering their relationships. For instance, while national government agencies have the legitimate authority to make rules that are binding on all citizens, a regional management group (such as a forest or township management authority) may have autonomy to self-organize within its own domain.\n\nSince change and disturbance can occur at various possible levels ranging from global to regional and to local, polycentricity is believed to be a better governance system that enhances the resilience of ecosystem services in several fundamental ways than other monocentric governance strategies. These fundamental aspects all link with previously discussed resilience-enhancing principles. First, a broadly inclusive system with governance at multiple smaller scales offers opportunities for experimentation at more localized levels, creating natural experiments for testing different policies (Brondizio et al., 2009) . Such processes encourage learning (Principle 5).\n\nSecond, by increasing the breadth of inclusion, a polycentric system can capitalize on scale-specific knowledge (e.g., traditional and local knowledge) to aid learning through sharing information, experience and knowledge across culture and scales (Olsson et al., 2004) , while also expanding channels for managing slow variables and feedbacks (Principle 3) and providing opportunities for broader participation (Principle 6). Local levels with more direct linkages to the provision and use of resources provide institutional diversity from which successes can be shared with others (Folke et al., 1998) . Furthermore, this decentralized design often serves to increase the legitimacy of the governance system at a given level with more scalespecific input into decision-making (Engle and Lemos, 2010), and also facilitates long-term monitoring and enforcement of rules at local levels, which are considered as important factors for building resilience in SESs.\n\nThird, a polycentric approach has been suggested to confer connectivity and modularity (Principle 2), response diversity and functional redundancy (Principle 1), and foster CAS thinking (Principle 4) that can help preserve key SES elements in the face of disturbance and change. These characteristics of resilient governance may in turn change in scope and be further strengthened with increasingly collaborative degrees of polycentricity.\n\nIt should be pointed out, however, that studies to date in complex systems have been largely diagnostic and lacked predictive power and precision. To better understand how to operationalize, the idea of polycentricity in governance of SES requires more research.\n\nIn polycentric governance, there are three key challenges that, if not resolved, may lead to degradation of ecosystem services at one or more scales. The first is that of the need to balance redundancy and experimentation, because overlapping authority and increasing transaction costs may result in inefficiencies in SES management (Parks and Ostrom, 1999) . The second challenge is that of settling trade-offs between various ecosystem-service users (Rodriguez et al., 2006; Robards et al., 2011) . Tradeoffs may occur between conflicting goals and needs among users of current or potential ecosystem services (S\u00f8reng, 2006) , or when impacts are incurred by those not affecting or benefiting from an ecosystem services (Chapin et al., 2006) . The third challenge is closely related to the second one and is about politics, or the process of resolving conflict and making collective decisions over how to allocate trade-offs. Note that the issue of who bears the costs and who benefits from enhancing resilience in favor of particular ecosystem services constitutes one of the largest problems in SES governance (Lebel et al., 2006; Robards et al., 2011) .\n\nFinally it should be pointed out that, under some situations, particularly short timescales or crises where coordination across scales impedes timely action, there may be other governance tools (including top-down coercion or market approaches) that alone may accomplish specific goals more effectively than through a polycentric system (Imperial and Yandle, 2005; Hilborn et al., 2006) . For more detailed discussions, refer to Schoon et al. (2015) .\n\nThis section discussed seven principles that enhance the resilience of ecosystem services in the face of change and disturbance. Although some principles are better understood than others, the evidence and resilience-enhancing mechanisms discussed show that all are important and require a good understanding of how, when and where they apply. Furthermore, as SESs are highly interconnected systems, the properties and processes associated with these principles often work together to become effective. Hence, context matters and promoting resilient ecosystem services depends as much on how the individual principles are applied as on achieving an appropriate combination of principles, keeping in mind their mutual interactions.\n\nAs interdependent CAS, the nature of SES calls for governance and management that enhances aspects of an SES that help shape future paths in favorable directions and enable adaptive responses to unexpected events. In essence, these principles help define the key features relevant for building resilience that should be considered when designing governance structures and management policies, which include:\n\nMore research is needed to better understand the individual principles, how they interact and how they can be operationalized and applied in different contexts. \n\nAbbreviations ATC air traffic control EAS emergencies and abnormal situations FRMS fatigue risk management system FSS financial services system FTL flight and duty time limitation STS sociotechnical system T 2 EAM taskwork and teamwork strategies in emergencies in air traffic management In this section, the resilience approach in sociotechnical systems (STSs) is introduced first, focusing on four main factors to build resilience in STSs. These factors represent four essential capabilities of a resilient system. Each of them is discussed through real case studies taken from various sociotechnical domains. For more detailed discussions, refer to the original publication ).\n\nSafety efforts are traditionally focused on the unwanted outcomes, injuries, and losses that are the result of adverse events. This fact is manifested by the common understanding of safety as \"freedom from unacceptable risk.\" The resilience approach, however, defines safety as the ability to succeed under varying conditions. As a result of this change in attitude toward safety, to learn from things that go right becomes equally important as to study things that go wrong. Hence, an understanding of the normal functioning of a STS provides the necessary and sufficient basis for understanding how it fails. Normally, it is both easier and more effective to improve safety by increasing the number of things that go right, than by reducing the number of things that go wrong.\n\nThe concept of resilience can be made more concrete by pointing to four abilities that are necessary for a system to be resilient. These are the ability to respond to events, to monitor ongoing developments, to anticipate future threats and opportunities, and to learn from past failures and successes alike; refer to Fig. 2. 2. The engineering of resilience is concerned with establishing and managing these four capabilities. For STSs, resilience is defined as:\n\nThe intrinsic ability of a system to adjust its functioning prior to, during, or following changes and disturbances, so that it can sustain required operations under both expected and unexpected conditions.\n\nNote that the key term of this definition is the system's ability to adjust its functioning. The following notes on the four essential capabilities of resilience give concrete meaning to the abstract concepts of this definition: G Knowing what to do, that is, how to respond to regular and irregular disruptions and disturbances either by implementing a prepared set of responses or by adjusting normal functioning. This is the ability to address the actual. Knowing what to look for, that is, how to monitor that which is or can become a threat in the near term. The monitoring must cover both that which happens in the environment and that which happens in the system itself, that is, its own performance. This is the ability to address the critical.\n\nKnowing what to expect, that is, how to anticipate developments, threats, and opportunities further into the future, such as potential changes, disruptions, pressures, and their consequences. This is the ability to address the potential.\n\nKnowing what has happened, that is, how to learn from experience, in particular how to learn the right lessons from the right experience-successes as well as failures. This is the ability to address the factual.\n\nTo be able to respond to what happens in a timely and effective mannerwhether it is a threat, or an opportunity-is essential for the survival of a system, organization, or organism. Responding means to detect and assess an event, and take relevant actions. To do so, it is necessary either to have prepared responses and the requisite resources, or to be flexible enough to make the necessary resources available when needed.\n\nA resilient system must have effective means to monitor its own performance as well as changes in the environment. Monitoring enables the system to address possible near-term threats and opportunities before they actually take place. In order for the monitoring to be flexible, its basis must be assessed and revised frequently. While monitoring makes immediate sense, a specific effort should be made to look at the more distant future as well. The purpose of looking at the potential is to identify possible future events, conditions, or state changes that may affect the system's ability to function either positively or negatively.\n\nIt is obvious that future performance can only be improved if something is learned from past performance. The effectiveness of learning depends on the basis for learning, that is, which events or experiences are taken into account, as well as on how the events are analyzed and understood. Since the number of things that go right, including near misses, is many orders of magnitude larger than the number of things that go wrong, it makes good sense to try to learn from representative events rather than from failures alone. For more detailed discussion, refer to Hollnagel (2011a).\n\nThree Practical Cases Illustrating Resilience in \"Real Time\"\n\n1. Lessons from the Hudson (Pari\u00e9s, 2011a) On January 15, 2009, US Airways Flight 1549, a twin engine Airbus A320 made an unpowered emergency water landing on the Hudson River, in New York, after multiple bird strikes caused both engines to fail. All 155 passengers and crew aboard the Airbus A320 successfully evacuated from the partially submerged airframe as it sank into the river; they were rescued by nearby boats. The incident came to be known as the \"Miracle on the Hudson,\" which occurred when the plane turned into a flock of Canadian geese about two minutes after takeoff, at about 3000 ft (912 m). Several of these huge birds were sucked into the twin engines, causing both engines to quickly lose power. As the aircraft lost altitude, the flight deck crew decided that the plane could not reach the closest airfield. They therefore diverted to the nearby Hudson River and executed a controlled water landing, about 3 minutes after losing power, saving all on board.\n\nIn this case study, the author discussed a \"defense in depth\" strategy against anticipated bird strikes by the aviation system, and the fact that this anticipation greatly contributed to survivability in the Hudson River event. This strategy is described as to lay three lines of defense: the first line in minimizing the frequency of bird strikes, the second line in assuring the ability of the aircraft and its engines to withstand some bird strikes, and the last line in enabling the crew-aircraft system to land or ditch safely after a total loss of power.\n\nCoping with uncertainty: Resilient decisions in anesthesia (Cuvelier and Falzon, 2011) In the context of research on patient safety in pediatric anesthesia, an empirical research was conducted in a pediatric anesthesiology service in a French hospital, based on the critical-incident technique (a set of procedures used for collecting direct observations of human behavior that have critical significance and meet methodically defined criteria). The objective of this study is twofold: to identify the different types of disturbances that anesthesiologists have to manage in their work, and to highlight the resilience factors, i.e., the strategies developed through practice by anesthesiologists to allow the system to function despite these disturbances.\n\nThe results highlight a distinction between potential situations in which the problem was envisioned beforehand by practitioners and unexpected situations which the anesthesiologists had not foreseen. This subjective classification based on \"the astonishment of the perceiver\" highlights two critical decisions made by anesthesiologists in order to manage variability. The first is the preoperative definition of an envelope of potential variability of the surgical intervention. The second concerns the occurrence of an event which trespasses that envelope and thus requires the mobilization of additional resources. The identification of these two critical decisions provides opportunities for research and actions to enhance resilience in the practice of anesthesia. 3. Training organizational resilience in escalating situations (Bergstr\u00f6m et al., 2011) To evaluate the possibility of building front-end organizational resilience in unexpected and escalating situations, an experiment was performed with Swedish fire safety engineers, using training programs that met methodically defined scenario guidelines. A 2-day crisis simulation exercise was used to practice the generic competencies of two experimental groups.\n\nThe experiment demonstrated that designing a training environment in which people actually faced the uncertainty and unpredictability of escalating situations generated \"resilient\" competencies that were not generated by current training strategies aiming at drilling correct behavior in known scenarios. It also showed that the nondomain specific training deepened understanding of the nature of escalating situations and the difficulties in managing them. The potential is great to apply this type of training in various industries that demand rapid and well-structured responses to escalating situations, although more research and further testing are needed.\n\nThis section focuses on the ability of a system or an organization to \"deal with the actual,\" that is, to respond to the demands of the current situation-a disrupting or shocking situation. At the \"sharp end\" of the system, \"responding to the situation\" implies a series of actions: assessing the situation, knowing what to respond to, and making decisions on what to do and when to do it. The readiness to respond mainly relies on two strategies: the proactive strategy and the reactive strategy. The first one is to anticipate the potential disruptive situations and predefine ready-for-use solutions (e.g., abnormal or emergency procedures, specific reaction skills, crisis response plans, and so on). The second is to generate, create, invent, or derive ad hoc solutions.\n\nPut differently, the three cases discussed \"real time\" resilience, but from a synchronic as well as diachronic perspective. Indeed, from the viewpoints of designers, managers, or trainers, issues related to \"real-time resilience\" include how to ensure that the required resources (people, competence, equipment, etc.) are available or can be established in time. Hence, a more relevant question would be how to establish (now) and maintain (tomorrow) a readiness to respond (at any time in the future). While the three cases focus on the \"readiness to respond,\" they also touch on some issues related to establishing and maintaining that readiness. Each of them presents a practical case study of a specific domain: commercial aviation, anesthesia, and rescue services. Beyond their obvious differences of perspective and domain, they share similar underlying theoretical questions on resilience. The first of these is the relationship between resilience and anticipation, which also runs throughout other cases in this study on STS. For more detailed discussion, refer to Pari\u00e9s (2011b).\n\nThree practical cases on monitoring for building system resilience 1. From flight time limitations to fatigue risk management system (FRMS)-a way toward resilience (Cabon et al., 2011) Fatigue is known to be a major risk for safety in aviation. The flight and duty time limitations employed in civil aviation are a traditional approach to prevent fatigue through the regulation of duty hours. However, besides the inherent rigidity of regulations from an operational point of view, this method often fails to take into account all the complex dimensions of fatigue. In order to cope with this complexity, FRMSs are progressively emerging. Rather than setting absolute duty time limitations, an FRMS approach evaluates each operation in terms of fatigue risk. The International Civil Aviation Organisation (ICAO, 2011) defines an FRMS as \"a data-driven means of continuously monitoring and maintaining fatigue-related safety risks, based upon scientific principles and knowledge as well as operational experience that aims to ensure relevant personnel are performing at adequate levels of alertness.\"\n\nMonitoring plays an important role in FRMSs. Some suggestions on the overall monitoring process are given below, including four scenarios for developing a coherent strategy for monitoring fatigue risk: a. \"Continuous\" mode: In this \"basic\" mode, there is continuous feedback from the systematic monitoring on the risk matrix. For example, a risk that was not identified in the risk matrix is added after specific events are identified through air safety reports.\n\nb. \"Probe\" mode: In this mode, a focused monitoring is conducted on a limited period (e.g., 1 month) and used to update the risk matrix. c. Proactive mode: In this mode, a focused monitoring is triggered after a significant change (e.g., introduction of a new route, schedule change). The risk matrix is updated on the basis of the results. d. Reactive mode: In this mode, a focused monitoring is triggered because of a significant change of an indicator of the systematic monitoring, e.g., frequency of air safety reports and associated aircrew fatigue report forms increased in the last months on a specific roster. The implementation of FRMS requires a combination of tools and methods in order to manage the complexity of the impact of crew fatigue on their safety performance. FRMS can be seen as a concrete way to engineer resilience because it requires the organization to adjust its functioning by reintroducing safety managed by humans in addition to safety controlled by regulations. 2. Practices for noticing and dealing with the critical-a case study from the maintenance of power plants (Lay, 2011) In high-risk, high-pressure complex work such as the maintenance of power plants, quality and safety incidents can happen and sometimes be extremely costly for both the service provider and the customer. Thus, consistent high-quality job performance with few incidents can be the most important differentiating factor when selecting a service provider. Conventional safety and quality programs are often limited in scope and tend to be micro-focused on specific, historical incidents or trends. Principles of resilience theories can be applied to design a broad, proactive strategy for noticing signs of a critical situation and moving into different actions to reduce the risk in order to improve performance in line with the plan.\n\nHighly resilient organizations can be characterized by the following four behaviors: a. They anticipate critical disruptions and situations and their consequences. b. They notice the critical disruptions and situations when they occur. c. They plan how to respond. d. They adapt and move into different actions.\n\nIn this particular case, the first step in building resilience was improving \"noticing\" of the critical, looking at both general situations of an outage and specific unexpected situations. One approach was to implement a \"pinging\" process. Pinging is the proactive probing for risk profile changes (Wreathall and Merritt, 2003) . Through workshops with experienced project managers and operational support staff, signs that an outage could be approaching an out-of-control situation or risk profile change were hypothesized. Another approach was to train the entire organization on concepts of resilience through study groups, by observing and recognizing the patterns and characteristics of resilience, and brittleness in every-day work, then discussing observations across situations and domains.\n\nField service is already seeing benefits from applying the concepts of resilience even though it is at the beginning of the journey to become more resilient. It seems that even just building the skill of \"noticing\" can help reduce loss. Noticing triggers action as people improvize responses even without a prescribed menu of help. 3. Cognitive strategies in emergency and abnormal situations training-implications for resilience in air traffic control (ATC) (Malakis and Kontogiannis, 2011) The management of emergencies and abnormal situations (EAS) in the ATC system entails substantial challenges for air traffic controllers. The fundamental assumption behind refresher training is to provide air traffic controllers with the required skills and knowledge to meet successfully a wide range of challenges imposed by EAS. Using cognitive systems engineering principles, a set of cognitive and team strategies was used to explore patterns of resilience in dyadic teams of operational controllers during real and simulated emergencies in a major European Area Control center. The investigation of real incidents revealed operational problems that were different from those encountered during refresher training. This case study presents initial findings with the potential of providing insights into cultivating sources of resilience that will supplement current refresher training.\n\nThe first stage of the study resulted in a performance model, termed \"taskwork and teamwork strategies in emergencies in air traffic management (T 2 EAM).\" The T 2 EAM model is an attempt to achieve a balanced and pragmatic approach to capture resilient processes during EAS episodes in the ATC system. The individual and joint cognitive strategies that correspond to T 2 EAM are listed below: a. Anticipation: to timely and accurately detect and respond to a threat b. Recognition: to timely and accurately detect early signs of an impending emergency and play out mentally the progression of events c. Managing uncertainty: to assemble and assess a model of the situation and establish safety-related goals d. Planning: to employ standard and/or contingency planning for the unfolding situation e. Managing workload: to timely and accurately organize the required tasks and respond to interruptions and distractions f. Team coordination and communication g. Error management and task management Based on the results of refresher training programs, it is concluded that these failuresensitive cognitive strategies provide important practical examples of the potential for resilience at two levels. First, they provide insights on how adaptations by controllers in the form of cognitive strategies are employed to support resilience in the case of safety-critical events. Second, these cognitive strategies are used as building blocks in the development of advanced safety training programs with the aim of cultivating sources of resilience in the ATC system.\n\nEvery organization concerned with safety uses one or more metrics to judge whether the levels of safety in the organization are acceptable. Using a common definition of safety, the organization needs to know if it is \"free from unacceptable risk.\" This metric is often the number or rate of accidents or injuries (or deaths) over some period of time or the time between events. While such measures may provide some assurance that safety has not been entirely out of control in the past, they are of little use when considering how to manage safety in the future and could even be harmful. Since the environment of the organization and its own internal processes are both dynamic, last year's safety performance (or last month's or yesterday's) is at best a weak indication of how today's and tomorrow's will be.\n\nAs shown by the well-known management adage, \"You can't manage what you don't measure,\" measurement of processes is an essential part of any organization. Though in practice it is rare to find explicit models that provide a formal basis for identifying measures, knowledge often exists about relationships that are important to safety and that can be used to create working models. In the study of fatigue (Case 1), a set of relationships among safety, fatigue, and its underlying mechanisms was discussed and those relationships were used to create metrics for monitoring the risks from fatigue. In the study of cognitive strategies in ATC (Case 3), the T 2 EAM performance model was developed based on existing research related to cognitive strategies in similar kinds of tasks, though often in other industries. This performance model provided the basis for developing measurement methods.\n\nSeveral studies have used ad hoc methods for selecting indicators. As described in Case 2, the critical factors associated with an outage indicating a potential to be problematic were identified through brainstorming workshops with experienced project managers and others often involved in supporting projects when they get into trouble. In other words, even though these were based on learning from events in the past, they were considered to be sufficiently robust to remain effective for the next cycles of outages (1 to 2 years), at least from the perspective of testing the concept. Similarly, in work described by Wreathall (2006) , the selection of indicators was based on factors most frequently associated with problems in human performance issues in the nuclear industry's collective experience. For more detailed discussion, refer to Wreathall (2011).\n\nThree practical cases involving anticipation for building system resilience 1. Measuring resilience in the planning of rail engineering work (Ferreira et al., 2011) The demand for increased capacity of the UK rail network has generated growing pressure to improve the planning and delivery of engineering work. As the owner of the UK rail infrastructure, Network Rail faces the challenge of delivering increasing volumes of work (maintenance, enhancements, and renewals) within more diverse and shorter opportunities for access to the infrastructure, while meeting the safety performance standards imposed by the regulatory bodies. Achieving a balance between productivity pressures and assurance of the required safety standards has become critical for the sustainability of the rail organization.\n\nResilience engineering was proposed as a framework for research aiming to improve the ability of the organizational system responsible for the planning of all engineering work to respond to these pressures. Within this scope, an approach to measuring resilience was developed by means of questionnaire. A factor analysis method was used to identify underlying trends from the questionnaire data, which could then potentially be used as measurable aspects of resilience in rail engineering planning. The identified resilience factors are listed below: a. Adaptability and flexibility: Planners are able to restructure their work in response to pressures and adapt to new arising circumstances through problem solving. b. Control: People feel they have the means necessary, in particular information, to appropriately control and steer their activities. c. Awareness and preparedness: The system generates feedback and provides support in such a way that people have a clear view of how they should contribute to responding to challenges.\n\nd. Trade-offs: Achieving a balance between safety and efficiency through decisionmaking. e. Time management: Having the time to be thorough when required by planning decisions. Self-reporting methods, such as questionnaires, may be insufficient to provide a robust measure of resilience. However, such methods can be an efficient way of monitoring the system's behavior in terms of resilience, particularly on a longer term basis through periodical applications. 2. The art of balancing: using upward resilience traits to deal with conflicting goals (Tj\u00f8rhom and Aase, 2011) This case study describes some of the processes involved in balancing conflicting goals (e.g., between safety and operation) in a change-intensive environment by using examples of the Norwegian civil aviation transport. The ability to handle multiple goals involves the use of both downward and upward resilience traits to address potential conflicts. Here, downward resilience means that macrolevel directions and solutions prepare for resilience through clear goal structures, infrastructure, and procedures that handle the trade-offs between safety and efficiency. Upward resilience means that decisions made at the microlevel in a system reflect a commitment to safety in case of goal conflicts. Changes, caused either by external or internal drivers, may alter these resilience traits by introducing loss of oversight. Changes made at the macrolevel of the system might have unintended consequences on the microlevel, and vice versa.\n\nThese findings have implications for different levels of the aviation transport system. The following actions are proposed to strengthen downward resilience: a. Development of clear safety goal rules at the governmental level. b. Downward resilience is threatened by unwillingness to state clear goal rules at the strategic level. After years of changes within the aviation transport system, employees need clear statements that give them a framework to remain flexible and committed to safety despite economic pressure. c. The goal rules should be based on worst-case scenarios using input from the entire aviation transport system. The institutional level of the system must be responsible for collecting information regarding trends that threaten resilience. d. Development of guidelines and requirements for addressing crossscale interactions. e. The training tools should include participants from different levels and professions.\n\nThe following actions are proposed to strengthen upward resilience: a. Foster perpetual awareness among operators. b. Without a constant sense of unease about the way to handle an operation, one might become lost in routine and fail to notice variations. Even a seemingly insignificant variance in operation must be taken as a potential indicator of a threat to resilience. c. Extend operators' collaboration with other parts of the system. d. A strong focus on professional values might have some downsides .\n\nWithin a profession, self-confidence may evolve to the level of overconfidence. In a trade-off situation, this may result in over-reliance on the individual's judgment-at the expense of cautious prudence. Technicians and airport operators might rely too heavily on experience and knowledge, thus taking unnecessary chances without fully embracing the body of rules. Interrelations necessitate an exchange of knowledge across professions. The tension between downward and upward resilience in the aviation system studied is balanced by a strong professionalism throughout the system, which functions as a buffer and makes safety goals prevalent over production goals. To uphold this art of balancing, it is crucial to develop strong but flexible goal rules at the macrolevel to demonstrate a commitment to safety that microlevel actors find trustworthy. At points of intensified production pressure and higher organizational tempo, extra investments in sources of resilience are required to keep production/safety trade-offs from sliding out-of-balance. In other words, safety investments are most important when least affordable (Woods, 2006) . 3. The importance of functional interdependencies in financial services systems (FSSs) (Sundstr\u00f6m and Hollnagel, 2011) The events of 2007\u00c02009 in the global financial markets clearly illustrated the need for an improved understanding of how the global FSS functions. In particular, the crisis made it clear that national FSSs, or components of such systems such as individual banks, were highly dependent on the normal functioning of other components of the global FSS.\n\nThe primary purpose of this case study was to illustrate how resilience engineering can provide the financial services industry with a different way to understand risk at both a macro-and microprudential level. To enable multiple stakeholders to have a shared view of the system, a common model of the FSS is required. In the present study, the functional approach to FSSs proposed by Merton and Bodie (1995) was employed. Such an approach enables stakeholders and decision makers to focus on behavior rather than on the specifics of an individual financial services institution. The modeling process typically consists of four phases: a. Identify what functions need to be modeled. b. Identify conditions that could lead to a change in performance. c. Identify areas where functional resonance could emerge. d. Identify how performance variance can be monitored and controlled.\n\nA key advantage of a functional perspective is that it becomes possible to discover risks created by functional interdependencies among individual institutions and system components. The concepts and functional modeling approach outlined in this study thus provide the basis for the development of a standardized method to capture and better understand risk in the financial services industry.\n\nThe ability to anticipate and adapt runs throughout the discussions of resilience in this section. The case studies identified several patterns in how resilient systems may anticipate that adaptive capacity is falling, that buffers or reserves may become exhausted, that goal priorities should be changed, etc. These patterns include: G Resilient systems are able to recognize that adaptive capacity is falling. G Resilient systems are able to recognize that buffers or reserves become exhausted. To be resilient, a system always keeps an eye on whether its adaptive capacity, as it is configured and performs currently, is adequate to meet the demands it may encounter in the future. Overlooking or discounting signs that adaptive capacity is degrading leaves that system vulnerable to sudden collapse or failures (Woods, 2009) . For more detailed discussion, refer to Woods (2011).\n\nThe following views on learning are adapted from Hollnagel's work, \"To learn or not to learn, that is the question\" (Hollnagel, 2011b) .\n\nIn order for learning to take place, three conditions must be fulfilled. The first condition is that there are reasonable opportunities to learn, that is, that situations where something can be learned occur with a sufficiently high frequency. The second condition is that the situations are sufficiently similar to allow generalizations to be made, especially regarding their reasons or causes. The third condition is that there must be sufficient opportunity to verify that the right lessons have been learned.\n\nWhen accidents, emergencies, and catastrophes occur, it is clearly important to find out why they happened but it is also clear that they do not offer the best basis for learning. Accidents do not happen very frequently, at least if the domain of activity is reasonably safe. Furthermore, accidents are usually different from each other, and the differences are often proportional to the magnitude of the outcomes. Lastly, because accidents happen so rarely, there is little opportunity to check whether the right lessons have been learned. Accidents therefore do not provide good conditions for learning, common stereotypes notwithstanding.\n\nIt follows from these arguments that learning can be more effective if it is based on events or conditions that happen more frequently and that-almost by virtue of that fact-are less extreme and less dissimilar (e.g., Herrera et al., 2009; Woods and Sarter, 2000) . Indeed, it is more efficient to learn from what goes right than to learn from what goes wrong, because the former happens far more often than the latter.\n\nThe four main capabilities of resilience are equally necessary and therefore equally important. Taking learning as a starting point, it is easy to argue that the ability to respond would be of little value without the ability to learn. Facing change and disturbance, it is imperative to learn new ways to respond and the system can learn only by observing and evaluating the efficiency of the responses.\n\nA similar kind of argument can be made for the relation between learning and monitoring. It is mainly by learning through practice that the proper basis for monitoring-the indicators that must be watched-can be established. The efficiency of monitoring depends on the efficiency of learning, just as the efficiency of learning depends on looking at the right kinds of experiences. Finally, learning is also necessary for anticipation, which is essential to produce a realistic, or even adequate, model or understanding of what may possibly happen in the future.\n\nWhat is the nature of resilience? If resilience is a system property, then it probably needs to be seen as an aspect of the relationship between a particular STS and the environment of that system. Resilience appears to convey the properties of being adapted to the requirements of the environment, or otherwise being able to manage the variability or challenging circumstances the environment throws up. An essential characteristic is to maintain stability and integrity of core processes, despite perturbation.\n\nThe focus is on medium-to long-term survival rather than short-term adjustment per se. However, the organization's capacity to adapt and hence to survive becomes one of the central questions about resilience-because the stability of the environment cannot be taken for granted. Therefore, the notion of being able to read the environment appropriately and to be able to anticipate, plan, and implement appropriate adjustments to address perceived future requirements, is important.\n\nResilience represents the capacity (of an organizational system) to anticipate and manage risk effectively, through appropriate adaptation of its actions, systems, and processes, so as to ensure that its core functions are carried out in a stable and effective relationship with the environment. Wreathall, J., 2011. Monitoring-a critical ability in resilience engineering. In: Hollnagel, E., Pari\u00e9s, J., Woods, D.D., Wreathall, J. (Eds.) In this section, a paradigm for designing new-generation resilient and evolving computer systems is presented, including its key concepts, elements of supportive theories, methods of analyses and implementation principles. For more detailed discussions, refer to the original publication (Castano and Schagaev, 2015).\n\nMost embedded systems, in which the computer is completely encapsulated by the device it controls and performs predefined tasks, are real-time systems (RTSs). Since their early applications in the 1960s, embedded RTSs have been woven into the fabric of modern society in ubiquitous ways, built into homes, offices, bridges, medical instruments, cars, airplanes and satellites, and even into clothes. They are being used in fields where their correct operation is vital to ensure the safety and security of the public and the environment: from automotive systems and avionics to intensive health care and industrial control as well as military operations and defense systems. There are time constraints imposed on these systems for responses, and their safety-critical nature demands the highest possible availability and reliability of system operation.\n\nThe technological achievements that have led to exponential growth of clock frequency (at which a chip like a central processing unit is running) and memory size have led to the development of microprocessors with increased transistor density. This growth has been made possible by the progressive miniaturization of electronic components predicted by Moore, whose 1965 paper described a doubling every year in the number of components per integrated circuit (IC) (Moore, 1965) . Eventually, technological development has been slowed by physical limitations: due to the size reduction of electronic components to nanometer scales and due to the increase in clock frequencies, supply voltages have been reduced to keep power dissipation manageable but thermal noise voltages have increased (Asanovic et al., 2006; Kish, 2002) .\n\nEnvironmental impact and radiation effects have long been a serious concern in aviation, aerospace and special mission electronics. Radiation-induced faults are frequent in outer space (Adams et al., 1982; Binder et al., 1975; Blake and Mandel, 1986) . Due to the reductions in size and voltage, the sensitivity of embedded systems to ionizing particles has increased considerably. Energizing particles can produce a number of faults at the hardware (HW) level, not only in harsh environments such as outer space but also in real-life environments, which has been verified by stress experiments in the laboratory using particle bombardment.\n\nHence, electronic components with lower power and noise margins are less reliable, which is why recent systems are more prone to transient faults induced primarily by radiation (Baumann, 2002 (Baumann, , 2005a Seifert et al., 2002; Shivakumar et al., 2002) . Although transient faults do not cause permanent damage in circuits, they can affect system behavior by corrupting stored information or signal communication (Karnik and Hazucha, 2004; Mavis and Eaton, 2002) .\n\nIn view of these new challenges, there is an increasing need to deal with faults and their consequences in the system. There are two classes of mechanism to cope with faults: fault avoidance and fault tolerance (FT) (Avizienis et al., 2004) . Fault avoidance means developing components/systems that are less likely to present faults, while fault-tolerant techniques enable the system to tolerate the effects of these faults. By definition, FT is the ability to provide uninterrupted services, conforming to the desired levels of reliability even in the presence of faults (Avizienis et al., 2004) . As complete avoidance of faults in a system is practically impossible, a balance of the two approaches is currently applied.\n\nDespite the efforts made to apply fault-tolerant techniques to commercial offthe-shelf computers to reduce costs, no substantial breakthrough has been achieved in efficiency or any other crucial property required of safety-critical systems (Antola et al., 1986) . The main focus of the research community is (1) identifying all possible mechanisms leading to accidents and (2) providing preplanned defense techniques against them. However, too little attention has been paid to developing potentially resilient systems that can withstand deviations from desired states due to change and disturbance.\n\nThis research on new-generation resilient and evolving computer systems is driven by observations on resultant limitations from the evolution of computer architecture, which have been motivated by technological and market choices as well as physical limitations. As the traditional concepts of reliability, FT and dependability (the ability to deliver a service that can justifiably be trusted) do not take into account the transient nature of some of the faults induced by radiation, a new concept of resilience has to be introduced for embedded systems, reflecting the changing environment and the different FT contexts. The proposed resilience theory possesses six attributes: reliability, security, safety, performability, robustness, and evolvability. Note that software faults as the source of errors are out of the scope of this work.\n\nIn computer science the term \"resilience\" has been traditionally used as a synonym of FT. Historically, the word has been used in various fields with multiple meanings. As a property, it has different implications. In social psychology resilience is about elasticity, spirit, resource and good mood. On the other hand, in material science resilience involves not only elasticity but also robustness. In the present study, the concept of resilience for safety-critical applications is extended in line with the material science usage. Therefore, the term resilience discussed in this section includes both attributes: robustness and elasticity.\n\nThe term \"robustness\" refers to the use of static techniques such as the use of very reliable materials or the use of rigid and predesign approaches to FT. Ideally, a robust system can deliver correct service in conditions beyond the normal domain of operation without fundamental changes to the original system. However, total reliability in the event of unforeseen faults other than the normal domain of operation is not feasible. Therefore, a resilient approach requires another important attribute, elasticity, to respond to deviations from desired states due to change and disturbance.\n\nElasticity is interpreted as the ability to \"spring back\" (or recover) without losing the intrinsic properties of the material or system. Applied to resilience, elasticity is understood as the ability to evolve, to successfully accommodate changes (evolvability). An evolvable system may perform changes to the system, decreasing its level of performance or reliability for a specific time range (1) to compensate for faults or (2) during exceptional circumstances (graceful degradation). In other words, a resilient system must have the ability to be adaptable, understanding adaptability as the ability to evolve while executing. Hence, adaptability is a subset of evolvability and requires the ability to anticipate changes prior to the occurrence of the resulting damage. Therefore, a resilient architecture must possess different mechanisms to acquire both attributes: (1) static predesign fault-tolerant techniques (robust) and (2) dynamic techniques (elastic) that may be achieved with the ability to reconfigure elements of the system (reconfiguration). Such a resilient approach in designing the computer architecture for safety-critical applications aims at achieving the ability to deliver correct service adapting to disturbance, disruption, and change within specified time constraints.\n\nA list of specific requirements or attributes is given as follows:\n\nContinuity of service (reliability) Readiness for usage (availability) Nonoccurrence of catastrophic consequences (safety) Nonoccurrence of incorrect system alterations (integrity) Ability to undergo corrective maintenance and recovery with maximum coverage of faults (testability and recoverability) Ability to perform in the presence of faults (performability) Ability to decrease the level of performance for a specific time range in order to compensate for HW faults (graceful degradation) Ability to regain operational status via reconfiguration in the presence of faults (recoverability via reconfiguration) Ability to accommodate changes (evolvability) Ability to anticipate changes (adaptability) Clearly, resilience is not a simple single concept, but is a function of all these attributes. Taking into consideration all these attributes, resilience is defined as follows: Fig. 2.3 illustrates the different attributes and measures of resilience. As seen, most of them are important design principles of traditional computer systems. For clarity, the definitions of reliability, safety, security, and performability as discussed here are given below: Reliability: Reliability R(t) is the probability that a system or component will perform its intended function (without failure) over the entire interval [1, t] under specified environmental and operating conditions. Safety: In safety-critical systems, safety describes the absence of catastrophic failures for users and the environment when a failure takes place. Security: Security is the concurrent existence of three attributes: integrity, maintainability and availability. Integrity can be defined as the absence of improper system state alterations. Maintainability refers to the ease and rapidity with which, following a failure, a repairable system can be restored to a specific operational condition. Availability can be simply defined as \"readiness for correct service\" (Avizienis et al., 2004) .\n\nPerformability: Performability is the ability of a system or component to accomplish its designated functions within specified constraints such as speed, accuracy or memory usage.\n\nHardware faults are a major concern in silicon-based electronic components such as static random access memory, dynamic random access memory, microprocessors and field programmable gate arrays, which have a well-documented history of faults mainly caused by high-energy nuclear particles. In the case of safety-critical systems, maximum reliability can be achieved by assuming susceptibility of those systems to faults produced by various internal (e.g., interconnect coupling noise) and external (e.g., cosmic and solar radiation) disturbances. The traditional reliability analyses of these systems assume failure rates of permanent faults. So far, design and reliability engineers have discounted the effect of transient faults. However, the density of modern silicon chips makes them vulnerable to particles of lower energy that can cause transient faults and consequently, catastrophic system failures (Constantinescu, 2003; Hazucha and Svensson, 2000; Hazucha et al., 2003) .\n\nThe term \"radiation\" commonly refers to the process in which energy in the form of waves or particles travels through space or a material medium, ultimately to be absorbed by another body. In general, radiation can be divided into ionizing and nonionizing radiation depending on its ability to ionize matter. Nonionizing radiation is not a concern here because normally it does not carry enough energy to produce changes to electronic circuitry. On the other hand, ionizing radiation carries enough energy to directly or indirectly remove electrons from atoms or molecules, thus causing the formation of ions. Sources of ionizing radiation include highly energetic protons, alpha particles, heavy ions, galactic cosmic rays, and others. This classification also includes neutrons because their collision with nuclei produces ionizing radiation, even though they are not ionizing particles.\n\nDue to the reduction in size of the transistors and the reduction in critical charge of logic circuits, the effects of ionizing radiation are becoming a serious concern in semiconductor devices in various settings, such as outer space, high altitudes and at sea level. Basic radiation damage mechanisms affecting electronics include atomic lattice displacements and ionization damage, which induce different types of failure such as total ionizing dose (TID), single event effects (SEEs), and displacement damage dose (DDD). Thus, when energetic particles collide with sensitive regions of the semiconductor, stored information can be distorted, potentially leading to logic errors. Ironically, as manufacturing technologies advance, the natural resilience of previous technologies to information corruption is decreasing (Baumann, 2002 (Baumann, , 2005a Seifert et al., 2002; Shivakumar et al., 2002) .\n\nTransient faults (Breuer, 1973) are the predominant faults in modern technologies, and can be caused by environmental conditions like temperature, pressure, humidity, voltage, power supply, vibrations, fluctuations, and electromagnetic interference due to crosstalk between long parallel lines in a chip. However, ionizing particles are the major source of this type of fault.\n\nAs mentioned above, there are two basic damage mechanisms to electronic elements due to radiation: atomic lattice displacement and ionization. Atomic lattice displacement occurs when an energetic particle undergoes a nuclear collision with one or more atoms of the electronic device, changing its original position (refer to Fig. 2.4) and thus the analog properties of the semiconductor junctions, potentially affecting the properties of the material and creating lasting damage. As shown in Fig. 2 .4, the displaced atom is referred to as \"primary knock-on atom\" (PKA), and its new nonlattice position is called \"interstitial,\" while its absence from its original lattice position is named \"vacancy.\" The combination of a vacancy and an adjacent interstitial is called a \"Frenkel pair.\"\n\nIn silicon, displacement of an impacted atom is possible if it is part of the crystalline structure and the incident particle is capable of inducing a minimum energy (displacement threshold energy) of around 20 eV (Miller et al., 1994) . In most cases, the displaced atom with enough energy is likely to further knock out a neighboring atom creating a more complex configuration known as a \"cluster,\" affecting the properties of the bulk semiconductor material. In silicon, clusters and vacancies are of unstable nature and tend to be filled by nearby atoms, forming more stable defects. In general, this migration leads to the most typical process, known as \"defect reordering\" or \"forward annealing,\" mitigating the amount of damage and its effectiveness. Yet, in some cases, depending on the time, temperature and nature of the device, \"reverse annealing\" can also occur, producing more efficient defects.\n\nIt is known that ionization damage is primarily induced by charged particles, usually leading to transient effects that cause temporary variation of the functionality of the system. This type of error is called a soft error because no permanent damage is induced in the electronic circuit. Ionization damage may also lead to small degradation and permanent errors, which are categorized as hard errors.\n\nA key factor in the damage process is the critical charge, or Q crit , which represents the smallest amount of charge that can cause a change of value in a cell. The effects provoked by the above damage mechanisms can vary depending on the type of radiation, radiation flux, total dose, critical charge of the device and manufacturing technology, making fault modeling difficult, and time consuming.\n\nThe resultant macro effects of radiation may be classified into three categories: TID, DDD, and SEEs. Based on the type of degradation that these macro effects generate on electronics, TID and DDD are considered as long term and SEEs as short term.\n\nTID is a measure of the cumulative effects of prolonged exposure to ionizing radiation. Metal oxide semiconductor and bipolar electronic technologies are affected by TID and the resulting damage is permanent: once the material is damaged, it will not return to its original state (Felix et al., 2007) . Typical TID effects include parametric failures, or variations in device parameters such as leakage current, threshold voltage, etc., or functional failures. The primary sources of TID are trapped protons and electrons, and solar protons (Barth et al., 2004) .\n\nDDD or \"bulk\" damage (Barth et al., 2004; Yu et al., 2005) occurs when highenergy particles displace atoms from the semiconductor lattice due to its prolonged exposure to nonionizing energy loss. DDD often has similar long-term degradation characteristics to TID, but is a separate physical mechanism. The damage mechanism is the result of collisions with atoms, which become displaced from the lattice, Atom Vacancy Interstitial Frenkel pair Ejected particle (PKA) Semiconductor lattice Incoming particle creating interstitials and vacancies. Over time, sufficient displacement can occur and may change the performance properties of the device or material. Consequently, DDD is an effect of concern for all semiconductor bulk based devices. DDD accumulation primarily occurs when the semiconductor material is exposed to neutrons, trapped protons and solar protons over time. Likewise, secondary radiation produced in shielding materials can cause DDD effects.\n\nAs revealed by its name, the term \"SEE\" emphasizes the fact that the effect is caused by an individual particle interacting with the material. In current semiconductor technologies SEEs cause a much larger problem than the combination of all long-term cumulative effects. SEEs are explained in the next section.\n\nSEEs are provoked by the strike of a single energetic particle (ion, proton, electron, neutron, etc.) in sensitive regions of the material. The particle travels through the semiconductor material, leaving an ionized track behind by depositing sufficient energy to generate an effect on a localized area of the electronic device. Although TID and SEE are both caused by ionizing radiation, their damage mechanisms are very different. While TID is a long-term effect that changes the electrical properties of the device, SEEs are the result of an instantaneous perturbation.\n\nNeutron and alpha (\u03b1) particles are the most common sources of SEEs in terrestrial environments, while in space SEEs are primarily caused by cosmic rays and heavy ions. SEEs affect many different types of electronic devices and technologies, causing data corruption, high current conditions and transient disturbances. Left untreated, harmful functional interruptions and catastrophic failures can easily occur.\n\nSemiconductor devices experience SEEs in two major forms: in the form of hard errors with destructive effects and in the form of soft errors with nondestructive effects. While the former leads to permanent degradation or even destruction of the device, the latter leaves no permanent damage.\n\nSoft errors are of temporal nature, implying that the physical functionality of the circuit is not affected, even though its temporal integrity is. Typical examples include undesired changes of logic value in sequential logic and undesired analogue pulses that temporarily change the output of combinational logic. Soft errors can be further divided into transient and static errors (Mavis and Eaton, 2002) . When a soft error occurs, it can result in a detected recoverable error, detected unrecoverable error, or silent data corruption (Kadayif et al., 2010; Weaver et al., 2004) . There are numerous types of SEE, which can be categorized depending on the type of degradation, recoverability, and technology susceptibility.\n\nTo increase the reliability of safety-critical systems, effective methods and techniques have to be employed to prevent or reduce the appearance of faults that could cause catastrophic system failures. For system design, faults can be dealt with by utilizing two different design strategies: fault avoidance and FT.\n\nSince a failure is the result of an error spreading, and an error is the result of a fault, eliminating faults would improve reliability. Targeting the source of faults, fault avoidance techniques attempt to prevent faults from occurring in the first place. Therefore, fault avoidance strategies can be used in the design stage of electronic devices. Examples of fault avoidance techniques are silicon on insulator and hardened memory cells. However, complete removal of faults via fault avoidance is not possible. Besides, these techniques and supporting technologies have drawbacks regarding cost, speed of operation, chip area, and power consumption.\n\nOnce a fault has been generated, it can be prevented from activating an error by using either static fault-tolerant techniques or dynamic fault-tolerant techniques. Hence, FT strategies are implemented at the time of job execution. As faults and tolerance to them can be considered at different abstraction levels of system description, FT strategies can be implemented at a system level or element level. Fig. 2 .5 shows the failure life cycle and different techniques employed to deal with faults at each stage of development.\n\nIn mainstream systems, fault avoidance design strategies are employed to achieve the projected failure rates of the system. Manufacturing companies assess and identify the sources and weaknesses that could lead to potential failures. Based on the assessments, preventive measures are taken to ensure that the overall reliability target is met.\n\nIn addition, fault avoidance strategies may also lead to modifications of conventional manufacturing processes by using technology and design mitigation techniques. These techniques involve the use of specific materials, modification of the doping profiles of devices and substrates, and optimization of deposition processes for insulators.\n\nMore specifically, technology mitigation techniques are based at the process level, and consist of IC process variations by either improving the manufacturing process or by improving the materials used. Design mitigation techniques operate at the layout level, such as the use of enclosed layout transistors. Furthermore, to prevent or reduce the effects of radiation, memory cells can be hardened by using contact and guard rings, and safety margins can be inserted into clock speed, operating temperature and supply voltage margins.\n\nFault tolerance by using redundancy\n\nThe key strategy of FT is redundancy, which is defined as the addition of information, resources or time beyond what is needed for correct system operation.\n\nRedundancy may include combinations of additional elements of HW and/or software, and fault-tolerant techniques rely on them to detect and/or recover from faults. These components are called redundancy because in a perfect system they are not needed. As a design strategy, artificially built-in or protective redundancy is considered as a system property, which is the incorporation of extra components in the design of a system so that its function is not impaired in the event of a failure. Redundancy may arise by design (artificially built-in redundancy) or as a natural by-product of design (natural redundancy). While the former has been deliberately introduced, the latter is not usually exploited. Here, only artificial design redundancy is considered.\n\nWhen the minimum reliability of a system is not satisfied, extra redundancy, not strictly necessary for the normal functioning of the system, can be added in order to increase the probability of normal functioning. Note that redundancy indicates the performance of the same task, not identical functionality. Therefore, heterogeneous HW performing the same work can also provide redundancy.\n\nFT is composed of actions such as fault detection, location of the faulty component, recovery and, if necessary, reconfiguration of the system. Fault detection and location mean, respectively, to determine the presence of faults and the time of occurrence, and to exactly locate the reason/origin of the fault. It should be emphasized that system recovery must be performed dynamically during job execution as if the system is \"as good as new\" in operational terms, even though some of the redundancy has been used up and this may limit the possibilities for future repairs.\n\nIn classifying redundancy, this work follows the approach proposed by Schagaev (2001) . Fig. 2.6 shows the different types of redundancy and how it is implemented in HW and software systems. In general, there are three types of redundancy: structural (S), involving multiplication of components, information (I), involving multiplication of information/data and time redundancy (T), involving multiplication of functions in time. Here, the focus is on the HW aspect of redundancy and FT.\n\nStructural HW redundancy involves multiple independent HW components and assumes execution of the same computation over such components at the same time. When this type of redundancy is used for reliability purposes the errors are exposed by checking and comparing the results of the independent executions. In general, redundancy can be applied at different levels of abstraction: from transistor level, gate or logic level up to microcode level and chip level.\n\nTwo different organizations of structural redundancy can be distinguished: 1. Static redundancy Static redundancy, also called masking redundancy, performs error mitigation. The term \"static\" is linked to the fact that redundancy is built into the system structure. Fault-tolerant techniques based on this type of redundancy transparently delete errors on detection. Triple modular redundancy (TMR) (Von Neumann, 1956) and its generalization N-modular redundancy (NMR) represent the most common form of HW redundancy. Fig. 2 .7 illustrates a TMR with a majority voter.\n\nThis type of redundancy is similar to static redundancy, but its voter logic is replaced with a switch that is controlled by an error detection block. Dynamic redundancy techniques are used in order to reduce the extensive space, energy and performance overheads of TMR and NMR systems. \n\nThis type of redundancy approach is formed by mixing fault masking, detection, location, and recovery to combine the advantages of static and dynamic redundancy (Johnson, 1989) . Hybrid approaches employ fault masking to prevent erroneous results from being processed and erroneous data being spread across the system. Hybrid techniques also use fault detection, location and recovery to improve FT by removing errors.\n\nInformation redundancy denotes the addition of new information to existing information, i.e., using more bits than needed to ensure fault-free functioning. The most common form of information redundancy is coding. Coding consists of adding check bits to the data allowing (1) the verification of data correctness and/or (2) the correction of erroneous data. Coding theory was initially motivated by the need to mitigate errors in information transmission (Shannon, 1948) and thus has a long history of application.\n\nThe main problem with the structural and information redundancy types discussed above is the penalty imposed in the form of extra HW. At the expense of using additional time, FT techniques based on time redundancy aim to reduce the amount of HW required for the implementation. Time redundancy techniques involve multiple executions of a program using the same piece of HW and comparing the execution results to determine if a fault has occurred. This approach is known to be effective in detecting errors caused by transient faults. There are various time redundancy techniques, such as using alternating logic, recomputing with shifted, rotated or swapped operands (the address in a computer instruction of data), etc.\n\nAssume M as the known model of a system that performs a given function F. To analyses methods for achieving a required level of reliability, with performance and power consumption constraints, the following three models are defined:\n\nM s : The model of the system M fault : The model of the faults that a real-time FT system will be exposed to M FT : The model of FT or the new structure that implements FT As shown in Fig. 2 .8, these models are mutually dependent. Note that in this approach solution costs are not considered. M fault represents all faults that a system must tolerate. Typical examples of HW faults include:\n\nByzantine fault: the behavior of a component that gives conflicting values to other components; this fault affects the entire system. Fault encapsulation approaches can facilitate fault handling: due to deliberate design solutions it is possible to ensure that severe faults in the system do not escalate and remain simpler to handle, therefore making the fault handling possible to implement in practice. Fig. 2 .9 illustrates the concept of the new approach with a feature of high reliability that deals with various faults in the system and presents a variety of possible solutions. The system model in Fig. 2 .9 has overlapped HW and system software (SSW) ellipses to represent the duality of the system: HW and SSW. Both of them must be involved in the implementation of FT, as the HW cannot cover all possible faults. Obviously, M FT plays the role of a \"conceptual deliverer\" of reliability for the real-time FT system. Therefore, it has to be effective during the whole operational lifetime of the computer itself.\n\nContrary to the usual conception in reliability modeling, one has to assume that a fault might exist in the system over an arbitrary long period of time (latent fault) and its detection and elimination is not possible \"at once.\" Following Dijkstra's approach (Dijkstra, 1965) of defining a function as a process described by its algorithm, here FT is also considered as a function described and implemented by an algorithm. There are several options to achieve FT, assuming the use of HW and SSW by employing various types of redundancy techniques mentioned above. However, the use of a certain type of redundancy might cause system performance degradation; this is especially true for software measures (Kulkarni et al., 1987; Oh et al., 2002) .\n\nIt is not realistic to describe all possible faults that may occur in a system individually. Hence, fault models (FMs) are developed and used to represent in a simple form the consequence of complex physical mechanisms that lead to faults. Faults are assumed to behave according to these FMs in order to make their evaluations possible. According to Dunn (1991) , a FM is considered as a way of summarizing many fault descriptions at once. Often, it is desirable to model many different faults simultaneously by summarizing their common characteristics.\n\nIn the case of electronic systems, modeling of faults can be implemented at two different levels: at the level of HW components or at the system level. The following classification of faults is primarily based on the work by Avizienis et al. (2004) .\n\nFaults can be classified based on the attributes related to their origin, including their cause, the level at which they occur, the phase of creation, nature, system boundaries, etc.\n\nFaults can also be classified based on the attributes related to their manifestation, including their response, dimension, reproducibility, extent, persistence, etc.\n\nWhen a fault occurs, extra redundancy is required to deal with it. Thus, redundancy and the ability to use it to remove the fault form a combination of tools and techniques that is required for implementing reconfiguration.\n\nWhen system and fault modeling are developed together, system behavior in the presence of faults and the control process of FT can be contemplated at earlier stages, taking into account mutual dependencies of solutions at every stage of the design and development process. This advantage can be fully exploited in designing embedded systems, which have many design constraints and very often mutually exclusive requirements in reliability, performance, and power consumption that can limit design options.\n\nNote that redundancy can be used for various purposes, and it can be involved as an essential part of reconfiguration of connected computers. System reconfiguration purposes include performance improvement, reliability enforcement and energywise use.\n\nAlthough FT in computer systems can be achieved by introducing static redundancy in HW and SSW, employing these traditional approaches is expensive in terms of time, information or HW overheads. To tackle these problems, Schagaev and Sogomonian (Schagaev, 1986a (Schagaev, ,b, 1987 Sogomonian and Schagaev, 1988) proposed to consider FT not only as a feature, but also as a process that can be implemented algorithmically. Fig. 2 .10 shows a process for implementing FT, which assumes dynamic interaction of existing redundancy types between elements. The original three-step algorithm (Sogomonian and Schagaev, 1988) has been further developed into the generalized algorithm of FT (GAFT), a five-step fault-handling algorithm shown in Fig. 2 .10:\n\nDetecting faults Identifying faults Identifying faulty components Hardware reconfiguration to achieve a repairable state Recovery of a correct state(s) for the system and user software.\n\nThe different types of redundancy (information, time, and structural either HWor software-based) can be used to implement every step of the GAFT. As transient faults occur an order of magnitude more often than permanent faults, handling of transient FT must be extremely effective. A good fault-tolerant system tolerates the vast majority of transient faults within the interval of instruction execution, making them invisible for other instructions. At the same time, when transient faults with longer time range or permanent faults are encountered, they might be detected and recovered differently, at the procedural or task level of SSW.\n\nIt should be emphasized that the dynamic interaction feature of the GAFT is vital to building a fault-tolerant system, which is well illustrated in Fig. 2 .11. with faults, where S represents one of the five possible states for a single processing element system: ideal, faulty, erroneous, degraded and failed; T shows transitional operations and M denotes mechanisms involved in FT. While in the GAFT approach FT is carried out in a dynamic process involving interactions between different elements, in the traditional approach a fault is treated as a single event and error mitigation is performed statically. Obviously, the GAFT strategy represents a resilient approach to handling faults and disturbances.\n\n2.3.6 Hardware and system software support of resilience Figure 2 .11 A comparison between the traditional approach and the GAFT approach in error mitigation. Source: Adapted from Fig. 5 .14 in Castano, V. and Schagaev, I. (2015) . Resilient Computer System Design. Springer. scalability, reliability, and resource awareness. A brief introduction to these principles is given below: G Simplicity: Complexity is difficult to implement and handle efficiently. In addition, big complex systems are more prone to faults, thus lowering reliability. G Reliability: The highest reliability of individual components is preferable, but always keeping in mind the cost efficiency of its implementation. G Redundancy: Deliberate introduction of HW and software redundancy provides the required level of reconfigurability to reach performance and reliability goals. G Reconfigurability: Apart from the simplicity, reliability and deliberate introduction of redundancy, it is essential to achieve balance between performance, reliability, and power. Reconfigurability serves three main purposes: performance, reliability, and power awareness. It allows the system to adapt twofold: first to recover from a permanent fault and second adjusting the requirements of the running application. G Scalability: Scalability should be kept in mind when designing a system so that it can be extended if the requirements change.\n\nG Power-awareness: Mission critical systems have significant limitations of HW resources and power consumption constraints (e.g., battery life). Thus, for wise resource use, reconfigurability must be introduced.\n\nBy following these principles a new HW architecture and SSW have been developed for safety-critical applications, a resilience approach to change and disturbance with the ultimate goal of delivering correct service within specified time constraints. Further discussion on the HW and SSW development is deemed out of scope.\n\nof that system. . . Resilience represents the capacity (of an organisational system) to anticipate and manage risk effectively, through appropriate adaptation of its actions, systems and processes, so as to ensure that its core functions are carried out in a stable and effective relationship with the environment.\n\nThis in-depth analysis summarizes all the fundamental aspects of the resilience theories reviewed above, clarifying the ultimate goal of resilience studies as to ensure a stable and effective relationship between the system and the system environment. His description contains four key ingredients: a system, the system environment, the relationship between the two, and a risk or disturbance. A mathematical generalization of resilience is given below.\n\nA system is defined as S(q 1 , q 2 , . . ., q n ), and [q] are key system variables. The system environment is expressed as V, which is directly influenced by the key system variables. A disturbance is denoted by D. An impact function is defined as I (q 1 , q 2 , . . ., q n ), which is a function of the key system variables and measures the impact of a disturbance to the system on the system environment. This impact function represents the relationship between the system and the system environment, and is assumed to be a smooth function.\n\nWhen a disturbance occurs, it releases a certain amount of destructive energy, E d , into the system. Let E s(1) , E s (2) , represent, respectively, the energy absorbed by the system and the inner energy released by the system into the system environment during or after the event.\n\nThen, the total energy absorbed by the system environment, E v , is obtained as\n\nWhen facing a disturbance, a resilient system completely absorbs the energy released by an adverse event, and prevents its potentially harmful system energy from being released into the system environment (e.g., the nuclear meltdown of a nuclear power plant, the toxic chemical pollution of a chemical plant, the crash of a commercial airplane), i.e., With no harmful energy being released into the system environment, the impact function retains its minimum value, i.e., dI\u00f0q\u00de dq \u00bdq5\u00bdq \u00c3 5 0 or I 0 \u00f0q \u00c3 \u00de 5 0 (2.5)\n\nHere, the characteristic system variables [q \u00c3 ] represent the resilience design of a system. Fig. 2 .12 shows the concept of the impact function of the system environment and examples of resilient and nonresilient systems. In general, a system design is subjected to various constraints or restrictions, and thus even a resilient system may exert a limited impact on the system environment, which is represented by I 0 in Fig. 2 .12. Nevertheless, compared with any other approaches that do not incorporate resilience strategies, the impact of a resilient system on the system environment is at the minimum.\n\nObviously, for a socioecological system or a sociotechnical system, the system environment of that local system refers to the much larger socioecological or sociotechnical environment interacting with that system, including the people who live and work in it. For an embedded computer system used in a pacemaker implanted in a heart-disease patient, the patient is the system environment; if it is used in a commercial airplane, the passengers and the crew are the immediate system environment, and so forth. The key system variables are the controlling system variables, through which the seven resilience principles of the socioecological system, the four essential resilience capabilities of the sociotechnical system, and the six resilience attributes of the computer system can be introduced into the system.\n\nFinally, a mathematical definition of resilience is attempted. In the event of any disturbance, if a system completely absorbs the destructive energy released by that event, i.e., E d 5 E s(1) , and does not release any harmful system energy into the system environment as a result of that event, i.e., E s(2) 5 0 and E v 5 0, then the impact function of the system environment retains its minimum value, satisfying I 0 (q \u00c3 ) 5 0. Under this condition of minimum impact, a system is defined as a resilient system.\n\nBased on this definition, the priority of the resilience approach is to minimize the negative impact of an adverse event occurring to a system on the system environment. Although there is no explicit mention of the system condition during or Impact function I(q)\n\nI\u0408(q)* = 0\n\n[q] = [q]* [q] I 0 A resilient system A nonresilient system after that event, it is implicitly defined by the system's energy conditions specified in Eqs. (2.2) and (2.3) . Obviously, these conditions can be met only when a system possesses the resilience characteristics specified by the various resilience theories discussed so far.\n\nThe impact-function-based definition of resilience focuses on the various energies exchanged between a disturbance and a system, and between the system and the system environment during an adverse event. This definition enables a unified physical interpretation of resilience in various fields of study. Several general examples of resilient and nonresilient systems with energy conditions specified are given below:\n\n1. In earthquake engineering, a resilient community is one that sustains limited damage despite a major earthquake, and is able to recover its normal function quickly, thus satisfying the conditions E d 5 E s(1) , E s(2) 5 0 and E v 5 0. 2. A resilient computer system for safety-critical applications must possess the resilience attributes described in Section 2.3. Such a system satisfies the conditions E d 5 E s(1) , E s(2) 5 0 and E v 5 0 in the event of faults and errors, such as those caused by radiation, by using any one of the fault-tolerant and error-mitigation techniques discussed previously. 3. The two nuclear accidents, one at the Chernobyl nuclear power plant in 1986 and the other at the Fukushima nuclear power plant in 2011, proved that the two power plants were nonresilient systems. Even though the causes of these accidents were completely different [the former due to a flawed reactor design with operation by inadequately trained personnel (WNO, 2016) , and the latter due to a natural disaster], the irreversible, worstscenario conditions of E s (2) . 0 and E v . 0 (or E s (2) 5 N and E v 5 N) occurred in both cases.\n\nThe impact function will be discussed further in the next chapter. Defining the ratio of E s(1) to E d as a resilience index, the impact function will be redefined as a function of this resilience variable. The specific form of the impact function will also be discussed.\n\nIn principle, exact solutions of complex dynamical systems that are the targets of resilience studies cannot be found, with respect to the equations of motion of systems that are often mechanical or physical in nature, as well as systems that arise in biology, economics, and elsewhere. This is because a precise mathematical description of the system behavior will inevitably lead to ill-defined dynamic equations that cannot be solved. Nevertheless, knowledge of the long-term qualitative behavior of a complex dynamical system is highly desirable for developing an efficient and reliable resilience approach. In the following review, a generalized two-step approximate solution to a nonlinear dynamic problem (Shi et al., 2014) is introduced first, aiming at its wider application to similar types of problems in complex dynamical systems. Then, this method is applied to studying the cracking behavior of a large concrete dam during a strong earthquake, and the resilience issue of a complex dam-reservoir system against natural disasters is briefly discussed.\n\nTo generalize the solution strategy of the two-step approach, a dynamic response problem in association with a time-dependent excitation or energy input is defined by using four functions, which are the target function, T(x, y, z); the excitation or energy function, E(t); the force function of T(x, y, z), F T (x, y, z, t); and the deformation function of T(x, y, z), D T (x, y, z, t). In general, the purpose of a linear or nonlinear response analysis of T(x, y, z) subjected to E(t) is to obtain D T (x, y, z, t).\n\nThe target function T(x, y, z) is a well-defined space function, representing a target of study such as a dam. The excitation function E(t) stands for a source of timedependent energy input, such as an earthquake, and subjecting E(t) to T(x, y, z) generates a dynamic response in the target function. The time-varying dynamic force function F T (x, y, z, t) is derived from the dynamic response of T(x, y, z), such as the function of inertia force in a dam under earthquake force. Acted upon by this force function, a deformation or variation of T(x, y, z) occurs, which is expressed by the deformation function D T (x, y, z, t). The aim of the response analysis is to obtain this time-dependent deformation function, from which important features in the dynamic response of the target function are studied, such as the cracking behavior of a dam during a large earthquake.\n\nIn mathematical terms, the above description of the problem may be expressed as T\u00f0x; y; z\u00de 3 E\u00f0t\u00de ! F T \u00f0x; y; z; t\u00de 3 D T \u00f0x; y; z; t\u00de:\n\n(2.6a) or T\u00f0x; y; z\u00de 3 E\u00f0t\u00de 5 F T \u00f0x; y; z; t\u00de 3 D T \u00f0x; y; z; t\u00de: (2.6b)\n\nT\u00f0x; y; z\u00de 3 E\u00f0t\u00de 5 F T \u00f0x; y; z; t\u00dej L 3 D T \u00f0x; y; z; t\u00dej L (2.7)\n\nIn general, an accurate solution of Eq. (2.7) is possible. For nonlinear response:\n\nT\u00f0x; y; z\u00de 3 E\u00f0t\u00de 5 F T \u00f0x; y; z; t\u00dej NL 3 D T \u00f0x; y; z; t\u00dej NL : (2.8)\n\nIn general, an accurate solution of Eq. (2.8) is impossible. This is because the nonlinear deformation function D T (x, y, z, t)| NL contains elements of change that can fundamentally transform the target function T(x, y, z) from its original form, such as a gradual change in the geometry or mass, or some other basic features, resulting in ill-defined dynamic equations that cannot be solved.\n\nThe two-step approach suggests obtaining an approximate function D \u00c3 T (x, y, z, t)| NL for D T (x, y, z, t)| NL based on F T (x, y, z, t)| L , assuming that F T (x, y, z, t)| L is a close approximation to F T (x, y, z, t)| NL , i.e., T\u00f0x; y; z\u00de 3 E\u00f0t\u00deDF T \u00f0x; y; z; t\u00dej L 3 D \u00c3 T \u00f0x; y; z; t\u00dej NL : (2.9)\n\nEq. (2.9) transforms the time history analysis in Eq. (2.8) from dynamic analysis to static analysis based on the known function of F T (x, y, z, t)| L , and it is believed that this redefined nonlinear problem can be accurately solved to obtain the function D \u00c3 T (x, y, z, t)| NL . The logical reasoning based on mechanics principles for this approach is that, if the linear force function F T (x, y, z, t)| L is a close approximation to the actual force function F T (x, y, z, t)| NL , then the approximate deformation function D \u00c3 T (x, y, z, t)| NL must be a good estimate of the actual deformation function D T (x, y, z, t)| NL . Fig. 2 .13 illustrates the flow of this two-step solution. The error of the approach may be estimated in percentage form as\n\nA nonlinear dynamic problem Solve as a linear dynamic problem to obtain F T (x,y,z,t)| L Solve as a static nonlinear problem to obtain D * (x,y,z,t)| NL\n\nStep 1\n\nStep 2 Here, W F is the total work spent in the process (such as the total fracture energy consumed for propagating all the cracks in a dam during an earthquake) that gives rise to the important features in D T (x, y, z, t)| NL which are the focus of the study, and W F \u00c3 is the corresponding value calculated from D \u00c3 T (x, y, z, t)| NL . For certain types of nonlinear response phenomena, due to some specific feature of the problem in question the nonlinearity in its deformation function D T (x, y, z, t)| NL may have little influence on the system's dynamic response (not vice versa; after all, the nonlinearity is caused by the dynamic response). Hence, the force function obtained by linear response analysis, F T (x, y, z, t)| L , is likely to represent closely the actual force function, F T (x, y, z, t)| NL , which in principle cannot be accurately derived from nonlinear response analysis.\n\nFinally, it is worth pointing out that though the target function T(x, y, z) is initially assumed to be independent of time; actually, it can also contain a time variable such as T(x, y, z, t), provided that this new target function is also a known function in terms of space and time.\n\nResilience assessment of a complex dam-reservoir system Figs. 2.14 and 2.15 show successful applications of this two-step solution in obtaining the discrete cracking behavior of a large concrete gravity dam based on the linearly obtained time history of the dam's inertia force field under inland type and oceanic-trench type earthquake motions. Note that in order to propagate large cracks in the dam two extremely strong earthquakes are assumed in the study. With Figure 2 .14 Display of continuously changing crack behavior within a split second during the rescaled Hitokura earthquake (inland type) (Shi et al., 2014). reference to a detailed study on the seismic cracking behavior of concrete dams (Shi et al., 2014) , the focus of discussion here is on the resilience issue of a damreservoir system, using the newly proposed resilience theory discussed in the preceding section.\n\nBased on the obtained cracking behavior, the resilience of the dam-reservoir system against disturbance can be classified into three categories: 1. A resilient system: Despite a large earthquake, the dam has sustained no serious cracking damage that could affect the normal functioning of the dam-reservoir system. Thus, the conditions E d 5 E s(1) , E s(2) 5 0, and E v 5 0 are satisfied, i.e., there is no harmful energy released into the system environment. Here, E d and E s(1) represent, respectively, the destructive earthquake energy and the kinematic energy of the dam during seismic motions.\n\nAs a result of a large earthquake, the dam has sustained severe cracking damage that could affect the normal functioning of the dam-reservoir system, such as flood control due to damage to the spillway. Note that seismically damaged dams can require several years before complete repair and functional restoration can be accomplished (Hartford, 2011) . In this case, E s(1) , E d , E s(2) 5 0 and E v . 0, implying a harmful effect on the system environment. Obviously, the fracture energy sustained by the dam under seismic forces, W F , represents that harmful effect, i.e., W F 5 E d 2 E s(1) , and E v 5 W F . 3. A failed system:\n\nAs a result of a large earthquake, the dam has failed catastrophically due to the occurrence of penetrating cracks, causing grave social, environmental and economic consequences to the downstream areas. In this case, obviously, E s (2) 5 N and E v 5 N. Figure 2 .15 Display of continuously changing crack behavior within a split second during the rescaled Urakawa earthquake (oceanic-trench type) (Shi et al., 2014) ."}