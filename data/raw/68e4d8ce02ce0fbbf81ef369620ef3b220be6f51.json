{"title": "Language model-based automatic prefix abbreviation expansion method for biomedical big data analysis", "body": "Big data analysis has opened the door to a new era in biomedical fields, such as healthcare [1] and disease diagnosis [2, 3] , etc. Abbreviations are appearing more and more frequently in these areas, which significantly hinders development in related research fields such as biomedical text analysis [4, 5] , large biomedical ontologies [6] . Abbreviations are used in almost all types of data (structured, semi-structured, unstructured). Regarding unstructured data, Ammar et al. [7] have informed that English Wikipedia articles contain an average of 9.7 abbreviations per article and that more than 63% of the articles contain at least one abbreviation. At the sentence level, over 27% of sentences in news articles contain abbreviations. In the biomedical domain, the situation is becoming worse. Clinical narratives are typically produced under time pressure, which incites the use of abbreviations and acronyms [8] . A study focused on the electronic discharge summaries from a large, tertiary teaching hospital in Australia revealed that abbreviations were common, occurring at a frequency of one in five words [9] . For structural (e.g., relational databases, etc.) and semi-structural data (e.g., XML (Extensive Markup Language), knowledge graph etc.), abbreviations are widely used for element names since the identifier length is limited (for example, the identifier length is limited to less than 30 characters in Oracle). Abbreviations are substantial obstacles in related fields such as ontology matching [10] and knowledge map construction.\n\nAbbreviations can be separated into two classes: standard abbreviations and ad hoc abbreviations. The former are widely used and accepted, including acronyms such as HIV (Human Immunodeficiency Virus) and SARS (Severe Acute Respiratory Syndromes). The latter are abbreviations that are used in special situations. For example, in intensive care unit notes, sentences such as ''61 y (year) .o(old). M(Male) pt(patient) with a hx(history) of COPD (chronic obstructive pulmonary disease), HTN (Hypertension). . . '' often occur. Physicians use many ad hoc abbreviations under heavy time pressure. In order to understand above text, the corresponding expansions are chosen to replace the abbreviations (called text normalization). In the existing works [8, 11, 12] , the operation of text normalization is divided into the following two steps: firstly, all possible expansions are selected from the dictionary; then, a suitable sense is selected according to the context to replace the abbreviations. If the abbreviation or the suitable expansion is not contained in the dictionary, above method could not work. This seriously hinders the effect of text normalization especially for the data with many ad hoc abbreviation such as EMR (Electronic Medical Record), structural and semi-structural data. If the expansion of the abbreviation could be generated by an automatic method, the effect of text normalization could be improved dramatically.\n\nAbbreviation is a short form of a word or phrase. It is comprised of certain characters in the original order in word or expression. In the existing works, the LCS (Longest Common String) rule is used as the basic criterion to judge the correct expansion. A direct strategy to generate the expansion is to list all the candidate words or phrases meet the requirements of LCS. This strategy has the following drawbacks 1. The search space is difficult to be determined. The search space consists of all words and phrases. But getting all the phrases is impossible.\n\n2. LCS is a loose rule for expansion, so not all the words or phrase meet LCS are rational expansion. For example, ''MODS'' is the abbreviation of phrase ''Multiple Organ Dysfunction Syndrome''. But it could be the abbreviation of ''modest'', ''normal goods'' etc. even if they meet LCS rule.\n\n3. The expansion result will contain many phrases with similar semantics. For example, ''deoxyribose nucleic acid'' and ''deoxyribose nucleic acids'' will be generated for ''DNA''. This will result in the number of expansion in the result is very large. Subsequent operations would be impacted.\n\nIn this paper, a new method called LMAAE (Language Modelbased Automatic Abbreviation Expansion) is proposed to get the expansion automatically. In LMAAE, the above three drawbacks are processed separately. The flow diagram of LMAAE is depicted in Fig. 1 .\n\nThe procedure of LMAAE is divided into three steps: partition, expansion and filter, cluster. In each step, appropriate measures are proposed to alleviate the corresponding drawback. Details as follows:\n\nPartition: In this step, the abbreviation is partitioned into several blocks. Each block is corresponding to a word of the phrase. For an abbreviation with n characters, the number of different partition is 2 n . In this paper, there are two means to reduce the number of partition. At first, the maximum number of words in phrase is limited according to the statistic of the phrase in abbreviation dictionary. And then, the rationality of partition is proposed to filter the partition. Through analysis of the number of rational block, the number of rational partition is restricted to a constant value.\n\nExpansion and filter: For each block, all the words in the dictionary meets LCS rule consist of the expansion set of the block. And then, the expansion of the abbreviation is made up of the Cartesian product of each block. At first, through analysis of the characteristic of the abbreviation in dictionary, more than 99% of the abbreviation meet prefix abbreviation rule: abbreviation consists of the prefixes of the key words of the phrase. So prefix abbreviation rule is adopted to reduce the search space of expansion in this paper. This greatly improves the accuracy of the expansion. Secondly, not all the word sequence meet prefix abbreviation rule is a meaningful phrase, so the language model is used to evaluate and filter each word sequence to further reduce the expansion result.\n\nCluster: The output of previous step is a set with many expansions. But there are many phrases in the set with the same meaning. The mean-shift clustering algorithm is selected to cluster the results to eliminate redundant expansions.\n\nThrough above optimizing process, most of the disturbance terms are filtered from the result. The experiment result shows that the expansion set could include most of the expansion in the dictionary. The contributions of this paper can be summarized as follows.\n\n1. For the first time, an automatic method is first proposed to generate expansion for the abbreviation not contained in dictionary. Collaborated with dictionary-based method, the LMAAE could improve the effective of text normalization dramatically.\n\n2. Theoretical analysis of the time complexity of the automatic expansion method is presented, and several optimizing processes are proposed to improve the effective of expansion set and the time complexity of method.\n\n3. An excessive number of expansions will cause poor ASD (abbreviation sense disambiguation) performance. Therefore, we introduce a new strategy for clustering expansions with similar semantics to improve the results.\n\nThe remainder of this paper is organized as follows. Section 2 introduces the related research work. The definitions related to abbreviations and expansions are provided in Section 3, as well as some preliminaries. In Section 4, an analysis of the time complexity of the LMAAE method is presented and a corresponding optimization method is suggested to improve it. To validate the LMAAE method, several experiments were conducted, as discussed in Section 5. Section 6 provides the conclusions and describes areas requiring further work.\n\nIn biomedical big data analysis, the main research achievements involve dictionary construction and ASD. This section provides brief summaries of these areas and introduces some other important fields related to abbreviations.\n\nAt present, there are many public abbreviation dictionaries, such as AllAcronym, 1 Abbreviations, 2 and Stedman [13] . Through continuous improvement for almost 20 years, the number of dictionary entries has reached over one million, and the entries are checked manually one by one. Although these dictionaries are widely used in natural language processing, the shortcomings are obvious. Firstly, the maintenance cost is very high, and secondly, such dictionaries are ineffective for ad hoc abbreviations and some specific abbreviations such as eDNA (which consists of a word ''extracellular'' and an abbreviation ''DNA'' (Deoxyribo Nucleic Acid)). An abbreviation dictionary contains pairs of abbreviations and their expansions in the form <abbreviation, expansion>, for example, <SARS, severe acute respiratory syndrome>. In recent years, numerous methods have been suggested to find expansions automatically. Most methods can be classified into two categories: pattern-matching techniques and machine learningbased methods. For pattern-matching techniques, the LCS is the most important standard for candidate pair judgment. Representative works include those on an acronym-finding program [14] and three-letter acronyms [15] . Because the LCS restriction is so loose that many intrusive expansions are introduced, the search range and abbreviation length are limited to a certain extent. For machine learning-based methods, the decision is made by the machine-learning algorithm. For pair classification, proper features are designed to learn the abbreviations and expansions. Typical works include those on SVM (Support Vector Machine)-based [16] , HMM (Hidden Markov Model)-based [17] , CRF (Conditional Random Field)-based [18] , and latent-state neural CRF-based [19] methods. Henriksson et al. [20] stated that different types of texts contain different features and that different models can extract different features. Based on this idea, they suggested a mixed multi-model and multi-corpus model to judge the final full forms based on candidates. The main purposes of above methods are to select best expansion for the abbreviation, the main differences are in the following three aspects: type of abbreviation, the search rule and search scope. the details of these three aspects for these methods are listed in Table 1 .\n\nHighly accurate abbreviation dictionaries have been applied widely in biomedical big data. However, there are some drawbacks. Firstly, it is time-consuming and laborious to maintain dictionaries, and secondly, dictionaries are not suitable for ad hoc abbreviations.\n\nFor standard abbreviations, the main task of ASD is to choose a suitable expansion from the dictionary according to the context. The machine learning is the most used methods for ASD. In [21] [22] [23] [24] , a classifier for each abbreviation is trained according to the context. In [21] , Moon et al. presented evaluations of three kinds of classifier (na\u00efve Bayes, SVM, decision trees) in terms of their features, context window sizes, orientations, and minimum training sample sizes and proposed the best configuration of these parameters. In [22] , Wu et al. described the use of word embedding to construct the context. Two new SBE (surrounding-based embedding) context modes called LR_SBE (left-right surrounding based embedding) feature and MAX_SBE (maximum surrounding based embedding) feature are integrated as the context. Then, the context is entered into the SVM to select the proper sense. In [23] , the authors discussed the use of two kinds of context mode (SBE and term frequency-inverse document frequencybased embedding modes) to model the context, followed by abbreviation disambiguation by calculating the cosine similarity to choose the most similar one from the given candidates. Hua et al. [24] established a profile for each sense of an abbreviation from the discharge summaries and admission notes. During disambiguation, the cosine similarities between the context vector of the abbreviation and the profile vectors are calculated, and the sense corresponding to the highest similarity score is selected as the correct one.\n\nIn contrast to the techniques employed in the abovementioned studies, deep learning methods [25, 26] have the obvious advantage that feature engineering can be avoided. In [26] , Ahmed et al. suggested a deep learning model to train a vector for the context of each sense of each abbreviation. For disambiguation, the cosine-similarities for the context vector of the sentence with the context vector of each sense are calculated, and the sense with the maximum value is selected. In [25] , Joopudi et al. proposed a convolutional neural network with one convolutional kernel, a max-pooling layer, and a fully connected feed-forward neural network layer followed by a fully connected softmax classifier. Except for the embedding of surrounding words, the location and part of speech information of the word are considered in the context.\n\nThe processing methods for abbreviations are different in different research fields. The handling details of abbreviations in some typical fields are described below.\n\nText Normalization The target of text normalization is to convert the informal text into standard formal form. It is a critical step in the variety of tasks involving speech and language technologies. In [27] , Zhang et al. test the effect of automatic normalization on dependency parsing by using automatically derived parse trees of normalized sentences as reference. It is shown that Table 1 Key detail of the methods.\n\nType of abbreviation Search rule Search scope [14] Acronym LCS of initial Any subsequence in ten words before the acronym [15] Acronym LCS of initial, heuristics rules Any subsequence in the sentence [16] Acronym SVM Any sequences in the sentence exceed the number of characters in the acronym in length [17] Acronym LCS and the same initials min(|A| + 5, |A| * 2) [18] Acronym CRF Any subsequence in the sentence [19] Acronym LNCRF All the subsequence in the document [20] Abbreviation Semantic distance of context All the words in the text the performance of the normalizer is directly tied to the performance of a downstream dependency parser. In [28] , Wu Y et al. presented a framework called CARD(clinical abbreviation recognition and disambiguation) to handle the abbreviation in clinical data that leverages previously developed methods, including:\n\n(1) machine learning based approaches to recognize abbreviations from a clinical corpus, (2) clustering-based semi-automated methods to generate possible senses of abbreviations, and (3) profile-based word sense disambiguation methods for clinical abbreviations. In [29] , Yonghui Wu et al. propose a hybrid strategy that combines a machine learning based method using SVM, a profile-based method using Vector Space Model, and a majoritysense method to resolve ambiguous abbreviations appeared at different frequency levels. In [30] , Pierre Zweigenbaum et al.\n\npropose a supervised abbreviation resolution system. The system learns a MaxEnt(Maximum Entropy) model from the training data, based on a simple feature set that combines UMLS(Unified Medical Language System) knowledge with information gathered from the EMR text. Software maintenance: Information retrieval techniques are being exploited by an increasing number of tools supporting software maintenance activities. Identifiers are among the most valuable sources of information for software maintenance. When a programmer introduces an abbreviation (e.g., rect) as an identifier, the difficulty of understanding the identifier is increased. Corazza et al. [31] proposed a method of automatically splitting identifiers into their composing words and expanding abbreviations. The solution is based on a graph model and performs linearly in time with respect to the dictionary size. Because only complete words are considered in the identifier, multi-word abbreviations cannot be handled effectively, but this approach regarding identifiers is the most commonly used. Alatawi et al. [32] proposed a Bayesian unigram-based inference to expand abbreviations automatically into their original words to enhance source code maintenance. In this technique, a list of candidate words is extracted automatically from the source code for a given abbreviation and the statistical properties of the unigram of the abbreviation are employed as evidence to find the best candidate word. Due to the use of the unigram model, this approach cannot be utilized to address abbreviations that are expandable into phrases. Alatawi et al. [12] presented a bigram-based approach that could be used to expand an abbreviation into a phrase automatically with multiple unigrams. In this method, the abbreviation is firstly divided into segments. Then, the candidates for each segment are generated, where the candidates consist of the sequences of full forms of segment. Finally, the best phrase is chosen from the phrase candidates according to the bigram language model (LM).\n\nSchema matching: Abbreviations, particularly ad hoc abbreviations, are used in schema more frequently because of the limitation of the length of attribute's name. Ratinov and Gudes [33] proposed a method in which a neural network is utilized to judge the relation between two elements (one is an abbreviation, and the other is a full form). Firstly, the relation between the abbreviation and full form is formalized into a sequence containing four operations, and a neural network is then trained to judge the correctness of the operation sequence. In this method, the relation between the abbreviation and full form is judged by the neural network, but the candidate full form must be prepared in advance, which is not the case in all fields. Sorrentino et al. [34] used four resources to evaluate each expansion and selected the expansion with the maximum evaluation value to normalize the abbreviation.\n\nIn addition to the abovementioned areas, some other fields are influenced by abbreviations, such as data integration [35] , speech recognition [36] and string join [37, 38] etc. In these fields, abbreviation dictionaries play an important role in abbreviation operation.\n\nIn order to describe the problem and the proposed scheme clearly, the symbols used in the paper are shown in Table 2 .\n\nExample 1. Naked eDNA, most of it released by cell death, is nearly ubiquitous in the environment.\n\nIn Example 1, the abbreviation Edna cannot be found in many acronym dictionaries. By analyzing the abbreviations, it could be seen that the abbreviation consist of an initial of word ''extracellular'' and a standard abbreviation ''DNA''. Therefore, even for these standard abbreviations, the dictionary could not provide their full forms. Example 2. Fig. 2 shows a part of class structure of NCIt (National Cancer Institute thesaurus). To understand the semantic information of the schema correctly, it is necessary to obtain the semantics of each element in the schema. But in this schema, the designer gives some abbreviations like ''EGFR-GRB2 (Epidermal Growth Factor Receptor-Growth Factor Receptor Bound Protein 2)'' which does not included in the dictionary. In fact, it is composed of two abbreviations ''EGFR'' and ''GRB2''. Similarity of phrase 1 Example 3. Fig. 3 presents a database schema. To understand the semantic information in a schema correctly, it is necessary to obtain the semantics of each element it contains. However, in this schema, the designer has used some ad hoc abbreviations, such as AnonEncID, FloMeasID, FloMeasName and MeasValue, according to his personal preferences. Obviously, the full forms of these abbreviations would not be included in an abbreviation dictionary, because they are only used by the designer rather than widely accepted.\n\nThe examples provided above demonstrate that abbreviations are always changing. Thus, the dictionary-based method is ineffective, especially for ad hoc abbreviations. After analyzing the characteristics of the abovementioned abbreviations, we found that the problem becomes easy if the correct partition of the abbreviation in question is obtained. For example, if WEBGL is divided into ''WEB'' and ''GL'', the real semantic can be found easily by using a dictionary. Based on this idea, this paper proposes an LM-based automatic abbreviation expansion method to enumerate all possible full forms of abbreviations.\n\nThe concepts related to abbreviation expansion can be described as follows. The automatic abbreviation expansion procedure is shown in Algorithm 1. Algorithm 1 takes S, D, and the LM as inputs. Firstly, the prefix abbreviation is partitioned and the partitions are evaluated and sorted according to certain rules. Then, each partition is restored according to D to obtain all possible candidate expansions. Next, the candidate semantic set is evaluated and filtered according to the LM to obtain the semantic set. Finally, by clustering the semantic set, the clustering semantic set is obtained as the expansion result. The details of this algorithm are provided in Section 4.\n\nBlock Arbitrary continuous characters constitute a block of Partition set: All reasonable partitions of S constitute the partition set of S, denoted as PARTISET (S).\n\nFor any S, the partition set can be obtained by employing Algorithm 2. \n\nHere, for each i, the value of P(LEN(S), i) is as follows:\n\n. . .. . .. . .. . .\n\nThe sum of the formulas above is\n\nAccording to Eq. (7), the number of partitions of S is 2 LEN(S)\u22121 , so the worst-case time complexity of a partition is O(2 LEN(S)\u22121 ). Usually, this time complexity is known as unsolvable. However, in this situation, length restrictions can be imposed on the abbreviation and partition to obtain the upper limit of the number of partitions.\n\nThe upper partition length limit corresponds to the number of words in the full forms. Intuitively, an upper limit N must exist. Fig. 4 presents the statistical results for the full forms (of which there are about 900,000) on a well-known abbreviation website. 3 Fig. 4 demonstrates that when the number of words exceeds 7, there are fewer corresponding phrases, but when the number of words is greater than or equal to 8, about 99.5% of the full forms can be included. In a practical situation, by setting the upper partition length limit to 8, the complexity of the algorithm can be decreased considerably while negligibly affecting the precision.\n\nLet the upper limit of words be N. Then, the number of partitions is given by\n\nAccording to Eq. (8), when the length of S exceeds a certain threshold, the complexity is decreased from exponential magnitude to polynomial magnitude.\n\nBased on the concept of reasonable partition, if and only if each block is a reasonable block, the partition is a reasonable partition. Therefore, in the segmentation process, the recursion can be pruned when an unreasonable block is encountered. Doing so can reduce the complexity, whose quantitative analysis is presented below.\n\nThere must be a correct partition corresponding to the expansion when segmenting any prefix abbreviation. Except for the blocks of the correct partition, those of the other partitions are distributed randomly across the character sequence space. Before presenting the quantitative analysis of the ratio of reasonable blocks in the random character sequence space, it is necessary to introduce the following concepts.\n\nTrie tree: A complete tree with a degree of 26. The 26 children of each node correspond sequentially to the 26 letters.\n\nWord node: For each node in a trie tree, if the string from the root to the node corresponds to a word in the dictionary, the word is called a word node.\n\nPrefix node: All of the nodes in the path from the root to the word node are called prefix nodes.\n\nPrefix number: For each node, the total number of word nodes in its subtree is denoted as the prefix number of the node.\n\nNon-prefix node: A node is a non-prefix node when its prefix number is zero.\n\nWhen dividing a character sequence, all of the blocks in the partition besides the correct one are randomly distributed in the dictionary tree. Assuming that the length of a random block is 5, the corresponding prefix number is 26 5 = 11, 881, 376. However, the number of the words in the dictionary is just 1,193,517, and the number of reasonable blocks with length 5 must be less than this value 4 . Consequently, only 10% of all prefixes with length 5 are reasonable. The actual statistics for the dictionary are shown in Fig. 5 .\n\nAccording to Fig. 5 , the ratio of reasonable prefixes is about 14% in a random letter sequence when the prefix length is 4 3 www.abbreviations.com. 4 The dictionary is generated from a 14 Gb corpus text from twitter, the number of the words is 552345. and 1% when the length is 5. Most of the random blocks with lengths of more than 5 are not reasonable. To analyze the number of reasonable partitions, we set the prefix length to 4 (it was assumed that a prefix was unreasonable when its length was more than 4; otherwise, it was considered to be reasonable). The number of reasonable partitions in this case is shown in Fig. 6 .\n\nIn Fig. 6 , the number of reasonable partitions increases slowly with increasing abbreviation length. When the length is 8, the number of reasonable partitions is less than 100. The number of reasonable partitions reaches its maximum of 9,867 when the length is 19, then drops rapidly with further increasing L. For abbreviations with lengths greater than 32, there is only one reasonable partition, the correct one.\n\nAccording to the analysis presented above, after pruning in advance based on the reasonability of blocks, the complexity is greatly reduced from the original polynomial magnitude to a constant magnitude, which is acceptable in practical situations.\n\nFor a given abbreviation, all reasonable partitions can be obtained by applying the algorithm in Section 4.1. When a user omits some characters in a phrase to form an abbreviation, some rules are always followed, so the partitions can be evaluated according to the rules. The next three rules are selected as the evaluation criterion for partitions.\n\nAccordingly, in the LMAAE method, Eq. (9) is utilized to calculate the rationality of each reasonable partition except the initial partition, denoted as Rationality(\u2202). The rationality of initial partition is set to a fixed value. \n\ncould be calculated using Eq. (10)\n\nAccording to Eq. (9), the rationality of each reasonable partition can be calculated. Then, the Max-N strategy is used to filter some partitions. A partition that is not filtered is an effective partition, and all of the effective partitions form an effective partition set.\n\nFor the effective partitions selected by using the Max-N strategy, it would be simple to enumerate all possible expansions by enumerating all possible expansions for each block and then determining their Cartesian product. However, the result is too large to handle. This section presents an analysis of the complexity of the expansion and suggests an optimized strategy based on statistical LMs. Theoretical analysis of the strategy is provided, demonstrating that this method can limit the result set to an acceptable range. Related definitions are listed below. , denoted as shown in Eq. (11) . In ParES\n\n, each expansion is a sequence of words denoted as WS = (w \u2202 1 \n\nFor each reasonable block \u2202 i j , LEN(PreES(\u2202 i j )) is the prefix number of the corresponding node of \u2202 i j in the trie tree. The average number of prefixes with length n can be estimated based on the statistics of the average number of nodes on the n-th floor of the trie tree. The real data are shown in Table 4 . By analyzing the problem and considering the data in Table 4, the length of PreES(\u2202 i j ) could be replaced by the average value\n\n. If \u2202 j is a partition of an abbreviation of length L and LEN\n\n). This value is too large to handle effectively. To solve this problem, this paper suggests a strategy based on statistical LMs.\n\nBased on the example in Table 3 , most of the expansions of S in CandSS(S) are meaningless phrases. A statistical LM [39, 40] has the function of distinguishing meaningless phrases from the candidate expansions. In the LMAAE method, each candidate is evaluated using the LM and the candidates with evaluation values under a threshold are filtered.\n\nCo-occurrence rate: The ratio of the words that can appear adjacent to word w in all word sets, denoted as \u03c1(w), is given by \u03c1(w) = |set of words next to w| |set of all words| .\n\nBecause every word has a different co-occurrence rate, the co-occurrence rate of every word is set equal to the average co-occurrence rate in the dictionary for the convenience of analysis. By employing the co-occurrence rate, LEN(ParES\n\n) can be calculated as follows:\n\nIn Wikipedia data, the average co-occurrence rate is 0.00046.\n\nAssuming \u03c1 = 0.0005, LEN(S) = 10, when the number of blocks M is 5, and the number of effective expansions is about 9 according to Eq. (14) . All of the effective expansions of each effective partition constitute the semantic set of S (denoted as SS(S)).\n\nBecause a root can yield numerous words with the same prefix and similar semantics, there are many phrases with close similar semantics in the set, which would disturb subsequent operations, such as ASD. A clustering method is introduced below to merge phrases with similar semantics.\n\nFor any two phrases (phrase 1 = {w 1 1 , w 2 1 , . . . . . . , w n 1 }, phrase 2 = {w 1 2 , w 2 2 , . . . . . . , w m 2 )}, the semantic similarity is defined as\n\n).\n\nIn Eq. (15), vector ( w i 1 ) denotes the word embedding of w i 1 , and EM is the Euclidean distance between two vectors. Compared with sentences, there are fewer words and the semantics are simpler in phrases. Therefore, this method can provide better results by taking the average value of the word vectors as the vector representation of a phrase.\n\nThe mean-shift clustering algorithm is selected to cluster phrases, as shown in Algorithm 3. In Algorithm 3, the expansion phrases in SS(S) are divided into several categories according to semantic distance. In each class, the phrase with the maximum statistical probability is selected as the semantic representation of that category, which consists of the clustering semantic set (CLUSS(S)) of S. \n\nThe LMAAE method can obtain all of the candidate expansions automatically. It can effectively handle ad hoc abbreviations, which the dictionary-based method cannot. For standard abbreviations, it can supplement the dictionary-based method when abbreviations are missing from the dictionary. To validate the effectiveness of the LMAAE method, the following experiments were conducted.\n\n1. In order to validate the precision of LMAAE method to standard abbreviation, a simulation experiment is conducted on the abbreviation in the dictionary. The result shows that more than 80% of the expansions in the dictionary are listed by LMAAE.\n\n2. In order to validate the precision of LMAAE method to ad hoc abbreviation, a set of ad hoc abbreviation is selected from OAEI 5 (Ontology Alignment Evaluation Initiative). The precision of Top-20 reached 75%. 3. In order to validate the effectiveness of LMAAE to the related fields, two simulation experiments are conduct on schema matching and text normalization. The result shows that LMAAE could increase the precision exceed 5 percent in related fields.\n\nIn some research fields such as ASD, users must choose the most suitable semantics from all of the candidate semantics, so the completeness of the set of candidate semantics is crucial. To validate the completeness of the LMAAE method, the following experiment was performed.\n\nTest data consisting of 1000 abbreviations and all of their full forms were extracted from an online abbreviation dictionary (www.abbreviations.com). Then, the LMAAE method was used to obtain the expansions of the abbreviations. The precision and recall given by Eqs. (16) and (17), respectively, were used to evaluate the results.\n\nPrecision The proportion of the full forms in the abbreviation dictionary that are listed by the automatic algorithm.\n\nRecall: The proportion of the full forms listed by the automatic algorithm that are in the abbreviation dictionary.\n\nIn Eqs. (16) and (17), AUTOFF (Abbr) is the set of all full forms listed by the automatic algorithm, and DICFF (Abbr) is the set of all full forms in the abbreviation dictionary. The experimental results are presented in Fig. 7 . Fig. 7(a) shows the precision of the LMAAE method. The horizontal axis corresponds to the precision of the automatic expansion results, and the vertical axis shows the percentage of abbreviations with a particular precision. The blue and red curves were obtained using the 2-and 3-gram LMs, respectively. For instance, on the 3-gram curve, when the vertical value is 35.3 and the horizontal value is 90, it means that 35.3% of the abbreviations have automatic expansion results with precisions between 90% and 95%. When the 2-gram LM is used, 2.2% of the expansion results include all of the full forms in the dictionary, and 89.4% of the expansion results have precisions greater than 80%. When the 3-gram LM is used, 3.5% of the expansion results include all of the full forms in the dictionary, and 96.6% of the expansion results have precisions greater than 85%. The data in Fig. 6 (a) demonstrate that most of the full forms in the dictionary could be obtained by the automatic algorithm. Fig. 7 (b) depicts the recall of the LMAAE method. The horizontal axis corresponds to the recall of the automatic expansion results, and the vertical axis shows the percentage of the abbreviations with a particular recall. The meanings of the curves are similar to those of the precision curves. When the 2-gram LM is used, only 4.1% of the expansion results have recalls greater than 60%, and most of the results have recalls around 30%. When the 3-gram LM is used, only 8.9% of the expansion results have recalls greater than 60%, and most of the results have recalls around 35%. The data in Fig. 7(b) indicate that most of the expansions generated by the algorithm are not listed in the abbreviation dictionary.\n\nAccording to the data in Fig. 7 (a) and 7(b), the automatic expansion results include most of the full forms in the dictionary but about 65% of the expansion result are not listed in the dictionary( called Non-dictionary expansion). Non-dictionary expansion is not the error, but the correct expansion abbreviated seldom. Because not any dictionary could contain all the abbreviation and its expansion, so even the standard abbreviation, dictionary-based method could not handle all of them. When the corresponding expansion is not contained in the dictionary, the automatic expansion set would give an effective supplement to increase the precision of ASD.\n\nAd hoc abbreviations appear in schemas, knowledge graphs etc., and generally are not included in abbreviation dictionaries. To validate the effectiveness of the LMAAE method at expanding ad hoc abbreviations, 531 ad hoc abbreviations were extracted from the data set used in the OAEI contest, and the correct full forms were generated manually (only 73 full forms could be found in the abbreviation dictionary). The experimental results are summarized in Table 5 .\n\nThrough the data in Table 5 , LMAAE shows good performance for ad hoc abbreviations. For 3-gram model, its TOP1 precision is 42.5%(for 42.5% of the abbreviation generate expansion by LMAAE, the first expansion is correct) and TOP3(the correct expansion is in the first three expansion) precision reaches 60.3%. When the number of candidate is expands to 20, it achieves 76.9% precision. For 2-gram model, the result is slightly low than 3gram model. The data shows that LMAAE could expand ad hoc abbreviation effectively.\n\nThe objective of schema matching is to find the correspondences between the elements of the schema. Abbreviations in the names of the elements in a schema is a significant obstacle in schema matching. In this section, we describe the use of the LMAAE method to handle the abbreviations in the schemas to verify its effectiveness in this application. Three schema matching tasks were conducted, as shown in Table 6 , and three classical schema-matching algorithms (Cupid [41] , COMA++ [42] , and SF (similarity flooding) [43] ) were selected.\n\nIn this experiment, the three algorithms were firstly used to obtain the matching results directly (the abbreviations were handled by a standard abbreviation dictionary such as Word-Net). Then, the LMAAE method was used to preprocess the abbreviations in the schemas. For each abbreviation, the LMAAE method generated its top 10 full forms. When calculating the name similarity between elements, the maximum name similarity was selected as the final value. Standard measurements including precision (defined as the ratio between the number of correctly detected result and the total number of abbreviations), recall(defined as the ratio between the number of results correctly detected by the system and the total number of abbreviations), and F1 score (calculated as 2*precision*recall/(precision+ recall)) were used to evaluate the matching results. The experimental results are presented in Fig. 8 .\n\nThe data in Fig. 8 demonstrate that the LMAAE method can improve the performance effectively. The LMAAE method provides a precision 7% greater than that of the classical algorithm on average. The improvement is different for each matching task: 5% for SM1, 9% for SM2, and 7% for SM3 and the maximum improvement is 84%-95% for SM2 with Cupid. For recall, the LMAAE method exhibits 6% improvement over the classical algorithm on average. The improvement is 4% for SM1, 7% for SM2, and 6% for SM3, and the maximum improvement is 74.7%-82.2% for SM2 with SF. The F1-Score is increased by 6% on average with the LMAAE method compared to the classical algorithm. The improvement 4.5% for SM1, 8% for SM2, and 6.5% for SM3, and the maximum improvement is 81%-90% for SM2 with COMA++. Thus, the precision and overall score are obviously improved by using the LMAAE method before the classical schema matching algorithm. The improvements are especially notable for SM2, which included more abbreviations. The LMAAE method could enhance the schema matching effectively. To highlight the effects for the elements with abbreviations, Table 7 provides a statistical overview of the abbreviation matching results. In Table 7 , the numbers in the cells represent the number of abbreviation matches in each task for different matching methods. For example, the data in the second row means that: there are 5 matching relations in ground-truth with attribute in element's name, Cupid and COMA++ generate 3 of them, SF generate 2 of them. But with the assist of LMAAE, Cupid and COMA++ generate all of them, SF generate 4 of them. Table 7 Statistics of the abbreviation matching results. Task  Ground-truth  Cupid  COMA++  SF  LMAAE-Cupid  LMAAE-COMA++  LMAAE+SF   SM1  5  3  3  2  5  5  4  SM2  40  17  18  17  34  36  32  SM3  85  26  28  23  62 65 60 Fig. 9 . Precision, recall and F1-score of Text Normalization.\n\nAs demonstrated by Table 7 , with the help of the LMAAE method, the classical algorithms could identify more correct matches for the abbreviations. For SM2, only 17 abbreviation matches were identified with WordNet, but with the LMAAE method, 34 relations were identified. For SM3, the precision was improved considerably. Thus, the LMAAE method could solve the abbreviation problem effectively.\n\nFor further verification of the effective of LMAAE, a hot research area text normalization is select in this section. One of the main targets of text normalization is to get the correct expansion of the abbreviation in the text. For this experiment, we leveraged the ShARe (Shared Annotated Resources) corpus, a subset of de-identified discharge summary, electrocardiogram, echocardiogram, and radiology reports from about 30,000 ICU (Intensive Care Unit) patients provided by the MIMIC (Multiparameter Intelligent Monitoring in Intensive Care) [44] . In this experiment, a data set with 80 clinical texts and 3024 abbreviations is selected from the corpus and three recent works (CARD [28] , UTHealthCCB [29] , LIMSI [30] ) are used to normalize the text. The same measurements as Section 5.3 are used to evaluate the result. The experimental results are presented in Fig. 9 .\n\nThe data in Fig. 9 show the result about three original methods and the original method assisted by the LMAAE to handle the abbreviation not included in dictionary(called non-dictionary abbreviation). The method LMAAE-TOP1 means that the LMAAE generate only one expansion for non-dictionary abbreviation. For the other two methods, LMAAE generate 5 or 10 expansions respectively. The result demonstrates that the LMAAE method can improve the overall performance of text normalization. But the improvement is different for each evaluation indicator. For precision, because all three kinds of original methods do not generate expansion for non-dictionary abbreviation, so the precision is decreased when LMAAE generate expansion for nondictionary abbreviation. The maximal decline is generated by CARD method, the precision is decrease from 80% of original to 73% of LMAAE-TOP1. But for recall, LMAAE could increase the performance greatly because the number of standard result is fixed. The maximal rise is generated by UTHealthCCB method , the recall is increased from 61% of original to 72% of LMAAE-TOP10. For the comprehensive indicator F1-score, LMAAE could increase the value for each method especially for UTHealthCCB method from 67% of original to 72% of LMAAE-TOP10. In summary, LMAAE could improve performance of the method for text normalization.\n\nThis paper proposed an LM-based automatic prefix abbreviation reduction method called the LMAAE method. With the widespread use of abbreviations, the dictionary-based method that is currently used to address the abbreviation problem has considerable limitations, especially for ad hoc abbreviations. By analyzing the common rules for abbreviating, it was determined that, compared with reductions, abbreviations conceal word division information (words are linked together) and the complete forms of words (only the prefix is extracted). Therefore, automatic abbreviation reduction can be used to restore latent information. In the proposed automatic reduction technique, the division information is firstly restored by dividing the abbreviations. Secondly, the complete forms are obtained by restoring each block. Finally, according to the LM, the reductions are filtered and clustered to obtain the final results. The time complexity of this algorithm is higher than the dictionary-based method. By analyzing the time complexity, combined with practical problems, an optimized partition algorithm was developed, with the time complexity decreased from exponential to constant. Simultaneously, the time complexity of the reduction algorithm was decreased considerably. The experimental results additionally demonstrated that the proposed method has higher comprehensiveness for general abbreviations. In other words, the reductions can cover most dictionary results. The proposed method also has higher accuracy for ad hoc abbreviations. So it is a good choice to complement for the dictionary-based method."}