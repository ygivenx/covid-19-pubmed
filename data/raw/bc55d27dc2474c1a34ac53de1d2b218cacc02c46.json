{"title": "Loeffler 4.0: Diagnostic Metagenomics", "body": "Abstract A new world of possibilities for \"virus discovery\" was opened up with high-throughput sequencing becoming available in the last decade. While scientifically metagenomic analysis was established before the start of the era of high-throughput sequencing, the availability of the first second-generation sequencers was the kick-off for diagnosticians to use sequencing for the detection of novel pathogens. Today, diagnostic metagenomics is becoming the standard procedure for the detection and genetic characterization of new viruses or novel virus variants. Here, we provide an overview about technical considerations of high-throughput sequencing-based diagnostic metagenomics together with selected examples of \"virus discovery\" for animal diseases or zoonoses and metagenomics for food safety or basic veterinary research.\n\nSince nucleic acids are the general genetic material of all organisms and viruses, shotgun sequencing is the most generic approach for the detection of any pathogen. Hence, it is logical to apply the different high-throughput sequencing technologies for the detection of novel or unexpected viruses. Therefore, already with the advent of the first commercially available second-generation sequencing platforms, diagnosticians and researchers started evaluating and optimizing the detection of pathogens by sequencing. Likewise, researchers interested in exploring the full diversity of the virome started applying shotgun high-throughput sequencing for the analysis of diverse samples to identify novel viruses and virus variants.\n\nExpanding our knowledge of the virosphere is not only scientifically interesting but also important for disease detection and control. As shown later in selected examples, various diseases of unknown etiology are caused by pathogens that were recently detected by novel technologies, namely, by microarrays or high-throughput sequencing. Regardless of the platform or the specific sample selection and preprocessing, an unambiguous association of the detected pathogen with the observed disease requires the fulfillment of Koch's postulates as he formulated them in a talk (Redactions Comit e, 1891) . Due to the problems that may arise in fulfilling these, a number of amendments of Koch's postulates have been suggested (Mokili et al., 2012) . Nevertheless, still the stringent criteria formulated in the late 19th century by Koch and coworkers are the golden standard. In some of the examples provided in a later section of the article and discussed in detail in the following chapters, Koch's postulates were fulfilled, showing that in veterinary science this important proof can be achieved.\n\nAnother advantage of metagenomics for diagnostics is the possibility to detect coinfections. This may be of rising importance as we progress with detecting new agents that may infect the host without causing clinical signs of illness in a certain host without a coinfecting agent but cause serious or varying symptoms when coming together with different other facultative pathogens, as for instance analyzed by Blomstr\u20ac om et al. (2016), Zhang et al. (2014 ), or Hanke et al. (2017 . In case of coinfections, Koch's postulates might be hard to fulfill if not even impossible. Nevertheless, selecting individuals with the same symptoms and from epidemiologically related vs unrelated cases may help gather evidence of the coinfecting agents to be the cause of the observed disease.\n\nThe choice of the samples is of utmost importance for successful virus detection. Even though a systematic assessment of the impact of the sample so far has not been conducted, published data imply this Pfaff et al., 2017a) . Moreover, it is logical that due to differing organ tropism of different pathogens different organs will be positive for pathogen nucleic acids. Noteworthy to mention that also the time point of sampling is crucial. Looking at the studies of Wernike and colleagues (Bilk et al., 2012; Hoffmann et al., 2012; Wernike et al., 2012 Wernike et al., , 2013 dealing with Schmallenberg virus (SBV), it is evident that owing to the short viremia sampling will yield genome copies only in a relatively narrow period and hence allow virus detection. Fig. 1 displays the impact of the pathogen content on detection probability. The plots on the one hand illustrate that at a certain pathogen load it is nearly impossible to fail with detection. On the other hand, Fig. 1 also shows that if the pathogen content is too low, astronomic datasets may be necessary for the detection. However, it is logical that the larger and more complex the dataset gets, the longer the data analysis to find hints at the potential causative agent takes. Although substantial efforts have been put into optimizing sample preprocessing for pathogen detection, the intention of all these is to bias the sequencing (Briese et al., 2015; Conceicao-Neto et al., 2015; Kohl et al., 2015) . However, this bias naturally comes together with information loss due to the selection of only a fraction of the nucleic acids, and this loss can render the analysis useless if the necessary information is lost.\n\nAs a consequence, the applied preprocessing procedures often sacrifice the generic character of the shotgun sequencing approach. Instead, selection of a more suitable sample can solve the problem of the detection limits. As shown in Fig. 2 for the variegated squirrel bornavirus 1 (VSBV-1), the probability of detection of a certain number of viral reads in different samples from the same individual greatly varies. This variation clearly is a result of the organ tropism of the virus in quest. Therefore, analyzing a variety of sample materials might be necessary for success. As also shown in Fig. 2 , samples from different animals infected with the same viral species may have a greatly varying pathogen content, as depicted for a pair of rabies virus (RABV) infected animals (data taken from Hanke et al., 2016) . This is to a lesser extent also true for a pair of samples from cases of ovine astrovirus (OvAstV) infected animals (data taken from Pfaff et al., 2017a) .\n\nIn some cases, pooling of samples might appear useful to reduce the expenses per sample. However, calculating the probabilities for detection of VSBV-1 in pooled samples shows (Fig. 2 ) that the sequencing effort increases, at least Theoretical probabilities for pathogen detection. The graphs display the probabilities (plotted on the ordinate) to detect at least one read (plotted black), at least 10 reads (magenta), at least 100 reads (blue), or at least 1000 reads (red) representing a pathogen present in a sample with a certain pathogen: host ratio (plotted as the title of each graph) in a dataset with a certain size (plotted on the abscissa). when negative samples or samples with a low virus content are part of the pool. As shown, eventually the expenses can be reduced without significantly reducing the probability of detection if a sample with sufficiently high pathogen content is part of the sample pool. However, when working with pooled samples and aiming for complete genomes, individual positive samples have to be identified after detection of the virus, and individual libraries and sequencing have to be conducted afterward.\n\nAnother issue that has to be considered carefully is the choice of the nucleic acids to sequence, DNA or RNA. Possible criteria to take into account for this decision are at first of course the expected type of virus, i.e., DNA or RNA virus. Next, if dealing with samples where virus replication can be expected, preferably RNA should be chosen since this will enable the detection of both DNA and RNA viruses. On the contrary, choosing DNA will make the detection of RNA viruses fundamentally impossible. In case of samples in which no viral replication can be expected, for instance feces or environmental samples, DNA and RNA must be analyzed and should preferably be processed separately (also see Section 2.2).\n\nDespite the fact that some HTS platforms are no longer available, there is still a considerable range of technologies and instruments from which researchers and diagnosticians can select. All of these have their specific pros and cons and all proved to be more or less suitable for pathogen detection. We will not evaluate the different platforms here nor will we argue for the use of a specific platform, rather we will only briefly discuss a number of characteristics to consider for platform selection. Inter alia, characteristics influencing the choice of the most suitable sequencing platform are the read length, which is a major determinant of the reliable taxonomic classification, the runtime to complete an analysis, the possible size of the datasets, or the overall cost for an analysis. Fig. 3 and Table 1 show an example for the impact of read length on the sensitivity of detection. The original dataset (from Hoffmann et al., 2012) and the same dataset with shortened reads were analyzed using the software pipeline RIEMS (Scheuch et al., 2015) with identical settings and databases. It is clearly visible that longer reads are more readily classified than shorter ones. On the contrary, to a certain extent errors can be tolerated and do not ) and those reads for which no sequence with significant identity could be detected (red) using an early version of the RIEMS analysis pipeline (Scheuch et al., 2015) . Data were analyzed using an early version of the RIEMS analysis pipeline (Scheuch et al., 2015) with identical settings and databases.\n\ninfluence a sensitive reliable classification (Scheuch et al., 2015) since in case of diagnostic metagenomics not exact genotyping but classification at higher taxonomic ranks is the aim. This tolerance depends on the software workflow in use (Scheuch et al., 2015) . Of course, if the error rate rises too high, no sufficient sequence identity will be detectable and hence no classification possible. Finally, it has to be concluded that there is a tradeoff between read length and error rates that can vary depending on the availability of reference sequences with sufficient similarity enabling recognition of the pathogen. Of utmost importance are the potential inherent sources of contamination like carryover between runs or missorting of/mislabeling with molecular barcodes due to impurity of the barcoded adapters (Sigma Aldrich, 2017) used for deconvolution of the datasets. Such missorting is the likely reason for errors seen in the INSDC databases. For instance, Mukherjee et al. (2015) reported on integration of the PhiX phage into bacterial genomes of diverse families, even those that had never before been associated with PhiX. The authors report that 10% of the contaminated genomes had even been published in literature. By thorough analysis, they found out that the integration of the phage genome into the bacterial genomes was by missorting of the raw data, which included the used sequencing control sample into all datasets. Fig. 4 depicts another example of probable missorting and its result in the generated sequence. In this case, the Influenza A neuraminidase gene was assembled into a genome sequence of a bacterium taxonomically classified as Bacillus sp. In the respective genome assembly, also the Influenza A virus Hemagglutinin gene was included (not shown).\n\nThese two examples of the incorporation of foreign sequences are most likely also a result of improper data deconvolution. Moreover, these examples show that care must be taken when analyzing datasets, since such errors will always occur and need manual inspection and correction of the database content by trained data curators.\n\nAfter sample selection, processing, and sequencing, with data analysis another important part of the detection process starts. Since the huge datasets that are generated by shotgun metagenomics may comprise mostly host sequences masking the relevant information, sensitive and specific workflows for classification of the obtained reads are urgently needed.\n\nA major pitfall of data analysis is the fact that if dealing with a novel pathogen, no suitable reference sequence allowing its easy recognition may be available. Hence, although generally seen as a method that works without a priori knowledge because the starting point of the complete procedure can be true shotgun sequencing, regardless of the chosen type of nucleic acid, DNA or RNA, metagenomics heavily relies on a priori knowledge. Until we overcome the necessity that initial identification of the pathogen is based on similarity of the sequencing reads with known sequences available in the INSDC databases, we will not have a system that is independent of a priori knowledge and hence truly unbiased. This stresses the need for algorithms that can determine the source of a sequence with sufficient reliability solely based on low-level characteristics without calculating sequence similarities. On the one hand, novel optimized procedures like for instance Kraken (Wood and Salzberg, 2014) or Diamond (Buchfink et al., 2015) have the potential to speed up the classification. On the other hand, if it is possible to determine characteristics for classes of genomes (classified by higher orders like for instance a superkingdom), e.g., composition rather than sequencebased characteristics, this may help increase the sensitivity of detection. Fig. 4 Example for a result of improper data deconvolution and review. An Influenza A virus neuraminidase gene included in the whole-genome shotgun assembly of a Bacillus sp. An Influenza A H5N8 neuraminidase amino acid sequence was searched using blastp (Camacho et al., 2009) with default settings in the NCBI nr database.\n\nApproaches for classification of sequences that do not rely on calculation of alignments but are based on oligonucleotide frequencies have been proposed and also shown to be suitable for classification (A. Belka et al., unpublished; Diaz et al., 2009; Gregor et al., 2016; Leung et al., 2011; McHardy et al., 2007) . The mentioned approaches let it seem likely that it will in the near future be possible to improve the detection of novel viruses without a suitable known reference. Furthermore, the increasing number of sequencing projects results in a fast growing number of sequences from novel virus species, genera, or even families. This new data sets continuously improve the analysis and detection rate for new pathogens.\n\nAnother important issue that is frequently stressed (Byrd et al., 2014; Wood and Salzberg, 2014 ) is the quality of the sequences used for building the reference datasets for classification. As pointed out in Section 2.4, this is at the time of writing a problem. This problem may even be increasing due to the increasing democratization of sequencing and the consequent increase of sequencing efforts. Therefore, it is urgently necessary to have (more) trained personnel for database curation to ensure the quality and reliability of the nucleotide sequence database content.\n\nEven after some years of continuous use, it is still not reliably possible to establish the sensitivity and specificity of metagenomic pathogen detection. With regard to the specificity, it is noteworthy to mention that with a truly unbiased sample preparation and sequencing protocol, only the final step of the identification procedure, i.e., the algorithms and databases used for taxonomic sequence classification, determines the specificity. On the contrary, all protocols including sample preprocessing will strongly influence the specificity of the overall process. All these procedures in the worst case will render detection of pathogens that are not in the focus of the preprocessing procedure nearly impossible. Nevertheless, due to common and divergent characteristics of groups of pathogens, the specificity of the preprocessing protocols cannot be assessed.\n\nAt the same time, sample processing and data analysis have a strong impact on the sensitivity. As an example, searching for an unknown pathogen but only using a viral sequence database for data analysis will cause the sensitivity for the detection of bacterial pathogens drop to zero, and vice versa. The same applies for novel pathogens for which no suitable reference sequence is available. In these cases, the sensitivity may also drop to zero when no suitable alternative criteria can be applied to determine the pathogen. Likewise, even in case of the combination of unbiased sequencing with unbiased data analysis, the algorithm at least in part determines the sensitivity (see for instance (Scheuch et al., 2015) for a comparison of the sensitivities of different software). In summary, a general assessment of the sensitivity and specificity of high-throughput sequencing-based pathogen detection is not possible.\n\nTaken together, in approximately 10 years of high-throughput sequencing-based diagnostic metagenomics, mostly driven by secondgeneration sequencing, a substantial number of viruses has been detected and genomically characterized, thereby proving the suitability of sequencing approaches for diagnostics.\n\nIn former times, the discovery of new viruses or novel virus variants happened mainly as the consequence of isolation on cell culture, embryonated chicken eggs, or in animal models. Virus growth was detected due to disease development, cytopathic effects, and the use of broad diagnostics like electron microscopy or antisera for neutralization or staining. Detection of completely new viruses was often by accident only, depending, e.g., on the possibility to grow the virus. Today, \"virus discovery\" is driven by the new developments in molecular diagnostics, mainly broad PCRs, microarrays, and new sequencing technologies. Especially for infectious animal diseases or zoonoses, and in the field of virus reservoirs, like viruses transmitted from bats or voles, tremendous progress in the detection and characterization of new viruses was achieved.\n\nA growing number of examples exists now where viruses were detected in samples from diseased animals or in animal reservoirs. Table 2 Germany. The impact of SBV was huge since it spread all over Europe within only a few years due to the na\u00efve population of ruminants. Metagenomics allowed the detection and whole-genome characterization of virus from acutely infected animals before the further spread and also before Lateral-shaking inducing neuro degenerative agent (LINDA) Lamp et al. (2017) malformed offspring was born in 2012. Furthermore, the sequence information from sequencing enabled the fast development of specific molecular diagnostics. SBV and the current situation is reviewed in a later chapter of this issue. Further examples include also the detection of diverse astroviruses from various ruminant species, especially sheep and cattle. Most importantly, these cases were all associated with neurological disease. Before, astrovirus-induced encephalitis was only reported for very few human cases. This additional and new knowledge about astroviruses and their impact on differential diagnostics of encephalitis cases is also presented in detail in the chapter \"The expanding field of mammalian astroviruses: opportunities and challenges in clinical virology\" by Boujon et al.\n\nAnother field in veterinary virology where metagenomics provided completely new insights and expanded our knowledge on animal diseases was the first detection of the so-called atypical porcine pestiviruses (APPV). Although initially not associated with clear symptoms and identified during the NGS-based screening of porcine samples in the United States, several groups could link APPV infection with the congenital tremor of newborn piglets. More details on the novel pestiviruses can be found in the chapter \"New leaves in the growing tree of pestiviruses\" by Blome et al.\n\nWith the same NGS-based analyses, the Orbivirus family was substantially expanded by a number of new serotypes of bluetongue viruses. The metagenomics-driven growth of the species BTV is shown in a dedicated chapter summarizing not only the detection but also the further characterization of isolated BTV in vitro and in vivo. A very similar \"growth\" happened in the genus Lyssavirus of the Rhabdoviridae family (see chapter about \"Novel lyssaviruses\" by Eggerbauer et al.).\n\nExamples which are not further elaborated in this issue but which are of importance comprise, e.g., a novel Herpesvirus from Penguins (SpAHV). This report is a good example for the use of novel sequences to complete the phylogeny of a virus family and to provide data for a better understanding of the relationship of similar viruses from different animal species (Pfaff et al., 2017b) . The same is true for all the novel members of the genus Hepacivirus with new complete genomes of viruses detected in dogs, horses, voles, rats, and cattle (Baechlein et al., 2015; Drexler et al., 2013; Kapoor et al., 2011; Quan et al., 2013) . These sequences changed the picture of Hepaciviruses as a virus restricted to only humans into that of a huge virus group present in many animal species including humans. This is one of the most impressive examples how NGS-based metagenomics can change our view on important viruses within years or even a few months. The same is true for pegiviruses (Baechlein et al., 2016; Quan et al., 2013) and influenza A viruses.\n\nEspecially the novel bat influenza viruses H17N10 and H18N11 expanded even the well-studied influenza A viruses (Tong et al., 2012) in a new reservoir host. Bat influenza virus is in addition a perfect example for the de novo generation of a replicating virus by using only the sequence information for reverse genetics and recovery of infectious virus particles without the need of virus isolation from positive sample material (Moreira et al., 2016) . High-quality whole genomes of viruses are therefore in many cases the basis of further studies like gene expression, construction of chimeric viruses, or generation of recombinant clones.\n\nBeside veterinary virology, food safety, which is closely linked to veterinary virology by, for instance, zoonotic pathogens, is another field for which diagnostic metagenomics can improve pathogen detection (Stasiewicz et al., 2015) . Foodborne pathogenic bacteria like Salmonella, Listeria, or toxin-producing Escherichia coli strains, but also norovirus and Hepatitis A virus and parasites (Trichinella, Giardia) can cause disease outbreaks accompanied by vomiting and more or less severe diarrhea. For instance, a severe outbreak of gastroenteritis and the hemolytic-uremic syndrome caused by Shiga toxin-producing E. coli in Germany in 2011 mostly affecting adult women was likely caused be the consumption of sprouts (Frank et al., 2011) . In another case, frozen strawberries caused a norovirus gastroenteritis outbreak in Germany in 2012. Samples from this outbreak analyzed by real-time RT-PCR assay revealed a combination of three different genotypes that had not been reported in Germany so far resulting in the suggestion that the strawberries were polluted from sewage rather than from a single infected food handler (M\u20ac ade et al., 2013) .\n\nGenomes of foodborne pathogens have been often sequenced for genomic characterization of strains of special interest and for comparative genomic studies, but food products were rather seldom handled and evaluated for metagenomics. To systematically evaluate the suitability of shotgun metagenomics for the assessment of the quality of animal-derived foods and foods in general, we conducted a pilot study in which we investigated conventional food samples of animal or plant origin. We analyzed various untreated and highly processed food sample using our in-house metagenomics sample processing workflow. The investigated sample matrices included rocket, mushrooms, ham, salmon, meat loaf, cheese, oat flakes pizza, chocolate, oysters, strawberries, tap water, and parasite-containing wild boar meat. The used workflow starts with RNA instead of DNA extraction. In general, we could obtain RNA from all tested food products. The resulting concentration and especially the quality of the RNA obtained from processed food samples were clearly lower since their RNA content and integrity is inherently very low. Special matrices like fatty (e.g., cheese) and fiber-containing foods (e.g., oat flakes) were challenging and possibly need sample-dependent pretreatments, like defatting. Nucleic acids of samples with a low pH value (e.g., frozen berries suspicious to be contaminated by norovirus) are difficult to isolate and should be pretreated with a neutralization step.\n\nNevertheless, we were able to detect viruses in different data sets resulting from both untreated and processed foods. In a rocket sample, we found a novel yellows virus (0.3% of reads) that was only about 91% identical (at nucleotide level) to yellows viruses known from turnip and brassica. Meat loaf is an example of processed food in which we were able to detect a well-known pepper mild mottle virus (0.1%) that might cause different symptoms like fever or pruritus (Colson et al., 2010) . Interestingly, we discovered a novel mycovirus in untreated (mushrooms) and processed samples (pizza with mushrooms). This virus is only 35% similar (at amino acid level) to known mycoviruses; however, mycovirus sequences found in both mushroom samples are nearly identical to each other. In the examples mentioned here, the detected viruses are RNA viruses, which prove our RNAbased strategy to be very successful in shedding light on the virosphere. In addition, parasite-specific sequences matching the expected pathogens could be detected in wild boar meat containing parasites (trichina 0.02% and liver fluke 0.000002%). Thus, the used protocols seem to be applicable over a wide range of foodborne pathogens and food categories. Routinely applied for foodborne pathogens, resulting metagenomics data can be helpful for surveillance as well as foodborne outbreak investigation and will improve the hazard identification by increased specificity and potentially by a fundamental change in the definition of the hazard being rather a specific virulent strain, subtype, or gene instead of a not well-specified species.\n\nWhile the aforementioned examples illustrate the impact of highthroughput sequencing on veterinary diagnostics and the closely connected field of food safety, there are of course diverse applications beyond. Whereas anthropologists already successfully analyzed ancient samples immediately after the early second-generation sequencers became available (Green et al., 2006 (Green et al., , 2010 Maricic and P\u20ac a\u20ac abo, 2009; Noonan et al., 2006) , researchers only recently started using shotgun high-throughput sequencing to systematically assess the pathogen content of historic samples available in museums or other collections and ancient DNA from archaeologic specimens. Systematic investigation of historic samples will not only help discover the diversity of the virome but will essentially contribute to the elucidation of viral evolution and the potential impact of vaccination on the evolving virome, for instance. At the moment, however, no reports on the analysis of historic animal samples have been published, but the studies of historic DNA samples are focused to human samples. For instance, in a recently published paper (Feldman et al., 2016) , the authors describe the analysis of an ancient Yersinia pestis genome. The DNA was recovered from a 6th-century skeleton, a putative victim of the Justinianic plague, found in a southern German grave. The authors were able to assemble a complete genome with high coverage. Their sequence analysis revealed a number of unique variants and some structural differences compared to the previously available Y. pestis sequences. In another report, Duggan et al. (2016) sequenced a complete variola virus genome from a 17th century child mummy from Lithuania. The authors were able to reconstruct the complete viral genome, which was found to be basal to all strains from the 20th century. The authors concluded that much of variola virus evolution and diversification occurred recently driven by the impact of vaccination. Another study published by Pajer et al. (2017) shed further light on the long-term evolution of variola virus. In their study, they report on the sequencing and analysis of two complete variola virus genomes from historic human specimens from a museum in Prague. Together these two studies provide a new level of insight into long-term virus evolution.\n\nThis impressively demonstrates the power of high-throughput sequencing for the investigation of the evolutionary history of viruses. Another example for the possibilities that high-throughput sequencing opens in this field is the work published by Toppinen et al. (2015) . They analyzed DNA obtained from bones of putative Finnish casualties from World War II for parvovirus B19 DNA. Interestingly, they found only viral genotypes that disappeared from Northern Europe in the 1970s or had never previously been reported to be found there. Moreover, using molecular clock analysis the authors were able to date the most recent common ancestors for all sequenced viruses back to the early 19th century. Noteworthy to mention that in addition to virus analysis, the obtained sequences also enabled tracing the origin of the casualties. The authors were able to show that one of the casualties was most likely of Russian origin. This highlights an additional facet of genome analysis using shotgun high-throughput sequencing. A similar approach was used for in-depth spatio-temporal investigation of arctic rabies viruses and their reservoir hosts in Greenland . The study explored virus dynamics over a period of roughly 10 years. The authors did not find a link between viral genotypes and the reservoir host population. All these studies together demonstrate the utility of shotgun high-throughput sequencing for investigations into virus dynamics and evolution.\n\nFor sensitive, reliable, and in the optimal case comparable results, a certain degree of standardization is necessary. This encompasses the procedures from sample to sequence and the subsequent analyses of the generated datasets for the detection of the viruses. An important determinant of success is the selection of suitable samples and the sampling and sample handling and storage conditions. To this end, awareness must be raised as to the preservation of samples that are intended for metagenomic analysis. A common preservation is formalin fixation followed by paraffin embedding. However, this minimizes the prospects of success for sequencing-based pathogen detection. Moreover, since the complete characterization of viruses and their unambiguous connection with the disease imperatively requires virus isolates, native material must be available. Therefore, it is necessary to preserve samples by other means than fixation and embedding. Possible procedures have to be evaluated for their suitability and subsequently need to be validated on a range of samples. Finally, this will help improve the complete procedure of diagnostic metagenomics.\n\nThe constantly growing number of available viral genome sequences will improve the chances of recognizing new pathogens. Moreover, the ongoing efforts for the implementation of algorithms to identify sequences of viral origin without sequences comparison will enable the identification of viruses even stronger deviating from those known.\n\nDespite all these successes, further improvements and standardization are urgently necessary for routine use of shotgun metagenomics in diagnostics. With the ongoing dissemination of high-throughput sequencing in diagnostic laboratories, the need for quality control will rise. Therefore, in the near future it will be of utmost importance to establish platform-independent quality measures and ring trials for diagnostic laboratories that use highthroughput sequencing for pathogen detection in general and for virus detection in particular. To make high-throughput sequencing-based diagnostics as generic as its foundation is, the necessary procedures must be designed in a way to abolish the necessity of differentiation between viral, bacterial, and parasitic pathogens for reliable detection."}