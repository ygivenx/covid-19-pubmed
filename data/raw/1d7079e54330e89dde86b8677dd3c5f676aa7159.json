{"title": "Outbreak detection model based on danger theory", "body": "The aim of an outbreak detection system is to assist epidemiologists monitor the progress of a disease and raise an alert when there is an impending outbreak. An accurate and fast alarm notification system allows healthcare professionals to set up an early prevention plan before an outbreak spreads to a wider geographical area. If an outbreak is uncontrollable, it may lead to high death tolls and worse still; it kills without warning [1] . For example, the spread of severe acute respiratory syndrome (SARS) [2] , influenza [3, 4] , and the avian virus [5] led to high mortality rates shortly after the first case was detected. Beside those viruses, there are seasonal types of diseases such as malaria, dengue, cholera and many more that can be classified as having the potential to escalate to outbreaks according to the World Health Organization and Centers for Disease Control and Prevention portal. One of the indirect effects of an outbreak is that it has a detrimental economic impact on a country. This is exemplified by the SARS and avian virus outbreaks in 2003, which led to a huge loss of around US$20 million for the tourism industry in most affected Asian countries such China, Hong Kong, Singapore and Vietnam because of an ineffective surveillance system [6] . For this reason, we need to be able to rely on an accurate detection system to monitor for signs of an emerging outbreak. The importance of having such a system has been emphasized in much of the literature, not least because it can help authorities to control the spread of an outbreak and reduce the mortality rate [4, 7] .\n\nAn outbreak is defined as a sudden spread of disease with a much greater number cases reported than expected over time, where the point of initial occurrence is a small area but quickly spreads to a wider geographic area [8] . A health surveillance system can monitor the progress of incidences of a disease over time: daily, weekly, or monthly. If there is an extraordinary pattern, this might indicate that an outbreak has started [9] . A sudden increase in the number of suspected cases from previous day is an indicator of a potential outbreak and it changes are viewed as abnormal characteristics; therefore an outbreak detection system is actually looking for anomalies in health data [10, 11] . In outbreak detection studies, an outbreak is a collective anomaly where it requires more than one case to be detected before it can be labelled as an actual outbreak [12, 13] . For example, if a single patient has been detected as having a certain disease, the alert system will not be activated until the number of similar cases has reached a certain default number.\n\nEach disease has a different default number which is determined by epidemiologists. For instance, in the case of a disease such as dengue just more than a suspected cases difference from previous day will raise an alarm while cholera requires up to 20 registered cases before it can be classified as an outbreak [14] . However, when the spread of an outbreak is slow, the number of cases is not the key indicator and further epidemiological investigation is required [15] . Asthe notification of outbreak is based on collective anomalies that need to be tailored with same locality and time, this makes outbreak detection different from other detection tasks such as intrusion, fraud and fault.\n\nIn recent years, many researchers have worked on various outbreak detection methods. Those methods can be classified into three main approaches: statistical [1, 9, [16] [17] [18] , artificial intelligence [15, [19] [20] [21] [22] [23] [24] [25] , and a combination of these two approaches into hybrid methods [26] [27] [28] . The statistical approach was applied in early outbreak detection models and to date it remains the preferred approach in many health surveillance systems [29, 30] . One of the key issues that needs to be addressed in existing models is how to obtain a high detection rate while at the same time reducing the false alarm rate. However, due to the weakness of the outbreak signal which always behaves under uncertainties, existing systems produce an inconsistent false alarm rate during detection. Here, uncertainty refers to inconsistent outbreak signals, where the outbreak pattern frequently changes and differs between years. This means that a trained model has less capability in terms of robustness particularly when unseen outbreak patterns vary from the trained model. Robustness is also lost when detection algorithms require a sample from both an outbreak and non-outbreak session for model development. However, in practice, it is a challenging task to define a sample for an outbreak period because most of the data available relate only to non-outbreak periods.\n\nPreviously, outbreak detection models were mostly based on the univariate surveillance approach [18, 31] . However, due to the weakness of the outbreak signal, the possibility of these models generating an imbalanced result between detection rate and false alarm rate is high because they rely on a single attribute [28] . As a possible solution to this problem of a weak signal, researchers have been investigating the use of multivariate surveillance by injecting the weak signal with a stronger signal, for instance by combining spatial and temporal data [1, 16, 32] . In addition, efforts have been made to combine multiple syndromic data to boost detection, for instance by combining emergency visit data with weather information, or clinical diagnosis results [33, 34] and by investigating social network status and internet tracking search [22, 35, 36] . Since an outbreak is observed over time, distraction factors such as the seasonal event effect always disturb the detection results [9] . In relation to the data issue, each disease has a different outbreak definition which is determined by its the environment, government policies, outbreak rate and the medium for spread. Information on aspects such as outbreak duration (outbreak period), outbreak magnitude (rate of outbreak cases) and onset date are required and they are different depending on the disease [7] . The above-mentioned factors indirectly influence the effectiveness of an outbreak algorithm. Fig. 1 depicts the general outbreak detection concept. In this conceptual model, an outbreak is defined based on three parameters: similar time, similar place, and huge number of reported cases. The aim of the outbreak detection model is to generate an accurate alarm which has high sensitivity and fast detection time. Since an outbreak is viewed as an abnormal period, anomaly detection analysis is used to detect the point at which data start to abnormally change. The quality of outbreak data and disease characteristic are the two factors that influence detection performance.\n\nTo improve the outbreak detection model's robustness when dealing with uncertain outbreak signals, we sought to identify an appropriate model from various disciplines, but especially from the field of biology, to detect early outbreak signals with good detection results. In this paper, an outbreak detection model based on danger theory is proposed. Danger theory is a bio-inspired method that replicates how the human body fights pathogens and the literature has shown that it can solve problems mainly related to the detection process. While not many works that apply danger theory have addressed the outbreak detection problem, danger theory has been successfully applied in intrusion [37, 38] , fraud [39, 40] , and fault detection problems [41] with good detection performance. Since the capability of danger theory as a good detector is proven in other areas, it motivates us to adapt the artificial immune system (AIS) as an outbreak detection model. The robustness of the immune system mostly lies in the ability of the dendrite cell to sense an early death cell (viewed as an outbreak signal), which can be replicated in an outbreak detection model to reduce the high false alarm rate. Moreover, danger theory offers a multivariate detection approach without relying on a training phase, which can improve a model's robustness.\n\nIn the proposed model, a signal formalization approach based on cumulative sum (CUSUM) is formalized and a cumulative mature antigen contact value (cMCAV) is proposed to suit the outbreak characteristic and danger theory. In experiments, the model is applied to two outbreak datasets; a real-world dengue outbreak and a synthetic SARS outbreak. To evaluate the model, four evaluation criteria are used: detection rate, specificity, false alarm rate, and accuracy. Then, the result is compared with three statistical control chart approaches: the CUSUM, Exponentially-weighted Moving Average (EWMA), and Moving Average (MA). The proposed model is also compared with two multivariate classifiers, Multilayer Perceptron (MLP) and Naive Bayes (NB).\n\nThe remainder of the paper is organized as follows: Section 2 outlines the concept of danger theory, covering the biological background, dendritic cell algorithm, and danger theory as a detector. Section 3 presents the proposed outbreak detection model and Section 4 describes the experiment set-up. Then Section 5 presents the main finding of the study and Section 6 discusses and elaborates on the results. The final section, Section 7, concludes this work.\n\nIn this section, the concept of danger theory is outlined. It covers the biological background of danger theory, the dendritic cell algorithm, and danger theory application as a detector.\n\nDanger theory relies on the idea that the human immune system is triggered by a danger signal produced by a necrotic cell which unexpectedly dies due to a pathogenic infection [42] . When a cell is infected, the distressed cell establishes a danger zone around itself to mitigate and localize the impact of the attack. The key point of danger theory is that the immune system only reacts to the danger signal which will do harm to health. This goes against conventional immunology theory that the immune system is based on discrimination between self cells and non-self cells that leads the immune system responding to the presence of foreign entities and not to the body's own cells.\n\nIn principle, danger theory views all cells in the human body as antigens which have a similar possibility of being infected by harmful antigens. It relies on the function of dendrite cells, a family of cells known as macrophages. As an antigen presenting cell, the dendrite cell controls the activation state of T-cells in the lymph nodes and it has three different states, namely immature state, semi-mature state and mature state. At the beginning of the detection process, the dendrite cells, which are initially born as immature cells in thymus, observe the progress of the body's cells. At this stage, the dendrite cell collects the body cell protein paired with its three signals; pathogen associated molecular patterns (PAMP), danger, and safe signal in cell tissue as shown in Fig. 2 . Based on the collected input throughout its life span, the dendrite cell will evolve from being immature into one of two maturation states: either a semi-mature (apoptotic death) or a mature state (necrotic death). At this phase, the dendrite cell is migrated from cell tissue to lymph node. Reaching a 'mature' state indicates that the cell has experienced more danger signals throughout its life span that have been caused by a foreign antigen, wound, etc. If this happen, it indicates that the harmful antigen has been detected and a danger zone will be released. From a mature state, T-cells are activated to release antibody. While a 'semi-mature' state indicates that apoptotic death has occurred and this is seen as part of normal cell function. The semi mature dendrite cells cannot activate T-Cells and they are tolerized to the presented antigen.\n\nBy integrating the appropriate mechanisms from danger theory, it is envisioned that danger theory will enhance the efficacy of the currently available outbreak detection systems. Compared with other outbreak detection approaches, danger theory provides inspiration for a robust, highly distributed, adaptive, and autonomous detection mechanism for early outbreak notification with better detection results. Since the identification of the cause of cell distress (either apoptosis or necrosis) is the key for antigen detection in the human body, it provides a solution to overcome the uncertainty problem in early outbreak signals. In this context, all outbreak datasets are considered as antigens and all of them have a similar chance of being affected by foreign antigens. Throughout the monitoring period, the maturity state of the dendrite cell is updated; if the outbreak movement becomes abnormally changed, then a danger/alarm zone will be released indicating that harmful antigens exist.\n\nThe characteristics of danger theory are that it is sensitive to any changes occurring inside the body and it can highly discriminate between harmful and normal cells. In line with this, danger theory considers all input data as similar under the assumption that they have a similar chance of being affected by foreign antigens. Therefore when danger theory is applied, the requirement to define input data into either a normal or outbreak class is not required, and moreover, no training phase is required. In a conventional immune system approach, some of the antigens have to be predefined as self cells (the training set) but in practice, the self cells are difficult to define and can change over time. Hence, when changes to these self cells occur during implementation, the error detection rate increases sharply.\n\nThe dendritic cell algorithm (DCA) is derived from the danger theory abstract model Greensmith [43] which is depicted in Fig. 3 . In the DCA, the dendrite cell acts as an agent that is responsible for collecting antigen coupled with its three context signals (PAMP, danger, safe) as input to the system. Within computer context, the antigen in DCA presents each record contains in dataset and the signals present the normalized value of selected attributes. Each dendrite cell accumulates the changes that occur in the monitored system and determines which antigen causes the changes. Using the accumulative function in Eq. (1), all input signals are transformed into three cumulative outputs signals; co-stimulatory molecules (CSM), mature, semi-mature. where W is the weight matrix, IS is the input signal, OS is the output signal, i is the input signal categories, and j is the output signal categories.\n\nAs a population-based algorithm, the dendrite cell samples input signals and antigens multiple times. This is analogous to sampling a series of suspected antigen in human body such that the dendrite cell will hold several antigens until it matures as shown in Fig. 4 . Throughout the sampling process, the experience of each cell is increasing whereby the entire experience is documented in CSM (OS 1 ), mature (OS 2 ), and semi-mature (OS 3 ) as output signals. The sampling process stops when the cell is ready to migrate. This occurs when the OS 1 of that cell reaches the migration threshold and it is then removed from the population for antigen presentation. After migration, the output values of OS 2 , and OS 3 are compared in order to derive a context for the presented item. The antigen is termed mature if OS 2 > OS 3 or semi mature if OS 2 < OS 3 . Then the migrated dendrite cell is replaced with a new cell to restart sampling and return to the population. This process is iterated several times.\n\nWhen learning ends, antigens appear in different contexts. In the last step, the potential anomalous antigen is determined based on the collected context. Termed as the mature context antigen coefficient (MCAV), the anomalous antigen is determined as: where i refers to the antigen type, Ag mi refers to the total number of mature antigens of antigen type i, Ag m is to the total number of mature antigens, and Ag sm refers to the total number of semi-mature antigens. If the value of MCAV i is higher than the anomaly threshold, then the antigen is categorized as an anomalous antigen.\n\nDanger theory has been used in many studies as one of the anomaly detection techniques. It has been applied mostly in computer networks to detect intruders. The early work in this field was initiated by Greensmith et al. [44] , who incorporated the biological danger theory approach into a DCA. This type of algorithm is able to detect network intrusion with better performance compared to negative selection algorithms. Later, a more sensitive version of the DCA was proposed which included new controllable parameters [45] . Following successful trials, danger theory has been applied to address a wide range of computer security network issues such as malicious code detection, misbehaviour in wireless networks, port scanning, spyware, and worm detection [37, 38, [46] [47] [48] [49] [50] [51] [52] [53] [54] . Besides providing accurate detection with a lower false alarm rate, the danger theory framework can be applied as an agent in huge real-time heterogonous networks [55] [56] [57] [58] . In line with this, other recent work demonstrates that the danger theory framework can be implemented in real-time flood detection in a big scalable environment with fast response to pattern changes [54] . Moreover, it has been found that detection sensitivity towards intruders increases when danger theory is hybridized with other detection approaches such a negative selection algorithm [38, 59] .\n\nBeside intrusion detection, a flurry of detection frameworks inspired by danger theory has also been reported in other areas. For example in fraud detection, the danger theory approach is hybridized with a negative selection algorithm to detect fraudulent online video-on-demand transactions [39, 60] as well as fraud in runtime malware for Windows processes [40] . Inspired by dendrite cell function, danger theory has also been implemented in the area of fault detection. For example, Ran, et al. [41] adopt the idea of the DCA to diagnose faults in robots and control systems. In line with their work, Prieto, Nino, & Quintana [61] have proved the ability of danger theory as an efficient detector in control systems. They propose the DTAL, which is a goalkeeper strategy in robot soccer. In addition, danger theory has also been successfully applied to detect faults in task scheduling where each job to be scheduled is considered as an antigen and aligned with the cell signal [62] . Other successful applications for fault diagnosis can be seen in Laurentys, Palhares, & Caminhas [63] and Zhou, Fan, & Zhang [64] . As well as acting as an anomaly detector, danger theory has also performed well in other data mining task such as filtering web documents [65] and image classification [66] . Based on the successful results in intrusion, fraud, and fault detection, danger theory seems to have the potential to detect outbreak signals with good results. Since, to the best of our knowledge, there does not appear to be a recent outbreak detection model based on danger theory, we propose an outbreak detection model based on danger theory in the next section.\n\nThe general outbreak detection process consists of a two-step procedure. First, the baseline period that represents the abnormal pattern is obtained and second, the score of the observed data is calculated such that the abnormal cases are detected when they exceed the maximum baseline [11] . Based on this process, the outbreak detection model based on danger theory is presented in Fig. 5 . It comprises four processes, data gathering, signal formalization, outbreak mining, and outbreak analysis. The danger theory algorithm (DCA) is used as the detection algorithm.\n\nThis step involves gathering data from original sources. Data can be of different types since the DCA supports real-time multivariate mining. As is usual, the data are pre-processed, where the numerical missing value is replaced with a mean value while Mode for categorical attribute.\n\nSignal formalization is the most important step, where data are prepared for the DCA. In this study, data are normalized into an appropriate form to suit outbreak data for the DCA environment. The normalization approach is inspired by the statistical control chart called cumulative sum (CUSUM). Several steps are involved in formalizing the signal: assigning the attribute into an appropriate signal, transforming the data into numerical form, normalizing the input signal, and forming an antigen. Fig. 6 shows the signal formalization steps.\n\nThe first step is to select and assign the attribute to an appropriate signal: PAMP, danger, or safe. To match attribute and signal, an expert judgement is required otherwise it can be taken from the literature. The PAMP signal is assigned when the attribute shows an anomalous situation indicator while the safe signal is assigned when no anomalous indicators are present in the attribute. However, where the attribute may or may not indicate an anomalous situation but the probability of an anomaly is higher under normal circumstances and is labelled as a danger signal [43] . Categorical data need to be transformed into numerical data since the DCA only functions in numerical form. For transformation, the categorical value of an attribute is replaced with the total reported cases in a day. For example, the reported symptoms attribute in the WSARE dataset [67] has four categories which are none, respiratory, nausea, and rash. The registered cases in day X are counted according to the attribute value, as shown in Fig. 7 .\n\nTo make outbreak data suitable for the DCA, a normalization approach based on CUSUM is proposed. CUSUM is a statistical approach primarily used to monitor the planned process in manufacturing operations. This approach monitors the mean of the process and assumes a process remains under control when the cumulative mean is within the control value. The process is considered out of control when a huge shift in movement occurs away from the target value. As recent outbreak activity has a relation with what has happened during the previous day's activities, the cumulative mean shift is taken into consideration for normalization. Since an outbreak is only indicated when there is a sudden increase in cases, the lower side CUSUM is ignored. So, to normalize, the upper side CUSUM is applied, as shown in Eq. (3):\n\nwhere the C + i is the upper cumulative value at i th observation, x i is the process at i th observation, 0 is the initial mean and K is Fig. 7 . Example of total cases for WSARE dataset. the allowance value which is chosen between the target 0 and out of control value 1 . K is expressed by K = (\u0131/2) = | 1 \u2212 0 |/2, where \u0131 is the shift size from standard deviation, . The C + i value accumulates deviation from 0 that is greater than K which is reset to zero on becoming negative. The starting value is C + i = 0. The normalized value of x i is C + i if the cumulative sum value is greater or equal to zero, otherwise the value is 0 as applied in Eq. (4):\n\nThe last step is to form an antigen that functions as a platform to represent the item being analyzed. The antigen is expressed in Eq. (5) as:\n\nAntigen(ID, Antigen ctx ) (5) where ID refers to the outbreak record to be analyzed and Antigen ctx represents the antigen context which is either normal or dangerous. Normally, the monitoring date or registered case ID is chosen as the ID.\n\nIn outbreak mining, the normalized outbreak data are mined using the DCA. It has two tasks; first, to determine the outbreak baseline and second, to mine the outbreak dataset with the DCA.\n\nThe outbreak baseline is the default value that must be reached before the DCA can raise an alarm. To determine this value, a previous outbreak dataset covering several years is taken into consideration for pre-mining. The formula for each disease may be different since each disease has a different outbreak definition. If no such definition is available, suggestions from experts can be used as a guide. This value is compared with the MCAV which is generated from the DCA. Eq. (6) shows the general outbreak baseline calculation used in this study:\n\nwhere O bs is an outbreak baseline, CS otB refers to the total cases classified as outbreak, and CS notB refers to the total cases classified as non-outbreak\n\nIn this phase, the outbreak data are presented to the DCA for detection. The aim is to generate a MCAV for each antigen that represents the final condition of an outbreak. The general process of this phase consists of setting the initial parameters, updating the input signal and antigen, calculating the MCAV, and categorizing the antigen. To categorize the antigen as outbreak or non-outbreak, a cumulative MCAV (cMCAV is proposed, where the average MCAV for antigens of similar date is calculated, as shown in Eq. (7), and the cMCAV i is compared with outbreak baseline O bs in Eq. (8):\n\nStatus(x i ) = +re; cMCAV i > O bs \u2212re; cMCAV i < O bs (8) where cMCAV i is the cumulative MCAV, x i is an antigen at i th observation, +re refers to an anomalous condition or outbreak, and \u2212re refers to normal condition or non-outbreak. The occurrence of an outbreak is notified when cMCAV i > O bs .\n\nOutbreak analysis is under health department control. When DCA raises an alarm, the notification will be verified by a team of health workers. After verification, an immediate prevention plan is arranged. In this stage, the health workers will follow the health surveillance system procedure in managing the outbreak period. A review session will be conducted once the outbreak ends.\n\nThis section explains the experimental data used, how an outbreak is defined and the parameter settings for the DCA. Two outbreak datasets are used to validate the proposed model, dengue outbreak and SARS outbreak data, and are described below: Table 1 . Both datasets are represented in a weekly-based format. A dengue outbreak in Malaysia is defined as the occurrence of more than one case in the same locality where the onset date between cases is less than 14 days [14] . Therefore, a dengue outbreak is considered to have started if there are additional cases in the observed week at least a case compared to the mean of previous two week cases. The allclear status is given when no new cases have been reported for 14 days.\n\nThese data are known as WSARE and have been used in [67] . The dataset consists of 100 sets of artificial SARS outbreak datasets with different outbreak release dates. In this study, WSARE7 has been selected for the experiment. The dataset covers the period from 2002 to 2003, and the 2003 records are used for monitoring. WSARE has 12 attributes which are all categorical. The attribute selected for mining is shown in Table 1 . In contrast to the dengue dataset, for monitoring purposes the WSARE is represented in a daily-based format. The outbreak definition for WSARE is not clearly defined in [67] ; in his work, a SARS outbreak is defined based on the actual date when the virus is released and it is assumed that the virus will remain in the community for several days, while detection is based on how fast the algorithm can sense the first outbreak day after the release date. Therefore, for the purpose of this study, an outbreak starts a day after the release date and remains present for 14 days, i.e. the algorithm will identify the whole 2 weeks as an outbreak period.\n\nA different approach is used for the datasets in selecting suitable attributes and assigning them to the appropriate signal. For the dengue dataset, attribute selection is based on expert suggestions while in WSARE, the selection is based on [28] . Table 1 summarizes the attribute-signal assignment. The attribute assignment to an appropriate signal is important since it will affect the capability of the DCA to discriminate any anomalies accurately. For example, in the WSARE dataset, the number of cases in winter and autumn (seasonal attribute) is assigned to the PAMP signal because flu incidence rates are high during this period . Conversely, in summer and spring the number of cases falls, therefore both of these seasons are assigned to the safe signal. The general guideline to select and assign attribute to appropriate dendrite cell signal is based on the signal behaviour itself as follows:\n\n\u2022 PAMP: Their existence indicates an anomalous situation \u2022 Safe: Their presence indicates no anomalous situation is present \u2022 Danger: Their occurrence may or may not indicate an anomalous situation but the probability of an anomaly is higher than in the normal situation.\n\nFor dengue dataset, the emergency visit attribute is selected to represent the PAMP and safe signal. To assign appropriate value for both signals, the dengue outbreak definition is referred such that the total case variant between current week and average previous 2 weeks is used as basis. The following rule shows the process. \n\nOutbreak threshold Refer to Eq. (6) Table 2 summarizes attribute selection for the DCA and for the other five comparative detection approaches examined in this study.\n\nThe parameter settings for the DCA are presented in Table 3 . The migration threshold is set to 50% of the median value of the input signal, as shown in Eq. (9) in the table.\n\nThis section presents the performance of the outbreak detection model based on danger theory. The model is evaluated based on four performance metrics: detection rate (DR), specificity (SPS), false alarm rate (FAR), and accuracy (ACC). The DR shows the model accurateness to detect an outbreak while the ability of the model to detect a non-outbreak is evaluated by SPS. The FAR measures how much false detection occurs when a non-outbreak case is detected as an outbreak. Lastly, the ACC checks the accuracy of the model in classifying both classes correctly. For comparison, three statistical detection approaches are selected, namely, CUSUM, EWMA, and MA. These approaches represent the univariate detection approach. In addition, MLP and NB are also compared with the proposed model and represent the multivariate detection approach. MLP and NB rely on a trained model developed from a training phase.\n\nThe first evaluation concerns the detection of dengue outbreaks. For the purpose of monitoring, the dataset from 2006-2009 (209 weeks) are presented to the DCA. The dataset is mined 50 times and the average is calculated for further analysis. Fig. 8 summarizes the dengue outbreak detection result in terms of DR, SPS, and FAR using DCA, CUSUM, EWMA, MA, MLP, and NB.\n\nBased on Fig. 6 , DCA ranks first and has the highest DR with a score of 0.9891. This indicates that the DCA has high ability to detect a true outbreak week as an outbreak. The DCA is closely followed by CUSUM with a score that is 0.0147 lower than that of the DCA. Although the DRs of both approaches do not differ much, the DCA performed better than CUSUM as the former consistently detected non-outbreak cases as non-outbreaks. This can be seen from the higher SPS score (0.8217) and lower FAR score (0.1783) for the DCA. In contrast, CUSUM failed to classify many non-outbreak cases as non-outbreaks with a high FAR (0.9847) recorded. When we compare the results of the MLP with the other methods, MLP seems to be the most accurate detector as it is able to detect nonoutbreak weeks as non-outbreaks with the highest SPS (0.9924) and lowest FAR (0.0076). However, MLP is less performed in detecting true outbreak weeks (DR is only 0.3256). The performance of NB seems comparable with MLP but with a higher DR and lower SPS. Meanwhile MA also shows similar capability as MLP but its ability to detect true outbreak weeks is the worst among others approaches; a score of only 0.2692 for DR is recorded. In the case of the EWMA's detection performance, an average score between DR, SPS, and FAR are recorded. In terms of ACC, DCA is the most accurate model with a result of 0.89; it performs better than all the other approaches tested, as shown in Fig. 9 .\n\nIn every experiment, DCA generates a consistent result for each performance metrics that shows it robust capability towards different dengue outbreak pattern. This can be seen in Table 4 where DCA has the lowest standard deviation in comparison to other approaches. Higher DR and ACC are recorded in other approaches.\n\nIn general, the stability between DR (high) and FAR (low) reveals that the DCA is the most accurate detection approach among the other approaches for this dataset. The information in Table 5 depicts the average true positive (TP), true negative (TN), false negative (FN), and false positive (FP) of the DCA after 50 experiments. The total number of outbreak weeks and non-outbreak weeks is 78 and 131, respectively. Fig. 10 presents the outbreak detection specifically from week 150 to week 209. The dengue outbreak threshold is set to 0.34. \n\nThe second evaluation concerns the detection of SARS outbreaks. For this part of the experiment, the WSARE7 is presented to the DCA and it has to monitor 365 days (2003). Among those 365 days, there are 14 days which are labelled as outbreak days. Fig. 11 summarizes the SARS outbreak detection result in terms of DR, SPS, and FAR using DCA, CUSUM, EWMA, MA, MLP, and NB.\n\nBased on Fig. 9 , three approaches score a high DR, which are DCA, CUSUM, and EWMA. CUSUM has the best DR result where it nearly detects all 14 outbreak days as outbreaks (0.9900). It is closely followed by DCA (0.9569) while EWMA ranks in third place (0.9286). The DR for MA and NB are not as good as in the other approaches scoring only 0.5714 and 0.5000, respectively. Nevertheless, they did show better performance in detecting non-outbreak days where they generated the least FAR (MA 0.0171; NB 0.0228) and higher SPS (MA 0.9829; NB 0.9772) compared to the other approaches. In the case of MLP, it has a capability similar to MA and NB in terms of SPS and FAR; however, it failed to detect true outbreak weeks as outbreaks and had the worst DR score (0.2857). A good detection model needs to have balance between a high DR, high SNS, and low FAR, but the results for MA, NB, and MLP fail to meet this condition. In line with this proviso, DCA is found to be the best model for this dataset, generating a lowest FAR (0.0036) and highest SNS (0.9964) while at the same time having a good DR (0.9569). Comparable to DCA, the EWMA also has a similar capability for discriminating outbreak and non-outbreak days when generating a consistently high SNS (0.9459), low FAR (0.0541) and high DR (0.9286).\n\nThe model performance in terms of ACC shows that most models generate high classification accuracy with a result of between 0.94 and 0.98. The DCA recorded a highest level of ACC (0.98) which is not significantly different from that of other approaches. However, CUSUM scored the worst ACC with 0.56. The ACC of each approach is shown in Fig. 12 The information in Table 6 compares the standard deviation between DCA and other approaches. In comparison to dengue outbreak detection, most models have smaller standard deviation mainly for SPS, FAR, and ACC. DCA remains constant as the most consistent approach when produces the lowest standard deviation in every performance metrics. Table 7 depicts the average TP, TN, FN, and FP of the DCA after 50 experiments. Fourteen days are classified as outbreak days while 351 are non-outbreak days. Fig. 13 presents the outbreak detection from day 300 to day 365. The SARS outbreak threshold is set to 0.055.\n\nFor a model to be a good detection model, it must have the ability to generate a balanced result for DR, FAR, and SPS when detecting anomalies. The above section showed that the proposed outbreak detection model based on danger theory generates a consistent result between high DR, SPS and lower FAR. The preference matrix in Table 8 reveals the ability of danger theory to act as a framework for an outbreak detection model when DCA has the lowest weight score after considering all performance metrics. Through the preference matrix [68] , the accumulative score of each of the performance metrics is given based on the priority of each metric; highest (DR, SPS, ACC) or lowest priority (FAR). A score of 1 is given for the best mining and score 6 for the worst result according to the priority metric. The accumulative weight score ( S1, S2) represents the total scores of all priorities. The lowest score indicates the best model. Based on the last column of Table 8 ( S1 + S2), DCA has the lowest score, which indicates the most accurate approach for both outbreaks.\n\nThere are several benefits that can be derived from the proposed immune system, as summarized in Table 9 . First, danger theory overcomes the inconsistent outbreak signal problem. This can be seen from how danger theory perceives the input data. In danger theory, all data inputs are considered as antigens where they have a similar chance of becoming infected with a harmful pathogen. Based on the generated MCAV that represents the antigen life experience, it determines whether the antigen has died due to infection (outbreak) or not (non-outbreak). Since outbreak and non-outbreak periods are hard to define in real life, danger theory eliminates the assignment of input data to both of these periods during model development. Therefore, there is no need for a training phase and or a pre-developed model in danger theory. This is in contrast to some other detection approaches such as MLP and NB, where the model must first be trained. As a result, the robustness of these models can decrease when the unseen pattern is not the same as that learned during the training phase. Based on the experiments, danger theory can highly discriminate between outbreaks and non-outbreaks with consistent detection results even though no training phase is involved. Moreover, since no training phase is required, the proposed model is able to process real-time input and has high potential for implementation as a real-time system. Second, danger theory is a multivariate detection approach. In a biological immune system, a dendrite cell processes three input signals (PAMP, danger, safe) into three output signals (CSM, mature, semi-mature). During this process, DCA may accept various inputs in term of cell protein and classify them either into PAMP, danger, or safe. As an outbreak signal is often weak and not consistent, the use of multiple input factors may improve detection performance in comparison to those methods that rely on a single predictive factor Table 8 The preference matrix.\n\nDengue SARS S1 + S2 DR  SPS  FAR  ACC  S1  DR  SPS  FAR  ACC  S2   DCA  1  3  3  1  8  2  1  1  1  5  13  CUSUM  2  6  6  6  20  1  6  6  6  19  39  EWMA  4  5  5  4  18  3  5  5  5  18  36  MA  6  4  4  5  19  4  3  3  2  12  31  MLP  5  1  4  2  12  6  2  2  4  14  26  NB  3  2  1  3  9  5  4  4  3  16 25 Table 9 Relationship between outbreak issues and danger theory as well as the benefits of the outbreak detection model based on danger theory. The dendrite cell process uses multiple input signals Accepts multiple input factors to improve detection performance rather than relying on a single predictive factor [28] . As an example, three attributes for a dengue outbreak (average humidity, average temperature and average rainfall) are taken as inputs for the danger signal while the emergency visit statistic is assigned to either a PAMP or safe signal. From this mixture of multiple input factors, the DCA generates a better result detection performance compared to the univariate approach (CUSUM, EWMA, and MA). Moreover, the experimental result for SARS outbreak detection also indicates a similar performance. Hence, the DCA indirectly improves on the CUSUM result. This can be clearly seen from in the results for both outbreak datasets where DCA and CUSUM generated a comparable DR; however, the DCA performs better than CUSUM in terms of FAR and SPS. This indicates that the multivariate approach offered by danger theory can increase outbreak detection performance.\n\nInspired by the success of other detection-based solutions in the literature, this study proposed a novel outbreak detection model based on danger theory. In the proposed model, a signal formalization approach based on CUSUM was formalized and a cumulative mature antigen contact value (cMCAV) was employed to suit the outbreak characteristic and danger theory. Based on experiments performed on two outbreak datasets, dengue and SARS, the proposed model was found to be more robust than other available models when dealing with inconsistent outbreak signals. Although there is no training phase involved, the danger theory-based outbreak detection model is able to handle new unknown outbreak patterns and can highly discriminate outbreak and non-outbreak cases with a consistent high DR, high SNS, and lower FAR. This can be seen in the case of the dengue outbreak dataset, where the proposed model produced significantly better results than CUSUM, EWMA, moving average, multilayer perceptron, and na\u00efve Bayes. Also, in the case of the SARS outbreak dataset, the proposed model produced a performance that was comparable to that of EWMA. Since danger theory is a multivariate approach, it offers a new alternative for outbreak detection that can deal with vague and inconsistent outbreak signals for different types of data. To further evaluate the effectiveness of the proposed model, further analysis will be conducted on different outbreak diseases, where the performance of the proposed model will be investigated when dealing with different outbreak shifts. The proposed outbreak detection model will also be extended as a real-time agent-based model."}