{"title": "A non-contact infection screening system using medical radar and Linux-embedded FPGA: Implementation and preliminary validation", "body": "With the widespread epidemics of influenza [1] , the Middle East Respiratory Syndrome (MERS) [2] , or the Ebola hemorrhagic fever [3] in recent years, it is imperative to diagnose the presence of infection at mass gathering places. Our previous studies were conducted focusing on the noncontact dengue fever detecting systems based on multiple vital signs using medical radar and thermography. For example, [4] [5] [6] [7] developed methods that processed the recorded signal to obtain heart rate (HR) and respiratory rate (RR) by built-in algorithms, namely Autocorrelation, Fast Fourier transform (FFT) and Multiple Signal Classification (MUSIC), on computer software. However, those research studies used built-in algorithms with fixed parameters regardless of the age and sex of subjects, which could reduce the accuracy of the system. Regarding the task of discriminating patients with infectious diseases, [6] employed the Linear Discriminant Analysis method on a small dataset of 18 samples, which achieved an indifferent accuracy level of 88.9%. Moreover, [8] [9] [10] employed sophisticated methods using Neural Networks. However, this approach required high computational complexity to train the Neural Network, partly because it was susceptible to the variability in age and gender.\n\nIn this study, several signal processing and machine learning classification algorithms were employed to improve the performance of both tasks. To measure vital signs, a Digital Butterworth filter [11] and a simple time-domain peak detection algorithm were deployed to process the signal acquired from medical radar. To deal with the variability in HR [12] and RR [13, 14] depending on age and gender, instead of the inflexible methods used in Refs. [4] [5] [6] [7] , filters with flexible cut-off frequencies were designed in this work. For the classification problem, two machine learning models, namely Support Vector Machine (SVM) [15] and Quadratic Discriminant Analysis (QDA) [16] were used, with a far larger dataset of 101 samples, than the 18-sample dataset in Ref. [6] , to increase the classification accuracy. Moreover, those algorithms were implemented on a Field Programmable Gate Array (FPGA), since one of the long range objectives of this study is to enable a low-cost, portable, and standalone system, which would be beneficial for public health centers in underdeveloped areas, where people have little access to healthcare, thus they have higher risks of succumbing to infectious diseases. FPGAs have been widely used for multiple signal processing problems, especially filtering [17, 18] , due to their flexible and reconfigurable structure. Moreover, research focusing on FPGA implementations of machine-learning problems, such as classification, has been thriving in recent years [19] [20] [21] , since the flexibility, parallelism and energy-efficiency of FPGAs are beneficial for accelerating complicated work. In this study, the FPGA called PYNQ-Z1 was employed to implement all signal processing and classification algorithms.\n\nThe PYNQ-Z1 board [22] is designed to be used with the PYNQ opensource framework, which is described in Fig. 1 . The framework enables embedded programmers to deploy All Programmable System-on-Chip (APSoC) platforms without having to use hardware description languages such as Verilog to design combinational or sequential logic circuits. At the hardware level, programmable logic circuits of the FPGA are presented as libraries called overlays. These overlays are analogous to software libraries and can be accessed through an application programming interface (API). On the other hand, regarding the software level, a high-level programming language, Python, with the Jupyter Notebook design environment [23] , as well as the kernels of the Linux operating system, can be deployed for high performance embedded application. The PYNQ-Z1 board combines all of the above elements of both software and hardware levels to simplify and improve APSoC design.\n\nIn this study, the capabilities of the PYNQ-Z1 were exploited to enable a low-cost and portable platform. Specifically, the software level of the board enabled the implementations of all algorithms in Python for both the signal processing and classification tasks. Instead of inflexible built-in algorithms on computer software [4] [5] [6] [7] , flexible digital filters were designed to process the radar signal. The filters were designed with SciPy [24] , a Python library for mathematics, science, and engineering. In addition, SVM and QDA classifiers could also supplant complicated Neural Networks [8] [9] [10] deployed on bulky and expensive computers or workstations. The two classifiers were implemented on the board using scikit-learn [25] , a framework supporting many machine learning algorithms. Fig. 2 provides overall information about the system. The medical radar (NJR 4262J, Japan), which has the frequency range from 24.05 to 24.25 GHz transmitted microwave to the person's chest and received the reflected wave, which contained both the cardiac and respiratory frequency components due to the Doppler effect [6] . The signal was monitored for 30 seconds, then it was transferred to the board for processing.\n\nA digital band-pass filter extracted the cardiac frequency component from the signal, then a peak detection algorithm was used to obtain HR and Standard deviation of heart beat-to-beat interval (SDHI). Similarly, the respiratory component was extracted by a digital low-pass filter, then passed to the peak detection algorithm to compute the RR.\n\nFrom a machine learning perspective, the signal processing stage performs as a feature extraction process [26] , which transforms raw data into a reduced set of features containing relevant information of the original. This process, therefore, reduces the complexity of following classification algorithms, which take the small set of relevant features as input. In this study, each digital signal was processed to generate 3 features, which were HR, SDHI and RR. Then, those 3 features were passed to a classification algorithm. With such a small number of features, machine-learning-based algorithms, such as SVM and QDA can perform well, and they should not be inferior to deep-learning complex structures like Neural Network. Therefore, in this study, the SVM and QDA classifiers were chosen and implemented.\n\nThe performance time of the system was also evaluated, which consisted of all amounts of time taken for running algorithms on the PYNQ-Z1 board. The performance time was measured after the 30-second recording time by the medical radar. Assessing performance time should be vital for practical purposes, since if negligible, the screening result of the subject could be displayed promptly; thus a real-time platform could be enabled.\n\nMachine learning algorithms often require a large dataset for training, depending on the complexity of the problem, the number of classes and input features. The size of each class in the dataset should be similar. In this study, experiments were conducted on the undergraduates at Hanoi University of Science and Technology (HUST), 41 males and 13 females, aged from 18 to 22, who did not have any illnesses over the measurement period, and 54 samples were collected for the healthy class. For the infected group, the 3 vital signs of 47 dengue fever patients, 26 males and 21 females, from 15 to 77 years of age, were measured at the National Hospital of Tropical Diseases (NHTD), Hanoi, Vietnam, from July 31 to August 2, 2017. In total, the dataset has 101 samples of both healthy and infected classes, which is far larger than the 18-samples dataset used in Ref. [6] . This joint research's data acquisition was approved by the Ethics Committee of the NHTD, HUST and the University of Electro-Communications (UEC).\n\nThe system consisted of two main stages: Digital signal processing and data classification. The first stage, including two forms of digital filters, band-pass and low-pass filter, and the peak detection algorithm in time domain, was designed to calculate the 3 vital signs HR, SDHI, RR of a person from the radar signal. \n\nThe signal recorded by the medical radar contained cardiac, respiratory information, and high-frequency noise [6] . The primary purpose of our filtering method was to eliminate unwanted frequency components. The frequency of the heartbeat was greater than that of the respiration [27] and lower than the noise [6] ; therefore, a band-pass filter was applied for calculating the HR. Similarly, a low-pass filter was employed for RR measurement, to remove both the cardiac and noise frequency ranges. The digital filters required a parameter of normalized frequency f n [28] , which was related to the analog frequency f a [Hz] and the sampling rate f s \u00bc 100 [samples/second] in this study by Equation (1) . The unit of the normalized frequency f n is half-cycles per sample (hcps).\n\nA third-order band-pass and a fifth-order low-pass Butterworth filter were implemented on the PYNQ-Z1 board. By comparing the HR and RR results calculated by different filter orders with the results calculated by reference methods, the third and the fifth orders appeared to produce the most reliable HR and RR outcomes, respectively.\n\nThe next step was to detect all peaks in the filtered signal. A lowcomplexity algorithm was designed, which found all local maxima in the filtered signal, whose differences with the two adjacent troughs were higher than a threshold. The elaborate procedure was described in Algorithm 1.\n\nAlgorithm 1. Time-domain peak detection.\n\nA person's heart rate normally ranges between 60 and 100 beats per minute (bmp) [29] , while patients suffering from dengue haemorrhagic fever have higher heart rates [30] . In addition, the heart rate range also varies according to this problem, since there is no parameter which can be optimal for all human cases. In this study, to address with the problem, band-pass filters were designed with different cut-off frequency ranges by sex for four age groups, and the heart rate range was expanded to cover the infected case, as shown in Table 1 .\n\nWith the sampling rate f s \u00bc 100 [cycles/sample], the normalized higher and lower cut-off frequencies of the band-pass filter were computed, as shown in Equations (2) \n\nApplying the peak detection algorithm for the filtered signal, the HR can be obtained by:\n\nIn Equation (4), n peaks is the total number of peaks that were detected, and t m is the measuring time in seconds.\n\nThe standard deviation is a statistical measure that indicates the amount of variation in a dataset. The heart beat-to-beat interval is the time between consecutive heartbeats, which can be measured as the duration between consecutive peaks in the filtered signal. According to Ref. [31] , the SDHI of a normal person should be higher than that of an infected patient, due to the phenomenon of respiratory sinus arrhythmia (RSA) [32] . Therefore, the SDHI should be a beneficial feature to improve the accuracy of diagnosis. Statistically, it is calculated by the following formula: \n\nIn Equation (5), n \u00bc (n peaks -1) is the number of heart beat-to-beat intervals, x i [s] is the ith heart beat-to-beat inter-val, which is the time between two consecutive peaks ith and (i\u00fe1) th , \u03bc[s] is the mean value of all heart beat-to-beat intervals x i in the band-pass filtered signal.\n\nThe respiratory rate range varies significantly according to age, especially from birth to 18 years of age [13] . The range of children and adolescents, from 18 to 30 breaths per minute [BPM], tends to higher than that of adults, which is from 12 to 20 [BPM] [14] . Therefore, the inflexible methods used in Refs. [4] [5] [6] [7] should be highly influenced by this variability. In this study, to measure the subjects' respiratory rate more accurately, a low-pass filter was designed with different cut-off frequencies: 30 [BPM] (0.5 breaths per second) if the subject is aged 18 or more, and 20 [BPM] (0.33 breaths per second) otherwise. Similar to the HR calculation method, the low-pass filter was applied with the normalized cut-off frequencies f Cn1 and f Cn2 obtained by:\n\nAfter the peak detection stage, the RR was computed by Equation (8):\n\nThe frequency responses of the band-pass and low-pass filters, with normalized cut-off frequencies of [0.017, 0.047] and 0.01 [hcps], were illustrated in Fig. 3 , in solid red and dashed blue curves, respectively. The normalized cut-off frequencies of the two filters were showed in thin dotted vertical lines as well.\n\nThe two classification algorithms proposed in this paper were SVM and QDA. This section briefly provides the mathematical concepts of those methods.\n\nThe QDA algorithm [16] assumes that the dataset follows a Gaussian distribution X ~ N\u00f0\u03bc k ; P k \u00de, with each class having different covariance matrices, P 1 and P 1 , as well as different mean values, \u03bc 1 and \u03bc 1 . One of the most fundamental concepts of the QDA is the quadratic discriminant function of each class, which is defined for class k as:\n\nThe aim of the training phase, as described in Algorithm 2, is to compute all covariance matrices P k with mean values \u03bc k , and \u03c0 k from the labelled dataset, so that the quadratic discriminant function \u03b4 k of an unknown point x with respect to class k can be calculated. The term \u03c0 k is the priori probability of class k to the entire dataset.\n\nAlgorithm 2. QDA Training phase Input: Data matrix X \u00f0m\ufffdn\u00de , containing m samples of n-dimensional vectors.\n\nOutput: Covariance matrices and mean values of all classes: \u00f0 P k ;\u03bc k \u00de, k \u00bc 1, 2, \u2026, m 1 Compute the mean of each class:\n\nm k is the number of samples of class k in the dataset.\n\n2 Calculate the priori probability of each class:\n\nCompute the covariance matrix of each class:\n\nIn the testing phase, given the unknown n-dimensional data x, by applying (9) the discriminant functions f\u03b4 1 ; \u03b4 1 g of x were obtained with respect to the two classes. The classification rule assigns the unknown data x to the class that has the greater discriminant functions, based on the following classification rule:\n\nThe idea of the SVM algorithm [15] is to find an optimized classifier that maximizes the geometric margins between the decision boundary and the two classes. All points x on the decision boundary must satisfy Equation (11):\n\nIn (11), \u00f0\u03c9; b\u00de are the parameters obtained through the training phase. The geometric margin \u03b3 i of the decision boundary with respect to a sample \u00f0x i ; y i \u00de in the training set is defined in Equation (12):\n\nThe need to maximize the geometric margin leads to the constrained optimization problem shown in (13) :\n\nThe problem above can be solved by Lagrange duality [33] and sequential minimal optimization problem (SMO) [34] , obtaining the optimal values of \u00f0\u03c9; b\u00de.\n\nThen, at the testing phase, the SVM classifies an unknown sample x based on the hypothesis function:\n\nThe labels 1 and -1 in (14) denote the healthy and infected classes, respectively.\n\nOn the other hand, the training phase produces a set of points which are closest to the decision boundary, called the support vectors. The benefit of those support vectors is that when predicting the label of a new point x, following the property of the Lagrange duality:\n\nAll of the Lagrange multipliers \u03b1 i of all points x i in the dataset are zero except for support vectors. Hence, many terms in Equation (15) are zero, and the computational cost is significantly reduced.\n\nIn machine learning, hyperparameters [35] are parameters which are set before the learning process commences. They are different from the parameters of a model generated after the learning process. Different algorithms require different hyperparameters. Hyperparameter optimization, or hyperparameter tuning [35] , is the process in which optimal hyperparameters are chosen for a learning algorithm, to produce the best model for a specific problem. There are several methods for optimization, such as grid search and random search [36] . Grid search finds the optimal choice by choosing exhaustively from a set of hyperparameters; thus it guarantees that the optimal hyperparameters in the set will be found. However, its drawback is that when the model needs a number of hyperparameters, the optimizing process can be very time-consuming. Random search, on the other hand, searches the given set randomly, ignoring a certain number of values; thus it is much less time-consuming, but there is no guarantee that the result is the optimal combination of hyperparameters.\n\nThe QDA algorithm does not require any hyperparameter, since its training and testing phase can be done sequentially by algebraic operations, while the SVM often has a penalty hyperparameter C of the error term [15] . Because there was only one hyperparameter that needed optimizing, a grid search was employed in this study to find the optimal value of C for the SVM model.\n\nFirstly, to assess the accuracy of our proposed approach in measuring vital signs, reference measurements were conducted simultaneously and independently for comparisons. Specifically, the heart rates of all 101 subjects calculated by our method were compared with the ones obtained from a photoplethysmography (PPG) sensor. The PPG signal were processed by a Heart Rate Analysis Toolkit [37] to measure the reference heart rates. For the respiratory rate, the reference values were measured by manually counting how many times the subject's chest rises during the 30-second monitoring period. Three staff separately did the count to obtain a reliable reference.\n\nOur filtering method, with different orders of the band-pass and lowpass Butterworth filters, namely the 2nd, 3rd, 4th, 5th and 6th, was compared with the reference measures. A Student's t-test [39] was employed to find the most appropriate orders. The test showed whether the means of two samples are different from each other, and how significant the differences are, based on statistical computation. In this study, to evaluate the performance of the signal processing stage, the t-test was applied for the vital signs obtained by our proposed method and the reference methods.\n\nSecondly, to evaluate the accuracy of the two classification measures, the k-fold cross-validation method [40] was employed. The entire dataset was shuffled randomly and split into k similar-in-size disjoint subsets. (k -1) subsets were used for training, and the remaining subset was used for testing, which is also called the hold-out cross-validation set. The model made predictions of all samples in the holdout set, and by comparing those with the ground truth class labels, all evaluation metrics such as training accuracy, precision, recall and f 1 -score [41] were collected. The process is repeated k times, which means each subset is used 1 time as the hold-out set and (k -1) times for the training stage.\n\nThe precision (P) and the recall (R) are defined as:\n\nThe precision score measures the accuracy of a machine learning model in terms of false positive cases, i.e., the number of infected patients predicted to be healthy. The lower the false positive cases, the higher the precision of the model. Meanwhile, the recall score gives information about false negative cases, which is the number of predictions that a person has an infectious disease but is actually healthy. In the context of disease screening, it appears that ignoring infected patients is far more severe than misdiagnosing healthy people; thus the precision makes more sense than the recall in terms of evaluating a model.\n\nThe f 1 -score is an overall statistical measure that combines and balances both the precision and recall values; thus it is the most suitable metric for model evaluation.\n\nIn this study, 4-fold cross validation (k \u00bc 4) was chosen, which means that each subset constitutes 25% of the entire dataset. To make a reliable assessment about the robustness of a classifier, especially when shuffling the dataset before splitting, the 4-fold cross validation was conducted 10 times repeatedly to obtain the average and standard deviation (STD) of evaluation metrics. However, the cross-validation method was not used to measure the training and testing time of the two classifiers, since this method reduced the size of the dataset used for training. Instead, the entire dataset containing the 3 vital signs of 101 objects was used for training, and only one test case was used for testing. . 4 illustrates the results of the digital signal processing stage and how the three vital signs were computed. In the second subplot, through the band-pass filter, all of the high-frequency noise and respiratory information in the original signal were eliminated, to produce the HR from the extracted cardiac frequency component. The third subplot illustrates the waveform of a reference cardiac signal, monitored by the PPG sensor simultaneously with the radar signal. Meanwhile, the fourth subplot shows the low-pass filtered signal, which contains information to calculate the RR. All red dots represent the peaks detected in the two signals after filtering, by applying Algorithm 1. Then using (4), (5) and (8) , with the measuring time t m \u00bc 30s, 32 peaks in the cardiac, and 8 peaks in the respiratory filtered signals, the 3 vital signs were obtained:\n\n. Table 2 and Table 3 show the assessment of the accuracy of our signal processing method, in comparison with the reference methods by the PPG sensor and manual counting for HR and RR, respectively. A t-test was conducted, with the null hypothesis that the two results are different from each other. t-value and p-value are two metrics of the test. The lower the absolute of the t-value, the more similarity between the results of the two methods. Meanwhile, if the p-value, from 0 to 1, was smaller than a threshold, typically 0.05, the null hypothesis that there is no significant difference between the two methods could be rejected. In Table 2 , the 3rd-order filter produced the best performance for heart rate measurement, with t-value \u00bc 0.11 and p-value \u00bc 0.03. Similarly, Table 3 indicates that the 5th, with t-value \u00bc 0.09 and p-value \u00bc 0.03, was the most appropriate order for calculating the respiratory rate.\n\nIn addition, three previous methods to measure vital signs from the digital radar signal, namely Auto-Correlation [4] , FFT [5, 6] and MUSIC [7] , were also evaluated by the t-test with the reference method. Those measures were operated on our dataset of 101 subjects. Tables 2 and 3 show that the performance of our proposed filtering method should be superior to the three methods, since [6] employed those three as rigid built-in algorithms, with fixed parameters, which were more susceptible to the variability of human cases. Table 4 compares the 3 signal processing phases in terms of the average amount of time taken to obtain the three vital signs from a signal recorded in 30 seconds by the medical radar. It is evident that the peakdetecting process accounted for by far the highest amount, approximately 80% of the total. Meanwhile, since the calculation of HR and SDHI require two same steps of band-pass filtering and peak detecting, the gross processing time of both the two features was reduced to approximately 35.87 ms. In total, the signal processing stage on the board took about 70 ms. Fig. 5 visualizes pairs of the 3 vital signs HR, SDHI, and RR calculated from the signal processing stage, of all healthy and infected people included in our research study. It is clear that the HR and RR of ordinary people appeared to be lower than those of the unhealthy, while the converse can be seen in the SDHI.\n\nThe grid search was used for tuning the hyperparameter C of the SVM Fig. 4 . Original, filtered and reference signals. model, ranging from 1 to 100, with the evaluating metrics f 1 -score. The results showed that the value C \u00bc 45 seems to be optimal, with which the SVM model achieved the highest f 1 -score of 97.9%. The solid green parabolas and dashed purple lines represent the 2-D decision boundaries of the QDA and SVM algorithms, respectively. The decision boundary of the linear-kernel SVM [42] is a straight line, while that of the QDA algorithm is a parabolic curve. It can be seen that the two classifiers distinguished the 2 classes quite separately, and subsequent results quantitatively show that both the SVM and the QDA measures achieved high precision, recall, and f 1 -scores. Table 5 compares the performance of the SVM and the QDA models. In the training phase, the SVM model generated higher accuracy than the QDA. In contrast, all the metrics with regard to the testing phase of the QDA were slightly higher.\n\nThe average predicting time of the QDA algorithm for one test case was approximately 1.63 ms, significantly higher than the 1.04 ms value of SVM. These results could be attributed to (9) and (15) , which show that the QDA required more complicated operations with the covariance matrices, such as determinant and inverse, than solely dot product and addition of the SVM. In contrast, the SVM required 403 ms for training the model, nearly 90 times as high as the training time of the QDA. It was probably because the SVM includes an iterative procedure to solve the constrained optimization problem, whereas training the QDA classifier only requires a fixed number of steps for basic matrix operations. In practical purposes, all machine learning models can be trained prior to testing; thus the predicting time appears to outweigh the training time. Therefore, on balanced, the performance of the SVM model appeared to be superior.\n\nIn Tables 2 and 3 , the small absolute t-value and p-value, around 0.10 and 0.03 respectively, show that our proposed methods that used flexible cut-off frequencies of Butterworth filters coped well with the variability in vital signs depending on age and sex. Therefore, the measurements of heart rate and respiratory rate were more accurate. Table 5 shows the negligible difference between the SVM and the QDA models regarding precision, recall, and f 1 -score. Overall, the two classifiers distinguished the healthy and infected classes quite accurately, as they both achieved f 1 -scores of about 98.0%, significantly higher than the accuracy of 88.9% of the Linear Discriminant Analysis in Ref. [6] . Those models were also not inferior to the complicated Neural Networks used in Refs. [8, 10] , which achieved an accuracy of 97.1% and 98.0%. Additionally, low standard deviations of all the metrics indicate that the two classifiers are robust and less susceptible to overfitting [43] . Moreover, the far larger dataset of 101 samples contributed to the robustness of our approach, compared with the dataset of 18 samples in Ref. [6] . Furthermore, mitigating the effects of variability in human cases in the preceding signal processing stage also benefited the performance of the two models.\n\nThe total performance time on the PYNQ-Z1 board was nearly 72 ms, including 70 ms of signal processing and about 2 ms to make a binary prediction, which was negligible compared with the 30s for monitoring the signal via medical radar. On one hand, the longer the measuring time, the better the estimates of vital signs, as both the HR and RR in (4) and (8) are susceptible to how long the radar signal is monitored. On the other hand, 30 seconds is rather long for practical applications. Furthermore, the dataset of 101 subjects with only 3 features per sample, as well as the narrow age range of the healthy group, might be other limitations of our study.\n\nIn this research, we developed the embedded infection screening system, exploiting the capabilities of the PYNQ-Z1 board. Specifically, algorithms for digital signal processing and data classification were implemented. The flexible parameters of Butterworth filters mitigate the effects of the variability of human cases, which improved the accuracy of measuring vital signs. Two machine learning models, SVM and QDA, were also proposed for classification, to increase the sensitivity and specificity of the system. The results show that the proposed algorithms achieved good performance.\n\nThe low-cost embedded system has considerable potential for practical purposes. In future work, we intend to measure additional vital signs such as blood pressure and oxygen saturation and increase the number of people from different age groups involved in our research, as well as reduce the monitoring time of the system, while maintaining the competent and robust performance.\n\nResearch supported by Grant-in-Aid for Scientific Research (B) (grant number 19H02385) funded by the Japanese Ministry of Education, Culture, Sports, Science and Technology. \n\nThis study was approved by the Ethics Committee of the National Hospital of Tropical Diseases (Vietnam), Hanoi University of Science and Technology (Vietnam) and The University of Electro-Communications (Japan).\n\nThe authors declare no conflict of interest."}