{"title": "Bayesian uncertainty quantification for transmissibility of influenza, norovirus and Ebola using information geometry: Supplementary Material", "body": "1 Solution of the linear compartmental model 1 \n\nConsider the continuous-time Markov chain X = (X(t)|t \u2208 R + ) with state space S = {i} m i=1 , m \u2208 N, and rates \u03b3 = (\u03b3 1 , . . . , \u03b3 m\u22121 ), \u03b3 i \u2208 R + , \u2200 i \u2208 S \\ {m}. For convenience we will set \u03b3 m = 0.\n\nThis has the following m \u00d7 m generator matrix\n\nwhich in component form reads\n\nX has an absorbing state at i = m, a unique stationary distribution \u03c0 * such that [\u03c0 * ] i = \u03b4 i,m , but it is not irreducible, ergodic or reversible (since p ij (t) = 0, \u2200 i \u2264 j \u2208 S).\n\nWe now try to solve the system d\u03c0 dt = M\u03c0, \u03c0 i (0) = \u03b4 i1 ,\n\nfor different relationships between the rates \u03b3.\n\nThe solution to (3) is simpler if all of the rates are distinct, i.e. if \u03b3 i = \u03b3 j \u2200 i = j \u2208 S \\ {m}. We will use two different methods to solve this system.\n\nHere we start by finding the eigenvalues of M, we solve the characteristic equation\n\nBy noting that the matrix M \u2212 \u03bb\u00bd is lower triangular, we see that its determinant is the product of its diagonal entries and so \n\nThe eigenvectors equation can then be written in component form as\n\nNow define a matrix A whose i-th column is the i-th right eigenvector of M so that M = A\u039bA \u22121 , where \u039b is a diagonal matrix with i-th diagonal element equal to \u03b3 i . Substituting for M ij as in (2) and rearranging, we have\n\nand so since \u03b3 i\u22121 = 0, A ij = 0 for i < j. If we set A ii = 1, then by induction we have\n\nand so the matrix A is also lower triangular. We then write the solution to (3) as \u03c0(t) = e Mt \u03c0(0) = Ae \u039bt A \u22121 \u03c0(0) = Ae \u039bt c ,\n\nwhere c is a vector obeying Ac = \u03c0(0). Clearly, c 1 = 1 and for i = 1\n\nLet us consider the possible solution\n\nThen for i = 1, (10) becomes\n\nand so\n\nNow we can show that expression (13) holds for all \u03b3 i = \u03b3 j \u2208 R, i = j.\n\nProof. Let there be a set {\u03b3 i } n i=1 s.t. \u03b3 i \u2208 C, n \u2208 N and \u03b3 i = \u03b3 j \u2200 i = j. Now consider the expression\n\nUsing the formula for partial fraction decomposition\n\nCombining this with the eigenvectors/values of M we can substitute into (9) to give\n\nFor the more general case, there are benefits to a Laplace transform approach, which we will introduce here. We define the Laplace transform of a function f :\n\nLaplace transforming (3) then gives s\u03c0 \u2212 \u03c0(0) = Q\u03c0 .\n\nIf s is not an eigenvalue of Q, then the matrix s\u00bd \u2212 Q is invertible and\n\nWe can then calculate the inverse Laplace transform using the residue theorem. Considering the explicit form of (17) for our model, if we let B(s) = s\u00bd \u2212 Q, where s = \u2212\u03b3 i \u2200 i \u2208 S, then B is invertible and we need to find B \u22121 \u03c0(0) where [\u03c0(0)] i = \u03b4 i1 , or in component form\n\nUsing BB \u22121 = \u00bd, and noting that B is lower triangular, we have that\n\nFor i = 1:\n\nand so\n\nThen finding \u03c0 i (t) reduces to calculatingL\n\nall the poles s = \u2212\u03b3 i off are order 1, and so using the residue theorem we hav\u00ea\n\nand so finally\n\nwhich is equivalent to (14).\n\nLets now take derivatives with respect to the model parameters {\u03b3 i }. This can be most easily done in the Laplace domain since we can use the fact that\n\nand so\n\nUsing the convolution theorem for Laplace transforms, we hav\u00ea\n\nwhich can be checked by computing the derivative directly from (14).\n\nConsider a general, pure birth chain with m = N + 1 states {I} N +1 I=1 , and M distinct parameters\n\nLet us find solutions to (3) using the Laplace transform method. Again defining\n\n. . , M }, and following the same procedure in \u00a71.2.2, we can write down\n\nwhere m I \u2208 {1, 2, . . . , M } and k I \u2208 {1, 2, . . . , n m I } are counting variables defined by the relationship\n\nThen to calculate \u03c0 I (t), we notice thatf (s) = (s + \u03b3 m I ) \u2212k I m I \u22121 j=1 (s + \u03b3 j ) \u2212n j has poles at s = \u2212\u03b3 1 , . . . , \u2212\u03b3 m I \u22121 , \u2212\u03b3 m I , of order n = n 1 , . . . , n m I \u22121 , k I respectively, and s\u00f4\n\nNow consider function g(s) such that\n\nand using a simplified form of Fa\u00e1 di Bruno's formula d p ds p e g(s) = e g(s) B p g \u2032 (s), g \u2032\u2032 (s), . . . , g (p) (s) ,\n\nwhere\n\nthen we can use the identity\n\nand so finall\u0177\n\nHere\n\nThis gives the overall solution\n\nwhere we define \u03b3 0 := 1, \u03b3 M +1 := 0, n M +1 := 1, m N +1 := M + 2 and k N +1 := 0 for simplicity of notation.\n\nAn analytic expression also allows us to calculate derivatives in a similar way to \u00a71.2.3. Taking a derivative of the Laplace transform, we get\n\notherwise.\n\nSo we need to calculate expressions of the formL \u22121 {(s+\u03b3 r ) \u22121\u03c0 j (s)}. Rather than use the convolution theorem as before, here we apply the residue theorem multiple times. The first application give\u015d\n\nwhereK I,r (t) is a n r \u00d7 n r matrix with components \n\nThe second application of the residue theorem give\u015d\n\nThis gives the full final expression as\n\n2 Metric for the shedding models\n\nWe first note the straightforwardly-obtained result that if \u03c6 is the pdf of a normal distribution with mean \u00b5(\u03b8) and standard deviation \u03c3(\u03b8), then the Fisher-Rao metric will contain terms like\n\nWe now consider how this metric is calculated for the four different shedding models considered in the main paper.\n\nFor the simple SIR model defined in the main paper (Eq. (14)) we have\n\nThe derivatives are therefore\n\nSubstituting into (41) gives the Fisher-Rao metric as having the following form:\n\nThis has the issue that there is full unidentifiability, reflected in the fact that this metric attributes zero distance to travel along constant-\u03c4 e \u2212\u03b3 curves. Our solution to this problem is to add an amount of distance in these directions to give full metric\n\nwhere \u03b1 is a constant parameterising the amount of distance added.\n\nWhile it was not necessary to do this for any of our other models, we suggest that as a general methodological point if an initial metric took a form similar to (45), but for very small \u03b1 that caused potential numerical issues with forming its inverse, then it may be advisable to increase the value of \u03b1 by hand.\n\nHere our likelihood is given by a product of normal probability density functions\n\nwhere\n\n\u03c0 i (t, \u03b3) =: \u03c4 \u03c0(t) , \u03c3(t) = \u03c3 t is given in data.\n\nGiven the results above for \u03c0 i (t), we can then write down that for influenza the metric has components\n\nFor Ebola, we have to consider both high-and low-viraemic pathways of infection separately, but otherwise the metric is\n\nOur norovirus model has the same form as (46) except that \u03c3(t) = \u03c3 is a model parameter.\n\nSince for these data we have a large number of approximately uniformly-distributed time points, we use integrals rather than sums over time for computational efficiency leading to expressions\n\nwhich can be straightforwardly computed from the results above.\n\nWe can include the contribution of a rate-\u03c1 a exponential prior on \u03b8 a through performing calculations for uniform / improper priors and then making the transformation\n\nFor SMMALA the metrics we have considered so far are all that is required, and have the primary benefit of introducing local second-order derivative information into the MCMC algorithm, but for WLMC we make additional use of the possibilities for reduction of global distances possible in a geometric approach.\n\n3 The WLMC algorithm\n\nThe 'HMC' dynamics introduced in the main paper can be generalised to a geometric approach in two ways. The 'RMHMC' approach of [2] involves Hamiltonian dynamics with a discretized integrator, the generalized leapfrog method [6, 2] , which requires significant numerical effort, in particular fixed-point iterations, to solve implicit equations. This step is potentially computational intensive (repeated matrix inversion of G(\u03b8) involves O(D 2.373 ) operations in dimension D), and can sometimes be numerically unstable [3] .\n\nTo address this issue, Lan et al. [4] propose an explicit integrator for geometric MCMC by using the following Lagrangian dynamics:\n\nwhere v(0) := G(\u03b8(0)) \u22121 p(0) \u223c N (0, G(\u03b8(0)) \u22121 ), and \u0393(\u03b8) are Christoffel Symbol of the second kind whose (i, j, k)-th element is \u0393 k ij = 1 2 g km (\u2202 i g mj + \u2202 j g im \u2212 \u2202 m g ij ) with g km being the (k, m)-th element of G(\u03b8) \u22121 .\n\nThe following explicit integrator can then be derived for these dynamics:\n\nwhere \u2126(\u03b8 (\u2113) , v (\u2113) )) kj := (v (\u2113) ) i \u0393(\u03b8 (\u2113) ) k ij . Such an integrator is time reversible but not volume preserving. The acceptance probability is adjusted to have the detailed balance condition hold [4] :\n\nwhere the Jacobian determinant is\n\nand E(z) is the energy for the Lagrangian dynamics defined as:\n\nThe resulting algorithm, Lagrangian Monte Carlo (LMC) is a valid exact sampler and has the same strength in exploring complex geometry as RHMC does. LMC is sometimes more efficient and stable than RHMC -for more details see [3, 4] .\n\nWhen the target distribution is multi-modal, derivative-based and geometric algorithms tend to fail as they are easily trapped in some of the modes without visiting all of them. Making proposals by numerically simulating Hamiltonian dynamics, the sampler has difficulty in passing through low probability regions [7] . Compared to HMC, the geometric RHMC and LMC methods perform even worse in relation to this issue because they are more adapted to the local geometry and more likely to be trapped in one mode.\n\nTo overcome this issue, some kind of global knowledge of the distribution needs to learned and incorporated. Lan et al. [5] proposed the idea of using wormholes for these geometric algorithms (HMC/RHMC/LMC) to work on multi-modal distributions. The proposed method comes in 2 parts: a distance-shortening metric and a mode-jumping mechanism.\n\nLet\u03b8 1 and\u03b8 2 be two modes of the target distribution. We define a straight line segment, v W :=\u03b8 2 \u2212\u03b8 1 , and refer to a small neighborhood (tube) of the line segment as a wormhole. Next, we define a wormhole metric, G W (\u03b8), in the vicinity of the wormhole. For a pair of tangent vectors u, w at \u03b8, wormhole metric G W is defined as follows\n\nwhere v * W = v W / v W , and 0 < \u03b5 \u226a 1 is a small positive number. To see that G W in fact shortens the distance between\u03b8 1 and\u03b8 2 , consider a simple case of a straight line: \u03b8(t) =\u03b8 1 + v W t, t \u2208 [0, 1]. In this case, the distance under G W is\n\nwhich is much smaller than the Euclidean distance.\n\nNext, we define the overall metric, G, for the whole parameter space of \u03b8 as a weighted sum of the base metric G 0 and the wormhole metric G W ,\n\nwhere m(\u03b8) \u2208 (0, 1) is a mollifying function designed to make the wormhole metric G W influential in the vicinity of the wormhole only.\n\nFor more than two modes, the above method alone could suffer from two potential shortcomings in higher dimensions. First, the effect of wormhole metric could diminish quickly as the sampler leaves one mode and moves towards another mode. Secondly, such a mechanism, which modifies the dynamics in the existing parameter space, could interfere with the native dynamics in the neighborhood of a wormhole, possibly preventing the sampler from properly exploring areas around the modes as well as some low probability regions.\n\nTo address the first issue, we add an external vector field to enforce the movement between modes. More specifically, we define a vector field, f (\u03b8, v), in terms of the position parameter \u03b8 and the velocity vector v = G(\u03b8) \u22121 p as follows:\n\nwith mollifier m(\u03b8) := exp{\u2212V (\u03b8)/(DF )}, where D is the dimension, F > 0 is the influence factor, and V (\u03b8) is a vicinity function indicating the Euclidean distance from the line segment v W ,\n\nAfter adding the vector field, we modify the Hamiltonian/Lagrangian dynamics governing the evolution of \u03b8 as follows:\n\nTo address the second issue, we allow the wormholes to pass through an extra auxiliary dimension to avoid their interference with the existing dynamics in the given parameter space. In particular we introduce an auxiliary variable \u03b8 D+1 \u223c N (0, 1) corresponding to an auxiliary dimension. We us\u1ebd \u03b8 := (\u03b8, \u03b8 D+1 ) to denote the position parameters in the resulting D + 1 dimensional space M D \u00d7 R. \u03b8 D+1 can be viewed as random noise independent of \u03b8 and contributes 1 2 \u03b8 2 D+1 to the total potential energy. Correspondingly, we augment velocity v with one extra dimension, denoted as\u1e7d := (v, v D+1 ). At the end of the sampling, we project\u03b8 to the original parameter space and discard \u03b8 D+1 .\n\nWe refer to M D \u00d7 {\u2212h} as the real world, and call M D \u00d7 {+h} the mirror world. Here, h is half of the distance between the two worlds, and it should be in the same scale as the average distance between the modes. For most of the examples discussed here, we set h = 1. Figure S1 illustrates how the two worlds are connected by networks of wormholes.\n\nOne can refer to [5] for full algorithmic details of Wormhole HMC/LMC, including the case where the modes are initially unknown. Figure S1: Illustrating a wormhole network connecting the real world to the mirror world (h = 1). As an example, the cylinder shows a wormhole connecting mode 5 in the real world to its mirror image. The dashed lines show two sets of wormholes. The red lines shows the wormholes when the sampler is close to mode 1 in the real world, and the magenta lines show the wormholes when the sampler is close to mode 5 in the mirror world.\n\nWe provide the following code sample provides as an example of how closed-form expressions for quantities of interest for the influenza model can be obtained using computer algebra. "}