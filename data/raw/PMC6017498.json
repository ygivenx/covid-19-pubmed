{"title": "RPiRLS: Quantitative Predictions of RNA Interacting with Any Protein of Known Sequence", "body": "The interactions of proteins with other proteins, peptides, DNAs and RNAs govern most the essential molecular function. RNA-protein interactions (RPIs) have a critical influence on post-transcriptional gene regulation [1,2,3], viral assembly [4,5,6], cellular defence [7], protein synthesis [8,9] and various other fundamental biological processes [10,11]. A significant portion of transcripts is long non-coding RNAs (lncRNAs) which are not translated into proteins and are longer than 200 nucleotides [12]. LncRNAs normally function with their interacting proteins [13]. For instance, the lncRNA HOTAIR regulated the HOXD locus in trans by interacting with PcG proteins [14]; several lncRNAs were shown to be able to interact with AUF1, a protein linked to aging and cancer [15]; lncRNAs binding to JARID2 protein were essential for the recruitment of PRC2 to the chromatin [16]; lncRNA GAS5 inhibited hepatitis C virus replication by decoying HCV NS3 protein [17]. Hence, the study of RPIs is essential for understanding their functions. Compared to those of protein-protein interactions and DNA-protein interactions, current knowledge regarding RNA-protein interactions, especially lncRNA-protein interactions is still limited. In this study, we propose a novel machine learning method, which we call RNA-protein interaction prediction based on regularized least squares (RPiRLS), to quantitatively predict the potential RNA-protein interactions.\n\nThe experimental determination of RPIs remains expensive and time-consuming [18,19,20], but fortunately, the accumulated RPI experimental data facilitate the development of computational models for RPI prediction [21,22,23]. In 2011, Pancaldi and B\u00e4hler [24] introduced a computational approach for RBP (RNA binding protein)-mRNA interaction prediction. They employed Support Vector Machines (SVMs) and Random Forests (RFs) based on more than 100 physical and functional features of RPIs, including gene ontology, chromosomal position, gene and protein physical properties, protein localization, experimental translation, mRNA properties, predicted protein structure, UTR properties and genetic interactions. Bellucci et al. [25] proposed a method called catRAPID for the prediction of protein lncRNA interaction. They evaluated the interaction propensities of protein-RNA based on their physicochemical properties, including secondary structure, hydrogen bonding and van der Waals. Muppirala et al. [26] developed a method called RPISeq, which predicted RPIs solely based on primary sequences. The RPISeq method still employed SVMs and RFs but exploited different features. They represented each sequence of proteins and RNAs as the normalized frequencies of the corresponding 3-mer and 4-mer, respectively. In 2013, based on the same feature vectors presented in Muppirala et al., Wang et al. [27] first reduced the dimensionality of feature vectors, and then performed the RPIs prediction by using naive Bayes classifier which assumed the independence of attributes and by using extended naive Bayes classifier which considered the correlation between attributes. Lu et al. [28] integrated the information on the secondary structure, hydrogen bonding propensities and Van der Waals of lncRNAs and proteins with Fisher\u2019s linear discriminant model. In 2015, Suresh et al. [29] developed a method called RPI-Pred to predict RPIs by considering the high-order 3D protein and RNA structure information. In 2016, Pan et al. [30] proposed a new method named IPMiner that integrated deep learning with stacked ensembling to improve the prediction performance of ncRNA-protein interactions.\n\nIn this paper, we classified RNA-protein pairs as interacting or non-interacting by integrating derived kernel with regularized least squares (RLS) [31]. The motivation is to relate the sequence information of proteins and RNAs to their biological functions, i.e., interactions. Our method attempted to extract discriminant subsequence features from amino acid sequences and nucleotide sequences. The derived kernel measures the similarity between two biological sequences by capturing nucleic acid or amino acid compositions and repetitive sequence patterns. We used regularized least squares in learning as the computations performed by RLS algorithms can be expressed using just inner products, hence allowing efficient implementation of kernel-based learning, in addition the RLS algorithms often perform comparable to the best batch classifiers [32]. Since the dimensionality of feature space increases exponentially with the template size, for computational sake, we set upper limit for template size. On the other hand, we categorized 20 amino acids into several groups based on their physiochemical properties [33,34,35], the reduced alphabet representation of the protein sequence allows larger template size and also decreases the dimensionality of feature space.\n\nWe considered the derived kernel with two-layer architecture, hence there were two template sets, denoted as TP and TR needed to be constructed for protein and RNA, respectively. Here we considered all possible substrings of the same length making up a template set. The template set TP for amino acid sequences was composed of substrings with k continuous amino acids, while the template set TR for nucleic acid sequences was composed of substrings with l continuous nucleic acids. In order to extract discriminant subsequence features from amino acid sequences and nucleotide sequences, we explored the effect on RPI prediction over a range of choices for the template sizes of protein and RNA. The training set RPI2662 was used to determine these parameters. In our case, we used different template sizes of protein and RNA chosen from set {1,2,\u2026,4}\u222a{1,2,\u2026,8} for RPiRLS and {1,2,\u2026,6}\u222a{1,2,\u2026,8} for RPiRLS-7G. With different combination of protein template size and RNA template size, the combined kernel K^2dk was integrated with RLS to predict RNA-protein interactions. The ten-fold stratified cross-validation has been verified to be the best algorithm for model selection on a large scale experiments [36], therefore on the data set RPI2662, we tuned the parameter \u03bb by ten-fold stratified cross-validation with the optional parameter set {\u03bb=en, n=\u221215,\u22ef,15}. The data set RPI2662 was divided into ten mutually exclusive folds and the mean response of each fold was approximately equal. In each test we merged 9 parts of the samples as the training set and left the other part as the test set. The parameter \u03bb was chosen by leave-one-out cross-validation on the training set. For RPiRLS, in all the ten sets, \u03bb=e\u22122 uniformly achieved the best performance in the training data. Table 1 and Table 2 showed the performance of the proposed method in terms of AUC and accuracy with different combination of parameters k and l, respectively. The experiment results showed that when the protein template size k=2 and the RNA template size l=5, the model performs best with AUC score of 0.926 and accuracy of 0.830. The other measurements of specificity (SP) and sensitivity (SE) were 0.771 and 0.890, respectively. While for RPiRLS-7G, \u03bb=e\u22123 uniformly performed best in all the ten sets. Table 3 showed that the method achieved the best prediction accuracy of 0.823 when the protein template size k=3 and the RNA template size l=4. The other measurements (AUC, SP and SE) were observed as 0.902, 0.761 and 0.884, respectively. The computational results showed that the RPiRLS classifier outperformed the RPiRLS-7G classifier in terms of various performance measurements, indicating that the diversity of amino acids at a sequence is important for the prediction of RPIs.\n\nIn order to evaluate the reliability and robustness of RPiRLS and RPiRLS-7G, we compared them with other two state-of-the-art methods RPI-Pred and IPMiner. The RPI2241 and RPI369 data sets after removing overlapping RPIs with the training data were evaluated. Both the RPiRLS and RPiRLS-7G classifiers were trained on the RPI2662 data set, and tested on the RPI2241 and RPI369 data sets, respectively.\n\nAs shown in Table 4 and Table 5, RPiRLS outperformed the RPiRLS-7G, RPI-Pred and IPMiner methods on both data sets. For the RPI369 data set as shown in Table 4, the performance of the RPiRLS method was 0.85, 0.92, 0.84 and 0.86 for predictive accuracy, AUC, specificity and sensitivity, respectively. While the predictive accuracy of the RPI-Pred and IPMiner methods were just 0.49 and 0.5, respectively which were much lower than RPiRLS\u2019s. The remaining measurements (specificity and sensitivity) were observed as 0.34 and 0.63, respectively for RPiRLS, and 0.52 and 0.48, respectively for IPMiner. The RPiRLS method outperformed RPI-Pred and IPMiner in terms of accuracy, specificity and sensitivity on the RPI369 data set.\n\nSimilar results were observed on the RPI2241 data set in Table 5. The specificity of the RPI-Pred and IPMiner methods was just 0.38 and 0.20, respectively, indicating there was a positive bias in their predictions of performance. A low specificity increases the labor, cost, and time needed to perform the required experimental tests, but our RPiRLS method achieved both reasonable specificity and sensitivity.\n\nFurthermore, we evaluated the RPiRLS classifier on large-scale RNA-protein pairs in the currently available RPIntDB data base. The RPiRLS method correctly predicted 35980 out of 43010 RPIs, reaching the predictive accuracy of 84%.\n\nTo explore the effectiveness of the proposed method on predicting ncRNA-protein interactions, a large-scale ncRNA-protein interaction data set (we called NRPI13153) was retrieved from the NPInter data base [37]. We trained RPiRLS and RPiRLS-7G on the RPI2662 data set, and tested it on the NRPI13153. Table 6 showed the prediction results compared with the RPI-Pred classifier on the NRPI13153 data set. The IPMiner method showed a significantly positive bias on predicting ncRNA-protein interactions, thus here we didn\u2019t include it into the comparison. The predictive accuracy for different organisms were separately computed. For the six organisms, our method RPiRLS performed best for the Homo sapiens and Saccharomyces cerevisiae, RPI-Pred performed best for Drosophila melanogaster, Escherichia coli and Mus musculus, and both methods obtained the same predictive accuracy for the Caenorhabditis elegans. RPiRLS outperformed RPiRLS-7G over all six organisms. For 13153 ncRNA-protein pairs, the RPiRLS method achieved an accuracy of 91% compared to 76% for RPiRLS-7G and 88% for the RPI-Pred method. We further tested RPiRLS and RPiRLS-7G on the LNRPI12114 data set which was a subset of the NRPI13153 data set and consisted of only lncRNA-protein interactions (lncRPIs). Our model achieved an overall accuracy of 92% compared to 77% for RPiRLS-7G and 89% for the RPI-Pred classifier as shown in Table 7. The predictive performance of RPiRLS was improved in 5 out 6 organisms compared with its performance on the NRPI13153 data set. The results indicated the effectiveness of our method to predict lncRNA-protein interactions only by using primary sequences of proteins and RNAs.\n\nPredicting lncRNA-protein interaction networks is useful to explore the molecular mechanisms that are regulated by lncRNAs [38,39]. In this experiment, we evaluated the performance of RPiRLS on building lncRPI networks and further compared its performance with RPI-Pred. On the basis of the data in NPInter, we analyzed the results of four organisms, i.e., Caenorhabditis elegans, Drosophila melanogaster, Escherichia coli and Saccharomyces cerevisiae, consisting of 4, 61, 78 and 437 lncRPIs, respectively.\n\nFor Caenorhabditis elegans, the RPiRLS method correctly identified all 4 lncRPIs (blue edges) while the RPI-Pred method correctly identified 3 out of 4. As shown in Figure 1, RPI-Pred made incorrect prediction for the pair of n342950-G5EBF5 (red edges). In Figure 2, RPI-Pred correctly predicted all 61 lncRPIs, whereas RPiRLS missed 7 out of 61 lncRPIs for Drosophila melanogaster. These 6 out of 7 incorrect predictions which were observed between two proteins P49963 and Q9VSS2 (yellow rectangle) and three signal recognition particle (SRP) RNAs n5330, n5333 and n389 (green ellipse), formed the SRP RNA-protein interactions. For Escherichia coli, the RPiRLS classifier made much more errors than the RPI-Pred method, with predictive accuracies of 45% vs. 86%. The performance of RPiRLS for Escherichia coli was much poorer than that for the other five organisms. In order to analyze why RPiRLS had relative poor performance on Escherichia coli, we estimated the amino acid composition of Escherichia coli compared with that of the other five organisms. As shown in Figure 3, we found that Escherichia coli had relative higher observing frequencies of Alanine and Valine as well as much lower content of Serine compared with that of the other five organisms. The amino acid composition bias in Escherichia coli probably leaded to poor results. As shown in Figure 4, among 43 incorrect predicted pairs, 39 RPIs corresponded to 7 protein hubs, e.g., P0A6H1, P0AFZ3, P0AG67, P0CE47, P0CE48, P21499 and P77398, each of which appearing as a yellow rectangle node was shown to interact with six transfer-messenger RNAs (tmRNAs), e.g., n3828, n1877, n5000, n435, n329 and N4292 (green ellipse). For Saccharomyces cerevisiae, as showed in Figure 5, among 27 incorrect predicted pairs, 10 RPIs were involved in 3 protein hubs (P57743, Q03338 and Q06819), in which each protein interacted with 4 small nuclear RNAs (snRNAs: n4610, n6134, n4606 and n6136), and other 13 RPIs corresponded to 7 protein hubs (P15646, P47083, P53941, Q04217, Q04500, Q08492 and Q12136), each of which interacted with three small nucleolar RNAs (snoRNAs: n5819, n4618 and n6159). The RPiRLS classifier correctly identified 410 out of 437 RPIs, achieving a high accuracy of 94%, compared of 87% (correctly predicted 379 out of 437 pairs) for RPI-pred. In this work, we illustrated the effectiveness and reliability of RPiRLS in predicting RPIs for eukaryotic organisms in networks which comprised a variety protein hubs and RNA hubs.\n\nMammalian cells contain more than 1000 different proteins interacting with RNA [3]. Normally, any individual RNA can interact with multiple proteins [11,40]. Conversely, most proteins are capable of interacting with multiple RNAs [41]. Given the number of RNAs and RNA-binding proteins, the number of possible RPIs is enormous. High-throughput sequencing methods have accumulated huge amount of RNA-protein binding experimental data and opened new possibilities to measure and understand RNA-protein interactions by computational methods. Most of the previous computational works on RPIs focus on the prediction of RNA-binding proteins or RNA-binding residues in a protein sequence [34,42,43,44]. To our knowledge, very limited works have been developed to predict the specific associations between RNAs and proteins, which play a critical role in post-transcriptional gene regulatory networks. Complex networks of RPIs mediate post-transcriptional gene regulation and therefore prediction of RPIs helps us to gain insight into regulatory networks [45,46].\n\nThe work presented here provided a computational method, called RPiRLS, to classify RNA-protein pairs as interacting or non-interacting by integrating a sequence-based derived kernel with regularized least squares. The derived kernel exploited the contextual information around an amino acid or a nucleic acid as well as the repetitive conserved motif information. Our results demonstrated that only the sequence structures of RNAs and proteins provide sufficient information to accurately predict RNA-protein interactions, especially long non-coding RNA-protein interactions. Specifically, the RPiRLS classifier considered each protein sequence comprising up to 20 diverse amino acids, while the RPiRLS-7G classifier encoded each protein sequence by using the 7-letter reduced alphabets according to amino acid physiochemical properties. The computational results showed that the RPiRLS classifier was superior to the RPiRLS-7G classifier in reliability and effectiveness, indicating that the diversity of amino acids at a sequence has critical impact on the function of RNA-binding proteins. On two non-redundant benchmark data sets extracted from the PRIDB, the RPiRLS method outperformed RPI-Pred and IPMiner in terms of accuracy, specificity and sensitivity. Compared with RPI-Pred and IPMiner, the RPiRLS method obtained a reasonable sensitivity at a lower false positive rate. Further, RPiRLS achieved an accuracy of 92% compared to 89% for RPI-Pred on the prediction of lncRNA-protein interactions. The RPiRLS method can be extended to construct RNA-protein interaction networks and therefore helps us to gain insights into post-transcriptional gene regulation.\n\nThe reason for the good performance of the proposed method may be due to several factors. Firstly, the use of similarity scores is a significant conceptual change in protein/RNA evaluation, quantifying the overall similarity between proteins, RNAs and their interactions. Combining kernels by tensor product for the set of RNA-protein pairs allowing to share information across the RNA-binding proteins considerably improved the prediction, especially in the case of RNAs with few known binding proteins. Secondly, we have found that contiguous k-mer frequencies alone captured rich statistical information on the repetitive conserved motif of RNA-protein pairs and the diversity of amino acids at a sequence has also contributed to an observed improvement contrast to RPI-Pred which just applied 1-letter frequency for both protein and RNA. Finally, a kernel works as a measure of similarity and supports the application of powerful machine learning algorithms such as regularized least squares which we used in this paper. The RLS mehod enables us to efficiently search for an optimized parameter \u03bb at essentially no additional cost [31]. Further, our model was trained on a large data set which contained 2662 RNA-protein pairs, and yielded more robust results. In contrast, IPMiner had much more model parameters to fit as combining deep learning with stacked ensembling, however, was trained on a small data set of just 488 RNA-protein pairs, and thus showed a significantly positive bias on predicting ncRNA-protein interactions. The main disadvantage of the proposed method is that the method is purely data-driven, in the sense that it relies solely on information derived from amino acid sequences and nucleic acid sequences, and thus does not consider higher structural information of protein and RNA. While this may be seen as an advantage, since it can predict any RNA-protein pair of known sequences.\n\nThe increase of the number of protein-RNA complexes in Protein Data Bank [47] has opened possibilities for researchers to develop secondary data bases and to gain valuable insight into the structure and function of these complexes. The Protein-RNA Interface Data base (PRIDB) V2.0 [48] identifies interfacial residues in RNA-protein complexes and also calculates atomic distances between interfacial residues. The RB344 and RB1179 are two precalculated data sets in the PRIDB, which respectively consist of 344 and 1179 RNA-binding protein chains. After combining the RB344 and RB1179 data sets, we obtained a total of 1750 experimentally validated non-redundant RNA-protein pairs, which had at least two atoms respectively coming from RNA and protein with distance no more than 4 \u00c5. Next, we removed redundant RNA-protein pairs, which are the same protein chains interacting with the same RNA chains. Further, we removed those RNA-protein pairs with amino acid sequence length <25 or nucleic acid sequence length <15. Finally, we obtained a positive sample set which consisted of 1331 experimentally validated RNA-protein pairs. So far there are no definite negative samples of RNA\u2013protein interactions that are available. To construct a balanced negative sample set (\u201cRNA-protein non-interacting pairs\u201d), we made it by randomly permute the proteins in the positive sample set but kept the RNA fixed. We repeated the permutation process until no negative pairs existed in the positive sample set. As a result, the training set, called RPI2662, was composed of 1331 RNA-protein interacting pairs and 1331 RNA-protein non-interacting pairs.\n\nSeveral data sets were employed to evaluate the performance of the proposed methods.\n\nOur RPiRLS method was first evaluated using two popular non-redundant data sets of RPIs studied in [27] . The RPI2241 data set consisted of 2241 experimentally validated RNA-protein pairs extracted from the PRIDB data base. While the RPI369 data set eliminated all RPI pairs with ribosomal proteins or ribosomal RNAs from the RPI2241 data set. To avoid overlapping between training and testing data sets, those RPIs overlapping the training data were removed, leaving the RPI2241 data set of 1832 RPIs and the RPI369 data set of 204 RPIs. Their corresponding negative pairs were generated by following the same steps as developing the training negative pairs.\n\nNext, we tested the performance of the RPiRLS method on a large scale data set extracted from the RNA-Protein Interaction Data Base (RPIntDB) (http://pridb.gdcb.iastate.edu/RPISeq/download.php). This data set consisted of 43,010 experimentally validated RPIs from several sources, including the RPIDB, NPInter data base and high-throughput experiments published in literature.\n\nThe fourth data set were extracted from the NPInter data base which we called NRPI13153. The NRPI13153 data set consisted of 13,153 experimentally validated ncRNA-protein pairs from six model organisms, i.e., Caenorhabditis elegans, Drosophila melanogaster, Escherichia coli, Homo sapiens, Mus musculus and Saccharomyces cerevisiae. We constructed the fifth data set called LNRPI12114 by extracting only lncRNA-protein pairs from the NPInter data base. This data set contains 12,114 experimentally validated lncRNA-protein pairs.\n\nThe derived kernel was proposed by Smale et al. [49] on images inspired by neuroscience of visual cortex. In what follows, we briefly described the construction of derived kernel on sequences.\n\nSuppose A is a finite set called the alphabet. In the work here A is the set of 20 amino acids (for RPiRLS), 7 alphabets (for, RPiRLS-7G) or 4 nucleic acids. Let A1=A and define Ak+1=Ak\u00d7A recursively for any k\u2208N. We say s is a string if s\u2208\u222ak=1\u221eAk, and s=(s1,\u2026,sk) is a k-mer (e.g., a sequence of length k) if s\u2208Ak for some k\u2208N with si\u2208A. The process of computing the derived kernel mainly includes three steps as below:Step 1.Set an initial kernel K^1 at the first layer. Here the initial kernel is defined as:\n(1)K^1(x,y)=1,x=y0,otherwise,\nwhere x,y\u2208Ak. x={x1,\u2026,xk} and y={y1,\u2026,yk} are substrings of the same length k. x=y if and only if xi=yi for i=1,\u2026,k.Step 2.Let f=(f1,\u2026,fn), Denote by f the length of f, so here f=n. Then define the second layer neural response of f at t :\n(2)N2(f)(t)=1n\u2212k+1\u2211h\u2208H1K^1(f\u2218h,t), t\u2208T1,\nwhere T1 is the template set at the first layer, here we consider all possible substrings of length k making up the template set T1, so here T1=Ak. H1 is the transformation set at the first layer. \u2200h\u2208H1, h:(1,\u2026,k)\u2192(i,\u2026,i+k\u22121), for 1\u2264i\u2264n\u2212k+1. The second layer neural response of f, denoted as N2(f), defines a map as N2(f):T1\u2192[0,1].Step 3.Compute the second layer derived kernel by normalizing the inner product of two neural responses:\n(3)K2(f,g)=N2(f),N2(g)L2(T1),\nwhere \u00b7,\u00b7L2(T1) denotes the L2 inner product with respect to the uniform measure 1T1\u2211t\u2208T1\u03b4t, where T1 is the cardinality of the template set T1 and \u03b4t is the Dirac measure; N2(f)(t)=1n\u2212k+1\u2211h\u2208H1K^1(f\u2218h,t), t\u2208T1.With correlation normalization:\n(4)K^2(f,g)=K2(f,g)K2(f,f)K2(g,g).\n\nThis process can continue if appropriate higher level templates are defined. At each layer (local) derived kernels are built by recursively pooling over previously defined local kernels. Here, for the 2-layer derived kernel, pooling is accomplished by taking an average over a set of transformations which calculating the frequency of a template that occurs in a sequence.\n\nIn this paper we deal with inner product kernels K which satisfies the Mercer Condition, are known to be an instance of reproducing kernels. Next with correlation normalization, K^ is also a reproducing kernel and K^(x,x)=1 for all x\u2208X.\n\nThe kernel function is symmetric (i.e., K(f,g)=K(g,f)), and non-negative (i.e., K(f,f\u2032)\u22650), therefore it can be interpreted as a measure of similarity.\n\nWe first apply the kernel to the set R which contains nucleic acid sequences, and denote it by K^2R, and then apply the kernel to the set of amino acid sequences P, denote it by K^2P, and lastly combine two kernels in a natural way by tensor product for the set of RNA-protein pairs . The reproducing kernel for two RNA-protein pairs (r,p),(r\u2032,p\u2032)\u2208R\u00d7P is defined by:(5)K^2dk((r,p),(r\u2032,p\u2032))=K^2R(r,r\u2032)K^2P(p,p\u2032).\n\nSince both K^2R(r,r\u2032) and K^2P(p,p\u2032) are positive definite kernels, K^2dk((r,p),(r\u2032,p\u2032)) is obviously a positive definite kernel too [50].\n\nAfter combining the kernel with other kernel-based supervised learning algorithm, we can predict RPIs to any RNA-protein pairs with known primary sequences.\n\nThe RLS algorithm is one of the most widely used models for regression. Let K be a kernel on a finite set X. Write HK to denote the inner product space of functions on X defined by K. Suppose z\u00af={(xi,yi)}i=1m is a sample set (called the training set) with xi\u2208X and yi\u2208R for each i. The RLS can be written as follows:(6)fz\u00af,\u03bb=argminf\u2208HK1#z\u00af\u2211(xi,yi)\u2208z\u00aff(xi)\u2212yi2+\u03bb\u2225f\u2225K2.\n\nWe integrated RLS with the combined kernel K=K^2dk, hence the main construction is to compute\n(7)f\u00af=argminf\u2208HK\u2211i=1m(f(xi)\u2212yi)2+\u03bb\u2225f\u2225K2.\n\nHerein, we aim to develop a novel method to distinguish RNA-protein interaction pairs from non-RNA-protein interaction pairs. Therefore, for the binary classification case with yi\u2208{\u22121,1} for each i, if f\u00af\u22640, the predicted class is \u22121 ( denotes non-interaction), otherwise it is 1 (denotes interaction).\n\nOne important step of RLS is to find a \u201cgood\u201d value of the regularization parameter \u03bb>0 in Equation (7). They were selected from an optional set \u039b by leave-one-out cross-validation [51] on the training data. We never used testing data for parameter selection which is under the risk of over-fitting.\n\nThe sensitivity (SE) and specificity (SP) are used to measure the ability of identifying positive and negative instances, respectively. They are defined by\nSE=TPTP+FN, SP=TNTN+FP,\nwhere TP,TN,FP and FN are the number of true positives, true negatives, false positives and false negatives, respectively.\n\nThe accuracy which is used to measure the prediction quality, is defined by\nAccuracy=TP+TNTP+TN+FP+FN.\n\nThe AUC (Area Under the receiver operating characteristic Curve) is further employed to measure the predictive performance, which is 1 for perfect prediction and 0.5 for random prediction."}