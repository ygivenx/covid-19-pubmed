{"title": "Translating Innovation in Diagnostics: Challenges and Opportunities", "body": "The last decade has seen rapid advances in diagnostic tools and novel therapies as a result of basic research in fi elds as diverse as genomics, drug discovery, information science, imaging, and nanotechnologies ( Mankoff et al., 2004 ; Reece, 2006 ) . These areas of advancing knowledge are changing the practice of modern medicine. However, their uptake in the health care setting is often slow and haphazard ( Graham et al., 2006 ; Kerner, 2006 ) .\n\nIt is clear that new knowledge and clever technologies are not enough to improve patient care. Basic discoveries must be linked to insights surrounding the development, delivery, deployment, and application of new information or techniques in health care for translation to successfully occur. The study of translating knowledge in health care or \"translational medicine \" has been characterized in many ways, but one defi nition is that efforts to fi nd cures for affected individuals are complemented by the pursuit to understand human diseases and their complexities ( Mankoff et al., 2004 ) . Within this idealized framework, advances in basic research are properly tested in the clinical setting, then knowledge gained is both fed back to basic researchers and effi ciently translated into new therapeutic strategies to treat, cure or prevent disease ( Marincola, 2003 ; Sonntag, 2005 ) .\n\nHowever, gaps in translation and the provision of new knowledge in the health care setting (so-called \"translational gaps\") do occur. They can adversely impact the quality of medical care, decrease patient safety, and escalate costs. An example of an important topic in translational medicine is the estimated 30-45% of all patients who do not receive medical care according to the best scientifi c evidence. In fact, it has been reported that 20-25% of medical care currently provided is either unnecessary or potentially harmful ( Graham et al., 2006 ) .\n\nEfforts to close these translational gaps have focused in large part on the perceived obstacles and challenges in translating basic research into viable diagnostic and therapeutic strategies. These include: communication and education between researchers from diverse fi elds, institutional support for translational research activities, regulatory hurdles, the high cost of developing diagnostics and novel therapeutics, the need for bioinformatics and complex data handling capabilities, adapting new technologies to high-throughput automation, intellectual property and licensing protection, and the willingness to move away from standard therapies and embrace new paradigms for diagnostic and therapeutic strategies ( Horig and Pullman, 2004 ; Horig et al., 2005 ; Humes, 2005 ; Marincola, 2003 ; Reece, 2006 ; Tsongalis, 2005 ) .\n\nThere is a need to recognize the key role of technological innovation in closing translational gaps in medicine as compared to incremental improvements with current practice. However, signifi cant health gains can only occur when the fi delity with which medical advances are delivered is optimized. These two processes can sometimes be seen as competing ( Woolf and Johnson, 2005 ) , but in actuality represent two factors necessary for medical progress. There may be no area where this is more clearly demonstrated than in translational diagnostics, a fi eld driven by innovation ( Ratner, 2006 ) . The implementation of genomics and other innovative diagnostics for personalized medicine requires the translation of a vast array of technologies and therapeutic strategies from basic science to clinical application. Delivery of these diagnostics to patients requires physician and patient education, as well as evidence-based demonstration of effi cacy for healthcare uptake, payment, and regulatory approval.\n\nHow have some of these challenges been addressed? Just a few years ago, academic medical centers were the primary mechanism to foster translational research and facilitate collaboration between researchers and clinicians ( Puderbaugh, 2006 ) . Multidisciplinary educational programs were funded primarily by private institutions, such as the Doris Duke Charitable Foundation (Mankoff et al., 2004 ) . In the United States, the National Institutes of Health (NIH) and the Food and Drug Administration (FDA) then began initiatives to identify, fi nance, and develop translational research programs. The NIH Roadmap for Medical Research aimed to identify and support research beyond the scope of any single NIH component ( NIH, 2006 ) . Roadmap initiatives sought to improve the development and availability of modern scientifi c tools and information resources, foster novel methods of research collaboration, and enhance the nation's clinical research enterprise. The NIH funded the fi rst 12 Clinical and Translational Science Awards (CTSAs) in 2006 for academic centers conducting projects on translational research ( NCRR, 2006 ) ; additional centers are being named in subsequent years.\n\nThe FDA plays a key regulatory role in medical practice and has promoted translational medicine. The FDA's Critical Path Initiative, subtitled \"Stagnation to Innovation, \" identifi ed key areas for development: better evaluation tools -Biomarkers and Disease Models; streamlining clinical trials; harnessing bioinformatics; moving manufacturing into the 21st century; and products to address urgent public health needs and at-risk populations ( FDA, 2006a ) . In early 2006, the FDA released The Critical Path Opportunities List, which outlined 76 science projects to bridge the translational research gap ( FDA, 2006b, c ) . Of note, the fi rst 33 items on the FDA's Opportunities List were in diagnostic-related areas, refl ecting the importance of diagnostics in driving innovation in medical practice ( Ratner, 2006 ) .\n\nTranslational diagnostics is the subfi eld of translational medicine concerned with diagnostic methods and information. Given the importance of diagnostics, it is crucial to look at translational issues affecting this area of innovation with the intent of closing the gap between basic research and the medical system. As the role of diagnostics continues to evolve, examining translational diagnostics from the view of the largest providers of labbased tests, the national reference laboratories, will be important. Reference labs translate everything from \"home brew \" technologies (also called laboratory-developed tests) licensed directly from academic institutions, to FDA-approved kits purchased from manufacturers, and sophisticated in vitro diagnostic multivariate index assays (IVDMIAs) used to generate patient specifi c risk data ( FDA, 2006d ) . They provide complex data analysis of laboratory tests, educate both physicians and patients, and in this and other ways spearhead the uptake of new diagnostic technologies.\n\nWith this in mind, this chapter will focus on translational diagnostics in the context of genomic and personalized medicine and on the practical lessons learned from bringing innovative diagnostics to market at larger reference laboratories. The large national labs in the United States -including Quest Diagnostics, Laboratory Corporation of America Holdings, Sonic Healthcare, Genzyme, Mayo Clinic, Associated Regional University Pathology (ARUP), and others -provide about 50% of all lab testing delivered in this country and complete several million assays a day in support of patient care. Understanding translational diagnostics as practiced by these national reference laboratories should shed light on both challenges and successes in bridging the translational diagnostics gap from bench to bedside. The potential for translational diagnostics in cancer, infectious disease, and other health care sectors will also be discussed.\n\nThe goal of translational diagnostics is to provide improved diagnostic procedures leading to more effi cacious therapies and improved medical outcomes. Early laboratory medicine consisted primarily of the direct examination of tissue or the analysis of simple analytes in blood. Molecular biology techniques such as positional DNA cloning, DNA sequencing and nucleic acid hybridization assays led to early molecular diagnostics. Southern blot hybridization assays were used to detect gene deletions in Duchenne muscular dystrophy, repeat expansions in fragile X syndrome, and gene rearrangements diagnostic for B-and T-cell lymphoma ( Tsongalis and Silverman, 2006 ) . The fi rst molecular diagnostic products to reach the market included tests for the detection of viral RNA or DNA, genetic tests for single gene diseases, and tests to determine risk for developing certain cancers, such as breast or colon cancer. Many of the new diagnostic technologies entering the clinical laboratory are part of an evolution from nucleic acid amplifi cation technologies for use in amplifying, identifying, and sequencing target single gene sequences towards a systems biology approach which looks at the expression of multiple genes in response to a variety of stimuli and conditions such as infectious disease, drug therapy, and cancer ( Billings and Brown, 2004 ) . Systems biology is often used as a synonym for functional genomics, a description of the genomic and epigenomic infl uences on a trait and interactions with environmental variants (see Chapter 6).\n\nTo understand translational diagnostics and the gaps that currently exist in the movement of knowledge, it is necessary to understand what technological innovations and potential applications are coming forth ( Table 31 .1 ).\n\nMicroarray technology typically involves tethering numerous probes to a solid substrate. For DNA microarrays, the probes usually consist of small oligonucleotides or complementary DNAs (cDNAs). DNA targets, commonly in the form of fl uorescently labeled cDNA or genomic DNA fragments, are then hybridized to the probe. Detection of a fl uorescent signal from directly labeled nucleic acid or protein samples is the most common microarray detection method ( Tefferi et al., 2002 ) . DNA microarray technology provides data on DNA sequence variation (mutations and polymorphisms) and gene expression levels (see Chapters 8, 9, and 13) .\n\nGene expression profi les can identify upregulated and downregulated genes, which are then targets for novel therapeutics. cDNA arrays have also been used to classify pathological subgroups of specifi c disorders. When polymorphisms are identifi ed, genomic DNA targets are probed with oligonucleotides which defi ne allelic differences. Recent advances in microarray technology involve the use of liquid-phase or microparticle-based arrays. One example would be bead-based multiplexing, which allows multiple analytes to be assayed in the same well. Molecular diagnostic applications of microarray technology include cancer diagnosis, typing, and prognosis; infectious disease identifi cation, biodefense applications, and pharmacogenomics ( Petrik, 2006 ) .\n\nAnother application of array technology is array comparative genome hybridization. This is a powerful technique that detects high-level amplifi cation and homozygous deletions in small genomic regions. Array technology has been used to detect highresolution copy number changes in breast, renal, and bladder cancer ( Gabriele et al., 2006 ; Petrik, 2006 ) . Using the genomic clone sequence data, this technique can lead quickly to the identifi cation and cloning of genes associated with disease for use in diagnostics and targeted therapeutics. Array technology promises to revolutionize the fi eld of medical cytogenetics by providing molecular karyotyping without the need to culture cells or stain chromosomes for visualization (see Chapter 9).\n\nProteomics seeks to understand the structure, function, and expression of all proteins encoded by a given genome (see Chapter 14). The human genome project identifi ed approximately 25,000 human genes. However, it is estimated that the number of protein products is 10-30 times higher due to alternative splicing and post-translational modifi cation of gene products ( Petrik, 2006) . Proteomics also encompasses protein expression profi les as a function of age, state, and environment. The basic technique of proteomics is the separation of proteins from a biological sample by various separation technologies (2-D PAGE, liquid chromatography, affi nity capture) followed by detection of proteins peptide fragments by technologies such as peptide fi ngerprinting by mass spectrometry, protein arrays, or antibody arrays ( Rice et al., 2006) . Proteomic analysis based on mass spectrometry may be better suited for the identifi cation of useful protein biomarkers, while technologies such as automated chip technology using protein arrays may have higher clinical diagnostic utility. Protein arrays use protein probes that in solution retain the ability to interact specifi cally with other proteins or molecules. Protein arrays can be used to identify protein-protein interactions, enzyme-substrate interactions, and antibody-antigen interactions.\n\nGlycosylation is a critically important post-translational modifi cation of cellular proteins. Glycan moieties are involved in a wide variety of intracellular, cell-cell, and cell-matrix recognition events. Glycomics is the study of the glycans produced by humans and their role in protein function in health and disease (Morrelle and Michalski, 2005 ) . Early technologies used separation methodologies and mass spectrometry analyses that were similar to proteomic methods. New approaches are being developed for clinical applications and include such technologies as microcapillary chromatography, lectin affi nity chromatography, and carbohydrate microarray and mass spectrometry ( Miyamoto, 2006) . Glycan analysis has contributed to drug discovery, clinical assays, and basic research into the underlying biology of cancer. However, its greatest promise will be to enhance genomic and proteomic analysis of clinical samples to determine the effects of the genetic, environmental, lifestyle, and nutritional state of patients on their health status. \n\nEpigenetics involves the study of heritable changes in gene function that occur without a change in DNA sequence (see Chapter 1). Epigenetic mechanisms include DNA methylation, histone acetylation, and RNA interference (see Chapters 5 and 11). Some neuropsychiatric and rheumatologic diseases and cancers may affect both the genotype and epigenotype. Small interfering RNAs (siRNAs) can induce specifi c post-transcriptional gene silencing in mammalian systems and hold great promise as therapeutic agents. Diagnostic assays will have to be developed to analyze the effectiveness of these new therapies. Cancer cells are known to undergo changes in 5-methylcytosine distribution including hypomethylation and hypermethylation of promoter CpG islands, which are associated with tumor-suppressor genes (Esteller, 2002 ) . An integrated analysis of the epigenetic state of cancer cells can involve DNA methylation, transcription factor binding analysis, and histone acetylation analysis. Treatments being evaluated include histone deacetylase inhibitors and DNA methyltransferase inhibitors. DNA methylation patterns are being developed as diagnostic biomarkers for detection and risk assessment ( Laird, 2005 ) (see Chapter 11).\n\nBiological imaging has begun to play an increasingly important role in the early detection and staging of disease (see Chapter 43). Recent advances in imaging include multislice computer tomography (CT) and whole-body magnetic resonance imaging (MRI) to provide earlier detection and more accurate determination of the extent of disease ( Schwaiger and Peschel, 2006 ) . Tracer techniques in combination with positron emission tomography (PET) have aided in discriminating between normal and tumor tissues. Such molecular imaging technologies seek to visualize biological processes and molecular binding sites. New advances such as PET-CT provide qualitative and quantitative assessment of tumor tissue. Taken together with other molecular diagnostic methods, biological imaging should aid in disease detection, be predictive of therapeutic effi cacy, and provide markers for disease treatments.\n\nThe near future of nanobiosensing envisions a merger of molecular diagnostic technology with nanotechnology to produce nanobiosensors capable of detecting biological processes at the molecular level with continuous operation in real time ( see Chapter 51 and Demidov, 2004 ) . Molecular beacons comprised of both nucleic acid probes and self-reporting optical transducers should be capable of providing feedback for monitoring therapeutic intervention or for periodic checkup. Nanoparticlebased molecular detection is being used currently to develop multiplexed molecular recognition arrays and label free ways to quantify specifi c binding events ( Ming- Cheng Cheng et al., 2006) . Quantum dot nanocrystals provides a nearly unlimited range of sharply defi ned color indicators that can be tagged to biomolecules of interest and provide long-lived, sensitive probes. Nanodevices using microfl uidic technologies may produce useful cancer, infectious diseases and other tests. These technologies will aid the continuing miniaturization of molecular diagnostic platforms; a necessary evolution if molecular diagnostics are to be expanded to point of care use and for system biology approaches that require the simultaneous detection of specifi c biological processes associated with disease.\n\nFactors affecting disease progression and therapy management in infectious disease associated with viral, bacterial, fungal, and parasitic pathogens include rapid detection and identifi cation, quantifying pathogen burden, and pathogen genotyping ( NIH, 2002 ) .\n\nAs with the Human Immunodefi ciency Virus (HIV), quantifying the viral load of a wide variety of viral pathogens is needed to monitor both the progression of the disease and the response to therapy. Pathogen genotyping is also essential in order to identify variants resistant to therapy, and to differentiate closely-related species or viral types. Currently, the gold standard for pathogen detection is growth of the pathogen in culture. This process is time-consuming and in many cases non-informative. Diagnostic methods for viral detection consist primarily of immunological identifi cation of antibodies produced in response to viral infection. Molecular diagnostics utilizing nucleic acid amplifi cation testing (NAAT) have been used for viral load monitoring and genotyping for HIV and screening for HIV1, hepatitis B and C (HBV, HCV), and West Nile virus (WNV) ( Paxton, 2004 ) . In HIV and HCV there is a window between infection and detection of antibody production in which immunological methods fail to detect viral infection. This is a very important issue for blood safety and for the identifi cation of acute infections which may account for a high percentage of new clinical cases. NAAT testing has been implemented for blood screening and has been estimated to prevent about fi ve HIV-1 and 56 HCV infections per year. Another advantage of NAAT testing lies with the fl exibility of the platform in the face of emerging infectious disease threats. When WNV became a public health concern in 2002, these platforms were converted for the detection and monitoring of WNV within 9 months. With the incorporation of DNA microarray technologies, rapid pathogen detection and genotyping may be accomplished simultaneously. Recently, a protein microarray for Coronaviruses, including the severe acute respiratory syndrome (SARS) virus, has been tested and found to distinguish between SARS and other Corona viruses ( Zhu et al., 2006 ) . Low-density microarray tests are being researched for use in subtyping infl uenza A and avian type H5N1. These types of arrays may soon have clinical utility for monitoring infl uenza worldwide, developing vaccines, and eventually for point of care applications ( Mehlmann et al., 2007 ) . This last application is very important, as determining the presence of infl uenza on solely clinical grounds is very diffi cult due to poor specifi city and sensitivity of clinical fi ndings, the other pathogens that cause similar symptoms, and the infl uenza subtypes that cause different symptoms. For example, up to 70% of patients with infl uenza symptoms are not infected with infl uenza virus, and up to 30% of those are due to Coronaviruses (Montalto, 2003 ; Brown, 2006 ) . CombiMatrix has developed a DNA microarray to detect and type infl uenza strains and Tm Bioscience Corp. (now Luminex) has developed a major human respiratory virus array. Molecular diagnostics can be utilized for sexually transmitted diseases such as Chlamydia trachomitis and Nisseria gonorrhea, as well as other bacterial pathogens such as Legionella pneumophilia (Legionnaire's disease), Borrelia burgdoreri (Lyme disease), Mycobacterium tuberculosis, and Bordetella pertussis and B. parapertussis (whooping cough). Despite effective vaccination programs, Bordetella pertussis remains an important and highly contagious disease due to the fact that newborns do not appear to gain passive maternal protection and vaccine-induced immunity is far less complete than expected. A real-time PCR assay that accurately detects and differentiates the DNA from both agents of pertussis syndromes has been developed ( Vincart et al., 2007) . This real-time PCR assay provides results within 24 h of sample receipt and provides clinicians with more timely and accurate information than that provided by culture, direct fl uorescence antibody, serological testing, or even conventional PCR methods. Additional applications include tissue product testing, bioterrorism monitoring, and characterization of host-pathogen interactions in the search for additional diagnostic biomarkers.\n\nStandard diagnostic procedures for human tumors use a combination of histopathology, special stains, immunohistology, radiology, and clinical data. Diagnostics derived in this manner provide data on tissue origin, tumor type, stage, and grade, along with information on completeness of surgical tumor removal ( Dietel and Sers, 2006 ) . However, these tests are neither sensitive nor specifi c enough to differentiate patients who will respond to treatment from those who will not. The application of high-throughput DNA array technologies, which provide gene expression profi les, has revolutionized tumor diagnostics and led to the identifi cation of novel tumor subgroups for otherwise indistinguishable tumors ( Dietel and Sers, 2006 ) . Researchers have also identifi ed genes the expression of which can be used to predict the metastatic potential of breast carcinomas. In the future, technologies that take a systems approach will be needed to identify patients who will benefi t from novel therapies. These methods must be capable of detecting multiple oncogenic pathways in tumors both before and during therapy and they must be adaptable to routine clinical diagnostic testing. Possible methods include: tissue microarrays; forward-phase protein microarrays, capable of simultaneously detecting multiple protein interactions with a single sample; and reverse-phase protein microarrays, capable of probing multiple analytes with specifi c antibodies ( Gulmann et al., 2006 ) . These testing modalities may be applied to primary or metastatic tumor biopsies, or to 'blood biopsies' of rare circulating tumor cells.\n\nUnderstanding the role of human genetic variation in disease susceptibility and in the selection and effi cacy of therapeutics is an essential component of optimizing care. Pharmacogenomics looks at the impact of patient genetic variation on response to therapeutics (see Chapter 27). Pharmacogenomics also infl uences the delivery and cost of healthcare ( Phillips et al., 2004 ) . Sensitivity, toxicity, and resistance to medications at standard doses are prime examples of the effect of genotypic heterogeneity within the patient population. Well known, clinically relevant examples include differential responses to warfarin, codeine, thiopurine antileukemic drugs, and succinylcholine. Genetic differences in receptor structure and affi nity, drug metabolism, and drug transport systems are now known to play an important role in both differential responses to drug therapy and adverse drug reactions (ADRs).\n\nAdverse drug reactions result in 6.7% of hospitalizations and 0.32% of mortalities ( Montgomery and Louie, 2001 ) . The genes most associated with ADR encode receptors, metabolic enzymes, and metabolite transport proteins, the same genes have been implicated in environmental toxin susceptibility and cancer predisposition. Drug effi cacy is directly related to the binding of the drug molecule to cell surface receptors. For example, it has been demonstrated that patients expressing high levels of beta-adrenergic receptors are more responsive to beta-agonists and antagonists. Conversely, those expressing low levels of this receptor require higher-drug levels to achieve a comparable pharmacological effect.\n\nOnce inside the cell, the drug is metabolized by a number of enzymes catalyzing alterations in the molecular structure of the therapeutic drug. Genetic variation in the genes encoding metabolic enzymes leads to differences in enzyme activity that can be characterized as extensive (standard dosage), intermediate (slower than extensive, altered dosage), poor (enzyme deficiency, do not treat with some drugs), and ultrarapid (break down drugs faster than extensive, no effect or reduced effectiveness from drug therapy). One class of metabolic enzymes is the cytochrome P450 superfamily (CYP) that comprises more than 40 isozymes. These enzymes metabolize a large number of drugs, small molecules, mutagens, and carcinogens by modifying parent molecule functional groups. Polymorphisms in the gene CYP2D6 can result in a homozygous recessive inactive genotype that cannot convert codeine into the active metabolite morphine ( Rabinowitz and Poljak, 2003 ) . In 2005, the FDAapproved Roche Diagnostic's AmpliChip CYP450 test, a P450 molecular array which detects mutations in both the CYP2D6 and CYP2C19 genes, which contribute to the metabolism of about a quarter of all prescribed drugs. The data provided by the AmpliChip aids physicians in determining drug selection and dosage. An example of the potential impact of CYP450 typing is illustrated by the drug warfarin which is metabolized by enzymes encoded by the gene CYP2C9. Polymorphisms in this gene infl uence warfarin dosing requirements and warfarin-associated bleeding ( Wittkowsky, 2002 ) . It has been demonstrated that the maintenance warfarin dose can be estimated from demographic, clinical, and pharmacogenetic factors ( Gage et al., 2004 ) . A second gene, VKORC1 , has been associated with the average weekly warfarin dose required to maintain patients at their desired anticoagulation target. With more information it is expected that predictive models for warfarin dosing can be developed and help the more than one million patients which each year take warfarin at appropriate dosage to avoid thromboembolic events ( Li et al., 2006 ) . Twelve percent of those users currently experience major bleeding episodes and death results in as many as 2%.\n\nAbacavir is a nucleoside analog that is a potent inhibitor of the HIV reverse transcriptase enzyme. However, between 4-8% of patients develop hypersensitive reactions which can result in life threatening hypotension. The possession of human leukocyte antigen (HLA) B*5701 is a risk factor for abacavir hypersensitivity. Use of HLA-B*5701 genotype screening has been shown to effectively reduce the incidence of abacavir hypersensitivity through the exclusion of patients positive for this allele from abacavir combination treatment ( Mallal et al., 2008 ) . Adoption of this pharmacogenomic test was impacted by drug safety issues and the demonstration of its cost-effectiveness (Hughes et al., 2004 ) . Additional associations between specifi c HLA genotypes and life threatening ADRs include Allopurinol and HLA-B*5801 ( Hung et al., 2005 ) , and carbamazeprine and HLA-B*1502 ( Dainichi et al., 2007 ) . In December, 2007, the FDA released guidelines calling for genetic testing of the HLA-B*1502 genotype prior to starting carbamazeprine therapy for patients with Asian ancestry ( FDA, 2007e ).\n\nCancer pharmacogenomics impacts patient safety and therapeutic effi cacy and can be used to select treatment choice based on tumor genomics and patient genotype. Methodologies such as molecular cytogenetics, somatic mutation detection, and gene expression profi ling have been used to examine genotypic differences in cancer tissue and in patient response to therapy.\n\nChronic myelogenous leukemia (CML) is one of the most common forms of leukemia. Most cases of CML result from a chromosome abnormality whereby DNA from chromosome 9, which contains most of the proto-oncogene c-abl , is translocated onto chromosome 22 within the breakpoint cluster region ( BCR) gene ( Billings and Brown, 2004 ) . This results in a gene fusion constitutive for expression of a protein with tyrosine kinase activity. This activity affects intracellular signaling pathways that result in unregulated cell proliferation. Molecular diagnosis of CML utilizes quantitative PCR. Fluorescence in situ hybridization (FISH) can also be used to visualize the translocated chromosomes. The oral drug imatinib (Gleevec) was specifi cally designed as a selective inhibitor of the BCR-ABL tyrosine kinase and has demonstrated therapeutic superiority over conventional drug therapy. Molecular diagnostic testing can be used to monitor responsiveness to Gleevec or the development of therapeutic resistance.\n\nHereditary nonpolyposis colorectal cancer (HNPCC) is the most common hereditary cause of colon cancer, accounting for about 2-5% of all colon cancer cases (see Chapter 73).\n\nIt is caused by mutations in any of at least fi ve DNA mismatch repair genes, and DNA tests are available for the most common genes. The HNPCC syndrome predisposes a person to developing colon cancer at a young age. Presymptomatic and predispositional testing in families have been conducted. Since approximately 90% of tumors from HNPCC patients show microsatellite instability, testing for this phenomenon alone can be a good guide to the necessity for further molecular characterization. Gene sequencing can be used to identify the precise mutation. Mutational data, along with DNA microsatellite instability testing results, can be used to identify fi rst-degree relatives with HNPCC. Once HNPCC has been implicated from clinical data, immunohistochemistry testing can be conducted on tumor tissue for confi rmation of diagnosis. In this way, members of families with HNPCC have a means to know if they need aggressive monitoring and treatment or not.\n\nThiopurines, thioguanine, and mercaptopurine are commonly used anticancer therapeutics. The thiopurine methyltransferase (TPMT) enzyme catalyses the methylation of thiopurines. The TPMT gene is polymorphic, and one in 300 patients is defi cient in enzyme activity. At standard doses, this can lead to toxic accumulation of thiopurines, which can be fatal. Three mutations account for the majority of mutant alleles, and genetic testing is available. Children with leukemia who receive these medications are routinely screened for these defi ciency genes ( Billings and Brown, 2004 ) .\n\nIn late 2005, the FDA-approved Third Wave Technologies ' Invader UGT1A1 molecular assay, which is used to identify patients who could experience adverse reactions to Camptosar (irinotecan), a colorectal chemotherapy drug ( Grebow, 2005 ) . The test detects mutations in the UGT1A1 gene, which produces a metabolizing enzyme active on the therapeutic. This was one of the fi rst pharmacogonomic tests to be FDA approved for use as a companion diagnostic to a specifi c drug.\n\nFinally, genetic testing to predict future disease risk based on an inherited germline mutation is also available. The BRCA1/2 mutations are associated with a higher risk of breast and ovarian cancer. Approximately 60-80% of women with BRCA1/2 will develop breast cancer. BRCA testing, the BRAC Analysis test, is expensive and provides information with limited therapeutic response. Tests such as these can experience initial resistance among insurers to provide reimbursement ( Phillips et al., 2004 ) .\n\nWhat are the factors that infl uence the uptake of pharmacogenomic diagnostic testing? Clinical validity and utility need to be demonstrated and communicated to physicians and patients. Rapid, reliable, inexpensive, and easily interpretable molecular tests will increase the clinical use of these diagnostic tests ( Shah, 2004) . Clinical relevance must also be demonstrated. This may limit the use of this technology to applications where there exists choice of treatments. In addition, due to the screening nature of these tests, cost-effectiveness must be demonstrated (Flowers and Veenstra, 2004 ) . However, determining cost-effectiveness of genetic technology will not be simple, a fact that has led to some opposition to their large scale uptake in healthcare (Rogowski, 2007 ) . Additional obstacles to the deployment of pharmacogenomic testing include obtaining reasonable reimbursement for testing, and the education of physicians and patients as to the value of tests and interpretation of the results.\n\nInvestigation of specifi c examples of translational diagnostics can reveal crucial issues and challenges faced as new methods enter health care.\n\nPrior to the 1990s, therapeutics that truly impacted the course of viral diseases were not available. The symptoms could be affected, but there was no antibiotic equivalent for treating viral diseases. However, by the mid-1990s, pharmaceutical companies had a number of antiviral compounds; some specifi cally tailored to HIV, well into clinical trials and needed an accurate biomarker to analyze effectiveness of the new therapies. This convergence of therapeutics and diagnostics would lead to the birth of the theranostic , a diagnostic test linked to the application of specifi c therapies ( Warner, 2004 ) . In 1996, Roche Diagnostics began the Roche Amplicor\u00ae Access Program to provide two free baseline HIV viral load tests to anyone in the United States with HIV ( James, 1996 ) . Reference laboratories helped execute this program, which began the successful uptake of HIV viral load testing in the marketplace. As with traditional diagnostics, molecular diagnostic tests needed to demonstrate utility and performance before their adoption into practice. First, there must be an unmet clinical need, which was clearly the case with HIV. Secondly, with the development of antiviral therapies, HIV viral load testing provided information that resulted in a therapeutic action, and also provided the means to continually monitor the effi cacy of the treatment. Without a therapeutic, the test would have provided information that would not have informed the treatment of HIV.\n\nAdditional factors sped the uptake of HIV viral load testing. Initial testing involved a relatively small group of highly focused clinicians. This made it much simpler to educate physicians in the use of the viral load test for the management of HIV. Highly motivated patients fueled demand for the test. The viral load test had a signifi cant role in early clinical trials of antiviral therapeutics and became an objective measurement tool, even before any protease inhibitors were approved for physician use. Publications on the antiviral therapeutics included the HIV viral load results. Thought leaders who participated in the trials became early adopters of the test, even before the test was FDA approved. The viral load test was put into a kit format and became FDA approved at approximately the same time as the fi rst protease inhibitors became available. Recommendations for viral load testing became part of standard guidelines for treating HIV infection ( Chesebro and Everett, 1998 ) . The theranostic became tied to the use of the drug. Roche's Amplicor Access Program led to rapid test utilization. As volumes increased, manufacturers developed automation. HIV management requires relatively frequent monitoring, as often as four times per year. Therefore, the test was not a \"one off \" test as in constitutional genetic testing. HIV viral load testing also obtained its own CPT code, facilitating reimbursement. Additionally, the kit manufacturer, the national reference labs, patients, and doctors all worked to ensure that the test was reimbursed. These factors helped make HIV viral load testing cost effective and a foundation of HIV prevention efforts. The combination of viral load testing and new therapeutics transformed HIV from a uniformly fatal disease to a chronic treatable disease. Therefore, HIV has become a disease with an \"actionable \" result. Recent Centers for Disease Control (CDC) recommendations now call for general population screening for HIV status. This example of translating a diagnostic into patient care was successful because so many of the hallmarks of successful diagnostic tests were in place. There was an unmet clinical need, an actionable result was obtained from testing, the test was ultimately cost effective, high-throughput analysis became available, and reimbursement issues were rapidly resolved.\n\nSimilar to the convergence of HIV therapeutics and the need for HIV viral load assays, the development of the anti-cancer therapeutic Herceptin required a clinical laboratory test to determine Her2 receptor status ( Tsongalis and Silverman, 2006 ) . Amplifi cation of the human epidermal growth factor receptor 2 gene ( HER2) in primary breast cancer carcinomas had been shown to correlate to poor clinical prognosis for breast cancer patients ( Ferretti et al., 2007 ) . The Her2/neu oncogene is active in 25-37% of breast cancers, and overexpression of the HER2 protein on cell surfaces was found to stimulate uncontrolled tumor growth. Herceptin (trastuzumab) is a recombinant DNA-derived monoclonal antibody that binds with high affi nity and specifi city to the extracellular domain of the HER2 receptor and inhibits the proliferation of HER2 overexpressing tumor cells. Herceptin alone was associated with an objective response in 15% of extensively pretreated patients with metastatic breast cancer overexpressing HER2, and 26% of previously untreated patients ( McKeage and Perry, 2002 ) . Cardiac dysfunction occurred in 13% of patients receiving Herceptin and paclitaxel and in 4.7% of patients receiving only Herceptin. Given the high cost of Herceptin therapy, the signifi cant side effects of Herceptin treatment for some patients, and the targeted nature of the therapeutic, it was necessary to restrict therapy to patients expected to respond to treatment. In 1998, the FDA approved both Herceptin and the HerCepTest for the treatment of metastatic breast cancer. HER2 protein is detected primarily by IHC or FISH methods. HercepTest and PathVysion, a FISH assay for HER2 amplifi cation, are now used to select patients for Herceptin therapy. Uptake of diagnostic testing for HER2 receptor status was greatly accelerated due to safety issues associated with the therapy. Additionally, the assay did not require new tissue samples as in many cases archived samples in paraffi n blocks were already being sent to reference labs for estrogen and progesterone analysis, and physicians could merely check another box on the test order sheet. Randomized clinical trials have recently demonstrated signifi cant differences in survival when comparing chemotherapy to chemotherapy plus Herceptin in women with HER2 overexpressing breast cancer in either the metastatic or adjuvant setting ( Ferretti et al., 2007) . Thus, what was once a prognosticator of poor outcome for the patient has become a predictive marker of response to therapy. Unfortunately, a lack of concordance among detection techniques, different scoring systems used to determine HER2 status, and a lack of lab standardization and quality based on test experience have led to signifi cant variation in HER2 testing (Nelson, 2000 ) . Recent CAP/ASCO guidelines, that include a testing algorithm, QA requirements, lab evaluations, and a scoring system, should help address these issues ( CAP/ASCO, 2007 ) .\n\nHuman papillomavirus (HPV) is the major cause of cervical cancer, a disease that kills more than 200,000 women worldwide each year ( Gottlieb, 2002 ) . In the United States more than 6 million new cases are reported annually and at least 20 million people in this country are already infected ( NIAID, 2006 ) . Approximately 40 of the 100 types of HPV virus can be sexually transmitted, but most rarely cause symptoms or disease. Types 6 and 11 are low-cancer risk types that cause genital warts. Lowand high-risk types can cause the growth of abnormal cells which can be detected when a Pap test is done during a gynecologic exam. The Bethesda system divides the most common clearly abnormal Pap results into either: low-grade squamous intraepithelial lesions (LSILs) which are mild cell changes associated with HPV; high-grade squamous intraepithelial lesions (HSILs) which are precancerous cell changes which should be treated by a physician; and cancer. Clinicians managing a notcompletely-normal Pap result, known as ASCUS (atypical squamous cells of undetermined signifi cance), may benefi t from the additional information provided by HPV testing.\n\nIn 2000, the FDA-approved Digene Corporation's hc2 High-risk HPV DNA test for use in women with abnormal Pap test results. In 2003, the FDA expanded the use of HPV testing in conjunction with the Pap test for routine screening. The test uses hybrid capture technology to directly detect HPV virus DNA and the current system allows high-throughput testing. A woman fi tting the appropriate clinical criteria with both a negative Pap and negative HPV DNA test has less than a one in one thousand chance of developing cervical cancer. In 2006, the FDA-approved Merck and Co's Gardisil, a recombinant vaccine which is designed to prevent the majority of HPV-related clinical diseases, those caused by HPV 6, 11, 16, and 18. HPV types 16 and 18 account for approximately 70% of cases of cervical cancer, while HPV 6 and 11 cause approximately 90% of genital wart cases. In clinical studies, Gardasil prevented 100% of HPV 16-and 18-related cervical cancer in women not previously exposed to the relevant HPV types. These studies were conducted on 21,000 women ages 16-26 and the vaccine was nearly 100% effective in preventing precancerous lesions and genital warts. Gardisil was evaluated and approved in 6 months under the FDA's priority review process, a process for products with potential to provide signifi cant health benefi ts ( FDA, 2006f ) . On June 28, 2006 the CDC's Advisory Committee on Immunization Practices (ACIP) recommended that all females between the ages of 11-26 receive the HPV vaccine as part of routine primary care.\n\nHPV diagnostic tests expanded in utility from a test limited to abnormal Pap test results, to a diagnostic test that in conjunction with the Pap test is now standard of care for routine screening for women 30 years of age or older. The move to a liquid Pap test from previously common smear methods removed the need for additional sampling for HPV testing and the development of an effective vaccine bolstered the public awareness of HPV. Technological innovations reducing sampling barriers and improving therapeutic options along with other factors aided the rapid uptake of HPV testing.\n\nDiagnostic tests are being developed at a rapid rate, and the technologies of diagnostic testing are expanding. However, not all diagnostic tests fi nd their way to application in health care. For some, clinical utility may not be demonstrated due to the lack of an actionable result, cost-effectiveness, high-throughput methods, or adequate reimbursement. For example, early asymptomatic glaucoma testing was widely abandoned because early detection did not affect the outcome. Other tests may not achieve the analytical performance characteristics required of a good diagnostic. Assay sensitivity, specifi city, and predictive value must be high (Gaeta, 2005 ) . Reproducible precision and a clinically reportable range must be demonstrated ( Irwig, et al., 2002 ) . For example, before microarray technology can be translated widely to diagnostic applications, problems with sensitivity and reproducibility will have to solved ( Petrik, 2006 ) . A rigorous evaluation of diagnostic tests prior to their introduction into clinical practice is the goal of the standards for the reporting of diagnostic accuracy (STARD) initiative ( Bossuyt et al., 2003 ) and evaluation of genomic applications in practice and prevention (EGAPP), a CDC project to establish an evidence-based process for assessing applications of genomic technology ( CDC and National Offi ce of Public Health Genomics, 2008 ). Ultimately, the role of diagnostics in clinical care must be balanced with other tools available to the physician, such as patient history and physical exam (Halkin et al., 1997 ) . Additionally, technological advancements must produce an increase in effi cacy of treatment in excess of that produced by improving the delivery of older treatments ( Woolf and Johnson, 2005 ) .\n\nThe future of innovative diagnostics holds great promise for providing rapid identifi cation of disease susceptibility and status, for monitoring disease progress and therapeutic effi cacy, for reducing ADRs and speeding appropriate therapy selection, for the development of personalized medicine, and for optimizing disease prevention and cures. To see this promise unfold, a number of challenges must be overcome. Diagnostic testing must address an unmet medical need, lead to an actionable event, and demonstrate its clinical utility. These diagnostics must also demonstrate high specifi city, sensitivity, and predictive value.\n\nFor a novel diagnostic to be ultimately translated effectively, additional obstacles must also be overcome. The diagnostic test must prove functional in the health care setting. For example, do new sample types need to be procured, or can the diagnostic test replace or add on to existing assays while retaining current sampling procedures? Will the clinical value of the test be clearly demonstrated to thought leaders, early adopters, patient advocacy groups, and other stakeholder; required for rapid uptake of the diagnostic? The role of professional society guidelines, CMS coverage, technology review by the insurance industry, and regulators must be considered. Will reimbursement issues delay the uptake and access of the test to a large sector of the patient population? Are there patient safety issues, within specifi c treatments, that warrant rapid clinical uptake? What are the training requirements? How does the clinical lab select the appropriate technology from a pool of technical approaches to the diagnostic problem? What is the best way to deliver the diagnostic to healthcare providers and users? How should a new diagnostic test best be used to optimize the healthcare benefi t, as a stand alone diagnostic, or linked to a treatment protocol (a companion diagnostic or theranostics)? Finally, will new accurate and reliable diagnostic methods make the biases and inaccuracies that have characterized the practice of clinical medicine and patient reports a thing of the past? And, if they do, what will be lost and what will be gained? These are crucial concerns and challenges that must be addressed for virtually any new diagnostic to reach its full potential to improve health care. Closing the translational gaps highlighted by these issues and others requires research, its own knowledge translation, fi nancial support, and an awareness of the lessons learned from previously translated diagnostics (Table 31 .2 ).\n\nEven though diagnostics infl uence 60-70% of healthcare decision-making, they make up less than 5% of hospital costs and 1.6% of Medicare costs ( Nordhoff, 2005 ) . In the next decade, new diagnostics are predicted to have an even greater impact on healthcare decision making by providing improved disease prediction and prognostication, and therapy guidance. Studies have found a 30-50% reduction in direct hospital and outpatient charges when changes in patient health status are accurately monitored. Yet, diagnostic tests recommended as standard of care are underused 51% of the time ( Olsen, 2006 ) . Low compliance with diagnostics-based quality measures for diabetes, cardiovascular disease, colorectal cancer, and breast cancer can be linked to 34,000 avoidable deaths and $899 million in avoidable healthcare costs ( Olsen, 2006 ) . Effi cient translation of new knowledge and technology is critical so that the promise of innovative diagnostics and personalized medicine can be realized. Improving the translation of innovative diagnostics will require not only an understanding of specifi c lessons learned from previous translations, but also an understanding of the conceptual framework within which diagnostic translations occur ( Table 31 .3 )."}