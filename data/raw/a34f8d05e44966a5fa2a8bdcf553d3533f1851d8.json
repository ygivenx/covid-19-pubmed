{"title": "Supplementary Figures", "body": "118 200 Scientific collaboration network of the Santa Fe Institute leadership [3] 32 96\n\nCollege students in a course about leadership prison [3, 4] 67 182 Social networks of positive sentiment (prison inmates) dolphin [5] 62 159 social network of dolphins polbooks [6] 105 441 A network of books about US politics football [2] 115 613\n\nThe network of American football games, Fall 2000. netscience [7] 1589 2742 A coauthorship network of scientists working on networks. netscience* [7] 379 914\n\nThe largest component of netscience.\n\nThe basic idea of compressed sensing is to reconstruct sparse data or a signal from a few observations whose number is much less than that of the original data sets [8, 9, 10, 11] . The observations are measured by linear projections of the original data on a few predetermined, random vectors. Since the requirement for observations is considerably less comparing to that in conventional signal reconstruction schemes, compressed sensing has been developed into a powerful technique to obtain high-fidelity signal, especially in cases where sufficient observations are not available.\n\nRestricted Isometry Property. To reconstruct vector X in the form Y = \u03a6 \u00b7 X by using the compressed sensing algorithm, the matrix \u03a6 needs to satisfy Restricted Isometry Property (RIP) [9, 10, 11] . Specifically, matrix \u03a6 is said to satisfy RIP if\n\nholds simultaneously for all s-sparse vectors X, for some \u03b5 s \u2208 (0, 1). Prior works have verified that RIP holds with high probability for sampling matrices \u03a6 whose entries are independently and identically distributed (i.i.d.) realizations of certain random variables, provided that the number of rows in the matrix is large enough. For our reconstruction problem, the randomness of matrix \u03a6 is induced by the stochastic effect in the spreading process, allowing us to successfully reconstruct the neighboring vector X from a small number of base strings using compressed sensing algorithm. The accurate reconstruction results obtained from a variety of complex networks suggest that the condition of RIP is met, due to the intrinsic stochastic nature of the spreading dynamics. With respect to data requirement, our experience suggests that the main factor determining the amount of data required is the sparsity of vector X, where a sparser vector X requires less data for reconstruction. Nevertheless, provided with sufficient data, one can recover even relatively dense vectors.\n\nCodes. The Matlab codes of the compressed sensing algorithm [12, 13] based on the L1-norm optimization that we used in this paper can be downloaded from [14] .\n\nFor the CP dynamics, the treatment is quite similar to that of SIS dynamics. Specifically, averaging over time t restricted by equation (14) on both sides of equation (7), we obtain\n\nSubstituting \u27e8P 01 i (t \u03b1 )\u27e9 by \u27e8S i (t \u03b1 + 1)\u27e9, we obtain equation (8) . According to equation (16) , a set of base strings at different timet can be identified. For each base string, repeating the above process gives rise to a set of equations at different timet \u03b1 (\u03b1 = 1, \u00b7 \u00b7 \u00b7 , m) that can be expressed in the matrix form\n\nCompared with the reconstruction of SIS dynamics, the matrix \u03a6 has the same form, but the vectors Y and X are different. In fact, reconstruction of CP is expected to be more accurate because fewer approximations are used in the derivation of the reconstruction form. For example, the derivation of equation (15) from equation (14) with approximation in the main text for SIS dynamics is absent for CP.\n\nWe provide details for finding base strings and other strings derived from these strings. Given a set of time series of length t, to reconstruct the neighboring vector X of an arbitrary node i, we first find all possible base strings according to the threshold \u0398. The procedure is as follows. Randomly choose a time t and set the string S \u2212i (t) to be the first base string S \u2212i (t 1 ). Then find the second base string\n\nRepeat this process until no more base strings can be found, which gives rise to a set of base strings\n\n, for which the normalized Hamming distance between each pair of base strings is large than \u0398.\n\nFor a base string S \u2212i (t \u03b1 ), we enumerate the time series to pick all strings\n\nwhich gives a set of t \u03bd (\u03bd = 1, 2, \u00b7 \u00b7 \u00b7 , l). We can then calculate two average values based ont \u03b1 and t \u03bd :\n\nThe two averaged quantities are used to build the vector Y and the matrix \u03a6, respectively, as shown in equation (6) in the main text and equation (3) for SIS and CP dynamics, respectively. For another base string S \u2212i (t \u03b2 ), we can find t \u03bd associated with the strings that belong to the base string S \u2212i (t \u03b2 ) in the same manner. Based ont \u03b2 and t \u03bd , we can obtain the two average quantities. Repeating this process for all the base strings gives rise to the reconstruction form Y = \u03a6 \u00b7 X. Figure 1 in the main text is a schematic illustration of the process. It is noteworthy that different base strings may share some strings in their contribution to the two average quantities in the sense that a string according to the threshold \u2206 may be subject to more than one base string. This will not have any negative impact on full reconstruction. More importantly, the sharing of strings by different base strings considerably reduces the minimum requirement of the length of time series and makes all available data sufficiently used, accounting for the slow increase of the minimum length t min of time series with the network size, as shown in Supplementary Fig. 4 .\n\nIdentifying links. After the neighboring vector X of each node has been reconstructed, we need to distinguish the existent links from null connections. This can be done when there is an explicit gap between the values of the reconstructed elements associated with existent links and those with null links so that a cut-off can be set in the gap. The elements with values larger than the cut-off value are regarded as corresponding to actual links, while those with values smaller than the cut-off value correspond to zeros in the adjacency matrix. However, ambiguity may arise due to the dispersion in the element values. It is thus necessary to determine the cut-off value reliably. In the ideal situation, the element values in X exhibit a bimodal type of distribution, with one peak (centered at some positive value) corresponding to actual links and another (centered about zero) to null connections. However, due to the fluctuations in the distribution, the two peaks may overlap, making it difficult to set a cut-off. Our idea is to use coarsegraining to suppress noise but preserve the useful information. As shown in Supplementary Fig. 1(a) , we calculate the local accumulation of the element values in the entire distribution region. The size of each bin can be adjusted according to the width of the peak centered at zero, with the constraint that the bin size should not be larger than the width of this peak. Empirically, 5 to 10 bins are effective to reduce noise while retaining the information about the link structure. Implementing such a coarse-grained process can in general make the \"valley\" between the two peaks distinct. The coarse-graining works even when there was no clear gap between the two peaks. In this case, we choose the lowest point in the valley between the two peaks to be the cut-off value.\n\nIn general, the predicted values of null connections are sufficiently close to zero, enabling us to set a cut-off for different networks, as shown in Supplementary Fig. 1(b) . We see that the four different types of networks share the same cut-off value of about 0.13. This cut-off value offers a reasonably way to separate the existent links from null connections, offering a robust procedure to identify actual links in the reconstructed vector X.\n\nLocating hidden source. Our method to ascertain and locate hidden source is based on a statistical analysis. In particular, from different segments of the time series, we calculate the structural variance defined in equation (10) in the main text. For any neighboring node of the hidden source, the value of \u03c3 i is usually much larger than those of nodes that are not in the immediate neighborhood of the hidden source. The \u03c3 i values can thus be used to reliably identify the neighboring nodes of the hidden source. Similar to the task of distinguishing actual links from null connections, we generate the statistical distribution of \u03c3 i and identify those nodes with abnormally large \u03c3 i values, which can be accomplished by using the coarse-grained method, as shown in Supplementary Fig. 1c . After accumulating the bins, a well-defined gap between the small and rare large values of \u03c3 i emerges. A cut-off can then be set within the largest gap, where the nodes associated with the large \u03c3 i values are identified as the immediate neighbors of the hidden source. The trade-off measures of locating the hidden source in Table I in the main text are result of this statistical analysis.\n\nAn alternative approach to identifying the immediate neighbors of the hidden source is to define the following quantity:\n\nwhere j denotes the column, x\n\nij represents the element values in the vector X i of node i inferred from the kth group of the data, \u27e8x ij \u27e9 = (1/g) \u2211 g k=1 x k ij is the mean value of x ij , and g is the number of data segments. As shown in Supplementary Fig. 1(d) , there is a clear gap between the \u03c3 * i values associated with the neighbors and those with nodes not in the immediate neighborhood. Analogous to the method based on \u03c3 i , different networks share approximately the same cut-off value of about \u03c3 * i = 0.9.\n\nReconstruction accuracy depending on nt for a variety of networks. Supplementary Figs. 2 and 3 show the success rates of reconstructing a number of model and real networks as a function of the fraction nt of base strings. For all networks studied, high recovery accuracy in a wide range of the values of nt is achieved. However, for nt close to unity, there is a slight decrease in the success rates. This can be attributed to the intrinsic characteristics of the compressed-sensing algorithm. Recall that compressed sensing is developed for solving under-determined systems with many more unknowns than the number of equations [12] . In order to assure a unique solution of the under-determined system, the constraint of sparsity must be imposed. Mathematically, if under such a constraint a unique solution indeed exists, the compressed-sensing framework enables recovery of the solution. For nt = 1, the matrix \u03a6 becomes a square matrix, indicating that the system is no longer under-determined. Although the compressedsensing algorithm is not restricted to under-determined systems, our results demonstrate that it performs better for under-determined systems than for nt = 1, as reflected by the decrease in the success rates when nt approaches to unity. However, since our method would typically be applied with limited data, the reconstruction accuracy can be still assured by using nt < 1 (so that nt = 1 is naturally avoided) in the process of setting up the compressed-sensing equation Y = \u03a6 \u00b7 X.\n\nMinimum length of time series. Figure 4 shows the minimum absolute length t min of time series required for achieving 95% success rate of the geometric averages of SREL and SRNC versus the network size N for SIS and CP dynamics on NW and BA networks. It can be seen that t min increases slightly as N is increased and the slope in a logarithmic plot is much less than 1, indicating low data requirement even for large networks. This can be explained by the fact that different base strings may share some common strings in the reconstruction. The relative minimum length n min t resulting from t min /N is shown in Figs. 3(c,d) in the main text. One can see that n min t is considerably reduced as N is increased. In general, when a sufficient number of base strings are available so that other strings can be chosen using the threshold parameters in time series, our reconstruction method can offer quite high accuracy, nearly regardless of the network size.\n\nSuccess rates affected by average node degree. Supplementary Fig. 5 shows the success rates as a function of the average node degree \u27e8k\u27e9 for SIS and CP dynamics on ER, NW and BA networks. The success rates decrease from 1 slightly as \u27e8k\u27e9 is increased to a large value, e.g., \u27e8k\u27e9 = 18. The slight decrease is due to two factors.\n\nFirst, for high values of \u27e8k\u27e9, the vector X is less sparse, leading to small prediction errors under the compressed-sensing paradigm. Second, for a node with a large number of neighbors, the probability of its being infected at each time is large, insofar as a fraction of its neighbors are in the infected state. In this case, it is difficult to ascertain which neighbor passes the disease to the node. For example, in the extreme case where a node remains infected all the time, it is practically impossible to determine from which of its neighbors the infection comes. The combined effect of the two factors accounts for the small decrease in the success rates with the increase of \u27e8k\u27e9.\n\nIn scale-free networks, a small fraction of hub nodes are usually more difficult to be reconstructed because of the many neighbors that each hub possesses, leading to slightly lower success rates than those associated with ER and NW networks of similar average degree (e.g., \u27e8k\u27e9 < 14). However, for larger values of the average degree, scale-free networks perform better than the other two types of networks. This can be understood, as follows. In a scale-free network, the heterogeneous degree distribution indicates that most nodes have small degrees. In contrast, in the ER and NW networks, the degree distributions are approximately homogeneous. Thus, for the same value of \u27e8k\u27e9, the degrees of most nodes in scale-free networks are less than those in the ER and NW networks, resulting in slightly larger success rates when reconstructing scale-free networks. In fact, in the presence of high-degree nodes, we can provide a heuristic method to improve the success rates by taking advantage of the \"conflicts\" arising when matching the neighborhoods of different nodes, since the neighborhood of each node is recovered independently of others. The errors in the reconstruction of the whole network will induce certain inconsistency in predicting the presence/absence of links among nodes. Due to the relatively large reconstruction errors associated with higher-degree nodes, it is reasonable to accept the predicted links centered at smaller-degree nodes rather than that centered at high-degree nodes whenever such a conflict arises. This way the errors associated with high-degree nodes can be considerably reduced. This method can further improve the reconstruction accuracy of scale-free networks or in any networks where a small set of high-degree nodes exist.\n\nReconstruction of vector X in the presence of partially unobservable nodes. Supplementary Fig. 6 shows the element values in the reconstructed vector X when there are missing time series. In principle, the local connection topology of nodes without time series are impossible to be reconstructed. We thus remove these nodes together with their links from the network and reconstruct the rest of network. Interestingly, we find that even when half of nodes' time series are missing, i.e., n m = 0.5, there is still a clear gap between the predicted values of actual links and null connections. This phenomenon suggests that missing time series only affect the width of the gap, but otherwise has little influence on the reconstruction. Indeed, in Figs. 4(c,d) in the main text, the success rates remain close to 1 as n m is increased.\n\nWe stress that the externally inaccessible nodes are somewhat similar to the hidden source in the sense that their time series are not available. The difference between the hidden source and the unobservable nodes lies in the fact that the source is always infected with relatively higher probability to pass the disease to its neighbors. The absence of connection from the source when establishing the reconstruction form of its immediate neighbors results in the large structural variance that becomes the fingerprint of the immediate neighbors. In contrast, the effect of unobservable nodes is relatively minor as compared to that of the hidden source, so their connections to the other observable nodes are difficult to be inferred. Consequently, in the presence of externally inaccessible nodes, the structural variance still allows us to exclusively locate the hidden source.\n\nRobustness with respect to fluctuation in infection rates. We have also investigated robustness of our reconstruction method with respect to errors in specification of the underlying diffusion process. This is of practical importance, since it is not possible to describe any realistic dynamical process by mathematical model with perfect accuracy. To be concrete, we assume that, in the spreading process, the infection rate \u03bb of each node exhibits random fluctuations in a range with probability p \u03bb at each time step. In this case, ideally the standard SIS and CP dynamics are no longer applicable. However, since such fluctuations are not known a priori, we reconstruct the dynamical process as if the infection rates were constant. This way the effect of model errors can be assessed. Supplementary Fig. 7 shows the success rate of network reconstruction as a function of p \u03bb . We see that quite satisfactory reconstruction of the network structure can still be achieved, regardless of the value of p \u03bb . This result holds for both SIS and CP processes on small-world and scale-free networks, suggesting that our reconstruction method is applicable to real propagation processes in spite of model errors.\n\nInferring infection rates. After all links have been successfully predicted, the infection rates \u03bb i can be recovered by equation (17) for SIS dynamics and equation (18) for CP dynamics in the main text. Supplementary Fig. 8 shows the standard box-plots of differences between the actual and predicted \u03bb i values for the two types of dynamics coupled with three types of network topologies. We find that for ER and NW networks, the predicted \u03bb i values of all nodes are quite close to the actual values. In contrast, for BA networks and SIS dynamics, the predicted values of \u03bb i associated with a small number of high-degree nodes have relatively large errors. This is due to the inaccuracy in the reconstruction of the neighboring nodes of the high-degree nodes. However, the prediction errors can be reduced by replacing links centered at high-degree nodes by the recovered links centered at the neighbors of these nodes. Insofar as the neighborhood of a node is precisely predicted, the value of \u03bb i associated with the node can be accurately reproduced as well.\n\nInferring recovery rates. The heterogeneous recovery rates of nodes can be inferred from time series in a more straightforward manner than the infection rates. This is because the recovery rate \u03b4 i of an arbitrary node i is an intrinsic characteristic of the node, which does not depend on the properties of its neighboring nodes. The information about \u03b4 i is contained in the transition from the infected to the susceptible states associated with the probability P 10 . We thus propose the following strategy to estimate \u03b4 i :\n\nwhere n is the length of the time series, P 11 is the probability for the node to remain infected, \u2211 n\u22121 j=1 S i (t j ) S i (t j + 1) denotes the number of 11 pairs and \u2211 n\u22121 j=1 S i (t j ) represents the total number of 10 and 11 pairs in the time series. Equation (6) means that the recovery rate can be assessed by the fraction of 10 pairs in the total number of 10 and 11 pairs in the time series. The predicted recovery rates are shown in Supplementary Fig. 9 for SIS and CP dynamics coupled with three types of networks. The standard box-plots demonstrate that the reproduced recovery rates are in good agreement with the true rates with negligibly small differences for all studied cases. Our method thus enables accurate reconstruction of the natural diversity of nodes in terms of the heterogeneous infection rates, the recovery rates, and the node degrees.\n\nIt is noteworthy that, when the recovery rates \u03b4 i (t) of individuals are time-dependent, our method can still be used to reconstruct the network structure and the quantitative measures of the individual diversity. This is because our reconstruction framework does not require any information about the recovery rates.\n\nIn fact, all useful information for uncovering the network structure and the infection rates is contained in the 00 and 01 pairs rather than in the 10 or 11 pairs, where the latter are used to estimate the recovery rates. The robustness of our reconstruction framework against stochastic fluctuations in the recovery rates of individuals renders applicable our framework to realistic stochastic systems.\n\nNumerical simulation of SIS. Initially, a fraction of nodes are set to be infected (state 1), where a node in this group can be either the external source or a node within the network. Nodal states are updated in parallel. For a susceptible node i of degree k i , at time t, if m neighbors are infected, the probability that it will be infected by each of the m neighbors is \u03bb i . For each infected neighbor, a random number is drawn and checked if it is smaller than \u03bb i . If yes, node i will become infected at the next time step; otherwise, nothing happens and the process is repeated for the next infected neighbor until i becomes infected or all infected neighbors have been tested. At the same time t, an infected node j has the rate \u03b4 j to recover to the susceptible state at the next time step.\n\nNumerical simulation of CP. The initial configuration and state updating rules are the same as those for SIS. At time t, for a susceptible node i, one of its neighbors is randomly chosen. If the chosen neighbor is infected, i will become infected at time t + 1 with the probability \u03bb i . Computationally, a random number is drawn to compare with \u03bb i : if the number is smaller than \u03bb i , i will be infected at t + 1; otherwise, i will remain in the susceptible state. The recovery rule is the same as that for SIS. A Monte Carlo time t is referred to the situation where all the states at t + 1 have been updated according to their states at t.\n\nWe have carried out a large number of simulations to validate our reconstruction framework. We find that our approach is robust and broadly applicable to various combinations of the network structure and spreading-dynamical processes. The results are generally independent of the parameter setting of the network model and the initial configuration of the diffusion dynamics.\n\nFor SIS dynamics, to derive equation (15) in the main text, we have used the following approximation:\n\nwhere P 01 i (t) is the infection probability of node i at time t. Equation (7) is valid if P 01 i (t) is relatively small. For simplicity, we denote P 01 i (t) byP and the ensemble average involvesP \u03bd for \u03bd = 1, 2, \u00b7 \u00b7 \u00b7 , l. We can write\n\nBecause of 0 <P \u03bd < 1, according to the binomial expression, we have\n\nIfP \u03bd is relatively small, keeping only the zero-order and first-order terms suffices, yielding\n\nInserting equation (10) into equation (8), we have\n\nSinceP \u03bd < 1,P \u03bd /l \u226a 1, we can omit O 2 terms. As a result, equation (11) indicates that approximation [equation (7)] is justified for relatively small values of the infection probability.\n\nWe have also numerically tested the approximation, as shown in Supplementary Fig. 10 . We can see that when the infection rate \u03bb i is randomly distributed in the range (0.2,0.4), both sides of equation (7) are quite close to each other with small deviation from the diagonal line. Moreover, even if the fitted line deviates from the diagonal line, the linear correlation can still assure the success of reconstruction based on equation (15) in the main text. The difference between the slope of the fitted lines and unity will become a small coefficient added to the neighboring vector X, which will slightly affect the element values in X, affecting only the width of the gap between actual links and null connections, but not the cut-off and identification of links.\n\nSupplementary Note 9: Dependence of success rates on threshold parameters In our reconstruction framework, there are two thresholds, \u2206 and \u0398, which are used in the calculation of the normalized Hamming distance. Here, we discuss the possible region in the parameter plane spanned by the two thresholds, which ensures full reconstruction of the network. Each threshold value is bounded in the unit interval, as stipulated by the definition of the normalized Hamming distance. Specifically, \u2206 is employed to approximate the law of large numbers because of the difficulty to find two strings with absolute zero normalized Hamming distance. With a string S \u2212i (t \u03b1 ) chosen to be a base, we select suitable strings through the threshold \u2206:\n\nWe can then use the states S i (t) of node i at time t belonging to the available strings to estimate the infection probability of i at t \u03b1 by\n\nThe threshold \u0398 is used to provide linearly independent equations containing structural information to be revealed by the compressed sensing algorithm. In general, an equation can be found based on a base string, e.g., S \u2212i (t \u03b1 ). We choose a series of base strings to construct a set T base , in which each pair of vectors satisfy\n\nwheret \u03b1 andt \u03b2 stand for the time of the two base vectors in the time series. The number of nontrivial equations is determined by the number of base strings. The dependence of the success rates on the values of the thresholds \u2206 and \u0398 is shown in Supplementary Figs. 11 and 12 for SIS and CP dynamics, respectively. If and only if both SREL and SRNC approach unity, the network is regarded as having been exactly reconstructed. In Supplementary Figs. 11 and 12, the smaller value of SREL and SRNC is defined to be the success rate. We see that for both SIS and CP dynamics coupled with four types of networks, there is a broad region in the (\u2206-\u0398) parameter plane that warrants full reconstruction with success rate close to 100%, indicating that our framework is reliable.\n\nThe broad region leading to full reconstruction in the parameter plane implies that our framework is robust against measurement noise embedded in the binary time series of nodal states. Suppose that a string belongs to a base string in the sense that the normalized Hamming distance between them is less than the threshold \u2206. In the presence of noise, some states of nodes in the strings are reversed, resulting in the change of the normalized Hamming distance that may become larger than the threshold \u2206. However, due to the broad region of the applicable values of \u2206, the change of the normalized Hamming distance is tolerable and the string is likely to still belong to the base string and can be used to estimate the infection probability P 01 (t), so the selection of strings with respect to a base string based on the normalized Hamming distance is robust against noise. Analogously, identifying different base strings in terms of the threshold \u0398 is insensitive to noise due to the broad region of \u0398. The other factor by which noise may affect the reconstruction accuracy is the process of averaging over node states for obtaining \u27e8S i (t \u03b1 + 1)\u27e9 and \u27e8S i (t)\u27e9. Since the error in measuring node states induced by noise will be naturally reduced by the average, there is resistance to noise. In general, noise can be tolerated in all steps required to construct the vector Y and matrix \u03a6, ensuring robustness of our reconstruction framework against noise in time series.\n\nSupplementary Note 10: Network models and data sets ER network model [15] . In the ER random-network model, N isolated nodes are given initially. Each pair of nodes are connected with probability p (0 < p < 1). The average node degree \u27e8k\u27e9 is approximately N p. The node degrees obey the Poisson distribution.\n\nWS network model [16] . Initially a ring-like structure of N nodes is constructed, where every node has the same number of local connections. Each link is then randomly rewired with probability p. For p close neither to zero nor to one, the network exhibits the small-world effect, i.e., highly clustered structures but with short average distance. In our work, p = 0.2 is used for obtaining all the results on the small-world WS networks.\n\nNW network model [17] . The NW model is capable of generating small-world networks while keeping the network as a single component. A ring-like structure of N nodes is generated initially, where the number of local connections of every node is the same as that of any other node. Random links are then added into the network. The average degree of the network is determined by the initial local connections and the number of shortcuts added. The initial ring is of coordinate number 1 and for each node, a link from the selected node is added to another randomly selected node. In our paper, the average node degree is \u27e8k\u27e9 = 4 for NW networks.\n\nBA network model [18] . The BA model generates scale-free networks with power-law degree distribution. Initially, a core consisting of a small number of nodes with random connections is constructed. At each time step, a new node with m links is added to the existent network according the preferentialattachment mechanism: the probability of a new node connecting to an existent node is proportional to its degree. Nodes are continuously added to the network until the pre-specified network size N is reached. The average node degree is \u27e8k\u27e9 = 2m.\n\nReal networks. The details of the real social networks studied in this paper are presented in Supplementary Table 1 , which includes the number N of nodes, the number L of links, and description of the networks."}