{"title": "Zoonoses in Wildlife: Integrating Ecology into Management", "body": "bovine tuberculosis. Ecologically based control, including the management of conditions leading to spill-overs into target host populations, is likely to be more effective and sustainable than simple reductions in wildlife populations alone.\n\nParasites and pathogens in wildlife are a natural part of biodiversity. An abundance of theoretical studies indicate that they have key roles in ecological and processes, including the regulation of population size May and Anderson, 1979) and the maintenance of genetic diversity (May and Anderson, 1983; Read et al., 1995) .\n\nOccasional field experiments have also demonstrated impacts of sublethal infections on reproductive rates (Munger and Karasov, 1991) and susceptibility of wildlife to predation (Hudson et al., 1992) . Yet the vast majority of empirical studies consider wildlife pathogens and parasites only if they threaten the health of humans or their domestic animals, and often overlook their natural history. Partly this reflects the priorities of funding agencies; and their concerns are not trivial. More than 70% of emerging (or re-emerging) pathogens of humans are thought to have wild animals as their natural reservoirs (Taylor et al., 2001) . Examples include SARS-CoV (severe acute respiratory syndrome coronavirus), avian influenza A (H5N1) virus (bird flu), NIPAH virus, hantavirus and West Nile virus. The economic imperative for controlling zoonoses that affect domestic stock can be very strong. For example, bovine tuberculosis (bTB) in the United Kingdom undoubtedly has a reservoir in wild badger populations, and the direct cost of the disease to agriculture is projected to reach \u00a31 billion by 2011 (Department for Environment, Food and Rural Affairs (DEFRA), 2004).\n\nIt has been recognised relatively recently that disease can also pose a serious threat to the survival of endangered wildlife (Lyles and Dobson, 1993; May, 1988; Smith, 1982) . This can either be through direct mortality, where losses are greater than the birth rate, or through effects on birth rate, longevity and survival, which suppress the population size to a level that renders it susceptible to extinction by stochastic effects (Table 8 .1). Generalist pathogens with a wide host range are particularly problematic, since even virulent species can persist in alternative hosts while driving the rare host to extinction (Begon and Bowers, 1995) . Although remaining low on the list of priorities compared with other threats such as habitat loss, efforts are therefore sometimes now made to control infectious diseases for conservation reasons. Examples of recent successes include the control of canine distemper virus in black-footed ferrets (Mustela nigripes) (Williams et al., 1988) , and rabies in African wild dogs (Lycaon pictus) (Hofmayer et al., 2004) and Ethiopian wolves (Canis simensis) (Haydon et al., 2004) .\n\nIn Europe alone, there are at least 35 zoonotic parasites and pathogens in wildlife that are known to be important either to public health or the agricultural economy (Artois et al., 2001) . For many other infectious agents, such as Cryptosporidium parvum, the epidemiological role of wildlife is unknown. Despite the many attempts to control zoonoses in wildlife, the success rate is poor. Typically, measures are adopted as crisis management (usually in the form of culling) following an outbreak, with little understanding of the ecology of species or its relationship to the pathogen. Crisis management also means that proper scientific designs with appropriate controls are often lacking; it is therefore difficult to evaluate the effectiveness of a given intervention. Even where there is a long history of attempts to control a disease through the management of a wildlife reservoir, the results have not been encouraging. For example, efforts over the last 30 years to control bTB in cattle in the United Kingdom by culling of badgers has failed to yield significant benefits, with analyses of the recent randomised controlled trial of badger culling concluding that culling could not contribute meaningfully to future control strategies for bTB (Donnelly et al., 2005) . Similarly, the culling of foxes has been discounted as a means of rabies control in western Europe (Blancou et al., 1991) . However, success has been achieved through the use of widespread vaccination (administered via bait) (Aubert, 1995) . Rabies is currently the only example of a widespread strategy of vaccination being favoured over the control of the host species (Artois et al., 2001) .\n\nThis review considers the ecology of zoonoses in wildlife and the links between infection in wildlife and humans or livestock. It proposes that a shift to ecologically based control, explicitly considering the natural history of wildlife hosts and their pathogens, is crucial in minimising the risk presented to humans, domestic animals and endangered species from zoonoses. This approach will also yield benefits for the conservation and welfare status of wild animals. Notwithstanding the complexities of specific relationships, the probability of a zoonosis being passed from wildlife into another host population, be it humans, domestic stock or an endangered wild species, is always influenced by several key parameters. These are the intensity of infection in the reservoir hosts; the size, or depending on the case, the density, of the infected host population; the degree and nature of the contact between infected individuals (or infectious particles in the environment such as infected faeces); and the susceptibility of the in-contact animal. (For indirectly transmitted parasites and pathogens, the role of vectors and/or intermediate hosts must also be considered.) Whether the zoonosis persists after initial invasion is also determined by the new host's population size. A great variety of models has been developed to describe the transmission dynamics of macro-and microparasites, taking into account the nuances of particular host and parasite population structures (Diekmann and Heesterbeek, 2000; Heesterbeek and Roberts, 1995; Scott, 1988; Scott and Smith, 1994 ). Yet empirical research has lagged behind the theoretical advances. The legacy of researchers like Elton and Chitty (Chitty, 1952 (Chitty, , 1954 Elton, 1931; Elton et al., 1931 Elton et al., , 1935 , who sought not only to describe pathogens but to understand their ecological role, has not been sustained. (There are a few notable exceptions, including the long-term studies of small mammals in the north of England (Beldomenico et al., 2008a,b; Feore et al., 1997) ; rodent reservoirs of hantavirus in the United States (Calisher et al., 2007; Mills et al., 1997) and macroparasite infections in laboratory models (Ehman and Scott, 2004; Scott, 1988; Scott and Anderson, 1984; Scott and Smith, 1987) .) This deficiency was noted in a key text in the field in 1995 (Grenfell and Dobson, 1995) and again in the follow-up publication in 2002 (Hudson et al., 2002) .\n\nWe lack even species lists of parasites and pathogens for most, if not all, wild animals. While pathogens that affect international trade are reported to the World Organization for Animal Health (OIE), and many of these affect wildlife (see Artois et al., 2001) for the lengthy list of those likely to affect wildlife in Europe), there is no agreed systematic programme of surveillance (Kulken et al., 2005) . Even where programmes exist, they lack integration with surveillance in humans and domestic animals at both local and international scales. Disease surveillance in wildlife is usually driven by outbreaks in humans or domestic animals (Childs, 2004) . Virulent pathogens are, therefore, more likely to be detected than more benign ones (Williamson et al., 1986) . Such studies are also, by their nature, not designed to screen for a range of pathogens, so opportunities to investigate the epidemiology and ecology of coinfections are often lost. Systematic surveillance of representative samples of the population is difficult and time consuming. Yet prevalence estimates can be seriously skewed if the only data available are derived from passive surveillance of carcasses. Not only are estimates likely to be too high if they are based on samples of wildlife found dead or sick by the public, but even road kills and game bags are likely to over-represent certain population classes (such as dispersing juvenile male mammals) and animals in compromised health.\n\nDisease-responsive surveillance also offers little information on the frequency with which transfer events are likely to occur. For example, many of the 'spectacular' epidemics derived from bat viruses, such as Hendra virus, Nipah virus, SARS-CoV-like virus, have been observed only a small number of times. We do not know why this should be the case. Is transfer of zoonoses from bats to terrestrial vertebrates generally rare due to a lack of appropriate contact? Or is there regular inter-specific transmission of other viruses but these go undetected because they lack the extreme pathogenicity of Hendra and Nipah viruses to stimulate screening efforts? Pro-active surveillance of wildlife and of apparently healthy human or livestock populations could help answer these questions.\n\nstructuring and species specificity of a pathogen in wildlife hosts?\n\nScreening for European bat Lyssaviruses in Europe is an exemplary case of research stimulated by public health concerns. The first recorded European bat rabies case was in Hamburg in 1954 and several other cases were identified subsequently (King et al., 2004 ). Yet surveillance of bats was not really pursued until a woman in Denmark was bitten by a serotine bat (Eptesicus serotinus) infected with European bat Lyssavirus 1 (EBLV1). Since then more than 800 rabies-positive bats have been identified across Europe; the vast majority being serotine bats infected with EBLV-1 (Harris et al., 2006) . In the United Kingdom, screening efforts were intensified following the death of a man in Scotland from EBLV-2 in 2002, after apparent contact with many bats in the United Kingdom and Europe (Fooks et al., 2003) . In contrast with classical rabies (RABV) there is now good evidence that at least some bats (and possibly other animals) can produce neutralising antibodies and survive EBLV infection for at least 6 years (Serra-Cobo et al., 2002; van der Poel et al., 2000) , and experimental models suggest that EBLV-2 might be inherently less virulent than EBLV-1 (Vos et al., 2004) . EBLV-2 also appears to have a much more restricted geographical range than EBLV-1, and small numbers of positive bats have been identified in the United Kingdom, Switzerland, the Netherlands, Denmark and Germany (Department for Environment, Food and Rural Affairs (DEFRA), 2008; Racey et al., 2004; Vos, 2007; Vos et al., 2007) . These cases have all been in the closely related Daubenton's (Myotis daubentoni) and pond bats (M. dasycneme).\n\nStructuring of EBLVs therefore is apparent from these data both across geographical areas and across species. The serotine bat occurs over most of Europe, extending north to 55 latitude (England south of the Wash estuary, Denmark and southern Sweden); Daubenton's bats are common across Europe; and the pond bat is present in a wide band across central and eastern Europe (between 48 and 60 latitude; absent from the United Kingdom (Schober and Grimmberger, 1997) ). Yet neither EBLV-1 nor EBLV-2 appears throughout their hosts' ranges. Some suggest that in the case of EBLV-1, this may be because long-distance travel is uncommon in serotines, the primary host (Vos et al., 2007) . Yet the species is widely distributed, and it is unlikely that there are gaps between populations that could not be travelled with relative ease; dispersing movements of up to 300 km have also been recorded (Hutterer et al., 2005) . Interestingly, EBLV-2 also appears to have a patchy distribution, despite its host species, at least in continental Europe, being migratory over long distances (Vos, 2007) . Whether the geographical distribution is, in reality, less patchy than it currently appears requires co-ordinated surveillance effort and a willingness by statutory authorities to publish test results even if they are negative. It is clear that active surveillance (systematic screening of bats in the wild) has been undertaken in a few countries only, and passive surveillance (submission of dead bats by members of the public for screening) has involved few, if any, animals in a number of European countries, including Portugal, Ireland, Greece, the Czech Republic and Slovakia (Racey et al., 2004) .\n\nA range of European bats, most of which are common and widespread, has been identified as having active EBLV infections in addition to the key hosts (Table 8 .2). It is striking then that the vast majority of reported cases come from just three species. Undoubtedly, the numbers of bats of each species submitted by the public does not match their abundance in the wild, but is influenced by the closeness of their contact with humans (and their cats, which are a major cause of bat mortality). For example, few woodland specialists have been submitted, whereas bats that frequently roost in houses, particularly pipistrelles, long-eared bats and possibly serotines, are over-represented (Harris et al., 2006) . Even active surveillance does not attempt comprehensive surveys of all species in proportion to their abundance: instead, it focuses on the three species already identified as being important sources of EBLVs, potentially failing to estimate properly the prevalence in others.\n\nDespite these limitations, the data clearly suggest that species partitioning occurs. The common pipistrelle bat is known to be susceptible to experimental infection with EBLV-1 (Kuzmin and Botvinkin, 1996) . Yet none of the more than 10,000 pipistrelles (P. pipistrellus and also P. pygmaeus, which is cryptic with P. pipistrellus) surveyed in the Netherlands, France and the United Kingdom (Harris et al., 2006; Picard-Meyer et al., 2006; van der Poel et al., 2005) has proved positive for the virus. Whether structuring across bat species driven by differing immunoresponsiveness to particular EBLV types, by a lack of transmission opportunities or by other mechanisms, is unclear. Multi-species summer, and particularly hibernation, roosts are known, though the amount of inter-specific direct contact appears to vary by season and species. For example, bats in houses and trees tend to use single-species roosts, even if more than one species is present at the site (Park et al., 1996) . There may be more potential for inter-specific contact at key underground sites used by bats. In a survey of more than 76,000 bats of 13 different species roosting in caves in Turkey, it was noted that multi-species clusters frequently occurred in the post-hibernation season, but not during hibernation; and the horseshoe bats (Rhinolophus spp.) only ever formed single-species clusters (Furman and \u00d6 zg\u00fc l, 2004) . Many bat species also use swarming sites-enclosed areas often in and around caves-for display purposes. At these sites, hundreds or thousands of bats of mixed species congregate (Glover and Altringham, 2008; Parsons et al., 2003) . The amount of contact, for example, via urine or aerosol droplets, between species at these events is unknown. More field research is needed to investigate the opportunities for disease transmission across bat species, and across geographical barriers.\n\nInterestingly, in the United Kingdom, only a single case of exposure to EBLV-1 has been found (the test was able to detect exposure rather than live virus), whereas in other European countries with EBLVs, the apparently more pathogenic EBLV-1 is more common (DEFRA, 2008; Racey et al., 2004; Vos, 2007) . To date, seven Daubenton' bats (Myotis daubentoni) in the United Kingdom have been found to have EBLV-2 infection (DEFRA, 2008) , the latest case being diagnosed in May 2008. It is notable that although a low prevalence (around 2%) of seropositivity was detected during active surveillance of Daubenton's bats in Scotland, live virus was not isolated from any of them (Brookes et al., 2005) . Similar results were found in an active-surveillance study in Spain, which found that up to 60% of individuals in some colonies were seropositive for EBLV-1, but the prevalence of active infection was less than 1.1% (Serra-Cobo et al., 2002) . It is currently difficult to interpret these results, but the vertical transmission of antibodies, as well as acquired immunity, is a possibility.\n\nWhile it is clear that EBLV-1 and EBLV-2 can cause deaths in unvaccinated humans, whether natural immune responses and cryptic recovery (i.e., without the virus having invaded the central nervous system and become symptomatic) are possible remains unknown. It is unfortunate that there was been no serological testing of bat workers in the United Kingdom to establish the natural prevalence of neutralising antibodies to EBLVs prior to 2002. Since that date, following the fatality in Scotland from EBLV-2, it has been officially recommended that bat handlers be vaccinated against rabies. The take-up rate of vaccination has been very high. This understandable management of the public-health crisis means that it is now not possible to gather information that would have helped indicate the pathogenicity of EBLV-2 to humans, and also whether exposure was more widespread than the single fatal case. It certainly appears that despite other species being susceptible in experiments, natural spillover into other non-bat hosts to produce clinical symptoms is rare, with the only known case for EBLV-1 being a single stone marten . There are no reports of spill-over for EBLV-2.\n\nThe apparently simple task of establishing the prevalence of a pathogen in wild animals can be fraught with difficulty. Even assuming that a reasonably random cohort can be sampled, there is usually no opportunity to repeat 'live-tests' in cases of diagnostic uncertainty. Establishing values for other key parameters is equally problematic. Fundamental data on the sex-and age-distributions of infection are often not recorded. Sometimes this is because the surveys (particularly for 'crisis management') were not designed with research in mind. Sometimes it is practically difficult for the data to be acquired. Bats, for example, can live more than 30 years, yet it is impossible in the absence of long-term banding studies on the particular population being surveyed, to judge the age of animals with much greater accuracy than 'juvenile', 'young of the year', and 'adult'. Weight is frequently used as a surrogate for age or maturity, particularly in studies of rodents, but there can be difficulties in distinguishing age from dominance effects, since both are correlated with body mass. The size of the population (or its density) is also often estimated with huge margins of error, as surveyors simply lack the time to undertake detailed ecological studies in addition to collecting clinical samples.\n\nDistinguishing between different burdens of infection (particularly for macro-parasites) and stages of infection (particularly for micro-parasites) is frequently overlooked. This makes it difficult to use the data to parameterise epidemiological models. For example, animals infected with bTB but in which the bacilli are encysted present no risk of transmission at that particular time point, yet these groups are often combined when data from post-mortem examinations are used. The fact that the disease may reactivate at some future time (measured by the overall prevalence) is not relevant to the calculation of the basic reproductive rate of the disease R 0 .\n\nBy conducting large-scale surveys of representative populations of wildlife on British farms, workers were able to build deterministic models to investigate the likelihood of the disease persisting in each host species. Initially they assumed that no between-species transmission was present. Using the prevalence of infectious individuals, together with field data on population structure and density derived from the same sites, they computed the basic reproductive number R 0 for each of the species. The analyses showed that even when the maximum likely prevalence was assumed (based on the upper 95% confidence limit), the R 0 (the basic reproductive rate of the disease) ranged from just 1.003 in wood mice to 1.05 in rats. (The lower confidence intervals for prevalence always gave R 0 values that were <1.0; Mathews et al., 2006b .) It is therefore unlikely that the disease would persist within single-host systems in the wild: the animals are unable to pass on the infection to their conspecifics at a rate high enough to maintain the disease. The findings are robust to underdiagnosis of infection: to affect the R 0 materially, the prevalences would need to be have been underestimated very substantially. If, instead of single-host models, we assumed multiple-host systems, then higher prevalences should have been observed in the field than those recorded. Alternatively, to achieve the prevalences seen in reality, the withinspecies transmission rate would have to be even lower than the very low value calculated. They have therefore been able to conclude that multi-species transmission of bTB within farmland wildlife communities appears unlikely.\n\nPerhaps the best example of long-term epidemiological studies in wildlife leading to epidemiological models of value to human health comes from studies of hantavirus infection in the United States. Large-scale studies of several thousand rodents were conducted by four separate research teams, but were co-ordinated by common methodologies (Calisher et al., 2007; Easterbrook et al., 2007b; Glass et al., 2007; Mills et al., 1997) . Using longterm datasets, with repeated trapping at set grids, the teams were able to explore key components of the transmission pathway. Seropositivity was higher in males and in heavier animals, suggesting horizontal transmission among adult males. Decreasing prevalence with age among the youngest deer mice suggests that infected dams confer passive immunity to pups. In the main host of Sin Nombre virus, the deer mouse (Peromyscus maniculatus), gender, age, wounding, season and local relative population densities were linked with the period prevalence of antibody (used as a marker of infection). Nevertheless, antibody prevalence and some of the risk factors associated with antibody prevalence, such as relative population density, gender bias and prevalence of wounding, varied significantly among sites and even between nearby trapping arrays at a single site. This suggests that local micro-site-specific differences play an important role in determining relative risk of infection in rodents and, consequently, in humans. These data are now being used in spatially explicit models of the risk of human disease outbreaks (Eisen et al., 2007) .\n\nAs described for bat lyssaviruses, the contact rates between infectious and susceptible individuals (or a vector and a susceptible) is a critical step in the transmission pathway. Yet compared with the effort that goes into improving, for example, the accuracy of a diagnostic test, very little attention is paid to measuring it in the field. This failure may offer some explanation for the difficulties faced in attempted disease control programmes.\n\nFor example, disease is a primary threat to the survival of the critically endangered Ethiopian wolf (Canis simensis). Since the early 1990s, outbreaks of rabies and canine distemper virus (CDV) have had significant impacts on wolf population dynamics Randall et al., 2004 Randall et al., , 2006 . These diseases are maintained in local domestic dog populations, and a programme of dog vaccination was therefore introduced in 1996, with the aim of reducing the population of susceptible dogs and hence the risk of transmission to wolves. Attempts were made to achieve coverage of more than 70% of susceptible dogs during annual vaccination campaigns. This was not an easy task since dogs in Ethiopia are used for guarding cattle and are not tame. Rabies vaccines have high efficacy, and in theory, this level of coverage should prevent rabies outbreaks 95% of the time (Coleman and Dye, 1996; World Health Organization, 2004) . Over 30,000 vaccinations have taken place, and at least initially, the number of rabies cases in dogs declined (Randall et al., 2006) . Nevertheless, a rabies outbreak occurred in wolves in 2003, and could be linked with more than 35 sympatric dogs with clinical symptoms consistent with rabies . Mathews has, therefore, been analysing the reasons for the apparent failure of the vaccination strategy, focusing on the population dynamics of the domestic dogs, using data collected by the Ethiopian Wolf Conservation Programme. The key factor appears to be the growth of the dog population, which, as in other African countries, is keeping pace with, or even outstripping, human populations (Cleaveland et al., 2000; Rhodes et al., 1998) . Eighty-six percent of all households owned dogs, rising to 93.5% in rural areas. Virtually nothing is known about the true contact rate between domestic dogs and Ethopian wolves. It is clear that interactions do occur as wolf-dog hybrids are seen. We might speculate that diseased dogs, and aggressive dogs that are difficult to vaccinate, might be even more likely to interact with wolves than would healthy ones.\n\nSome data are available on the demography of the dogs surrounding the Bale Mountains National Park-one of the strongholds of the remaining Ethiopian wolf population-as a result of a questionnaire survey administered by the Ethiopian Wolf Conservation Programme. The rate of increase in the dog population size appears to be around 5% per annum, and the turnover rate is also high. This creates a constant influx of new susceptibles into the dog population. It is difficult to keep pace with these, given the financial and logistical constraints on the numbers of visits veterinarians can make to each village. There also appears to be some geographical clustering of vaccination effort, and the implications of pockets of unvaccinated dogs on the probability of rabies transmission to wolves is currently being explored.\n\nThe vast majority of efforts to control zoonoses in wildlife hosts, rather than in domestic animals, rely on culling strategies. In simple terms, the idea is to depress the population of the reservoir host to a level at which the disease can no longer be sustained, because the density of infected and susceptible hosts is too low. Few of these culling programmes have systematically examined either the total population size or the level of population reduction likely to be required to achieve the desired endpoint. Even where this has been done, it can be difficult practically, as with vaccination, to achieve the level of coverage desired.\n\nThe strategy to control bTB in badgers and cattle in the United Kingdom has had the culling of badgers as its cornerstone for more than 30 years. When it became evident that gradual badger culling was having little or no impact on the incidence of the disease in cattle, a formal review of the programme was introduced, culminating in a large-scale randomised trial of badger culling. This has demonstrated that there is no clear-cut reduction in bTB in cattle. On the contrary, whereas the incidence of bTB in farms at the centre of 100 km 2 badger culling zones fell by around 19%, the incidence in farms up to 2 km away from the borders of these zones increased by around 29% (Donnelly, 2005) . Similar results were found in comparable areas where clusters of badgers were removed reactively, following nearby outbreaks of bTB in cattle (Donnelly et al., 2003) .\n\nAt least part of the explanation for the failure to achieve effective bTB control is likely to be the alteration in contact rates between infected and susceptible badgers, and also between infected badgers and cattle, as a result of the culling. There has been only one detailed study of M. bovis epizootiology in undisturbed badgers (culling having been suspended at the site, Woodchester Park in Gloucestershire, England in 1978; Delahay et al., 2000) . This study showed that bTB does not spread rapidly at high incidence through badger populations, but rather is distributed patchily among a minority of individuals. Social groups are relatively stable, and long-term dispersal movements are uncommon, though shorter movements do occur more regularly (Rogers et al., 1998; Vicente et al., 2007) . There is a correlation between rates of inter-group movement and the incidence of new infections (Rogers et al., 1998; Vicente et al., 2007) . While spatial clusters of infection exist, there is no strong synchrony between neighbouring groups, suggesting that there is only limited transmission between adjacent social groups (Delahay et al., 2000) . Both individuals and groups are more likely to be incident cases where the social group was diminishing in size, although there is no apparent relationship with group size itself, suggesting that it is the change in group size, and possibly the associated social dynamics, that influences disease risk (Vicente et al., 2007) .\n\nBadger culling operations have clear impacts on the behavioural ecology of the survivors. Woodroffe et al. (2006) found that badger social group ranges increased among survivors within reactive and pro-active culling areas and along the perimeters of pro-active culling areas. Their finding, at a large scale, accords closely with the observations of of individual and group behaviour in two zones of badger removal in England, as well as those of O' Corry-Crowe et al. (1996) in Ireland. In all cases, the spatial organisation of social groups was considerably altered following the culls, with a large increase in the extent of overlaps between social groups (e.g., Fig. 8.1) . The numbers of ranges with which each group overlapped also increased. There was a rather chaotic alteration in population densities (e.g., Fig. 8.2) . In the examples shown, culling was conducted in 1995, largely targeting areas of highest badger density (in effect, the largest social groups). One year later badger density was, unsurprisingly, lower in the culled areas, whereas there had been some increases elsewhere. In 1997, although the population as a whole had not grown, the density remained low, or even fell further, in two removal areas, but increased elsewhere. By 1998 and 1999, the distribution of badgers in the study area was radically different from that at the outset, with some previously high density, but culled, areas remaining depauperate . Thus, while the population density recovered as a whole, the badgers built up in a different place. This sort of radical redistribution has not been reported in undisturbed populations. The changes have not only implications for absolute contact rates, but also the nature of contacts. For example, bite-wounds-an important route of bTB transmission-were more common in the Macdonald et al. ( ) study following social perturbation. 1995 1996 (post-cull) FIGURE 8.1 Badger social territories before and after the selective removal of social groups following bTB incidents in local farms.\n\nSocial structure also plays an important role in hantavirus transmission. In deer mice, both wounding and Sin Nombre virus antibody prevalence increased with mass. Although it occurred in both sexes, the A 1995 C 1997 E 1999 B 1996 D 1998 Badgers removed in BRO increase was much more pronounced in males. Wounding was more frequent in adult males than in adult females, and adults had more wounds than juveniles. The highest rate of infection was seen in individuals with the most wounds. Similarly, in rats (Rattus norvegicus) hantavirus infection (Seoul virus) was associated with both wounding and elevated testosterone levels (Easterbrook and Klein, 2008; Easterbrook et al., 2007a) . It is therefore evident that changes to social structure-for example, by the removal of a dominant male-could have important implications for the epidemiology of a disease.\n\nRather than attempt to control disease by vaccination or culling, an alternative approach is to understand the factors leading to disease outbreaks in the first place and to manage these (Dobson, 2005) . Habitat changes that lead to alterations in population structure or migratory patterns, for example, are likely to affect the risk of zoonotic disease transmission (Dobson and May, 1986) . The effect of habitat fragmentation on disease processes has rarely been investigated, but it has recently been shown that Trypanosoma cruzi infection rates are higher in fragmented than continuous Atlantic forest (Vaz et al., 2007) ; and the risk of Lyme disease in New York is also apparently increased by fragmentation (Allan et al., 2003) . Interestingly, the division of endangered Ethiopian wolf population into small sub-populations, joined by habitat corridors, has been shown to allow rabies control to be achieved using a low-coverage vaccination strategy (Haydon et al., 2004) . The strategy operates by eliminating the largest outbreaks of disease, and so enhances meta-population persistence, rather than by the conventional objective of reducing the reproductive number of the disease to less than one (Haydon et al., 2004) . Human activities that artificially increase, rather than decrease, animal densities also influence disease processes. These increases can be the result of losses of absolute habitat area, or from the provision of supplementary food or water. In the United States, the practice of supplementary feeding of house finches and white-tailed deer has lead to an increase in the incidence of mycoplasmal diseases and bTB, respectively (Hartup et al., 1998; Schmitt et al., 1997) , presumably because of greater opportunities for disease transmission, and possibly also immunosuppressive effects of aggression at the feeding sites. bTB has also been an intractable problem in the British cattle herd, with the incidence rising inexorably since 1979. With a cull of badgers recently being ruled out, somewhat controversially, as offering no meaningful contribution to the long-term control of the disease (Donnelly et al., 2005) , it is worth asking whether consideration of the ecology of the badgers and cattle might help generate workable solutions. Over the past 30 years, along with the increase of bTB, there has also been an increase in badger densities, and it is likely that this contributes at least in part to the disease in cattle. So why have badger populations risen? Might the answer lie in changes in land use? Macdonald and Newman (2006) speculate on a possible role for climate change, with milder winters and hence greater earthworm availability improving survival rates.\n\nChanges could also have occurred in the susceptibility of badgers to bTB and/or of cattle to bTB. For example, the average milk production of a dairy cow rose from 3,750 l in 1970 to 5,395 l in 1995 (Farm Animal Welfare Council, 1997) , possibly to the detriment of the animal's immune status. Similarly, stress resulting from cull-associated social perturbations, or from other changes to habitat, food availability or population density, may have influenced the innate immune response of badgers. Little is known of the physiological responses of free-living wild mammals to poor environmental quality or other potential stressors. An argument has been made for polychlorinated biphenyls (PCBs) and other pollutants contributing to phocine distemper outbreaks (Ross et al., 1996 (Ross et al., , 2000 , but this is has been questioned (O'hea, 2000) . While the role of toxicants is not clear, recent work indicates that high population densities in wild field voles is associated with compromise in haematological and immunological indices. Poor body condition appeared to affect the inflammatory response (as indicated by lower neutrophil and monocyte peaks) and lower immunological investment (as indicated by lower lymphocyte counts (Beldomenico et al., 2008a,b) .\n\nI have found, with co-worker Jon Blount, preliminary evidence of increased oxidative stress (measured by serum malondialdehyde concentration) among non-infected badgers from farms with recent bTB in cattle, compared with those at sites free of bTB ( Fig. 8.3 ). There is also a considerable literature from farm (Moberg and Mench, 2000) , laboratory (Galloway and Handy, 2003) , and free-living aquatic animals (Liney et al., 2006) showing that environment has a strong impact on stress responses, and that these can lead to pathological and pre-pathological alterations in immune function and overall health status.\n\nWhile it might be difficult to intervene directly to reduce the causes of stress in animals, ecologically based interventions that reduce both disease susceptibility and the opportunities for transmission may be possible. For example, in a study of 120 British farms it has been shown that habitat management and cattle herd size were strongly associated with the risk of bTB in dairy cattle (Mathews et al., 2006a) . Reduced risk of bTB was associated with the management of farmland in ways favourable to wildlife, including greater hedgerow availability, a lack of gaps in hedgerows, increasing hedgerow width and the presence of ungrazed wildlife strips adjacent to hedgerows. All of these measures are encouraged by recent European Common Agricultural Policy reforms (2005) . Broadly, habitat could influence cattle contact rates or be associated with agricultural management practices in ways relevant to bTB transmission (such as reduced herd size). Favourable habitat may lower the susceptibility of badgers to bTB or reduce the number of inter-group excursions; alternatively, cattle on hedgerow-rich farms may be at reduced risk of ingesting contaminated soil.\n\nTaking for simplicity just one of the parameters contributing to the effects-the total length of hedgerow-an increase of 1 km/100 ha was associated with a decrease in the odds of bTB by about 12.5% (95% confidence interval 0.3% increase to 26.3% decrease) in univariate analysis. In absolute terms, this equates to the annual risk of bTB changing from the current rate of 9.2% (2,152 confirmed incidents in 23,471 herds in 2004) to 8.1% (1,901 incidents) for herds in the west of England if a policy of moderate hedgerow density increase were adopted. This would mean a reduction of 251 infected herds per year. By comparison, systematic badger culling appears able to reduce the odds of bTB by a maximum of about 19% and may even increase the prevalence in neighbouring areas (Donnelly et al., 2005) .\n\nChange in land use has also been linked to the emergence of two henipaviruses, Nipah virus and Hendra virus in the 1990s, and land use management may therefore offer part of the solution. Both viruses appear to be asymptomatic in their natural hosts, fruit bats (genus Pteropus). They are amplified in domestic animals, pigs and horses, respectively, where they cause mortality, and can then be passed on to humans (Chua et al., 2000; Halpin et al., 2000) . The closeness of RNA sequence match between Pteropus sp., livestock and human isolates of each virus suggests that a sudden change in virulence is a less likely explanation of their rapid emergence into domestic animals and humans than is the ecological change that have affected the habitat of their natural hosts. Many flying fox species are in decline, with roosting and feeding sites being deforested, and converted to agricultural or urban use. A number of hypotheses have been proposed to explain exactly how Nipah virus emerged (see Breed et al., 2006) , all of which involve the establishment of piggeries in previously forested regions still used by fruit bats (Chua et al., 2002) . Increased contact rates are also the likely explanation for the emergence of Hendra virus in Australia, with many Pteropus populations having relocated into urban areas (Hall and Richards, 2000) . With no vaccine available, and Pteropus in need of conservation, ecologically based strategies to limit contact rates between bats and livestock offer the best prospects of controlling the disease (Field and Mackenzie, 2004) .\n\nManaging the risks from zoonoses to the health of humans and domestic animals is complex. It is also fundamentally important: virtually all emerging infectious diseases have originated in wildlife. Superficially, the simplest method of control is via a reduction in reservoir host-disease prevalence, this being achieved by culls of host populations. However, effective reductions in population densities can be difficult to achieve in practice and may be undesirable where the target is of conservation concern. For example, most bat species are threatened, and yet they appear to be particularly important sources of emerging viruses (Calisher et al., 2006; Dobson, 2005) ; and despite not being endangered, badgers in the United Kingdom are legally protected. An alternative, and possibly complementary, strategy is to manage the ecological conditions leading to disease spill-overs. This will not only benefit the health of humans and their domestic stock, but must surely also lead to benefits for the conservation and welfare of wild animals."}