{"title": "The benefit of modeling jumps in realized volatility for risk prediction: Evidence from Chinese mainland stocks", "body": "The rapid growth of trading activity and financial instability in recent years has led to an expansion in risk management interest and capability. Value-at-Risk (VaR), which refers to an asset or a portfolio's maximum loss over a fixed time horizon with a given probability, has become established as the most commonly used measure of risk exposure. In light of the practical relevance of the VaR concept, the need for reliable VaR estimation and prediction strategies arises. 1\n\nIn line with the promotion of the realized volatility concept, a large number of papers discuss the importance of jumps in realized volatility series and develop models to describe it. This section introduces the procedure of extracting jumps from realized volatility, and presents several popular and commonly used realized volatility models with or without jump components.\n\nWe assume that the scalar logarithm of the asset price within the active part of the trading day evolves in continuous time as a standard jump-diffusion process\n\nwhere \u03bc(s) denotes the drift term with a continuous and locally bounded variation, \u03c3(s) is a strictly positive spot volatility process, w(s) is a standard Brownian motion and \u03ba(s)dq(s) refers to the pure jump part, where dq(s) = 1 if there is a jump at time s and 0 otherwise, and k(s) is the size of jump. The corresponding discrete-time within-day geometric return is denoted by \n\nwhere M refers to the number of intraday equally spaced return observations over the trading day t, which depends on the sampling frequency. As such, the daily return of the active part of the trading day has r t \u00bc \u2211 M j\u00bc1 r t j : Realized volatility (RV) is defined by the sum of the intraday squared returns as RV t \u2261\u2211 M j\u00bc1 r 2 t j : As noted in Andersen and Bollerslev (1998) , Comte and Renault (1998) , Andersen et al. (2003) , and Shephard (2002a, 2002b) , RV is a consistent estimator of quadratic variation of a standard jump-diffusion process, which includes both continuous integrated volatility and the sum of squared discontinuous jumps.\n\nIn order to disentangle the continuous and jump components, we need a consistent estimator of integrated volatility which is robust even in the presence of jumps. Shephard (2004, 2006) propose realized bipower variation (RBV), defined as the sum of the product of adjacent absolute intraday returns standardized by a constant, to consistently estimate integrated volatility. That is\n\njr t j jjr t j\u22122 j\u2192\u222b t t\u22121 \u03c3 2 s \u00f0 \u00deds;\n\nwhere \u03bc 1 \u2261 ffiffiffiffiffiffiffiffiffiffi 2=\u03a0 p \u22480:79788 is the expected absolute value of a standard normal random variable. Relative to the original measure considered in Barndorff-Neilsen and Shephard (2004) , the bipower variation measure defined above involves an additional stagger value (see Huang and Tauchen, 2005) to correct market microstructure bias. The difference between realized variance and realized bipower variation consistently estimates the part of quadratic variation due to jumps as\n\nHere we use a jump detection test to select statistically significant jumps. 3 Under the null hypothesis of no jumps, the test statistic is defined as\n\nwhere TQ t is the realized tripower quarticity which is estimated by\n\nwhere \u0394 = 1/M. Therefore, non-parametric measures of the jump component and the continuous component of the logarithm price process respectively are\n\nand\n\nwhere I(.) denotes the indicator function and \u03a6 \u03b1 refers to the appropriate critical value from the standard normal distribution.\n\nThe empirical findings suggest that the logarithm of realized volatility is approximately normal and it appears to be a long-memory process. To deal with this, Andersen et al. (2003) propose the use of an Autoregressive Fractionally Integrated Moving Average Model (ARFIMA) developed initially by Granger and Joyeux (1980) , to estimate and forecast realized volatility. However, the estimation of ARFIMA models is non-trivial. As Pong et al. (2004) and Gallant et al. (1999) consider an ARMA(2,1) model as a simple alternative to the long memory ARFIMA model, 4 the first realized volatility model I considered here is an ARMA (2,1) model of the logarithm of realized volatility (RV t ), which is written as\n\nwhere \u03a6(L) =1 \u2212 \u03d5 1 L \u2212 \u03d5 2 L 2 is the AR polynomial and \u0398(L)= 1 +\u03b8 1 L is the MA polynomial.\n\nHeterogeneous Autoregressive Realized Volatility (HAR) model proposed by Corsi (2009) is an alternative way to describe long memory property of realized volatility. Unlike the ARMA model that has little theoretical motivation, the basic idea behind the HAR model relies on the \"Heterogeneous Market Hypothesis\". 5 It suggests that the financial market consists of participants with a range of time horizons, which cause different types of volatility components. By assuming that market dynamics are completely determined by the behavior of the participants, current realized volatility and the expectations of volatility over longer horizons can effectively form volatility expectations for the next period. Therefore, the corresponding model for the logarithm of realized volatility is specified as a component model that contains a daily, weekly and monthly logarithm of realized volatility component, and it takes the form as\n\nwhere log(RV t d ) denotes the logarithm of daily realized volatility, log RV w t\u22121\n\n\u00deis the logarithm of weekly realized volatility and log RV m\n\n\u00derepresents the logarithm of monthly realized volatility. Like the ARMA (2,1), the HAR model is parsimonious and easily estimated. 4 The special lag structure in the ARMA (2,1) model is related to the fact that the sum of two AR(1) processes gives an ARMA (2,1) process, and an ARMA (2,1) can be interpreted as a component model with a transitory AR(1) component and permanent AR(1) component (see Hamilton, 1994; Gallant et al., 1999) . 5 See M\u00fcller et al. (1997) for further details on this hypothesis.\n\nIn the above two models, realized volatility is treated as a single variable. As many asset prices are best described by a combination of a smooth and very slowly mean-reverting continuous sample path process and a much less persistent jump component, as well as the theory relating to the decomposition of realized volatility into continuous and jump components has been well developed now, Andersen et al. (2007a,b,c) proposed a new HAR-RV-CJ model which explicitly allows for jumps in the realized volatility modeling. In contrast with the above two models, the HAR-RV-CJ model separately incorporates jumps and the continuous sample path of the asset price as two explanatory variables for realized volatility as\n\nwhere log C t\u22125;t\n\n\u00de denote the logarithm of weekly continuous component and the logarithm of monthly continuous component respectively. log J t\u22125;t \u00fe 1 \u00bc\n\n\u00de denote the logarithm of weekly jump component and the logarithm of monthly jump component respectively. As there are zero observations in jumps, and log(0) is undefined, we use log(J t + 1) to ensure a well-defined measure that equals zero, when jumps are zero. The separation strategy introduced by this model significantly improves the out-of-sample volatility forecast.\n\nFollowing the line of separately considering jumps and continuous sample path in the realized volatility models, Andersen et al. (2011) propose another model, which is referred to as HAR-CJN model, to account for jump dynamics. This model decomposes the total daily return variability into the continuous sample path variance, the variation arising from discontinuous jumps and the overnight return variance, and then model and forecast each component respectively. The HAR-CJN model is actually composed of four separate models, including the HAR-C model for the continuous sample path, the ACH model for the probability of jump occurrence, the HAR-J model for the squared jump size and the GARCH-t model for overnight returns. My current study only focusses on the first three models, and leaves modeling of the overnight returns for further research. Like realized volatility models, the HAR-CJN model is able to effectively incorporate intraday data into a reduced form for daily returns. Meanwhile, the HAR-CJN model can approximate long memory well by simply adding together volatility components for different time horizons. Innovatively, the HAR-CJN model attempts to capture the predictability of jumps and the dynamic persistence of jump intensity and jump size by combining the Autoregressive Conditional Hazard (ACH) model and a log-linear model with GARCH errors.\n\n2.5.1. HAR-C model for continuous sample path variation Andersen et al. (2011) apply the Heterogeneous Autoregressive Model proposed by Corsi (2009) to the continuous sample path for capturing its long memory dynamic dependencies. The HAR-C model is specified as\n\n. We work with the logarithm of the continuous sample path and jumps rather than their level, due to the fact that the unconditional distribution of the logarithm of realized volatility is closer to a normal distribution. Furthermore, by using the exponentiation of log(C t+1 ' ), we can ensure that the implied continuous variation is always positive. Since log(0) is undefined, we use log(J t ' + 1) to ensure a well-defined measure that equals zero, when jumps are zero. Time-varying volatility-of-volatility has been documented in numerous papers, and hence there is no reason to neglect it in the context of realized volatility. A conditionally t-distributed GARCH error structure therefore appears in the above model as\n\n2.5.2. ACH model and HAR-J model for jump variation The models for jump variation are composed of two parts: a model for the probability of jumps coupled with a model for the squared jump sizes. The Autoregressive Conditional Duration (ACD) model proposed by Engle and Russell (1998) is a standard way to model the dynamics of variables that are observed at irregular times. Jumps arrive at irregularly spaced time intervals, and their duration is ideally suited to be modeled by an ACD model. However, the ACD model only updates the conditional expected durations on days when jumps arrive. From a forecasting perspective, it is desirable to continuously incorporate new information as it becomes available. The Autoregressive Conditional Hazard (ACH) model of Hamilton and Jorda (2002) was explicitly designed for this purpose. 6 Consider a counting process N(t) representing the number of jump days that have occurred up until day t, with the hazard rate of jump occurrence naturally defined as\n\nwhere F t \u2212 1 is the available information set up to day t \u2212 1. The simple ACH (1,1) model, which is actually an intensity model, without any information updating between jump days is\n\nwith\n\nwhere d N(t) \u2212 1 is the actual duration, which is the length of time between the N(t) \u2212 2 and the N(t) \u2212 1 jump arrival times, that is,\n\nThe expected duration is a weighted average of the last actual duration d N(t) \u2212 1 and the last expected duration \u03c8 N(t) \u2212 1 . The augmented ACH (1,1) is designed to update conditional expected durations further by a set of exogenous variables as\n\nwhere \u03b4z t \u2212 1 accommodates new information which could significantly influence the hazard rate, such as the latest macroeconomics announcement or company earnings report. The augmented ACH model establishes a natural link between ACD models and intensity models by allowing the ACD model to include time-varying covariates that may change during a duration spell. In terms of the size of the jump, most continuous-time parametric jump diffusion models assume that they are i.i.d. distributed through time, but the present framework affords us much greater flexibility to model jump sizes. Following the same basic idea underlying the HAR-C model, Andersen et al. (2011) parameterize the conditional jump sizes as a function of the past continuous sample path variations, past jump sizes, as well as raw and expected durations as follows:\n\n6 Details of the ACD and ACH frameworks are explained in Engle and Russell (1998) and Hamilton and Jorda (2002) .\n\nwhere S t(i) is the squared jump size and t(i) records the corresponding trading day t in which the ith jump occurs.\n\nCombining the above three models, the forecast of realized volatility can be expressed as\n\nwhere E C \u2032 t F t\u22121 j \u00de represents the conditional mean of the continuous component of volatility calculated from the HAR-C model, and E J \u2032 t F t\u22121 j \u00de represents the conditional mean of the jump component of volatility, which can be calculated by\n\nis the conditional mean of jump size if there is a jump at time t calculated from the HAR-J model, and h t is the conditional probability of the occurrence of a jump at time t calculated from the ACH model. I t is an indicator function which is equal to one if there is jump occurrence at time t, otherwise it is zero. Since the models for C \u2032 t and S t are formulated in logarithmic terms, the two conditional expectations\n\nboth involve a Jensen's inequality correction.\n\nThis section introduces the procedure of VaR forecasting based on the above realized volatility models and backtesting for evaluating VaR forecasts from these models.\n\nA series of asset returns (r t ) is typically modeled by a location-scale family of probability distributions, as discussed in the introduction. To compute a h-step-ahead VaR with \u03b1 probability for the daily returns r t , I use conditional realized volatility obtained from the four different models as an input to estimate a return equation. This is\n\nThe first parameter needs to be estimated in the above equation as the mean component \u03bc t , which can be either fitted as an autoregressive process or treated as a constant. \u03c3 2 is an additional parameter to ensure that the rescaled innovation process z t has a unit variance. According to the distribution assumption, there may be some shape parameters to be estimated. Once the return equation is specified, h-step-ahead VaR can be calculated as\n\nwhere \u03bc t|t \u2212 h is the h-step-ahead forecasting of mean of the distribution of r t , \u03c3 t|t \u2212 h is the h-step-ahead forecasting of standard deviation of the distribution of r t (i.e.,\n\n), and Q \u03b1 (z) is the \u03b1 quantile of the assumed distribution of z. As the return r t of my empirical data is not persistent based on their Ljung Box Q-statistics, I set the mean u t as a constant for simplicity and focus on the forecast of standard deviation \u03c3 t .\n\nThe VaR predictive performances of these model candidates can be assessed by a two-step backtesting framework. This framework is designed to evaluate models by not only examining whether the models are able to produce VaR forecasts appropriately, but also by comparing the magnitude of the losses beyond VaR.\n\nIn the first stage, I employ the unconditional coverage test (see Kupiec, 1995) , independence test (see Christoffersen, 1998) and conditional coverage test to monitor the statistical adequacy of the VaR forecasts from these realized volatility models. These tests examine whether the average number of violations is statistically equal to the expected coverage rate and whether these violations are independently distributed. In the second stage, the loss function (see Lopez, 1999 ) based on another risk measure called Expected Shortfall (ES) is utilized to rank each model's predictive ability.\n\nDefining a hit sequence as H t = I(r t b \u2212 VaR t \u03b1 ) and specifying F t \u22121 as the information set up to day t \u2212 1, it is straightforward to show that an accurate VaR measure at a given level \u03b1 implies that the hit sequence is identically and independently distributed as a Bernoulli random variable with a hit probability of \u03b1. In other words, the VaR violations in each period will occur with the correct conditional and unconditional probability. If the hit sequence generated by a model holds those properties, it should pass the unconditional coverage test, independence test and conditional coverage test.\n\nThe unconditional coverage test asks whether the observed number of violations is statistically equal to the desired level by testing\n\nThe likelihood-ratio test statistic is\n\nwhere N is the number of days over a period T in which a violation occurred and \u03b1 is the desired coverage rate. Under the null hypothesis, LR uc \u223c \u03c7 1 2 and the model is rejected if it generates too many or too few deviations from the desired violation rate. An important shortcoming of the unconditional coverage test is that it will accept a model which generates violation clustering if the overall number of violations is close to the desired coverage level. Violation clustering and associated extreme losses incurred in consecutive periods may cause bankruptcy or at least serious financial problems for institutions or individuals. Therefore, Christoffersen (1998) proposes a relatively strict criterion in order to simultaneously examine the unconditional coverage rate and the independence of the sequence H t . Under the null hypothesis, the number of violations is not only statistically equal to the desired coverage rate, but also a violation today has no influence on the probability of a violation tomorrow. Namely, the probability of a hit in the next period does not depend on whether a hit occurred in this period. The alternative hypothesis models the \"hit\" sequence as a binary first-order Markov chain with a transition probability matrix given by\n\nThe appropriate likelihood ratio test statistic is\n\n\u00f0 \u00de n 00 \u03c0 n 01 01 1\u2212\u03c0 11 \u00f0 \u00de n 10 \u03c0 n 11 11 \u00c0 \u00c1 \u22122ln 1\u2212\u03c0 1 \u00f0 \u00de n 00 \u00fen 10 \u03c0 n 01 \u00fen 11 1 ;\n\nwhere n ij represents the number of transitions from state i to state j, \u03c0 ij \u00bc n ij \u2211 j n ij denotes the corresponding transition probability from state i to state j, \u03c0 1 defines the probability that a hit occurs in the next period whatever state it is in this period, and \u03c0 0 defines the probability that no hit occurs in the next period whatever state it is in this period. Under the null hypothesis that the hitting sequence is independent, LR in \u223c \u03c7 1 2 and the model is rejected if it generates violation clustering.\n\nThe conditional coverage test combines the above two likelihood ratio statistics to test the joint assumption of unconditional coverage and independence of failures. The test statistic is computed as\n\n\u00f0 \u00de n 00 \u03c0 n 01 01 1\u2212\u03c0 11 \u00f0 \u00de n 10 \u03c0 n 11 11\n\nUnder the joint null hypotheses that the observed number of violations is statistically equal to the desired level and the probability of violation in each period is independent, LR in \u223c \u03c7 2 2 and the model is rejected if it generates too many or too few deviations from the desired violation rate, or if the violations cluster.\n\nAfter assessing the statistical adequacy of these model based VaR forecasts, I use Expected Shortfall (ES) based loss function to rank each model's predictive ability. Given that a VaR violation has occurred, Expected Shortfall (ES) is a probability-weighted average of tail loss, which measures the expected value of the loss. The two loss functions employed here are:\n\nwhere T 0 is the end of the estimation sample, K is the size of out-of-sample VaR forecasts, and 1[\u039b] is the indicator function of the event \u039b.\n\nA small simulation study is conducted to compare the VaR forecasting performance of the above four realized volatility models, given a variety of underlying asset price process specifications. The various price processes and variations of them considered in the simulation are described below.\n\nThe initial asset price is set equal to one and the subsequent log prices of asset Y t = log(S t ) are simulated from a jump-diffusion process as\n\nwhere W 1 and W 2 are two independent Brownian motion processes, V t denotes the volatility of the asset price return at time t and follows a stochastic process, N t y and N t v are two Poisson processes with constant intensity \u03bb y and \u03bb v , and \u03be y and \u03be v are the jump sizes in returns and volatility, respectively. This specification nests many of the popular asset price models including Eq. (1) When \u03bb y \u2260 0 but \u03bb v =0, the above equation reduces to Bates' (1996) stochastic volatility model with random jumps occurring in the prices (SVJ). Jump sizes are assumed to be normally distributed as \u03be y \u223c N(\u03bc y ,\u03c3 y 2 ). Essentially, the jump component adds mass to the tail of the return distribution. Increasing \u03c3 y adds tail mass to both tails, while a positive \u03bc y implies relatively less mass in the left tail and vice versa; Eq. (2). When volatility is also allowed to jump, but jumps to prices and volatility are arrived independently, that is, \u03bb y and \u03bb v are two independent Poisson processes, and neither of them are zero, the above equation represents a stochastic volatility independent jumps (SVIJ) model with jump sizes in the price that are normally distributed as \u03be y \u223c N(\u03bc y ,\u03c3 y 2 ) and jump sizes in the volatility are from an exponential distribution \u03be v \u223c exp(\u03bc y ). Eq. (3) When jumps in return and volatility are driven by the same Poisson process, N t y =N t v = N t , and jump sizes in return and volatility are correlated as \u03be v \u223c exp(\u03bc v ) and \u03be y |\u03be v \u223c N(\u03bc y + \u03c1 J \u03be v , \u03c3 y 2 ), the above equation is referred to as stochastic volatility with correlated jump (SVCJ) model. In SVJ model, large price moves stemming from jumps that have no impact on volatility, while the SVCJ specification corrects this shortcoming to allow that price jumps will simultaneously influence both prices and volatility. The added volatility jump component typically adds right skewness in the distribution of volatility, and hence, overall fatten the tails of the return distribution.\n\nJump intensities are treated as a constant in the above specifications. In the simulation, I also consider a stochastic volatility model with state-dependent or time-varying correlated jumps (SVSCJ) in price and volatility. This model generalizes the SCVJ model described above to allow the jump intensity to depend on its past values or the current volatility. That is,\n\nwhere \u03bb t is the intensity of a Poisson counting process N t y (and N t v ) at time t, and V t is the instantaneous volatility at time t. In essence, the SVSCJ model can better describe jump clustering, and allows jumps to be arrived more frequently in high-volatility regimes when \u03b1 2 is positive. These generalizations are supported by the empirical facts.\n\nThe values of the parameters in my simulation for the above processes are reported in Table 1 . For all the data generating processes (DGPs), I simulate the time series of asset prices of length T = 1500 days with K = 14400 s per day, and the simulation is replicated 1000 times. In each replication, I use five minutes as a sampling frequency to construct the time series of realized volatility, bipower variation and jumps. To assess the VaR forecasting power of all the realized volatility models described in Section 2, I use a rolling window of 500 (or 496) observations to estimate all the models, and obtain out-of-sample one-step-ahead (or five-step-ahead) VaR forecasts with 1% and 5% coverage levels. At the end, I have 1000 VaR forecasts from each model, and the VaR violations can be calculated by comparing the 1000 VaR forecasts with the last 1000 daily return observations.\n\nThe ideal number of 1% (or 5%) VaR violations is 10 or 50 in each replication. I will analyze the distribution of the number of VaR violations from each model to compare their VaR forecasting performance. As there are 1000 replications, such a distribution of the number of VaR violations will have 1000 points.\n\nPanel A and Panel B in Tables 2 and 3 respectively report mean and variance of the number of violations over one-step-ahead and five-step-ahead 1% VaR forecasts from each model. In general, under all the DGPs, the VaR forecasts provided by all the models are positively biased. The ARMA-RV model gives the most biased forecasts with the highest variance, and the HAR-CJN model provides the least biased forecasts with the lowest variance. More importantly, the two realized volatility models (HAR-CJ model and HAR-CJN model) which explicitly model jumps produce less biased and variant VaR forecasts than the other two realized volatility models. To assess the trade-off between bias and variance, I also employ the Mean Squared Error (MSE) to compare the VaR forecasting performances of these models. For example, the MSE of 1% VaR is defined as\n\nwhere X i is the number of violations in the ith replication, and 10 is the ideal number of violations for 1% VaR at each replication. Panel C in Tables 2 and 3 reports MSE of one-step-ahead and five-step-ahead 1% VaR forecasts of all the models. It is clear that under all the DGPs, ARMA model and HAR model perform a bit similarly. However, the HAR-CJ model and HAR-CJN model always have the lower MSEs than the other two models. Moreover, when the jump intensity is time-varying and state-dependent, the HAR-CJN model has lower MSEs than the HAR-CJ model, as the jump intensity dynamics are clearly modeled in the former. The similar tendencies are observed at 5% VaR forecasts, and the results associated with this can be provided upon request. These results provide a strong evidence that explicitly modeling jumps in realized volatility model can better describe the tail behavior, and thereby provide better tail point forecast of the asset return distribution.\n\nIn this section, I compare VaR forecasts of the four realized volatility models by using empirical data. The analysis is based on intraday data of eight individual stocks on the Chinese Stock Exchange. 7 These stocks are from four different industries, and more details are provided in Table 4 . The raw transaction prices were obtained from the China Stock Market & Accounting Research (CSMAR) database provided by the ShenZhen GuoTaiAn Information and Technology Firm (GTA). Trading in the Chinese Stock Exchange is conducted through the electronic consolidated open limit order book (COLOB), and it is carried out in two sessions with a lunch break. The morning session is from 09:30 to 11:30 and the afternoon session is from 13:00 to 15:00. Before the morning session, there is a 10-minute open call auction session from 09:15 to 09:25 to determine the opening price. The afternoon session starts from continuous trading without a call auction. The closing price of the active trading day is generated by taking a weighted average of the trading prices of the final minute. The market is closed on Saturdays and Sundays and other public holidays.\n\nCompared with most Western developed stock markets in the context of institutional setting and trading rules, the Chinese mainland stock market differs in three main ways. First, there is a five-minute break between the periodic auction for the opening price and the normal morning session of continuous trading. In addition, there is a lunch break at midday between the morning and afternoon sessions, as in most other Asian stock markets. Second, the Chinese market is a limit order-driven market using electronic trading without market makers. Floor trading between member brokers and short selling are strictly prohibited. A further difference lies in the relatively immature infrastructure that embodies inadequate disclosure, and the co-existence of an inexperienced regulator with a limited number of informed investors and an enormous number of uninformed investors. Given these differences, data from the Chinese mainland stock exchange is expected to provide a picture of an emerging stock market, which contrasts with western markets in interesting ways.\n\nThe actual construction of the series needed for our analysis follows the realized volatility decomposition discussed in Section 2.1. I am only concerned with the active trading period, and overnight information is beyond the scope of this study. Parallelling many previous studies, I use five-minute intervals as the sampling frequency to strike a reasonable balance between accurate measure and microstructure noise. 8 Since there are no transaction records in the first 15-minute interval of many trading days, and also to avoid opening effects, my dataset spans 09:45-11:30 and 13:05-15:00 on each working day (excluding weekends, public holidays and firm-specific trading suspensions) from January 2, 2003 to December 27, 2007. My sample excludes some inactive days that contain only a few transactions to avoid complicating the inference.\n\nFirstly, based on the previous-tick method, I calculate five-minute prices from the tick immediately before the five-minute time stamp throughout each trading day. Secondly, I obtain five-minute intraday returns as the first difference of the logarithmic prices. The open-to-close daily return is naturally defined as the sum of the intraday returns, i.e., r t \u00bc X M j\u00bc1 r t j \u00bc ln p tM \u00c0 \u00c1 \u2212ln p t 1 \u00c0 \u00c1 . In addition, realized volatility is constructed as the sum of all squared intraday 5-minute returns RV t \u2261 X M j\u00bc1 r 2 t j . The time interval between 7 There are two official stock exchanges in the Chinese mainland financial market: the Shanghai Stock Exchange (SSE) and the Shenzhen Stock Exchange (SZSE). Three of the stocks in our analysis (SH600085, SH600351 and SH600050) are from SSE, while five of the stocks (SZ000919, SZ000419, SZ000987, SZ000581 and SZ000088) are from SZSE. All eight stocks are A share stocks. 8 There is a trade-off between measurement accuracy and microstructure contamination when using high-frequency data to measure financial volatility. For further details concerning the optimal sampling frequency, see Andersen et al. (2007a,b,c) .\n\ntrades is \u0394 \u00bc 1 M \u00bc 0:023. Because some days involve fewer than 44 intraday observations, I scale up the variance measure based on the available 5-minute returns. Thirdly, according to jump tests and bipower variation, I decompose the realized volatility into its continuous sample path C \u2032 t , and variation due to jumps J \u2032 t based on a significance level of \u03b1 = 0.01. 9 The duration between two consecutive jumps, D N(t) , is calculated as the number of days between the (N \u2212 1)th and the Nth jump arrival times, that is D N t \u00f0 \u00de \u00bc t N t \u00f0 \u00de \u2212 t N t \u00f0 \u00de\u22121 ; and the jump size S t(N) is the squared jump size within the day when at least one jump occurs. Finally, I adopt an expanding estimation window to specify the models and produce one-step-ahead and five-step-ahead forecasts by starting from the in-sample estimation period covering 694 days from Table 6 indicate more significant serial correlation in the jump component, particularly in the jump size. Therefore, jumps play a greater role in this emerging market and exhibit a more predictable pattern than that in the developed US market. The estimated models discussed in the next section provide an attempt to explain the main features of realized volatility and the continuous and jump components of realized volatility in these Chinese stocks.\n\nTwo-step model estimation is carried out in this section. In order to perform the VaR analysis, the four realized volatility models are estimated in the first step to obtain a set of realized volatility forecasts. 10 In the second stage, realized volatility forecasts are used as a proxy for variance in the daily return equation to estimate other parameters needed for VaR calculation. I use the first 694 days of the sample from January 2, 2003 to December 30, 2005 for model development, and then use the last 472 days of the sample from January 4, 2006 to December 27, 2007 for out-of-sample forecast analysis. Once these models have been developed, I use an expanding window to re-estimate their parameters and construct out-of-sample forecasts that incorporate information as it becomes available. 9 In practice, the number of detected jumps will depend on the chosen significance level. We follow most of the literature and use China United Telecommunications Co., Ltd Shanghai Utilities Table 5 Descriptive statistics for eight Chinese stocks. Note: rt denotes the daily return; rv t denotes the daily realized volatility; C t ' denotes the continuous sample path variation; J t ' denotes the jump component; D N(t) denotes the jump duration, and S N(t) denotes the size of jumps. N(t) records the calendar time t when cumulative number of jumps is N.\n\nLjung Box Q-statistics for eight Chinese stocks. To investigate jump behavior in Chinese market, I report estimation results for the ACH model in Table 7 and estimation results for the HAR-J model in Table 8 for the eight Chinese stocks to analyze their jump intensities and jump sizes. The ACH model presented here only includes the significant explanatory variables and all the insignificant covariates are deleted. I originally experimented with augmenting the ACH (1,1) model by past actual duration and expected duration along with five categories of exogenous variables. The five variables that were added include the logarithm of the number of days until the next news announcements of the Consumer Price Index (CPI) in the model, which turns out to be the most important macroeconomic variable to influence the stock market in this emerging market. 11 CPI announcement dates were collected from the National Bureau of Statistics of China. We also included the logarithm of the number of days until the next announcement of the firm's earnings report, which is released quarterly and collected from the Shanghai Stock Exchange official website. Unscheduled announcements are included in our analysis as well since insiders may know what and when information will be released, and this can have a potentially high effect on stock prices. In order to accommodate day-of-week effects in the jump occurrences, four weekday dummies for Monday, Tuesday, Wednesday, and Thursday were generated and the estimated coefficients in front of them represent the extra probability of jump occurrence relative to Friday. With respect to the specific industry the three individual stocks that belong to, the outbreak of epidemic disease should have an influence on the medicine manufacturing industry. It is well known that Severe Acute Respiratory Syndrome (SARS) affected China at the end of 2002 and lasted until the middle of 2003. Therefore, I generate a dummy variable, which equals 1 from January 2, 2003 to July 31, 2003, to observe its effect in jump occurrence. Finally, Nowak (2008) suggested that the length of time since the latest jump could be an additional information component to predict the chance that the jump will show up in the next period, I incorporate this innovative idea in the ACH model and use a step function to measure the no-jump duration L t ,\n\n. The estimated results of the ACH model presented in Table 7 confirm the strong dynamic persistence in jump durations or equivalently the hazard rate of jump occurrence, as the previous literature suggests. The corresponding coefficients of past actual duration and expected duration are both positive and significant. One exciting but not surprising result is that the no-jump duration since the latest jump has a statistically significant positive relationship with the hazard rate of jump occurrence. This suggests that the more no-jump time has passed since the last jump, the more likely that jumps will appear in the next period. However, in contrast to the U.S. fixed-income market in which Andersen et al. (2011) found a statistically significant negative relationship between the hazard rate of jump occurrence and the number of days until the release of macroeconomic announcements, there is no evidence from the estimated results that fundamental information such as released CPI and firm specific news affect the jump hazard rate in these Chinese stocks. Meanwhile, I cannot find a statistically significant day-of-week effect in these three stocks. Table 8 shows the estimated results for the HAR-J model. These clearly indicate a strong persistence in jump size, which is in contrast to findings in previous literature. Early studies in the stochastic volatility literature assume that jump size is identically and independently distributed with no evidence of predictability, and Andersen et al. (2011) found that jump size is statistically influenced by the past one-month continuous sample path variation, but has no significant own persistence. It is not clear yet what drives this difference in the Chinese stocks.\n\nThe estimation results of the return equation with conditional realized volatility from the four models are reported in Table 9 . I assume that the return has a Student-t distribution, 12 since this type of distribution can model the fat tail better than the normal distribution by using a shape parameter \u03bd. Smaller values of \u03bd indicate fatter tail behavior. It is clear that the return equation with realized volatility provided by HAR-CJ and HAR-CJN models in all stocks have relatively large values of \u03bd than other models, which is an evidence that modeling of jumps is able to better describe the tail behavior of returns. In terms of the conditional mean in 11 Andersen et al (2007) suggest that CPI and employment reports, representing the nominal side and real side of the economy respectively, are the two most important macroeconomic news announcements related to financial markets. However, employment reports in emerging markets are unreliable indicators of the real economy because they do not account for selfemployment and underground jobs. 12 The skewed Student-t distribution is popular in the recent literature for VaR forecasting due to its ability to capture both kurtosis and skewness, and to distinguish between the long position and short position VaR risk once the distribution is asymmetric (see Giot and Laurent, 2004) . However, as we have mentioned in Section 2.4, short selling is not permitted in the Chinese stock market. Therefore, our analysis only needs to focus on long position VaR risk, and the Student-t distribution should be adequate in this case. \n\nHere we only report the significant coefficients. Newey-West robust standard errors of the corresponding coefficients are reported in brackets. Note: The HAR-J model is specified as:\n\nHere we only report the significant coefficients. Newey-West robust standard errors of the corresponding coefficients are reported in brackets.\n\nthe return equation, the correlogram of the daily return indicates that there is no evidence of ARMA structure in the return series. I consequently use a constant to measure this component.\n\nOut-of-sample VaR calculation and backtesting I adopt the VaR calculation methodology introduced in Section 3.1 to compute one-step-ahead and five-step-ahead 1% and 5% VaR forecasts of each stock for all the realized volatility models, and the two-step backtesting procedure presented in Section 3.2 is employed to compare these VaR forecasts.\n\nThe statistical adequacy of the VaR forecasts from these realized volatility models is monitored by the unconditional coverage test, independence test and conditional coverage test with a significance level of 5% in the first stage. The percentage of return violations above the VaR forecasts provided by all the models for each stock, and P-values of the three tests on the sequence of violations are presented in Tables 10-13. In terms of the unconditional coverage property, it is clear overall that the HAR-CJ model and the HAR-CJN model are fairly well, while the ARMA model and the HAR model are a bit flawed as the p-values of the unconditional coverage test on the VaR forecasts provided by these two models are below 5% significance level for some stocks. The percentage of VaR violations implies that the ARMA model and the HAR model tend to underestimate the extreme point of the return distribution in these flawed cases. As all the realized volatility models do account for volatility clustering, most of them are able to produce i.i.d VaR violations, causing me to not reject independence of the hitting sequences and the accuracy of the conditional coverage for all the models that survive from the unconditional coverage test. 13 Based on the loss functions introduced in Section 3.2.2, Table 14 reports the MSE and MAE for all the realized volatility models with respect to one-step-ahead and five-step-ahead VaR predictions at the 1st and 5th percentile. The noticeable feature of these results is that the HAR-CJN model provides VaR forecasts with the lowest MSE and MAE for all the eight stocks. I use Diebold and Mariano (1995) tests (conducted at the 5% level of significance) to examine two specific questions of interest. First, I want to know whether the explicitly modeling jumps in the realized volatility models lead to VaR forecasts that are statistically superior to those obtained from the models which do not separately consider jumps. I 13 I do not proceed the independence test and conditional coverage test on the models which do not pass the unconditional test. compare the MSE and MAE from the HAR-CJ model and the HAR-CJN model with those from the ARMA model and the HAR model, and find evidence for this in all the eight stocks studied here. These are indicated by superscript (a) if the HAR-CJ model outperforms the ARMA model and the HAR model, and superscript (b) if the HAR-CJN model outperforms the ARMA model and the HAR model in Table 10 . Next, I want to determine whether separate treatment of jump intensities and jump sizes leads to VaR forecasts that are superior to forecasts derived from the model which treats jumps as a single variable. I compare the MSE and MAE from the HAR-CJN model with those that are produced by the HAR-CJ model, and find that the separate treatment of jump intensities and jump size leads to improvement in VaR forecast performance in five out of eight cases. These cases are indicated by superscript (c) in Table 10 . \n\nThe VaR predictive performance of several recently advanced and some new realized volatility models has been examined in this paper. Some previous studies (Andersen et al., 2007a (Andersen et al., ,b,c, 2011 Liao et al., 2010) reveal that decomposing realized volatility into continuous sample path variation and jumps, and separately modeling each component, outperformed the realized volatility models which treat realized volatility as a single variable with respect to out-of-sample realized volatility forecasting. However, some realized volatility models, particularly the ones which do not account for jumps, may be able to provide good mean point forecast, but fail in tail point forecasting. My analysis here explores the ability of four realized volatility models to predict the tail quantiles of the return distribution. In addition, this paper uses Diebold and Mariano (1995) tests of the null that VaR forecasts are equal is rejected at the 5% level of significance. The alternatives are: a) HAR-CJ model provides superior forecasts than ARMA model and HAR model; b) HAR-CJN model provides superior forecasts than ARMA model and HAR model; c) HAR-CJN model superior forecasts than HAR-CJ model. intraday data covering a recent five-year period of eight individual stocks from four different industries in China as an empirical analysis. This provides a good opportunity to examine different jump patterns and how they impact the risk predictive ability of the realized volatility models in an emerging stock market. From the results of the Monte Carlo simulation and the out-of-sample forecasting results in the real-data studies, I find that in some cases, the ARMA model and the HAR model of realized volatility suffer from excessive VaR violations, implying an underestimation of market risk. Most notably, the other two realized volatility models, HAR-CJ model and HAR-CJN model, in which the jump component is explicitly considered, can produce more accurate VaR forecasts. This finding regarding the importance of the jumps is interesting, because it implies that separately modeling jumps and continuous component in realized volatility do indeed improve the description of the tail behavior in asset return distribution.\n\nFinally, this work finds that jumps in Chinese market are more frequent and account for larger proportion of realized volatility than those of developed markets. Meanwhile, the times between two adjacent significant jumps of these Chinese stocks are highly persistent processes, and a surprising dynamic dependence of jump sizes is also detected in this emerging market. Therefore, modeling jumps in realized volatility plays a more important role in risk prediction of this market. It would be interesting to further study the jump behavior and investigate how jumps affect the risk prediction in other emerging markets."}