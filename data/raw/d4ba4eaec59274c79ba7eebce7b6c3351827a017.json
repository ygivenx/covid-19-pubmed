{"title": "Some remarks on protein attribute prediction and pseudo amino acid composition", "body": "are very important to both basic research and drug target development. For instance, given an uncharacterized protein sequence, what is its folding rate? Which structural class and quaternary structural attribute does it belong to? Which subcellular location site does it resides? Can it simultaneously exist in or move between two and more subcellular locations? How can we identify it as an enzyme or non-enzyme? If it is an enzyme, to which enzyme functional class does it belong? Is it a membrane protein or non-membrane protein? If the former, to which membrane protein type does it belong? Is it a protease? If it is, to which protease type does it belong? Is it a G protein-coupled receptor (GPCR)? If it is, to which GPCR type does it belong? Which part of the protein serves as its signal sequence? Where are its cleavage sites by proteases such as HIV (human immunodeficiency virus) protease and SARS (severe acute respiratory syndrome) enzyme? And so forth. Although the answers to these questions can be determined by conducting various biochemical experiments, it is both time-consuming and costly by relying on experimental approaches alone. As a consequence, the gap between the number of newly discovered protein sequences and the knowledge of their attributes is continuing to expand. To bridge such a gap and acquire these kinds of information in a timely manner, scientists are challenged to develop computational methods for predicting various attributes of proteins based on their sequence information alone.\n\nTo establish a really useful predictor in this regard, one usually needs to accomplish the following procedures: (1) construct a valid benchmark dataset to train and test the predictor; (2) formulate the protein samples with an effective mathematical expression that can truly reflect their intrinsic correlation with the attribute to be predicted; (3) introduce or develop a powerful algorithm (or engine) to operate the prediction; (4) properly perform crossvalidation tests to objectively evaluate the accuracy of the predictor; and (5) establish a user-friendly web-server for the predictor that is accessible to the public. This review will discuss each of the above five procedures, with a special focus on procedure 2, particularly on how to use various different modes of pseudo amino acid composition to represent protein samples by incorporating their core and essential features.\n\nTo develop a statistical prediction method for a given attribute, the first important thing is to construct a benchmark dataset S according to its possible classification, i.e.\n\nwhere S 1 represents the subset for category 1 of the attribute, S 2 for category 2, and so forth; while [ represents the symbol for ''union'' in the set theory, and M the number of different categories for the attribute concerned. For example, when the attribute concerned was about the protein structural classification as investigated in Chou (1995a) , Chou and Zhang (1994) , Chou (1989) , Levitt and Chothia (1976) , Nakashima et al. (1986) and Zhou (1998) , M would be four as illustrated in Fig. 1 ; when the structural classification was defined according to the SCOP database (Murzin et al., 1995) or investigated in Chou and Cai (2004b) , M would be seven as shown in Fig. 2 ; when the attribute was about the membrane protein type as investigated in Chou and Shen (2007d) , M would be eight as illustrated in Fig. 3 ; when the attribute was about the subcellular localization of eukaryotic proteins as investigated in Chou and Shen (2010a) , M would be 22 as illustrated in Fig. 4 .\n\nTo avoid homology bias and redundancy, it is important to introduce a cutoff threshold when constructing a benchmark dataset. Different cutoff threshold values were used, such as 90% (Reinhardt and Hubbard, 1998) , 80% (Small et al., 2004) , 40% , and 25% (Chou and Shen, 2010a; Chou and Shen, 2010c) . When a benchmark dataset was constructed with the cutoff threshold of 25%, none of the proteins included would have Z25% pairwise sequence identity to any other in the same subset (category). Accordingly, the smaller the cutoff threshold is, the more stringent the benchmark dataset will be in excluding the homology bias.\n\nThe benchmark datasets constructed in the earlier stage (see, e.g., Cedano et al., 1997; Chou, 1989; Nakashima et al., 1986) usually consisted of a learning (or training) dataset and an independent testing dataset, as can be formulated as\n\nwhere S L is the learning dataset, S T the training dataset, + the empty set, and \\ the symbol for ''intersection'' in the set theory. The learning dataset is used for training the predictor's ''engine'', while the testing dataset used for evaluating the predictor's accuracy via a cross-validation. As we can see from Eq. (2), none of the proteins in the testing dataset S T should occur in the learning dataset S L . Therefore, S T is also called an independent dataset for performing cross-validation. However, as will be shown later, there is no need to artificially separate the benchmark dataset into a learning dataset and a testing dataset when the cross-validation is performed by the jackknife test, in which case one benchmark dataset can serve both the training and testing purposes. \n\nTwo kinds of models were usually used to represent protein samples. One is the sequential model, and the other the discrete model. The most straightforward sequential model for a protein sample is its entire amino acid sequence, as expressed by\n\nwhere R 1 represents the 1st residue of the protein P, R 2 the 2nd residue,y, R L the L-th residue, and they each belong to one of the 20 native amino acid types. To get the desired results, the sequencesimilarity-search-based tools, such as BLAST (Altschul, 1997; Wootton and Federhen, 1993) , are usually utilized to conduct the prediction. However, this kind of approach failed to work when the query protein did not have significant sequence similarity to any attribute-known proteins. Thus, various non-sequential models, or discrete models, were proposed, as illustrated below.\n\nThe simplest discrete model used to represent a protein sample is its amino acid (AA) composition or AAC (Nakashima et al., 1986) . According to the AAC-discrete model, the protein P of Eq. (3) can be expressed by (Chou, 1995a )\n\nwhere f i (i\u00bc 1, 2 ,y,20) are the normalized occurrence frequencies of the 20 native amino acids in P, and T the transposing operator. Many methods for predicting various protein attributes were based on the AAC-discrete model (see, e.g., Cedano et al., 1997; Chou, 1999 Chou, , 2000 Chou, , 2005b Zhang, 1992, 1995; Elrod, 1999, 2002; Chou, 1989; Feng et al., 2005; Jahandideh et al., 2007a; Klein, 1986; Klein and Delisi, 1986; Liu and Chou, 1998; Metfessel et al., 1993; Nakashima and Nishikawa, 1994; Niu et al., 2006; Zhou, 1998; Zhou and Assa-Munt, 2001; Zhou and Doctor, 2003) . However, as one can see from Eq. (4), all the sequence-order effects would be missing using the AAC-discrete (1) type I transmembrane, (2) type II, (3) type III, (4) type IV, (5) multipass transmembrane, (6) lipid-chain-anchored membrane, (7) GPI-anchored membrane, and (8) peripheral membrane. As shown in the figure, types I, II, III, and IV are all of singlepass transmembrane proteins; see Spiess (1995) for a detailed description about their difference. model, and hence the prediction quality thus obtained might be limited. This is the main shortcoming of the AAC discrete model. To avoid completely losing the sequence-order information, a completely different discrete model, or the so-called ''pseudo amino acid composition'' (PseAAC) model (Chou, 2001) , was proposed to represent the sample of a protein, as formulated by\n\nwhere the first 20 elements are associated with the 20 elements in Eq. (4) or the 20 amino acid components of the protein, while the additional L factors are used to incorporate some sequence-order information via various modes. Typically, these additional factors are a series of rank-different correlation factors along a protein chain, but they can also be any combinations of other factors so long as they can reflect some sorts of sequence-order effects in one way or the other. For the convenience of users, a web-server called ''PseAAC'' (Shen and Chou, 2008) was established at http://www.csbio.sjtu.edu.cn/bioinf/ PseAAC/, by which some commonly used PseAAC forms can be automatically generated. The concept of PseAAC has been widely used to study various problems in proteins and protein-related systems, such as predicting enzymes and their family/sub-family classification Qiu et al., 2010; Wang et al., 2010b; Zhou et al., 2007) , protein subcellular location prediction Cai, 2003c, 2004e; Gao et al., 2005; Li and Li, 2008b; Pan et al., 2003; Shi et al., 2007 Shi et al., , 2008 Xiao et al., 2006b; Zhang et al., 2008c) , apoptosis protein subcellular location prediction Jiang et al., 2008b; Kandaswamy et al., 2010; Lin et al., 2009a; Liu et al., 2010b) , mycobacterial protein subcellular location prediction , predicting protein subnuclear localization (Jiang et al., 2008a; Li and Li, 2008a; Shen and Chou, 2005b) , predicting protein subchloroplast locations (Du et al., 2009) , predicting protein submitochondria locations Nanni and Lumini, 2008; Zeng et al., 2009) , predicting membrane proteins and their types Chou and Shen, 2007d; Liu et al., 2005; Shen and Chou, 2005a; Wang et al., 2004; Wang et al., 2006) , discrimination of outer membrane proteins (Gao et al., 2010; Lin, 2008) , identifying transmembrane regions in proteins (Diao et al., 2008) , identifying proteases and their types Zhou and Cai, 2006) , predicting protein solubility (Xiaohui et al., 2010) , identifying GPCRs and their classes (Gu et al., 2010a (Gu et al., , 2010b Lin et al., 2009b; Qiu et al., 2009; Xiao et al., 2009b Xiao et al., , 2010b , prediction of nuclear receptors (Gao et al., 2009) , prediction of cyclin proteins (Mohabatkar, 2010) , identifying bacterial secreted proteins , identifying risk type of human papillomaviruses (Esmaeili et al., 2010) , prediction of cell wall lytic enzymes (Ding et al., 2009) , prediction of lipases types , predicting conotoxin superfamily and family (Lin and Li, 2007a; Mondal et al., 2006) , predicting the cofactors of oxidoreductases (Zhang and Fang, 2008) , predicting DNA-binding proteins , predict protein structural classes (Chen et al., 2006a; Chen et al., 2006b; Ding et al., 2007; Li et al., 2009; Lin and Li, 2007b; Wu et al., 2010; Xiao et al., 2008a; Xiao et al., 2008b; Xiao et al., 2006a; Zhang and Ding, 2007; Zhang et al., 2008d) , supersecondary structure prediction (Zou et al., 2011) , protein secondary structure content prediction , predicting protein quaternary structural attributes (Chou and Cai, 2003a; Shen and Chou, 2009b; Xiao et al., 2009a; Xiao et al., 2010a; Zhang et al., 2008b; Zhang et al., 2006) , fold pattern prediction Shen and Chou, 2009a) , and others (e.g., Georgiou et al., 2009) .\n\nMeanwhile, various modes of PseAAC by extracting different features from protein sequences were proposed, including stochastic signal processing mode (Pan et al., 2003) , Fourier spectrum analysis mode (Liu et al., 2005) , special functions mode (Gao et al., 2005) , complexity measure factor mode (Xiao et al., , 2006a , cellular automaton mode (Xiao et al., 2006b (Xiao et al., , 2008b , geometric moments mode (Xiao et al., 2008b) , gray dynamic mode (Xiao et al., 2008a) , approximate entropy mode (Jiang et al., 2008a) , continuous wavelet transform mode , discrete wavelet transform mode (Qiu et al., 2009 (Qiu et al., , 2010 , sequencesegmented mode , evolutionary information and von Neumann entropy mode (Zhang et al., 2008c) , and so forth.\n\nHowever, according to its original concept, the essence of PseAAC is to keep using a discrete model to represent a protein yet without completely losing its sequence-order information. Therefore, in a broad sense, the PseAAC of a protein is actually a set of discrete numbers that is derived from its amino acid sequence and that is different from the classical AAC and able to harbor some sort of sequence order or pattern information. Therefore, the PseAAC for a protein P should be generally formulated as\n\nwhere the subscript O is an integer, and its value and the components c 1 , c 2 ,y will depend on how to extract the desired information from the amino acid sequence of P (cf. Eq. (3)). The form of Eq. (6) can cover all the aforementioned modes of PseAAC. For example, when\n\nwe immediately obtain the formulation of PseAAC originally introduced in Chou (2001), where the meanings for w, y j , and l were clearly elaborated and hence there is no need to repeat here. When we obtain the formulation for the amphiphilic PseAAC (Chou, 2005a) , where the meanings of w, t j , and l were also clearly given.\n\nIt is instructive to point out that, with the general formulation of Eq. (6), the PseAAC can be used to reflect much more essential core features deeply hidden in complicated protein sequences through the following modes.\n\nThe functional domain (FunD) is the core of a protein. Therefore, in determining the 3-D (dimensional) structure of a protein by experiments (see, e.g., Call et al., 2010; Pielak and Chou, 2010; Schnell and Chou, 2008; Wang et al., 2009) or by computational modeling (see, e.g., Chou, 2004a; Chou, 2004b) , the first priority was always focused on its FunD.\n\nUsing the FunD information to formulate protein samples was originally proposed in and Chou and Cai (2002) based on the 2005 FunDs in the SBASE-A database (Murvai et al., 2001) . Since then, a series of new protein FunD databases were established, such as COG (Tatusov et al., 2003) , KOG (Tatusov et al., 2003) , SMART (Letunic et al., 2006) , Pfam (Finn et al., 2006) , and CDD (Marchler-Bauer et al., 2007) . Of these databases, CDD contains the domains imported from COG, Pfam, and SMART, and hence is relatively much more complete (Marchler-Bauer et al., 2007) and was adopted in most of the recent publications (see, e.g., Shen, 2010a, 2010c; Shen and Chou, 2009d) . The version 2.11 of CDD contains 17,402 characteristic domains. Thus, when using the general formulation of PseAAC (Eq. (6)) to incorporate the FunD information, we have O\u00bc17,402, i.e.\n\nwhere T has the same meaning as in Eq. (4), and\n\nFor the detailed procedure of how to find the hit for P in CDD, refer to Chou and Shen (2010a) .\n\nSimilar approaches of representing protein samples with the FunD mode were also used for predicting protein subcellular localization (Chou and Cai, 2002; Chou and Cai, 2004d) , membrane protein types , enzyme functional classes , protease types Shen and Chou, 2009c) , GPCRs types (Xiao et al., 2010b) , protein structural class (Chou and Cai, 2004b) , protein fold pattern (Shen and Chou, 2009a) , and protein quaternary structural attributes (Shen and Chou, 2009b; Xiao et al., 2009a Xiao et al., , 2010a .\n\nGene ontology (GO) database (Ashburner et al., 2000) was established according to the molecular function, biological process, and cellular component. Accordingly, protein samples defined in a GO database space would be clustered in a way better reflecting some of their important attributes, such as subcellular localization and biological function Shen, 2007c, 2008b) .\n\nThe GO database (version 70.0 released 10 March 2008) contains 60,020 GO numbers. Thus, when using the general formulation of PseAAC to incorporate the GO information, we have O\u00bc60,020, i.e.\n\nFor the detailed procedure of how to find the hit for P in the GO database, refer to Chou and Shen (2010a) .\n\nThe information extracted from the GO database (Ashburner et al., 2000; Camon et al., 2004; Harris et al., 2004 ) was used to formulate PseAAC for predicting protein subcellular localization Chou and Cai, 2003b; Chou and Cai, 2004d; Chou and Shen, 2006a; Chou and Shen, 2006b; Chou and Shen, 2006c; Chou and Shen, 2007a; Chou and Shen, 2007b; Chou and Shen, 2007c; Chou and Shen, 2008b; Lee et al., 2005; Shen and Chou, 2007b; Shen and Chou, 2007c; Shen and Chou, 2007d; Shen et al., 2007) , enzyme functional class (Chou and Cai, 2004a; Chou and Cai, 2004c) , membrane protein types , protease types (Zhou and Cai, 2006) , and protein-protein interactions .\n\nBiology is a natural science with historic dimension. All biological species have developed continuously starting out from a very limited number of ancestral species. It is true for protein sequence as well (Chou, 2004b) . Their evolution involves changes of single residues, insertions, and deletions of several residues (Chou, 1995b) , gene doubling, and gene fusion. With these changes accumulated for a long period of time, many similarities between initial and resultant amino acid sequences are gradually eliminated, but the corresponding proteins may still share many common attributes, such as having basically the same biological function and residing in the same subcellular location.\n\nThe general formulation of PseAAC can be used to incorporate this kind of information via its sequential evolution mode, i.e.\n\n1 L\u00c0l \n\nwhere l is an uncertain number that will be further discussed later, L is the length of P (counted in the total number of its constituent amino acids), and E i-j represents the score of the amino acid residue in the i-th position of the protein sequence being changed to amino acid type j during the evolutionary process (Schaffer et al., 2001) , which can be derived by using PSI-BLAST (Schaffer et al., 2001) to search the Swiss-Prot database as described in Chou and Shen (2010c) . Here, the numerical codes 1, 2,y,20 are used to denote the 20 native amino acid types according to the alphabetical order of their single character codes.\n\nThe above equations were used to identify membrane proteins and their types , enzymes and their functional classes , proteases and their types , protein quaternary structural attributes (Shen and Chou, 2009b) , as well as protein subcellular localization (Chou and Shen, 2010a; Chou and Shen, 2010b) .\n\nBesides the aforementioned PseAAC modes, there may be some other feature extraction methods to represent protein samples, but they can always be formulated with the form of Eq. (6), the general formulation of PseAAC.\n\nIt is instructive to point out that, regardless of which kind of PseAAC mode is adopted for protein samples, the query proteins and the proteins used to train the prediction engine must be defined in the same infrastructural frame with exactly the same dimension. For instance, if a query protein is defined in the 17402-D FunD space (see Eq. (9)), then the prediction should be carried out based on those proteins in the training set that can be defined in the exactly same 17402-D FunD space as well. If a query protein is defined in the 60020-D GO space (see Eq. (11)), then the prediction should be carried out based on those proteins in the training set that can be defined in the exactly same 60020-D GO space as well. If the query protein in both the 17402-D FunD space and 60020-D GO space is a naught vector and hence must be defined instead in the sequential evolution space (see Eq. (13)), then all the proteins used to train the prediction engine must also be formulated in the same sequential evolution space. It is particularly important to follow such a self-consistency principle when hybridizing different PseAAC modes or building an ensemble classifier by fusing many individual classifiers (Chou and Shen, 2006d) .\n\nThe problem of predicting protein attributes can be generally described as follows. Suppose a system containing N proteins (P 1 ,P 2 ,y,P N ), which have been classified into M subsets (categories) as formulated by Eq. (1), where each subset S m (m\u00bc 1,2,y,M) is composed of proteins with the same attribute category and its size (the number of proteins therein) is N m . Obviously, we have N\u00bcN 1 +N 2 +?+N M . According to Eq. (6), we can suppose without losing generality that the k-th protein in the subset S m (see Eq. (1)) is expressed by\n\nwhere c k m,j \u00f0 j \u00bc 1,2,. . .,O\u00de is the j-th component of the k-th protein in S m . Now, for a query protein P as defined by Eq. (6), how can we identify which subset it belongs to? Many different prediction algorithms have been introduced to address this problem, such as discriminant algorithm Chou and Elrod, 1999) , neural network algorithm (Cai et al., 2000; Cai et al., 2001) , support vector machine (SVM) Cai et al., 2004; Chou and Cai, 2002) , and K-nearest Neighbor algorithm Chou and Shen, 2006b) . In this paper we shall focus on the K-nearest neighbor algorithm (Denoeux, 1995) and show how to generate a powerful ensemble classifier by fusing many individual basic classifiers characterized with different control parameters.\n\nThe K-nearest neighbor (KNN) classifier is quite popular in pattern recognition community owing to its good performance and simple-to-use feature. According to the KNN rule (Denoeux, 1995; Keller et al., 1985) , named also as the ''voting KNN rule'', the query protein should be assigned to the subset represented by a majority of its K nearest neighbors, as illustrated in Fig. 5 There are many different definitions to measure the ''nearness'' for the KNN classifier, such as Euclidean distance, Hamming distance (Mardia et al., 1979) , and Mahalanobis distance (Chou, 1995a; Mahalanobis, 1936; Pillai, 1985) . Usually, the following equation was adopted to measure the nearness between proteins P and P k m (cf. Eqs. (6) and (15)):\n\nD\u00f0P,P k m \u00de \u00bc 1\u00c0 \n\nwhere PUP k m is the dot product of the two vectors, and :P: and :P k m : their modulus, respectively. According to Eq. (16), when P P k m we have D\u00f0P,P k m \u00de \u00bc 0, indicating the ''distance'' between these two proteins is zero and hence they have perfect or 100% similarity. In using the KNN rule, the predicted result will depend on the selection of the parameter K, the number of the nearest neighbors to the query protein P, as described below.\n\nThe nearest neighbor classifier (Cover and Hart, 1967) , also called NN classifier, is a special case of KNN classifier with K \u00bc1 (Fig. 5) . With the NN classifier, the protein P will be predicted belonging to the same attribute category of the protein in the learning dataset that has the shortest ''distance'' to P, i.e., the query protein will be classified in the m-th attribute category if\n\nwhere min P k m A Sm means taking the minimum value of D\u00f0P,P k m \u00de for the proteins in the subset S m (cf. Eqs. (1) and (16) there are two and more arguments leading to the same minimum value, the query protein will be randomly assigned to one of the subsets associated with these arguments although this kind of tie case rarely happens. Owing to its simplicity and apparent efficiency, the NN classifier is still a favorite method used by many investigators (see, e.g., Chen et al., 2010; He et al., 2010; Huang et al., 2010) .\n\nWith the KNN classifier when K 41, the attribute of the query protein P will be determined by the majority of its K nearest neighbors via a vote (Fig. 5) , as can be formulated as follows. Suppose \u00f0P \u00c3 1 ,P \u00c3 2 ,. . .,P \u00c3 K \u00de are the K proteins in S that have the closest distances to P, the query protein will be predicted belonging to predicted belonging to category 2 as its nearest protein does; when K\u00bc 3, the query protein is predicted belonging to category 3 because two of its three nearest proteins belong to that category; when K \u00bc9, the query protein is predicted belonging to category 2 again because the majority of its nine nearest proteins belong to category 2. the m-th subset (attribute category) if\n\nwhere m is the argument of m that maximize\n\nwhere A is a symbol in the set theory meaning ''member of''. If there is a tie for the voting results, the query protein will be randomly assigned to one of the locations associated with the tie case. Generally speaking, the greater the K (the number of the nearest neighbors counted), the less likely the tie case occurs. As mentioned above, the sequential evolution PseAAC mode of Eq. (13) contains a parameter l, which is associated with what tier of sequence correlation is taken into account for the PseAAC. As we can see from Eq. (14), the only constraint to l is that it must be smaller than L, the number of the amino acids in the protein concerned. Suppose the length of the shortest protein investigated is 50, then l can be any of the following 50 numbers: 0, 1, 2,y,49. Although in principle we can include all these possibilities for l by enlarging the dimension of the PseAAC to contain 20 \u00c2 50 \u00bc1000 components, it may cause various unfavorable problems for statistical prediction, such as ''high dimension disaster'' and ''overfitting redundancy'' (Wang et al., 2008a) . Actually, it may reduce the cluster-tolerant capacity (Chou, 1999 ) and lower down the success rate of cross-validation if the PseAAC contains too many trivial components. Accordingly, for a given training dataset, there is an optimal number for l. However, it would be time-consuming and tedious to find the optimal l by changing its value and doing tests one-by-one. Likewise, the KNN classifier (cf. Eq. (18)) also contains a parameter K, the number of the nearest neighbors to a query protein (Fig. 5 ). It will affect the predicted result by choosing a different value for K. In other words, for a given training dataset, there is an optimal value for K as well.\n\nThe parameters such as l and K are called uncertain parameters.\n\nThe number of the uncertain parameters depends on which model is used to represent the protein samples and what classifier is used for the prediction engine. It can be seen from Eqs. (9), (11), (13), and (18) that one uncertain parameter, K, needs to be determined if using KNN classifier based on the FunD (or GO) mode of PseAAC, and that two uncertain parameters, K and l, need to be determined if using KNN classifier based on the sequential evolution mode. It would be much more tedious and time-consuming to determine the optimal values for two uncertain parameters. To deal with this kind of uncertain parameters, let us introduce the fusion approach.\n\nFor most cases in using the KNN classifier to predict protein attributes, when K 420, the success rate by the KNN classifier would decrease remarkably. Therefore, the basic individual classifiers to be considered can be generally expressed as KNN x P \u00bc M 1 \u00f0K,P\u00de A S, when P is in the P FunD or P GO mode\n\nwhere KNN represents the KNN classifier that is a function of K, the symbol x is the identification operator meaning using KNN to identify the attribute of the query protein P among the M subsets of S in Eq. (1). Suppose the accumulated score thus obtained (with K \u00bc1,2,y,20) for the protein P belonging to the m-th subset S m A S is given by Y \u00f01\u00de m \u00f0P\u00de \u00bc\n\nThus the query protein P is predicted belonging to the subset with which its score of Eq. (21) is the highest, i.e., the query protein P is identified as belonging to the m-th subset if\n\nwhere m is the argument of m that maximizes the score function Y \u00f01\u00de m of Eq. (21). If there are two and more arguments leading to the same maximum value, the query protein will be randomly assigned to one of the subset associated with these arguments although this kind of tie case rarely happens.\n\nWhen the KNN classifier is operated on the query protein formulated with the sequential evolution mode (cf. Eq. (13)), we are facing a problem with two uncertain parameters, K and l. In general, the shortest protein sequence investigated is 50 amino acids Chou and Shen, 2010c) , hence we can set the maximum value allowed for l is 49. Thus, the basic individual classifiers to be considered would become as follows: \n\nand the query protein P l Evo is predicted belonging to the subset with which its score of Eq. (25) is the highest, i.e., the query protein P is identified as belonging to the m-th subset if\n\nwhere m is the argument of m that maximizes the score function Y \u00f02\u00de m \u00f0P\u00de of Eq. (25). If there are two and more arguments leading to the same maximum value, the query protein will be randomly assigned to one of the subcellular locations associated with these arguments although this kind of tie case rarely happens.\n\nIf a basic individual classifier involves with three or more uncertain parameters, by following the similar procedures as described above, we can perform three or higher dimensional fusion.\n\nAfter a prediction method has been developed, a subsequent and natural question to ask is: What is its accuracy?\n\nIn statistical prediction, it would be meaningless to simply say a success rate of a predictor without specifying what cross-validation method and benchmark dataset were used to test its accuracy.\n\nIn literatures, the following three cross-validation methods are generally used for examining the effectiveness of a statistical prediction method: (1) the independent dataset test, (2) the subsampling (G-fold such as 5-or 10-fold cross-validation) test, and (3) the jackknife test (Chou and Zhang, 1995) .\n\nFor the independent dataset test, although all the proteins used to test the predictor are outside the training dataset used to train it so as to exclude the ''memory'' effect or bias, the way of how to select the independent proteins to test the predictor could be quite arbitrary unless the number of independent proteins is sufficiently large. This kind of arbitrariness might result in completely different conclusions. For instance, a predictor achieving a higher success rate than the other predictor for a given independent testing dataset might fail to keep so when tested by another independent testing dataset (Chou and Zhang, 1995) . Accordingly, the independent dataset test is not a fairly objective test method although it was often used to demonstrate the practical application of a predictor (see, e.g., Cedano et al., 1997; Chou and Elrod, 1999; Chou and Shen, 2006c; Chou and Shen, 2007a) .\n\nFor the subsampling test, the concrete procedure usually used in literatures is the 5-fold, 7-fold, or 10-fold cross-validation. The problem with the G-fold cross-validation test as such is that the number of possible selections in dividing a benchmark dataset is an astronomical figure even for a very simple dataset. This is because for a benchmark dataset as formulated in Eq. (1), the number of possible combinations of taking one G-th or 1/G proteins from each of the subsets in Eq. (1) will be\n\nwhere N m is the number of proteins in the m-th subset S m , and the symbol Int is the integer-truncating operator meaning to take the integer part for the number in the brackets right after it. For example, without losing generality let us consider the case of 5-fold cross-validation (i.e., G\u00bc5) for a very simple benchmark dataset that contains 250 proteins, of which N 1 \u00bc65 belongs to subset S 1 , N 2 \u00bc60 to subset S 2 , N 3 \u00bc55 to subset S 3 , and N 4 \u00bc 70 to subset S 4 . Substituting these figures into Eqs. (28-29), we have that the number of possible combinations of taking one-fifth proteins from each of the four subsets will be \n\nindicating that for such a simple and small benchmark dataset, the number of possible combinations of taking one-fifth proteins from each of the four subsets for 5-fold cross-validation will be an astronomical number. Now let us consider a moderate-size dataset that consists of 640 proteins classified into M\u00bc8 subsets with each containing 80 proteins, i.e., N 1 \u00bc N 2 \u00bc ?\u00bcN 8 \u00bc80. According to Eqs. (28-29), the number of possible combinations of taking one-fifth proteins from each of the 8 subsets for 5-fold-cross-validation will be \n\nIf the above benchmark dataset is slightly larger and complicated, i.e., the number of proteins is increased from 640 to 800, and the number of subsets from 8 to 10 with each still containing 80 proteins, then the number of possible combinations of taking one-fifth proteins from each of the 10 subsets for 5-fold-cross-validation will be P \u00bc P 1 UP 2 UP 3 \u00c1 \u00c1 \u00c1 P 10 \u00bc 80! \u00f080\u00c016\u00de!16! 10 4 the maximum number allowed to be calulated in a computer\n\nActually, many typical benchmark datasets contain more than 1000 proteins (see, e.g., Chou and Shen, 2008a; Chou and Shen, 2010a; Chou and Shen, 2010c) . Therefore, in any actual subsampling crossvalidation tests, only an extremely small fraction of the possible selections are taken into account. Since different selections will always lead to different results even for a same benchmark dataset and a same predictor, the subsampling test (such as 5-fold crossvalidation) cannot avoid the arbitrariness either. A test method unable to yield a unique outcome cannot be deemed as an ideal one.\n\nIn the jackknife test, all the proteins in the benchmark dataset will be singled out one-by-one and tested by the predictor trained by the remaining protein samples. During the process of jackknifing, both the training dataset and testing dataset are actually open, and each protein sample will be in turn moved between the two. The jackknife test can exclude the ''memory'' effect. Also, the arbitrariness problem as mentioned above for the independent dataset test and subsampling test can be avoided because the outcome obtained by the jackknife cross-validation is always unique for a given benchmark dataset. As for the possible overestimation in success rate by jackknife test because of only one sample being singled out at a time for testing, the answer is that as long as the jackknife test is performed on a stringent benchmark dataset in which none of proteins has Z25% pairwise sequence identity to any other in a same subset such as those mentioned in the Section 2, it is highly unlikely to yield an overestimated rate compared with the actual success rate in practical applications, as demonstrated in Chou and Shen ( 2010c) and Shen and Chou (2010) . Besides, when the jackknife test was used to compare two predictors, even if there was some overestimate due to using a less stringent benchmark dataset for one predictor, the same overestimate would exist for the other as long as they were both tested by the same dataset.\n\nAccordingly, the jackknife test has been increasingly and widely used by investigators to examine the quality of various predictors (see, e.g., Anand and Suganthan, 2009; Cai et al., 2010; Chen et al., 2008a; Chen et al., 2008b; Chen and Han, 2009; Du and Li, 2008; Du et al., 2009; Fang et al., 2008; Feng and Luo, 2008; Gu and Chen, 2009; Gu et al., 2010a; Jahandideh et al., 2007a; Jahandideh et al., 2007b; Jahandideh et al., 2009; Ji et al., 2010; Kannan et al., 2008; Li et al., 2009; Lin, 2008; Lin et al., 2009a; Liu et al., 2010a; Munteanu et al., 2008; Nanni and Lumini, 2008; Nanni and Lumini, 2009; Rezaei et al., 2008; Shao et al., 2009; Shi et al., 2008; Shi and Hu, 2010; Vilar et al., 2009; Wang and Yang, 2010; Wang et al., 2010a; Wang et al., 2008b; Yang and Jiang, 2010; Yang et al., 2009; Zhao et al., 2008; Zhou et al., 2008) .\n\nHowever, even if using the jackknife approach for cross-validation, the same predictor may still generate obviously different success rates when tested by different benchmark datasets. This is because the more the stringent of a benchmark dataset in excluding homologous and high similarity sequences, the more the difficult for a predictor to achieve a high overall success rate (Chou and Shen, 2010a) . Also, the more the number of subsets (attribute categories) a benchmark dataset covers, the more the difficult to achieve a high overall success rate. This can be easily conceivable via the following consideration. Suppose a benchmark dataset consists of two subsets (attribute categories) with each containing the same number of proteins. The overall success rate in identifying their attribute categories by random assignment would be 1/2\u00bc50%. However, for a benchmark dataset consisting of 20 subsets, the corresponding overall success rate by the random assignment would be 1/20\u00bc5%, which is only one-tenth of the former.\n\nEven if a powerful predictor has been developed by accomplishing the above four procedures, namely constructing a valid benchmark dataset, formulating protein samples with PseAAC to successfully catch their essential and core features, introducing a powerful and efficient algorithm or engine to operate the prediction, and achieving a high overall success rate by jackknife test on a stringent dataset in which none of the proteins included has Z25% pairwise sequence identity to any other in the same subset (attribute category), it does not mean that the predictor has been really completed. This is because we are living in the Internet Age. To make a new prediction method really useful for the majority of people, it is an important direction or necessary procedure to provide a user-friendly and publicly accessible web-server for the method (Chou and Shen, 2009) . Technically speaking, a web-server means a computer program that is responsible for accepting Hypertext Transfer Protocol (HTTP) requests from clients. By means of webservers, many computational prediction methods, regardless how difficult their mathematics or how complicated their algorithms are, can be easily used by the vast majority of scientists to generate their desired data without the need to understand the mathematical details.\n\nIn order to timely utilize the huge amount of newly discovered protein sequences generated in the postgenomic era for basic research and drug development, scientists are anxious to know their biological attributes. Many studies from various research laboratories around the world have indicated that mathematical analysis, computational modeling, and introducing novel physical concept to biology and medicine, such as graphical analysis (Andraos, 2008; Myers and Palmer, 1985; Zhou and Deng, 1984) , modeling three-dimensional structures of targeted proteins/peptides for drug design (Sharma et al., 2008; Zhou and Troy, 2003; Zhou and Troy, 2005a; Zhou and Troy, 2005b; Zhou et al., 2004) , diffusion-controlled reaction simulation (Zhou et al., 1981; Zhou and Zhong, 1982; Zhou et al., 1983) , cellular responding kinetics (Qi et al., 2007) , and biological functions of solitons in DNA (Zhou, 1989) can provide useful insights for both basic research and drug design and hence are widely welcome by science community. In view of this, it is highly desirable to develop automated methods by introducing new concepts and approaches for fast and accurately predicting the attributes of uncharacterized proteins based on their sequence information alone. During the past two decades or so, many statistical methods for predicting various protein attributes have been proposed. In this review, the key steps for establishing a powerful predictor in this regard have been analyzed in hopes that the points raised here may help stimulate the further development of new and more powerful predictors in this area. It is anticipated that the general form of PseAAC as formulated in this review may further stimulate the efforts to find various new modes of optimal PseAAC, which is one of the most important future directions we should focus on in order to substantially improve the power of predicting protein attributes."}