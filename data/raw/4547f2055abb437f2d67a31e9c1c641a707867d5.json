{"title": "Constructing the effect of alternative intervention strategies on historic epidemics", "body": "During the outbreak of an epidemic, decisions are taken on how to intervene in order to mitigate its impact (Anderson et al. 2004) . The time scale in which a decision must be taken and the paucity of information on key epidemiological parameters early in the epidemic make the choice of intervention or control difficult (Ferguson et al. 2001a,b) . The decision is often controversial (Cunningham et al. 2002; Kitching et al. 2006; Wingfield et al. 2006) , particularly so if the control strategy effected involves pro-active culling of non-symptomatic animals or crops or, for human diseases, travel restrictions. Following the cessation of the outbreak, one question that naturally arises is: was the right choice of control made? To answer this, we need to determine what would have occurred had an alternative, mooted choice of control been implemented. Moreover, we need to do this while taking account of uncertainty in both the parameters governing the process and the underlying, partially observed trajectory of the outbreak (definitions of terminology used in the paper are provided in table 1).\n\nWith a little thought, it becomes apparent that the question is ill-posed, since there are at least two interpretations, as follows. Was the best choice made given what was known at the time of the decision? Was the best choice made in the light of what is known now? We call these prospective and retrospective questions of the appropriateness of the choice of control, respectively. The prospective situation is the common one that confronts epidemiologists and decision makers in an emerging epidemic. Since there is little information to inform choices made at an early stage, though, the prospective question amounts merely to querying the professional competence of the person or persons who made the decision.\n\nIt is the retrospective appropriateness that is of most general interest when looking back at an historic epidemic. The retrospective question addresses the ultimate effectiveness of the actual and mooted actions in the light of what has passed (cf. table 1). It is the retrospective question, too, that interests those who have been personally affected by an intervention. A farmer whose entire herd or crop is culled will want to know whether swifter implementation of control measures might have saved the enterprise, not in a long-run population of outbreaks, but in the specific, actual outbreak in which the losses occurred.\n\nIn assessing the retrospective question, it is imperative that full use is made of the information provided by the historic epidemic, of which there are two types: parametric, i.e. information about the process, and trajectory based, i.e. information encapsulated in the epidemic's trajectory (figure 1 for schematic). Until now, however, it has seemed to many researchers that studying populations of outcomes independent from reality is the only way to address notional controls. This semi-retrospective approach was taken by Riley et al. (2003) for the SARS epidemic of 2003 and by Keeling et al. (2001) for the foot-and-mouth disease (FMD) outbreak in the UK in 2001. Riley et al. (2003) noted the difficulty in interpreting the results, describing the notional epidemics of SARS generated with parameters estimated from Hong Kong cases as representing cities with 'Hong Kong-like characteristics' rather than Hong Kong per se. One of the authors of the foot-and-mouth paper later pointed out a paradox of the approach, namely that implementing control measures more swiftly than happened in reality leaves 'a significant probability of a worse outcome than was actually observed' (Woolhouse 2003) . The cause of this paradox is illustrated by analogy to the following example. Suppose that one mooted intervention during the 2001 FMD epidemic in Great Britain (Ferguson et al. 2001a,b) , in addition to the controls implemented in reality, was to vaccinate all livestock on the Isle of Man (itself unaffected by the epidemic despite its proximity to the heavily affected areas of Dumfries, Galloway and Cumbria). It is reasonable to believe that there would be no effect of this alternative intervention strategy on the actual outbreak. This should be reflected in predictions that the notional epidemic be the same as the actual one. Ignoring the information content of the observed trajectory of the epidemic, however, leads to a distribution of possible outcomes following the spurious vaccination, some of which have greater and some less disease than the true outbreak, and which is not necessarily centred on the actual outcome. This paradox and the awkwardness of interpretation can be rectified by incorporating trajectory-based information in analyses to compare alternative control strategies. an intervention strategy is any action that may change the way an epidemic invades a population. Non-intervention is also considered to be an intervention strategy the actual or historic epidemic is the one that did occur in a given place and time under the actual or historic intervention strategy a mooted or alternative intervention strategy is one that may differ from the actual intervention strategy (it may also be the same) a notional epidemic is one that did not occur but might have occurred had a mooted intervention strategy been effected rather than the actual one parametric information encapsulates the uncertainty in the parameters of a model. A single vector of parameter values is capable of generating multiple epidemic trajectories or realizations (though not all are equally likely), of which only one occurs in one temporal and spatial locality trajectory information encapsulates the uncertainty in a single epidemic trajectory. A single trajectory could have been generated by multiple parameter values, though again not all are equally likely. Note that this information does not have to provide a complete representation of the outbreak a prospective analysis of the effect of a mooted intervention strategy uses current parametric and trajectory-based information to determine what the possible future effects would be, and often is undertaken while the outbreak is at an early stage a retrospective analysis of the effect of a mooted intervention strategy uses current parametric and trajectory-based information to determine what the possible past effects would have been, and might be carried out once the outbreak has ceased a semi-retrospective analysis of the effect of a mooted intervention strategy uses up-to-date parametric information and partial trajectory information to determine what the possible past effects would have been in an ensemble of realizations of alternative realities, which have trajectories that, once the first change in the intervention occurs, are independent of the historic trajectory. Such an analysis also might be carried out once the outbreak has ceased infection actual epidemic (X) notional epidemic (X\u2032) time mooted actual actual control: mooted control:\n\nprospective: X semi-retrospective: retrospective: X X Figure 1 . Diagram representing the different approaches described in the paper. In this simple scenario, the actual intervention (control) r is implemented at time t actual , lasts until the end of the outbreak and results in epidemic trajectory X. The choice of r makes use of parametric (q) and trajectorybased (X ) information until time t actual . After the outbreak is over, an alternative mooted intervention r 0 is considered which would have been implemented at time t mooted . This would have resulted in the notional trajectory X 0 , which has a distribution reflecting our uncertainty in the parameters and trajectory. In the semi-retrospective approach, all available parametric information is used, but the only trajectory information used is that occurring before the intervention. In the (fully) retrospective approach described in this paper, all available information (on X and q) is used.\n\nA heuristic, retrospective solution has been proposed by Haydon et al. (2003) and applied to FMD. This involves constructing an epidemic tree to summarize the course of infection by linking each infected farm to the donor that infected it. Contact tracing was used to identify some links, while other links were unknown and were constructed using ad hoc data-driven rules, such as the nearest potential donor, or a randomly selected farm from the set of potential donors weighted by a function of distance to the recipient. The notional effect of mooted interventions, such as swifter implementation of the national movement ban, could then be considered by removing branches of the tree. The approach enabled Haydon et al. (2003) to obtain estimates of disease levels under mooted interventions. Haydon et al. (2003) recognized these to be underestimates, reflecting an inherent bias in their methods, which allows only a single incoming branch to any recipient, so that when a mooted intervention results in the removal of a branch, the notionally non-infected host unit is considered safe from further infection, regardless of the infective pressure exerted upon it by other farms. Note that although the approach of Haydon et al. (2003) does eliminate the paradox of potential increase in disease levels under greater control, this is partly due to the inherent restriction that branches are capable of being removed but not of being inserted.\n\nThe current paper formalizes the heuristic approach of Haydon et al. (2003) by introducing and testing a novel framework for evaluating mooted interventions on historic outbreaks. The new method makes full use of all information available in a statistically coherent fashion that overcomes the bias of previous work and is applicable to a very broad class of models. The framework treats mooted interventions consistently regardless of whether the difference between them and the actual intervention be slight or major. Section 2 introduces the background for the approach and the way it differs from the standard method for prospective analysis. We follow this by introducing the methodology that allows historic and notional epidemics to be coupled. This is done by matching the latent processes generating epidemics that differ only through the effect of the intervention strategies used. The new methodology is illustrated with two examples-one simple, the other more complex-based on historic data: the general SIR model applied to common cold data and a spatio-temporal SI model applied to data on a spatially extended arboreal disease. In the latter, interventions take the form of physical removal of symptomatic and, possibly, also asymptomatic hosts. In the concluding discussion, we compare the approach with previous attempts and discuss some of the inferential issues that arise.\n\nThe question of the prospective appropriateness of differing control strategies is, in principle at least, easy to answer using statistical decision theory (Berger 1993) . We denote the unknown parameters by q, the actual epidemic process by X, the original choice of intervention by r and observed data by D(X ). The utility function, characterizing the costs and benefits of the intervention r, is denoted U(X, r). We also consider a mooted alternative intervention r 0 and the resulting notional epidemic process X 0 ; there are natural generalizations to more than two alternatives. Throughout we work within the Bayesian paradigm (Lee 2004 ) and use p to denote both probability mass and density. The best prospective choice of intervention (r or r 0 ) given what is known at a time T 1 during the outbreak is the one maximizing the expected utility conditional on this knowledge\n\nAlthough the problem is easily posed, carrying out this integration may be computationally challenging. One approach utilizes Monte Carlo simulation-draw values of q from p\u00f0qjD\u00f0X t!T 1 \u00de\u00de, use these to generate samples from p\u00f0X tOT 1 jD\u00f0X t!T 1 \u00de\u00de and p\u00f0X 0 tOT 1 jD\u00f0X t!T 1 \u00de\u00de, evaluate utilities and take averages. The expected effect of different strategies can then be compared.\n\nIt may initially appear that a similar approach can be used retrospectively to find the best decision at time T 1 based on what we know at the present (time T 2 , say), replacing D\u00f0X t!T 1 \u00de by D\u00f0X t!T 2 \u00de in the algorithm above. This is inappropriate, though, if the distribution of X 0 tOT 1 is in any way dependent on X tOT 1 , which will be the case if both parametric and trajectory-based information are to be fully utilized. We term this the semi-retrospective approach, since it conditions on some information but disregards other information.\n\nAn alternative approach is needed to make full use of all information at our disposal. This requires that pairs of epidemics be coupled so that the distribution of the effect of one intervention conditional on that of another can be determined.\n\nImagine for a moment that the 'interventions' are just alternative ways of observing the system for the purposes of collecting data for inference. We assume that collecting the data has no bearing on the epidemic outcome (no pathogens are inadvertently spread by the collectors, for example) so that the actual epidemic process X is maintained regardless of whether r or r 0 is carried out, although D(X ) and D 0 (X ) differ. This scenario holds when we attempt to devise retrospectively optimal designs of observation schemes (Cook et al. in press) . Then the distribution of D 0 (X ) conditioned on D(X ) is\n\nwhich may be sampled using Markov chain Monte Carlo (MCMC) and data augmentation (Gibson & Renshaw Alternative intervention strategies A. R. Cook et al. 1205 Cook et al. 1998 O'Neill & Roberts 1999) or may be deterministic if D 0 (X ) is fully specified by D(X ). By analogy, one way to evaluate the effect of a notional intervention strategy on an historic epidemic is to seek the joint distribution of two epidemics X and X 0 that differ in some sense only through the effect of their intervention strategies r and r 0 . The distribution of X 0 conditioned on our knowledge of X then gives our best prediction of what would have happened had the notional intervention been chosen rather than the actual one.\n\nThe joint distribution p(X, X 0 ) cannot, however, ever be validated empirically, since it is impossible to observe both X and X 0 , although both marginal distributions can be found by repeated sampling. The impossibility of observing the effects of two treatments on the same sampling unit has long been noted (Rubin 1974) and has been called the fundamental problem of causal inference (Holland 1986 ). Some assumption is therefore necessary to circumvent the problem. Causal inference underlies the approach of Haydon et al. (2003) , who assumed that inferred branches of their epidemic tree that were not removed by a mooted intervention would be invariant to that intervention, and therefore would be maintained in the notional outbreak. This is also the approach we take in this paper, although our approach allows potential contacts that did not cause infection in reality also to be invariant to changes in the intervention strategy.\n\nSuppose, therefore, that we can identify a latent or underlying stochastic process Z whose sample path is unaffected by the choice of intervention and which, together with the intervention, determines the outcome of the epidemic X Z g\u00f0Z; r\u00de; \u00f03:2\u00de X 0 Z g\u00f0Z; r 0 \u00de: \u00f03:3\u00de\n\nIt then follows that the marginal for the notional outcome conditioned on the actual epidemic is\n\nwhere Z is the space of Zs consistent with X and X 0 . The problem is that there are many different choices of Z, and in general they give different results. In selecting a Z process to match different interventions, we propose the following desiderata:\n\n(D1) Z should represent something we might reasonably expect to be invariant to changes in r.\n\nSince the value of Z is unknown in practice, we should be able to evaluate in a straightforward way its distribution conditional on the observed part of X, perhaps numerically.\n\nThe most important of these is the first one, i.e. the validity of the reasoning that the physical nature of Z should be maintained for differing interventions.\n\nOne common way of modelling an epidemic is as a modified Poisson process. If hosts mix homogeneously and contact sufficient for disease to spread occurs at a constant rate b, say, between each pair of hosts, then the occurrence of contacts in the population is a collection of Poisson processes of rate b. Non-homogeneous mixing of hosts may also be accounted for by allowing b to vary with the distance (in space or social space) between hosts (as in the example in \u00a75), for example. Infection and hence disease is assumed to spread across a contact if at that time one host is infectious and the other susceptible. Interventions may take the form of actively removing hosts before they spread infection, or reducing the number of contacts. Under the Poisson construction, Z is the infinite set of contact times and the hosts involved.\n\nThe infinite nature of the Poisson Z process is computationally undesirable, so we seek an alternative that is more manageable and yet functionally similar. We therefore consider a construction due to Sellke (1983) , which is an equivalent way of formulating standard stochastic epidemic processes. (The approach is connected to the idea of non-centred parameterizations (Papaspiliopoulos et al. 2003) , since there is a one-to-one relationship between an individual host's Sellke threshold and the cumulative distribution function of the infection time of that host.)\n\nSellke's construction assigns to each individual j in the population a threshold or resistance to infection Z j wExp (1) that must be overcome before j becomes infected. The threshold is overcome by the accumulation of infective pressure-if the rate of infection of j from all sources at time u is f j (u) (that may vary according to host heterogeneity and the evolving contact structure, cf. Cook et al. 2007 ), then the time t j at which j is infected is the solution of \u00d0 t j 0 f j \u00f0u\u00de duZ Z j . (When no solutions exist because Z j O \u00d0 N 0 f j \u00f0u\u00de du, then there is insufficient infective pressure to infect j and the host escapes infection.) In the general stochastic epidemic model with homogeneous mixing of hosts, the rate of where b is the rate of infection from one infectious host to one susceptible host, 1fAgZ 1 if A is true and 0 otherwise, and S(t) and I (t) are the sets of susceptible and infective hosts at time t, respectively. Sellke (1983) shows that this is equivalent to the standard formulation of the infection process of the general stochastic epidemic model, such as the Poisson process approach described previously or Gillespie's (1977) algorithm. Under the Sellke construction, Z is the set of thresholds, with one threshold per host.\n\nDenoting distributions under the Poisson construction as p P and under the Sellke as p S , then although p P \u00f0X\u00deZ p S \u00f0X\u00de and p P \u00f0X 0 \u00deZ p S \u00f0X 0 \u00de (i.e. the distribution of an epidemic X is identical under the two approaches), it is not generally true that p P \u00f0X; X 0 \u00deZ p S \u00f0X; X 0 \u00de. This is illustrated with a simple example (figure 2). Two hosts (A and B) are infected by background sources at rate b. Once one host is infected, it infects the other also at rate b. Suppose that A is infected first. The two interventions considered are r: do nothing, and r 0 : remove the first host to become infected immediately upon its infection. The distribution of the time A is infected is\n\nand under r 0 , \u00f0t B K t A jt A \u00de wExp \u00f0b\u00de. Now suppose that we have observed X completely and thus know t A and t B . Under the Sellke construction, there is a one-to-one mapping between the latent process Z and infection times given the parameters, and so the notional infection time for B takes a point mass at t 0\n\nUnder the Poisson construction, however, B was infected by the background source with probability 1/2, in which case its infection is unaffected by the removal strategy and t 0 B Z t B ; otherwise it was infected by A in reality and so its notional distribution is thus \u00f0t 0 B K t B jt B \u00de wExp \u00f0b\u00de, since there is no information on the next infectious contact from the background source to B. The difference is illustrated in figure 2. (7) I(7) + R (7) 20 40 60 80 100 120 I(7) + R(7) Alternative intervention strategies A. R. Cook et al. 1207 In practice, it is justifiable to use the Sellke as an approximation to the Poisson construction. Infection times are not observed precisely in reality; instead they are typically censored, recorded to the nearest day, for example. In the presence of censoring, event times assume a joint distribution conditional on what is observed (D(X ) rather than X ), and the two approaches give very similar conditional distributions p\u00f0X 0 jD\u00f0X\u00de\u00de (not shown).\n\nThis similarity of behaviour is illustrated through the following simulation example based upon the general SIR epidemic (Bailey 1975) . Motivated by the case study in \u00a74, consider a homogeneously mixing population of size NZ262 with rate of infection per S-I pair (as in equation (3.5)) bZ0.003 and rate of recovery per infected individual gZ0.66. We start with 10 individuals infected at time 0. (The high initial number of infectives is for convenience of representation and does not affect the generality of the comparison.)\n\nA total of 100 000 realizations of Z is generated for both Sellke and Poisson constructions (see appendix A in the electronic supplementary material). These are used to obtain the distribution of the number of infective and removed hosts at time tZ7 under the interventions r: do nothing, and r 0 : halve b from time tZ3 onwards (which we implement in the Poisson construction using R\u00e9nyi's splitting theorem (R\u00e9nyi 1964; Srivastava 1971) to discard each contact with probability 1/2). These are shown in figure 3, both jointly and marginally. Also shown is the relationship taking the notional disease trajectory to be independent of the true one after times tZ0 and 3. For this case, the Sellke construction provides an excellent approximation to the joint distribution of the number of infectives I(7) and I 0 (7) generated under the Poisson construction, with the two quantities strongly correlated. This provides prima facie justification for using the more tractable Sellke construction in practice. Additional simulations (not shown) indicate that conditional on a complete realization of the epidemic, the resulting notional trajectory under Sellke matches the mean of the distribution of Poisson trajectories closely. In contrast, the mean using the semi-retrospective approach matches the Poisson trajectories well only when a major intervention occurs so that little information from the actual outbreak is relevant in constructing the notional trajectory.\n\nWe compare the semi-retrospective approach with the (fully) retrospective approach by applying both to historic data on the common cold on the remote island of Tristan da Cunha during an outbreak starting January 1965 (Hammond & Tyrrell 1971; Shibli et al. 1971 ) and considering the effectiveness of two alternative interventions.\n\nThe general SIR stochastic epidemic model (Bailey 1975 ) is fitted to the daily numbers of removed individuals (i.e. those whose symptoms have ceased and have moved from the I to the R class) using standard data augmentation and MCMC integration techniques to obtain the joint posterior distribution of parameters and unobserved event times (e.g. Gibson & Renshaw 1998; O'Neill & Roberts 1999 , and appendix B in the electronic supplementary material). The model has infection as in equation (3.5), i.e. infection occurs at rate b per S-I pair in the absence of intervention. The control strategies whose effectiveness we wish to investigate take the form of reductions to b 3 days after the first removal, perhaps due to warnings issued by the island's physician to reduce contact with other islanders. Letting bx be the rate after 3 days, the two strategies we consider are r 1 : xZ0.5 and r 2 : xZ0.9. Infected individuals recover, leaving I (t) and entering the set of removed individuals, R(t), at rate g. The data are fR\u00f0t\u00deZ jR\u00f0t\u00dej : tZ 0; .; 19g, the population size of NZ262 and the fact that no subsequent infections occurred in the outbreak. The initial infection is assumed to arise by some other, unmodelled, process. Flat priors are taken for b and g on the region [0, 100] 2 .\n\nAs part of the parameter estimation routine, the set of Sellke thresholds is calculated and used to estimate the distribution of the number of removals with time, under the two control strategies of interest. Noninfected hosts are given randomly generated thresholds, conditional on being non-infected, making use of the memoryless property of the exponential distribution. Details of the implementation may be found in the electronic supplementary material, appendix B.\n\nPosterior medians and credible regions (Lee 2004 ) are shown in figure 4b,d. This figure also shows predictions using the semi-retrospective approach (figure 4a,c), with infection trajectories diverging from the actual one on day 3, when the intervention strategy begins. The problems with the latter approach are clear when one considers the predicted effect of a small reduction to the rate of infection-paradoxically, the expected amount of disease increases (figure 4a). Under our fully retrospective approach, however, we would instead predict a small decrease in the numbers of infections (figure 4b). When the change is more marked (xZ0.5, figure 4c,d ), the expected behaviour under the two approaches is similar, but taking the semi-retrospective approach inflates the variance considerably relative to the fully retrospective approach, yielding considerably poorer predictions.\n\nCitrus canker is an economically important disease of citrus trees caused by the bacterium Xanthomonas axonopodis pv. citri (Graham et al. 2004) . It was found to have been reintroduced to Florida, USA, in 1995, following a successful eradication programme from 1986 to 1992. From October 1997 to July 1999, Gottwald et al. (2002) collected spatio-temporal data on the locations and disease status of citrus trees in five residential districts of Florida. A detailed analysis will appear elsewhere; in this paper, we use the data to evaluate how effective a strategy of regular monitoring and removal of symptomatic trees, and potentially nonsymptomatic trees in their vicinity, would have been at reducing the number of trees infected.\n\nLet the rate of infection of j, a susceptible tree, be f j \u00f0t\u00deZeC X i bf \u00f0d i;j ;a\u00de1fi 2 I\u00f0t\u00deg1fj 2 S\u00f0t\u00deg; \u00f05:1\u00de\n\nwhere d i, j is the distance in kilometres separating trees i and j and f (d, a) is a dispersal kernel. We use the exponential kernel f \u00f0d;a\u00deZexp\u00f0Kd=a\u00de; results are robust to sensible choices of kernel (not shown). Rates of host-to-host infection are also governed by b, while e is the per capita rate of infection from external sources outwith the study area and potentially from anywhere within the infected area of the state. We analyse disease progress maps made at 30 day intervals over 360 days from 26 October 1997 to 20 October 1998 in a 10.3 km 2 area of Miami, Dade county (site D1 in Gottwald et al. 2002) . The trees in the study area were sampled extensively, yielding a very rich dataset, with 1124 of 6056 trees becoming infected. See Gottwald et al. (2002) for a fuller description of the data collection and some caveats regarding their interpretation. Note that the epidemic was ultimately interrupted by a prolonged period of dry conditions; prior to that, infected trees were effectively continuously infectious. During this year, removal efforts focused on clearing a backlog from elsewhere in the state and so no trees were removed from the epidemic from the population in question. We investigate the effect of notional removal strategies in which, at intervals of D days, all trees are assessed for infection. Infection is detected with probability q (including qZ1): trees found to be infected are removed immediately along with all other trees within a circle of radius r metres centred on the infected tree.\n\nIn reality, tree disease status was assessed by a team of phytopathologists (Gottwald et al. 2002) . In the mooted interventions, we allow for the possibility of non-detection accounting for less formal disease assessment. For the purposes of illustrating the methodology, here we focus exclusively on a simple criterion for the effectiveness of intervention: the total number of trees removed relative to the numbers infected in the historic epidemic. This does not take account of the force of infection generated by the population, nor of the costs of surveying and removing trees.\n\nWe used an MCMC routine to sample the posterior distribution of (a, b, e) (taking uniform priors over the region of interest) and the infection times ft i : iZ 1; .; 1124g. In figure 5, we present mean and 95% credible regions for P i 1fi 2 I\u00f0t\u00degC 1fi 2 R\u00f0t\u00deg (i.e. the number of trees lost to disease and/or removal) against time, under three interventions: removals every 60 days (DZ60, qZ1, rZ0), removals every 120 days (DZ120, qZ1, rZ0) and 40% removals every 120 days (DZ120, pZ0.4, rZ0). In table 2 we present the ratio of trees lost for these and several other removal strategies during a 360 day period relative to the actual number infected during this interval. This ratio is greater than 1 if pre-emptive removal of susceptible trees is carried out (rO0); otherwise the maximum the ratio may take is unity, as all other intervention strategies lead to a strict non-increase in the infective pressure on all hosts.\n\nFrom table 2 it is clear that removal of symptomatic trees would have substantially reduced the amount of trees being infected in the study area. The more frequent the surveying and removal, the less disease would have resulted, although losses depend nonlinearly upon surveying frequency. These results seem to suggest that pro-active removal of asymptomatic trees in the vicinity of a known infective is not an effective strategy, since it is predicted to increase total losses; indeed, the broadest removal radius considered by us was predicted to result in almost all trees being removed. We consider this ostensible inefficacy to be an artefact of analysing a non-isolated population. The study area considered here forms part of a greater population of citrus statewide (Gottwald et al. 2002) and pro-actively culling trees only in some areas allows disease to be reintroduced from elsewhere. However, this and the other study areas were unusual in that disease was allowed to increase without intervention by the regulatory agencies specifically so that the epidemic could be studied to aid the development of intervention strategies. Within all other locations throughout the state intense culling of symptomatic and surrounding trees was practised. From a statewide perspective, culling is desirable, for although culling results in heavy to complete losses of tree populations locally, the practice effectively lowers e at the geographic scale.\n\nThis paper introduced a novel approach to analysing retrospectively the effects of mooted interventions on historic epidemics. Coupling epidemics by matching their latent processes allows us to make full use of both parametric and trajectory-based information. By so doing, we avoid paradoxical results that may often occur when information about the epidemic trajectory is only partially used; these include predictions of a high probability of a more severe epidemic occurring with more effective controls than were used in reality (figure 4). Our method is easy to implement as part of a parameter estimation routine using standard MCMC techniques. The choice of latent process to match is subjective, but may be guided by desiderata, which the Sellke construction satisfies.\n\nThere is a logical inconsistency in using information from the whole of an outbreak to parametrize the model but only part of the information on the trajectory of the outbreak: that coming before a mooted intervention. This inconsistency is most evident when we wish to assess the effect of an incremental change to an actual strategy. Our fully retrospective approach is a natural alternative that preserves patterns and pathologies in the data. It preserves patterns wholly when the mooted intervention has effects identical to the actual one, and partly and decreasingly so when the mooted intervention changes more and more from the actual one. When the mooted intervention is drastically different from the actual one, all trajectory information is lost, and our approach gives predictions that match those using the semi-retrospective approach.\n\nThe mechanism used to couple epidemics was to match latent processes that we assume to be unaffected by control. We did this using the Sellke construction to transform the effect of one intervention strategy (including non-intervention) to another. The choice of matching-process is subjective. Even the semi-Alternative intervention strategies A. R. Cook et al. 1209 retrospective approach rests on an assumption: that the notional invasion trajectory is independent of the actual one, i.e. that the occurrence of events in the observed epidemic has no correlation with their occurrence in the notional outbreak. This is one extremum of the set of possible assumptions regarding the relationship between two epidemics. Indeed, any division of unobservables into 'parameters' and 'outcomes' is inherently arbitrary.\n\nThe main philosophical issue with any approach to evaluating the benefits provided by the actual intervention compared with the outcome of an alternative strategy is that the relationship between a notional and the actual epidemic can never be verified. This is not a new issue in modelling: in making any predictions based on a model, we implicitly trust that the model provides a reasonable description of reality and may be extrapolated to future or alternative conditions. Indeed, non-verifiability of cause and effect is an old philosophical issue dating back to Hume. The theory of causal inference (e.g. Rubin 1974 Rubin , 2007 Holland 1986; Cox 1992; Greenland & Brumback 2002) has been developed to overcome (at least partially) this obstacle and is frequently used in medicine, the social sciences and econometrics when randomization of sampling units is not possible. Most of the causal inference literature focuses on scenarios in which treatments are applied to a sample of units that respond independently. Clearly this is inappropriate for contagious infections since the infectious status of individuals in the population are not independent (Halloran & Struchiner 1995) . Our approach differs in that we are interested in the effect of 'treatment' on a population rather than an individual, and only one 'treated' population is observed. We therefore tackled the problem by using the Sellke construction to decompose the epidemic into simpler components and then again to reconstruct the notional outbreak under a mooted intervention. As in causal inference (see Cox 1992 ) the assumptions we make can be justified from first principles but cannot be independently tested.\n\nWe have applied the method in two ways. First, we used temporal data and a simple and accessible model. Here we made simplifying assumptions, assuming no latent period (cf. Arruda et al. 1997; Heikkinen & J\u00e4rvinen 2003) and homogeneous mixing (cf. Becker & Hopper 1983) and susceptibility (cf. Heikkinen & J\u00e4rvinen 2003) , for example. The validity and tractibility of the method are not, however, dependent on these simplifying assumptions. To illustrate the generality of the approach, we also used a spatio-temporal model to analyse the effect of removal of trees on an economically important disease of citrus in Florida.\n\nThe most pressing extension of the work is to incorporate economic factors such as treatment costs, in order to identify economically optimal strategies (Forster & Gilligan 2007) . This may be effected within the current framework by suitably adapting the utility function. This remains the subject of future work.\n\nThe scope of potential applications is broad. Vital questions about the effectiveness of varying the timing and intensity of control may be evaluated in diseases of humans (e.g. SARS (Riley et al. 2003; Wallinga & Teunis 2004) , Spanish influenza (Chowell et al. 2006a,b) and Ebola (Lekone & Finkenst\u00e4dt 2006) ) and other animals (e.g. FMD) as well as plants. We note, in particular, that a very nice aspect of the approach of Haydon et al. (2003) -their incorporation of known infectious contacts-could easily and consistently be incorporated within our framework also. By using an appropriate latent process such as the Sellke, our method also allows assessments to be made of the risk of inaction following outbreaks in which control measures were actually deployed. The method described in this . Colours distinguish three notional control strategies: r 0 1 : sample the population every 60 days and remove all infected trees (corresponding to (b)); r 0 2 : sample every 120 days and remove all infected trees (c); and r 0 3 : sample every 120 days but only detect and remove infection with probability 40% (d ). The three lower panels show maps of the areas, with coloured symbols indicating the posterior mean probability that the corresponding tree would have been infected by time 360 had the intervention taken place (legend in top panel). Table 2 . Posterior effect of various intervention strategies on the number of trees infected or removed during a period of 360 days. (The removal efficacy is labelled q, the removal frequency D and the radius of removal r (38.1 mZ125 ft being the original radius around detected infections used to determine asymptomatic trees for removal elsewhere in the state, later extended to 579 mZ1900 ft): if no entry is present in the r column this indicates only the tree with infection detected is removed. The mean ratio of notional to actual losses as well as 95% credible bounds are tabulated.) Alternative intervention strategies A. R. Cook et al. 1211 paper allows such important issues to be tackled taking full account of all available sources of information.\n\nThe data and CCC routines used in the common cold example are also available in the electronic supplementary material."}