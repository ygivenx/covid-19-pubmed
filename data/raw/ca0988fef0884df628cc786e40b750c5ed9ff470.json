{"title": "Genomic Analysis of Viral Outbreaks", "body": "This chapter is based on the following work: Wohl S., Schaffner S.F., Sabeti P.C. Annual Review of Virology, 2016 [1] . My advisor, Pardis Sabeti, gave me the opportunity to write a review on viral genomic analysis, and I conducted a literature review on this topic, drawing examples and ideas from work I had already done on the subject, described in Chapters 2 and 3. I then wrote the paper and refined its contents with Steve Schaffner.\n\nOf course, the methods and tools presented in this review are merely a snapshot of a rapidly changing field. Throughout this chapter, I note additional tools and ideas that have emerged since the original publication of the review in 2016, and highlight more recent applications of these methods (see Sections 1.2.2 and 1.4.2). While techniques for generating and analyzing genomic data continue to evolve, the fundamental questions during disease outbreaks -where did the virus originate, how is it changing, and where will it go next -remain the same. By detailing the ways in which genomic data can help answer these questions, I hope to provide a resource that may assist study of future viral outbreaks.\n\nIn subsequent chapters, I demonstrate the use of many of these methods to analyze recent For decades, epidemiological methods such as detailed contact tracing and mathematical modeling have been used to support these aims [2] [3] [4] [5] . Although those methods have worked well for stemming outbreaks of low-prevalence diseases like severe acute respiratory syndrome (SARS), their effectiveness is limited for large outbreaks, diseases with long latent periods, and outbreaks that occur in remote areas [6] . For these kinds of outbreaks, it is difficult to collect the detailed observations needed to parameterize epidemiological models with predictive power.\n\nApplying molecular biology tools to traditional epidemiology has greatly improved outbreak monitoring and prevention for all types of viral diseases. These tools include genotypic and phenotypic methods to determine the specific strain or type of virus circulating in a population. They can be used to improve diagnostics, to guide treatment programs and vaccine development, and to trace the spread of pathogens [7] [8] [9] [10] .\n\nMoving to full genomic analysis expands our capacity to understand viral outbreaks even fur-ther, because nucleotide-level resolution can distinguish isolates of the same viral strain. For example, when the World Health Organization issued a global alert for SARS in 2003, the diseasecausing agent was still unknown. Subsequent sequencing of isolates and identification of SARS coronavirus as the pathogen responsible led to development of sequence-based diagnostics necessary for the remarkable containment of the outbreak [2, [11] [12] [13] . In 1992, viral sequencing was also used to supplement epidemiological investigation when a patient claimed she had contracted HIV through a dental procedure. Phylogenetic analysis of HIV from the dentist and five of his dental patients -which showed that the viruses were closely related -made it clear that the dentist had indeed transmitted HIV to his patients [14] . In these two examples, whole-genome viral sequencing led to advances in diagnostics and in understanding transmission, respectively, that were not possible using other methods.\n\nThe 2014-2016 EBOV epidemic in Western Africa has provided one of the first applications of near-real-time whole-genome viral sequencing to understand a disease outbreak from its onset.\n\nGenomic analysis during the outbreak was made possible by recent advances in high-throughput sequencing, computational methods, and data processing. This epidemic also spurred the development of numerous methods for exploiting whole-genome sequencing in future outbreaks.\n\nHere, we compile and describe existing methods for analyzing genomic data from viral disease outbreaks. We focus on fundamental questions and how genomic data can be used to answer them.\n\nAlthough we include examples from a number of viruses in our review, we use the EBOV epidemic as the primary example throughout, both because rich genomic data are available for that outbreak and because many of the analyses described here were applied during it.\n\nAccurate sequencing is key to producing high-quality genomes for analysis. Dramatic improvements in high-throughput (also known as next-generation) sequencing technologies and in virusspecific sequencing [15] [16] [17] [18] in the past decade have enabled sequencing of viruses, known and novel, from all kinds of samples. We briefly review current sequencing technologies and discuss methods for sequence processing.\n\nIt is essential that patient samples be processed and sequenced in a way that will provide the highest quality data for downstream analysis. For RNA viruses, timing is especially important: degradation can occur quickly in clinical samples and is common, so the time between sample collection and sequencing should be minimized ( [19] ; see [20] for procedures used in an EBOV diagnostic laboratory). In general, all sample processing should consider both sample preservation and researcher safety [21] .\n\nSequencing itself has progressed from technologies tailored to a specific viral sequence to sequence-independent, high-throughput approaches. Amplicon-based sequencing is the most common sequence-dependent method and was used early in the EBOV outbreak [22] . In that study, viral genomes were amplified in long (often \u22652 kb) overlapping fragments by reverse transcriptase polymerase chain reaction (RT-PCR) with EBOV-specific primers; these fragments were then sequenced by Sanger sequencing. This method is popular for detecting and studying viruses because it is fast and can be used to amplify very small amounts of material.\n\nThe speed and accuracy of amplicon-based sequencing has made it an effective method for on-site sequencing even in remote field settings [23] , and the resulting genomes are sufficient for pathogen identification and basic analysis. However, amplicon-based sequencing does have drawbacks. First, Sanger sequencing is not conducive to the deep coverage needed to detect lowfrequency variants. Second, designing PCR primers requires prior knowledge of the viral sequence, which introduces bias and precludes metagenomic analysis. Third, it can be difficult to design primers that produce full-length genomes for all samples, given the high sequence diversity of many viruses. Lastly, degraded samples prevent full-length amplicon production necessary to obtaining whole-genome sequences.\n\nHigh-throughput sequencing platforms resolve many of the issues of amplicon-based approaches. These platforms, which produce short reads, are better able to capture fragmented or partially degraded samples. They also allow for the ultra-deep sequencing needed to detect lowfrequency within-host variants [24] . Sequence independence is essential for studying outbreaks caused by new or unknown pathogens, and for metagenomic analysis. Instead of virus-specific primers, these methods rely on random priming followed by high-throughput sequencing [15, 17, 25] . Combining sequence-independent primer amplification with selective RNase H-based digestion of contaminating RNA (mainly host ribosomal RNA) enables rapid, unbiased deep sequencing of viral samples, as was done during the EBOV epidemic [26] .\n\nOther high-throughput sequencing approaches can contribute to viral genomic analysis. Hybrid selection has been used to enrich the viral content of sequencing libraries with high host con-tamination even after RNase H digestion [18] , and is an active area of development [27, 28] . Refining this technology will improve viral genomic analysis during outbreaks, when sample quality may be variable. Other potentially useful technologies still in development include long-read sequencing, which could allow for phasing of variants, and technologies optimized for rapid on-site sequencing. These cheap and portable approaches [29] are useful for rapid diagnostics, but have high error rates that may preclude some detailed genomic analysis.\n\nDespite the drawbacks of amplicon-based approaches, their importance for sequencing low-titer viruses was highlighted in the 2016 Zika virus outbreak in the Americas [30] [31] [32] [33] . Instead of using long amplicons, the amplicon-based sequencing method used in these papers employs a larger number of shorter amplicons (specifically, 35 primer pairs, each creating a~400-nucleotide amplicon) tiled across the Zika virus genome [34] . Shorter amplicons allow for pathogen identification even in degraded or extremely low titer samples, because large intact regions of the genome need not be present. However, many of the other drawbacks of amplicon-based sequencing remain, such as the inability to reliability detect within-host variants ( [31] , see Chapter 4) .\n\nHybrid selection, another approach to sequencing samples with low viral content, has also seen recent improvement and more widespread application [18, 35] . This approach relies on small RNA probes that hybridize to the viral genome; washing away any unbound material enriches the proportion of the virus(es) of interest in the sample, thus improving sequencing quality and decreasing cost. Recently, methods have been developed that target a wide variety of viruses at once, allowing a user to enrich viral content without knowing the specific identity of a disease-causing pathogen [27, 28, 36] . More recently, however, Siddle et al. [37] have developed an algorithm for probe design that maximizes strain diversity while minimizing the number of required probes.\n\nEasy probe design is key to more widespread adoption of hybrid selection, and we have already applied the method and resulting probes to both Zika and mumps viruses (see Chapters 4 and 5) .\n\nNew methods to design probe sets reflect both the need to capture newly-identified viruses [38, 39] and the more varied use of target viral capture methods in outbreak and non-outbreak settings.\n\nAfter sequencing, care should be given to the assembly and alignment of genomes. Some of the necessary steps and best practices for processing high-throughput sequencing reads are shown in Figure 1 .1 (see also Appendix A). After completing these steps, reads that do not map to the database of possible viruses (Figure 1 .1, step 2) can be investigated using one of several taxonomic analysis tools [40] [41] [42] . Such reads can be de novo assembled and further investigated using a nucleotide or protein homology search. For a detailed example of how these methods were used to discover a novel flavivirus and two novel rhabdoviruses, see [38, 39] . Alternatively, comprehensive metagenomics pipelines [43, 44] can be used if rapid pathogen identification is the primary goal.\n\nRecombination can affect downstream phylogenetic analysis, so the final sequence alignment should be screened for recombination. Many methods that check for recombination have been compiled into a single software package, RDP4 [46] . As described below, there are alternative phylogenetic tools that should be used when recombination is present, but analysis of recombinant \n\nSingle-nucleotide polymorphism (2) depleted of host reads [45] and mapped to a database of possible viruses. (3) Reads from each sample are de novo assembled, and (4) all reads from each sample are mapped onto their own assembly. (5) The consensus sequence is determined for each sample and then (6) aligned to all other samples using multiple sequence alignment. See Appendix A for available software for each step. viral sequences is still an area of active development.\n\nSequence differences between viral genomes mark the evolutionary history and relationships between samples. Single-base substitutions (single-nucleotide polymorphisms, or SNPs) are the simplest variants. Given high-quality consensus sequences aligned to a reference, it is relatively easy to manually identify SNPs. However, more complex approaches -such as those implemented in packages such as GATK [47] or Samtools [48] -are helpful when samples contain insertions or deletions or when regions of the genome have poor quality, low coverage, or high diversity. Individual SNPs should be annotated -classified as nonsense, missense, or intergenic -and located relative to genes and other genomic elements. Many annotation tools are available online, each requiring only a list of SNPs and an annotated reference genome [49] [50] [51] .\n\nAt this stage, it is also useful to identify variants within individual samples (intrahost singlenucleotide variants, or iSNVs), indicating the presence of multiple viral quasispecies. Powerful tools exist for calling low-frequency variants in heterogeneous viral populations [52, 53] . To avoid calling sequencing errors as iSNVs, we suggest discarding variant calls with fewer than five forward or reverse reads and those for which the number of reads differs greatly between the forward and reverse strands (see Supplemental Methods in [26] ). Because PCR errors during library construction can introduce false variants, replicate libraries should be prepared and sequenced whenever possible to confirm the presence of within-host variants at comparable frequencies. The importance of properly filtering within-host variants is extensively discussed in Chapter 4.\n\nUnderstanding how and when an outbreak began is critical to curtailing it and to preventing future outbreaks. If an outbreak can be traced to a particular transmission route, steps can be taken to eliminate that route. For example, phylogenetic analysis of human influenza A H5N1 in the 1997\n\nHong Kong outbreak showed that the virus likely arose through reassortment between an H5N1 virus in terrestrial poultry and a similar virus in quail. This finding led to legislation prohibiting the sale of live quail together with other poultry in Hong Kong [54] and is one of many examples of phylogenetic analysis illuminating the origins of an avian influenza outbreak [55] .\n\nPhylogenetic methods all start with the creation of a phylogenetic tree -a reconstruction of the relationship of viral samples to one another -based on nucleotide substitutions in samples from the current outbreak. These phylogenetic relationships can then be used to determine the evolutionary order of sequences and to identify the first cases of an outbreak.\n\nPhylogenetic trees can be constructed using maximum likelihood [56, 57] or Bayesian [58] approaches. All methods require only a sequence alignment and a nucleotide substitution model. The nucleotide substitution model describes the rate at which one nucleotide is replaced by another and is used to estimate the evolutionary distance between sequences. The model is used in calculating the likelihoods of various possible phylogenetic trees, and it therefore may greatly affect results [59] . A general time-reversible model (typically referred to as a GTR model) is often used for phylogenetic analyses because it is the most general and makes no assumptions about nucleotide substitution rates or base frequencies [60] . Alternatively, several groups have written statistical software to compare substitution models for a given data set [61] .\n\nWhen constructing or reading trees, it is important to keep confidence values in mind. Confidence in maximum likelihood trees is commonly represented by bootstrap values [62] . Bootstrapping estimates uncertainty by sampling from a dataset with replacement. In this case, the bootstrap value for a node is the proportion of bootstrap trees in which that particular branch topology occurs. Although there is some debate about the accuracy of bootstrap values [63] , reporting these values, at least for important nodes, is common practice. Confidence values are built into Bayesian phylogenies and are the posterior probabilities. A Bayesian approach can be thought of as a faster version of a bootstrapped maximum likelihood approach, though the concordance between the two types of confidence values is variable [64] .\n\nBoth maximum likelihood and Bayesian methods were used to determine the phylogeny of Ebola viruses sequenced during the outbreak [22, 26, 65] . These two methods are often used together to check for agreement: major differences in the resulting trees may suggest a complex evolutionary relationship not fully captured by one or more methods.\n\nWithout further information, a maximum likelihood tree will be unrooted: It will show the relationship of branches relative to one another and the overall topology, but it will not identify the base of the tree or the direction of evolution. It thus cannot be used to identify which samples are ancestors and which are descendants. Because ancestry is very important to determining the origin of an outbreak, it must be determined by rooting the phylogenetic tree.\n\nThere are two primary methods of rooting phylogenetic trees: midpoint rooting and outgroup rooting. Midpoint rooting is done by finding the longest tip-to-tip distance in the tree and setting the root halfway between these tips. This method assumes that evolutionary rates are constant throughout the tree, meaning the root should be equidistant from all tips (it also assumes contemporaneous sampling). As discussed in the next section, this assumption (known as the molecular clock assumption) is often incorrect. Therefore, viral outbreaks are typically rooted by selecting an outgroup -that is, a set of sequences known to be more distantly related than anything else in the tree. This can be comprised of published sequences from previous outbreaks of the same virus, virus sampled from another host species, or a closely related viral species. Outgroup genomes must be distinct from outbreak genomes, but rooting trees using highly divergent sequences can also be problematic [65] . If only outbreak sequences are available, it is also possible to use a particularly divergent cluster of outbreak sequences as the outgroup, if one exists. Once an outgroup is selected, the phylogenetic tree is reconstructed using these additional sequences; the root is the point of divergence between the outgroup sequences and the rest of the tree (Figure 1.2) . In some cases the root of the tree is ambiguous, as in the recent EBOV outbreak. Dudas & Rambaut [65] explained how viral substitution rates and linear regression can be used to select the most likely root for a viral outbreak. be used to date the true origin of the outbreak. This is done using a strict molecular clock model, which assumes that nucleotide substitutions accumulate at a constant rate [66] . The number of substitutions on each branch of a tree with dated tips can be used to estimate the nucleotide substitution rate, which then can be used to extrapolate backward to the date of origin of a particular outbreak strain [67] . Maximum likelihood methods [68] can calculate substitution rates given a phylogenetic tree and sampling dates.\n\nAlthough it is a helpful simplification, the strict molecular clock does not always accurately model real viral evolution; evolutionary rates can vary over time, over space, or between different branches. To address this, more flexible models have been developed that allow for variation in the substitution rate over time [69] . The Bayesian Evolutionary Analysis by Sampling Trees (BEAST) package [70] implements a Bayesian Markov chain Monte Carlo method to determine changing substitution rates over time; this is referred to as a relaxed molecular clock. This framework can be used to coestimate the phylogeny and divergence times given sequence data and sampling dates.\n\nDuring the EBOV outbreak, BEAST was used to estimate when outbreak viruses split from lineages documented in other outbreaks [65] and to estimate the date of entry of the virus into Sierra Leone from Guinea [26, 71 ].\n\nThe same phylogenetic methods can be used to determine the type(s) of transmission causing an outbreak (human-human or animal-human), but this analysis requires sequences from appropriate hosts and/or time periods (Figure 1.3) . For example, analysis of EBOV patient samples showed that there was substantial genetic variation between EBOV outbreaks, but limited variation within each outbreak. This suggested that the virus evolves separately in an animal reservoir, and that a single zoonotic transmission was responsible for the start of each outbreak. This hypothesis was supported by the divergence times calculated by BEAST: The lineages from the two most recent outbreaks diverged from a common ancestor significantly before the start of either outbreak [26] . [19, 26] ) from a single recent ancestor. This topology suggests that each outbreak began with a single zoonotic transmission but was subsequently sustained by human-to-human transmission. (B) Lassa virus (LASV) tree containing S segment sequences from both human (circle nodes) and Mastomys natalensis (rodent nodes) hosts. Samples are from Sierra Leone [72] , where LASV is endemic. Sequences do not cluster by time or by host, indicating frequent animal-to-human transmission and a lack of discrete outbreaks.\n\nAlthough phylogenetic tools have been used successfully to understand many viral outbreaks, significant challenges remain in correctly establishing the origin of an outbreak. A detailed review of current challenges in phylogenetic methods can be found in [73] . Here we highlight those challenges particularly relevant for determining the origin of a viral outbreak.\n\nFirst, although relaxed molecular clocks allow for some rate variation, current models may still fail to capture the full variation in evolutionary rates. For example, an analysis for pandemic HIV-1 group M found that the time to the most recent common ancestor varies significantly when subtypes are analyzed separately compared with jointly, perhaps because closely related viral lineages have different substitution rates [74] .\n\nAdditionally, the methods described above cannot account for recombination that occurs in many viruses, because the ancestry of these viruses cannot be represented by a simple branching process. Instead, different parts of a single sample's genome can be the product of different genealogical trees and are better modeled by a phylogenetic network or ancestral recombination graph that allows for complicated evolutionary relationships [75] [76] [77] . Because many phylogenetic tools cannot account for recombination, it is important to restrict analysis to parts of the viral genome or tree where recombination is limited. Important advances in the field will come from continued development of phylogenetic tools that can incorporate viral recombination.\n\nLack of data also poses significant barriers to analyzing many viral outbreaks. For example, lack of sampling dates essentially rules out divergence time estimates, and lack of informative out-group sequences -perhaps due to limited past sequence data, or because a zoonotic reservoir has yet to be determined -prevents accurate rooting of a phylogenetic tree. Nonrandom sampling over time or space may significantly bias results [73] . Finally, understanding the ecological factors leading to an outbreak at a particular place and time requires detailed surveys of the outbreak location, both during and before its start [78, 79] . Without detailed epidemiological surveys, it may be impossible to determine the index case of a viral outbreak.\n\nUnderstanding the spread of the virus, including the mechanism, speed, and direction, is essential to controlling a viral outbreak. In many cases, this is done with epidemiological modeling. During the EBOV epidemic, many groups used case counts to estimate epidemiological parameters and the eventual size of the epidemic [80] [81] [82] [83] [84] [85] [86] [87] [88] [89] [90] . The varied approaches taken by these groups illustrate that there is no standard way to parameterize and use these epidemiological models. Additionally, in the absence of very detailed contact tracing and other epidemiological metrics, these models often cannot capture the complexity of an outbreak.\n\nEven without whole-genome sequencing, studying different viral strains as they move through time and space can be used to determine transmission patterns, especially for viruses with distinct subtypes, like HIV or influenza virus. However, this type of analysis does not always have adequate resolution to answer important questions about transmission routes. In the case of possible HIV transmission from a dental procedure, as described in Section 1.1, both contact tracing and molec-ular methods failed to prove a link between dentist and patient: contact tracing led to the dentist, but it was not conclusive because there was no evidence of shared bodily fluids. Similarly, two individuals with the same HIV subtype do not indicate a direct transmission link. Sequencing of the viruses from the patient, dentist, and several local individuals finally provided significant evidence for direct transmission [14] .\n\nUsing genetic data to reconstruct transmission routes is also especially important for post- [96] ); generation time: gamma distribution with mean = 8.4 and sd = 3.8, based on values from [2] ). Red and yellow circles correspond to the two Singapore clades identified in (A); lined red circles are samples with only sequence data (no contact tracing). Arrows are labeled with (number of SNPs between samples) / (posterior probability of transmission). (C) Transmission tree created during the SARS outbreak by contact tracing, as reported by [97] . Gray circles are unreported cases assumed to be part of the transmission chain. Comparison of panels (A-C) shows that the three methods generate similar relationships between samples. [96] ; several of these methods combine genetic and epidemiological data (e.g., sampling dates and locations) into a single likelihood function that is used to sample possible transmission trees [92, 96, 98, 99] . Jombart et al. [96, 100] have developed an R package that constructs transmission trees from genetic and any available epidemiological data (Figure 1.4B ).\n\nIt has recently been recognized that within-host genomic data constitute an essential component of phylogenetic analysis and transmission tree reconstruction [67, 99, 101] . One challenge of incorporating this type of data is that it requires an understanding of the characteristic within-host dynamics for a given virus before it can be used to effectively inform statistical and epidemiological models. Specifically, it is important to know the underlying viral mutation rate and the typical within-host nucleotide substitution rate, as well as how much diversity is transmitted during an infection event (the bottleneck size). Because studying within-host dynamics requires both high sequencing depth and longitudinal sampling, limited information exists for most viruses.\n\nWithin-host studies are most common in well-studied chronic viral infections such as HIV infection [102, 103] , although similar studies in other viruses are beginning to appear [104] . In the same vein, the average size of the transmission bottleneck is known in HIV [105] but is still under investigation in most other viruses. Deep sequences of Ebola viruses published during the 2014-2016 outbreak suggest that the bottleneck size is greater than one [19, 106] , but more precise estimates are still needed. Within-host viral dynamics studies, along with the development of robust phylogenetic methods that incorporate within-host variation, are a crucial next step in outbreak research.\n\nThe last few years have, in fact, produced a number of new methods that incorporate within-host variation into transmission analysis [107] [108] [109] . Allowing for within-host variation in transmission models was an essential step in fully connecting transmission and phylogenetic trees; previously, transmission trees were often estimated without phylogenetic information (such as in [96] ) or transmission was reconstructed given a fixed phylogenetic relationship between samples [99, 110] .\n\nConsequently, many recent studies [107] [108] [109] feature simultaneous sampling of phylogenetic and transmission trees. Simultaneous sampling over both types of trees allows these methods to better account for missing samples, thereby improving transmission reconstruction. Despite these improvements, dealing with unsampled cases remains a challenge in the field, and several recent papers have addressed and suggested potential solutions to the problem [99, 111, 112] .\n\nAnother active area of development is relaxing the assumption -made in all three of the simultaneous sampling methods mentioned above -that the transmission bottleneck size is equal to one. Within-host data may play an important role in this, as previously suggested by Worby et al. [113] . Even with extensive within-host data, it is important to remember that transmission reconstruction is ultimately limited by the mutation rate of the virus [114] , since it is difficult to infer patterns from identical sequences. This suggests the important role of epidemiological data in transmission inference: as discussed at length in Chapter 5, an approach combining genomic and epidemiological data, or genomic epidemiology, has the potential to be more powerful than one using either datatype alone.\n\nThe basic reproduction number (R 0 ) -the number of secondary cases from a single infectionis a useful measure of the infectivity of a pathogen and is usually estimated from epidemiological models. However, this number can also be estimated from a detailed transmission chain or from genomic data [115, 116] . This value often frames the discussion about containment for a disease outbreak and can be used to predict outbreak dynamics and eventual size in the presence or absence of various control measures (for its use in the EBOV outbreak, see [82, 117] ). Calculation of epidemiological parameters such as R 0 is part of the new and growing field of phylodynamics, the study of infectious disease behavior that arises from a combination of evolutionary and epidemiological processes [102] . Incorporating epidemiological metadata can enhance genetic analysis, and vice versa, in outbreak situations.\n\nAlthough joint evolutionary and epidemiological analysis has greatly advanced the field of outbreak investigation, there are still challenges associated with determining the route and rate of viral spread. Major hurdles include sampling bias and the difficulty of allowing for spatial and temporal complexity in phylodynamic models [73] . For example, Lloyd-Smith et al. [118] highlighted problems with using the same reproduction number for all individuals and the resulting implications for outbreak control. \n\nThe mutation rate is a major determinant of the overall rate of evolutionary change during and between outbreaks; it is the number of genetic mutations that occur per viral genome replication. It is largely determined by a virus's biological properties, such as the fidelity of its polymerase, the speed at which it replicates its own genome, and whether the genome is RNA or DNA [119] . In general, RNA viruses mutate fastest and DNA viruses slowest. The mutation rate must be measured experimentally because natural selection affects the number of mutations identified in genetic data. For mutation rate estimates for a number of specific viruses, see the reports by Drake [120] and Drake & Hwang [121] .\n\nThe mutation rate should not be confused with the nucleotide substitution rate, which is the rate at which nucleotide substitutions accumulate in a viral lineage. This rate is determined by the mutation rate and by other factors, including natural selection and the effective viral population size. This is the rate most commonly discussed in an outbreak situation, both because it can be calculated from sequence data and because it can be used to understand selective pressures on a viral population during an outbreak. For example, it may be useful to compare the substitution rate within an outbreak to that in a zoonotic reservoir. The virus should have the same intrinsic mutation rate in both hosts, so differences in substitution rate could be due to selection.\n\nWhereas calculating the viral mutation rate requires careful experimentation, the substitution rate can be calculated given a phylogenetic tree and sampling dates. This can be done with maximum likelihood methods [68] or Bayesian methods [70] . Bayesian methods such as BEAST are more statistically rigorous than most maximum likelihood implementations because they can allow the substitution rate to vary between branches, but they are computationally intensive [119] .\n\nFor approximate substitution rates for various viruses, see the compilation by Jenkins et al. [122] .\n\nThe caveat to all of these methods is that they assume all substitutions are fixed in the population. Because many mutations on recent branches are mildly deleterious and will disappear from the population over time, the substitution rate for any tree containing recent samples may be artificially high. This is common during outbreaks, when a majority of viral genomes may be terminal branches. This test requires only a codon alignment and is often applied to viral sequences to identify domains or genes under selection [123, 125, 126] . However, it was originally developed to analyze sequences from divergent species, not to detect selection within a single population [127] . The d N /d S ratio is not applicable within single populations, and the results obtained from this test are often misleading when applied to microbes [128] . Therefore, although this ratio can still be used to analyze sufficiently divergent, separately evolving outbreaks, users should be wary of using this test to detect selection within a single outbreak population.\n\nIf data from other outbreaks are available, the d N /d S statistic can be used to identify sites or regions under selection. During the EBOV epidemic, several groups found d N /d S > 1 in the gene encoding the glycoprotein (GP) -or, more specifically, the disordered, mucin-like region of GP - [19] ). (C) Synonymous constraint for every codon position in the West Nile virus genome (sliding window = 20 nucleotides) [124] . Red bars mark regions of excess constraint. Asterisks mark two known RNA structural elements (orange = structural proteins, yellow = non-structural proteins), a hairpin in the capsid gene and a pseudoknot element within non-structural protein 2A. Panel (C) adapted with permission from [124] when including sequences from all known EBOV outbreaks [19, 129] (Figure 1 .5B). This is unsurprising because GP is the envelope protein: as the only surface-exposed protein on the viral particle, GP is the target of host cell antibodies. Because of this biology, it is suspected that GP undergoes diversifying selection or relaxed purifying selection as a response to host immune pressure [130] .\n\nIn general, comparing nonsynonymous to synonymous mutations between sites or between timescales (i.e., comparing inter-and intrahost substitutions, as in [72] ) can be used to suggest regions under selection. It is possible to identify selection using only synonymous substitutions by looking for regions of excess synonymous constraint in a virus [124] (Figure 1 .5C). In viruses, many protein-coding regions contain overlapping or embedded functional elements. Because any type of substitution may disrupt an overlapping element, these regions are often characterized by an unusually low synonymous substitution rate. This method was used during the EBOV epidemic to find a constrained region in a known editing site.\n\nTests based on mutation frequency spectra and tree topology can also be used to identify selection in single populations. The basic principle behind these methods is that selective pressure on a population leaves a distinct mark on overall genetic diversity and tree symmetry [102, 131, 132] . Statistics that test for selection, or non-neutrality, using these principles include Tajima's D, Fu and Li's D, and tree-imbalance metrics [133] .\n\nTajima's D is a statistic that compares the average number of pairwise differences between sequences to the total number of variable sites within the set of sequences [134] . A negative value of [136] , the cherry count [137] , and Colless's tree imbalance index [138] . More asymmetry than expected in a phylogenetic tree suggests non-neutral evolution. These methods have been successfully used to understand selection in HIV, influenza virus, and other viruses [133, 139] .\n\nThe major drawback of both frequency spectra and tree imbalance methods is that it can be hard to differentiate between selection and epidemiological effects such as changing population size. For example, a very negative Tajima's D or Fu and Li's D can be due to exponential growth rather than non-neutral evolution. Drummond & Suchard [133] addressed this problem by incorporating a demographic model when analyzing three RNA virus data sets and showed that it is possible to use these tests to identify selective pressure on viral populations.\n\nDetecting selection in viruses is challenging because most statistical methods have been created for the comparison of divergent populations or species, rather than for analysis of a single population that may be rapidly evolving and expanding. Additionally, viruses are very biologically diverse and have highly variable mutation and substitution rates. This makes it difficult to use the same selection tests for all viruses. For example, slow-mutating viruses, which usually have low substitution rates, may require extended sampling to achieve the population diversity needed to identify evolutionary trends. Unfortunately, not all viruses and data sets make good subjects for evolutionary analysis, and even when they do, the results may be relatively uninformative or uninteresting.\n\nOne important role of genomic data is to inform experimental studies, which are necessary for understanding the biology of pathogenic viruses. In many cases, genetic analysis of outbreak sequences generates hypotheses about particular regions of a virus that may play a role in transmission or pathogenesis. Validating or refuting these hypotheses experimentally leads to a more complete picture of the virus, which may directly inform treatment and prevention measures or be used to improve epidemiological and evolutionary models.\n\nImmediately after sequencing, viral samples can be used to answer one pressing question:\n\nwhether one or more mutations have impaired the ability of clinical diagnostic tools to detect the virus. This is particularly a concern for the real-time PCR assays commonly used for viral detection, because SNPs and other mismatches in primer binding sites have been shown to greatly reduce assay performance [140] . For example, during the EBOV epidemic, various groups periodically compared the most up-to-date list of mutations in the EBOV genome with recognition sites for diagnostic probes, as well as for existing and candidate therapeutics [141] [142] [143] . Mutations in the binding regions of diagnostics or therapies should be carefully tested experimentally to ensure that binding still occurs.\n\nBroadly, the results of phylogenetic and evolutionary analyses can be used to identify variants most likely to have a functional effect on the virus. For example, clade-defining mutations -mutations shared by large clusters on a phylogenetic tree -are prime candidates for experimentation.\n\nThese mutations may have fixed within a cluster of samples simply by genetic drift and patterns of transmission, but they could also represent sites under strong positive selection. For example, genomic analysis of EBOV sequences demonstrated the presence of four viral lineages circulating in Sierra Leone, each defined by one to four deviations from the reference genome, that rose to prevalence in the population at some point during the outbreak [19, 26, 144, 145] . Because of their prominence, these mutations were targeted for experimental study soon after the outbreak started [146] .\n\nVariants or genomic regions identified by the evolutionary analyses described in the previous section should also be considered for experimental testing. Analyses suggesting that the glycoprotein in EBOV might be under selection are an excellent example of how genomic analyses were not able to definitively classify selective pressures, but were able to identify the most promising region for functional validation.\n\nAnother common question during an outbreak is whether mutations correlate with clinical outcomes. Therefore, before conducting experiments, it may be informative to explore mutations in relation to clinical and other types of data. This requires additional data, such as information about symptoms, survival, or viral load. Correlations cannot prove causation but can be used to refine a set of mutations for experimental analysis and to suggest a function or mechanism that can be tested experimentally.\n\nGenomic analysis can answer many urgent questions during an ongoing viral outbreak, including ones related to where the outbreak originated, how the virus is transmitted, and how the virus might be evolving. This was successfully done during the EBOV outbreak, and the same techniques are being applied to past and ongoing outbreaks of other infectious diseases. However, all of these analyses are limited by the quality and availability of data.\n\nData quality may be improved by updated sample preparation and sequencing methods. These methods are especially important for viruses that are present at low titer, such as Zika virus. To best utilize sequencing data, many of the analyses discussed could be further refined with better information about the virus itself [147] . For example, biological investigation of the evolutionary and transmission processes unique to specific viruses would improve the quality of many withinoutbreak analyses.\n\nAlthough these technical challenges remain, logistical issues are a major barrier to effective outbreak response because phylogenetic methods depend on specific types of data: determining the time an outbreak started is difficult if a suitable outgroup is not available, reconstructing transmission is impeded by inaccurate or missing sample dates, and all of these techniques are limited by sparse sampling during an outbreak. Missing data have been a major challenge in viral genomics largely because the usefulness of real-time sample collection and sequencing for outbreak control was not recognized until recently. Now, with easier, cheaper sequencing and the development of computational methods to harness that sequence data, it should be evident that genomic data will be a powerful tool in understanding and controlling future outbreaks.\n\nEven when data are collected, decentralized sample collection and analysis mean that those data may not be readily available. This problem was highlighted in the EBOV epidemic, during which many different groups were conducting studies all over Western Africa, and the data did not always become immediately available. Sharing outbreak data is a necessary component of an efficient response [148] . Another lesson from the EBOV epidemic is the usefulness of extensive collaboration. Many of the techniques discussed in this review are computationally intensive (BEAST, for example), and deep sequencing of isolates is very expensive; both require substantial technical expertise. Large collaborations and shared resources and data seem to be the best ways to respond quickly to an outbreak situation. Although this has not been the normal approach of many research groups, informal collaborations, such as the online forum Virological Ebola virus (EBOV) first emerged in Guinea, West Africa in early 2014, having previously been observed only in Central Africa [1] . The virus then spread to Liberia, Sierra Leone, and Nigeria, ultimately resulting in over 28,000 cases and over 11,000 deaths [2] . Throughout the outbreak, a number of groups sequenced EBOV from patient samples [1, [3] [4] [5] [6] [7] [8] [9] [10] [11] , totaling over 1,600 published EBOV genomes that could be used to analyze spread of the outbreak throughout Western Africa [12] .\n\nIn Gire et al. [3] , we released 99 EBOV genomes from 78 patients in Sierra Leone during the outbreak and identified three major phylogenetic clades (SL1, SL2, SL3) circulating within the country. As an author on the publication, I helped analyze these genomes, looking for patterns in the sequences that could provide clues in understanding transmission during the outbreak. I also analyzed within-host variants identified in these EBOV sequences and used them to assess sequencing methods used in the study. Early in the epidemic, genome sequencing provided insights into virus evolution and transmission and offered important information for outbreak response. Here, we analyze sequences from 232 patients sampled over seven months in Sierra Leone, along with 86 previously released genomes from earlier in the epidemic. We confirm sustained human-to-human transmission within Sierra\n\nLeone and find no evidence of import or export of EBOV across national borders after its initial introduction. Using high-depth replicate sequencing, we observe both host-to-host transmission and recurrent emergence of intrahost genetic variants. We trace the increasing impact of purifying selection in suppressing the accumulation of nonsynonymous mutations over time. Finally, we note changes in the mucin-like domain of EBOV glycoprotein that merit further investigation. These findings clarify the movement of EBOV within the region and describe viral evolution during prolonged human-to-human transmission.\n\nThe 2014-2016 Western African EVD epidemic, caused by the EBOV Makona variant [13] , is the largest EVD outbreak to date, with 26,648 cases and 11,017 deaths documented as of 8 May 2017 [14] . continued to perform active diagnosis and surveillance in Sierra Leone following our initial study [3] . After a six-month delay of sample shipment due to regulatory uncertainty about inactivation protocols, we again began to determine EBOV genome sequences. We have sequenced samples at high depth and with technical replicates to characterize genetic diversity of EBOV both within (intrahost) and between (interhost) individuals. To support global outbreak termination efforts, we publicly released these genomes prior to publication as they were generated, starting with a first set of 45 sequences in December 2014 and continuing with regular releases of hundreds of sequences through May 2015.\n\nHere, we provide an analysis of 232 new, coding-complete EBOV Makona genomes from Sierra\n\nLeone. We compared these genomes to 86 previously available genomes: 78 unique genomes from Sierra Leone [3] , three genomes from Guinea [1] , and five from healthcare workers infected in Sierra [20] and utilizes a generalized workflow engine to run on a wide variety of computer hardware configurations [21] . Through a partnership with DNAnexus, this pipeline is also available in a secure cloud-compute environment to enable consistent analyses across laboratories with limited computational resources (see Methods, Section 2.5).\n\nUsing this pipeline, we successfully assembled 232 EBOV Makona coding-complete genomes ( . We also observed five single-base insertions and two double-base insertions in noncoding regions. We mapped all of the variants to primer-binding sites for known sequence-based diagnostics [15] and found no mutations in these sites that were present in more than one Sierra Leonean sample.\n\nWe constructed a second, independent genome library for each of 150 high-quality samples from the KGH cohort to reliably determine iSNVs at low frequencies [3] . We identified 247 iSNVs [22] and to identify human-to-human transmission chains [3] . In the current dataset, which includes 85 samples with at least one iSNV (Figure B \n\nWe previously reported that new mutations accumulated more rapidly in the viral population early in the outbreak than over the long-term in the reservoir [3] . We hypothesized then that the higher rate early in the outbreak resulted from incomplete purifying selection -that is, we were detecting transient nonsynonymous variants that would later be removed by purifying selection [23, 24] .\n\nThe observed evolutionary rate is thus not an estimate of the underlying mutation rate since some deleterious mutations are purged by selection before they can be detected. But neither is it an estimate of the long-term substitution rate since other deleterious mutations have not been eliminated by selection at the time of analysis. We hypothesized that the EBOV Makona evolutionary rate would decline following the addition of genomes covering a longer evolutionary timescale. Such a decline is well characterized in members of other species [25, 26] . With the present dataset, we were able to examine the evolution of the virus over a longer time period. We found that the most \n\nEstimates of EBOV evolutionary rates at three timescales: decades (yellow, all known EVD outbreaks), months (blue, Baize + Gire + Park), and weeks (red: Baize + Gire). (B) Purifying selection. We estimated nonsynonymous (red) and synonymous (blue) substitution rates on external (unique to an isolate, potential dead end) and internal (shared by multiple isolates, evidence of human-to-human transmission) branches. Nonsynonymous mutations accumulate faster on external branches than on internal branches. For synonymous mutations, the difference between external and internal branches is less pronounced. (C) Enrichment for nonsynonymous mutations at shorter timescales. Intrahost (all variants that appear within a single host at less than 100% frequency); unique interhost (SNPs fixed in exactly one individual); shared interhost (SNPs fixed in two or more individuals); shared between EVD outbreaks (internal branch SNPs on a between-outbreak tree). See also Figure B We calculated the fractions of nonsynonymous (NS) and synonymous (S) consensus SNPs and iSNVs within experimentally determined B cell epitopes (data from ViPR [27] ). Dotted line represents the fraction of GP amino acids in ViPR epitopes. Nonsynonymous SNPs (p = 0.004) and iSNVs (p = 0.037) in GP occur more frequently in epitopes than expected by chance (two-sided exact binomial test). Numbers indicate fraction of each variant type within GP epitope regions. Error bars represent binomial sampling intervals. (C) Local enrichment of T-to-C mutations within GP B cell epitopes. We observed five sequences with short stretches (<200 nucleotides) of concentrated T-to-C mutations. Of these five sequences, two (shown here, samples 20141582 and G5119.1) contain stretches of T-to-C SNPs (blue points) within GP epitopes (light blue bars). Additionally, we observe a T-to-C mutation at amino acid position 485 (blue diamond) in three samples (one shown here, G4955.1), which is otherwise completely conserved among members of all ebolavirus species [28] . (D) Genome-wide increase in T-to-C mutations. We observe more T-to-C transitions within the 2014-2016 outbreak than any other transition, after correcting for nucleotide content. Error bars represent binomial sampling intervals. (E-F) Elevated T-to-C rates are genome wide but are limited to a subset of sequences. Accumulation of mutation increases linearly with time. However, some individual samples show more genetic distance than expected based on sample date. Samples with short stretches of T-to-C mutations (orange) show a significant enrichment of T-to-C mutations, as expected. Excluding these samples, the top 5% of samples by genetic distance (yellow) lack localized stretches but still show moderate enrichment of T-to-C mutations genome wide. The bottom 95% of samples (beige) show no enrichment of T-to-C mutations. Error bars represent binomial sampling intervals. only surface-exposed viral protein on EBOV virions, and as such, it is the primary target of antibodies [29] . This finding therefore raises the possibility that antibodies might be driving diversifying selection and rapid evolution in this region. This observation is based on a very small number of substitutions (eight nonsynonymous and four synonymous within the outbreak), however, and is not statistically significant (posterior probability that d N /d S is elevated within-outbreak = 92.9%);\n\nthe situation should be clarified as more sequencing becomes available. If diversifying selection is occurring here, then the observed changes are very unlikely to represent population-level selection for transmission among humans; this would only occur if previously infected individuals were frequently being exposed to new infections. Instead, we hypothesize that these changes represent within-host selection for EBOV to escape a developing humoral immune response.\n\nTo test the hypothesis that antibodies drive diversifying selection of GP, we looked for enrichment of mutations within B cell epitopes within that protein. Effective humoral immunity depends on antibody binding to specific B cell epitopes [29, 30] . Using experimentally determined B cell epitopes obtained from the Virus Pathogen Database and Analysis Resource (ViPR) [27] , we found that nonsynonymous mutations in GP do indeed occur more frequently in epitopes than expected by chance (Figure 2 .4B). This correlation supports the hypothesis that humoral immunity exerts selective pressure on the virus, driving immune evasion via accumulation of nonsynonymous mutations within GP B cell epitopes.\n\nVisual inspection identified a subset of sequences that are more likely to contain B cell escape variants (Figure 2 .4C). In particular, three sequences (e.g., G4955.1) had a threonine-to-alanine mutation at GP amino acid position 485, a conserved threonine that is required for in vivo protection by the 14G7 antibody [28] . Additionally, two sequences had short stretches of T-to-C mutations in GP (four or more T-to-C mutations within a 200 nucleotide region; Figure 2 .4C), both of which occur within B cell epitopes.\n\nSimilar patterns of excess T-to-C mutations within short regions were also observed by Tong et al. [4] . In our dataset of 318 genomes, five possessed obvious stretches of T-to-C mutations within\n\nshort regions. We also tested more broadly whether excessive T-to-C mutations occurred in all sequences and found a significant enrichment of T-to-C transitions relative to all other types of transitions ( Figure 2 .4D). To determine whether viral sequence divergence is related to T-to-C transition enrichment, we compared relative T-to-C transition rates in sequences with stretches of T-to-C mutations (n = 5) to the top 5% of remaining sequences by sequence divergence (n = 15) and to the bottom 95% of sequences (n = 298) (Figure 2 .4E). While the sequences with T-to-C stretches showed the strongest T-to-C enrichment, we found moderate enrichment of T-to-C transitions in the 5% most divergent sequences.\n\nOur findings from 232 EBOV Makona genomes sampled in Sierra Leone over seven months during the 2014-2016 EVD outbreak in Western Africa demonstrate the value of continued sequencing throughout an epidemic. We tracked the movement of EBOV throughout Sierra Leone and determined the frequency of EBOV movement into and out of that country. Although it is not unlikely that the virus continued to cross the national borders of Sierra Leone throughout the epidemic, these observations suggest that, at least in late 2014, cross-border introductions were not an important factor in the development of the epidemic. We were unable, however, to draw any conclusions about export to Guinea since few EBOV sequences from there were available at the time.\n\nThe sequence data display EBOV Makona evolution in the context of prolonged human-tohuman transmission and provide an updated view of genomic diversity. Based on the rates of nonsynonymous and synonymous changes that are shared or are unique to an individual host, we concluded that purifying selection becomes increasingly effective over time, as it has more opportunity to remove deleterious mutants.\n\nWhile the effects of purifying selection in this extended EVD outbreak are clear, these evolutionary changes do not imply that positive selection or adaptation to humans are occurring.\n\nRather, the data suggest that evolutionary changes over time through natural selection are sufficient to remove newly arisen alleles that are less fit in the human environment.\n\nIt is important to recognize, however, that the long-term human-to-human transmission observed during the 2014-2016 EVD outbreak is historically unique for EBOV. At the beginning of each EVD outbreak, EBOV enters the human population with little or no genetic diversity. In the case of the current EVD outbreak, EBOV has now maintained fitness while expanding across a much larger space of genetic diversity than in previous EVD outbreaks, the largest of which comprised only 318 human infections. This degree of diversity will undoubtedly affect researchers' ongoing efforts to develop or improve candidate diagnostics, vaccines, and therapeutics for EVD, many of which are targeting EBOV sequences directly (PCR, nucleic-acid based therapeutics) or indirectly (antibody cocktails).\n\nThe mucin-like domain of the EBOV glycoprotein, in contrast to the rest of the EBOV genome, appeared to be under diversifying selection based on a high ratio of nonsynonymous-to-synonymous mutations. While not statistically significant because of the small number of SNPs in the region, our observation is in agreement with many previous studies [31, 32] . As the EBOV GP, especially the mucin-like domain, is the target of many antibodies, a plausible hypothesis is that the humoral immune response exerts selective pressure on GP, resulting in an accumulation of nonsynonymous mutations. In support of this hypothesis, regions of GP corresponding to experimentally determined B cell epitopes are significantly enriched in nonsynonymous, but not in synonymous, variants. There are two important caveats to this analysis: (1) these epitopes are determined in vitro and therefore may not be epitopes in vivo if they are not immunodominant, and (2) there is no experimental evidence to suggest that the majority of observed variants disrupt antibody binding to these epitopes.\n\nWhile further experimental testing is required to validate an immune evasion hypothesis, we have highlighted a few prime candidates to consider. Genomes from three samples share a threonine-to-alanine mutation at GP amino acid position 485, a position that is conserved among all members of the Ebolavirus genus. This position is indispensable for binding of the protective antibody 14G7 [28] ; the observed variant at this site may therefore be the result of escape from antibody-mediated selection. Additionally, two samples each possess multiple mutations within a single experimental B cell epitope in GP, which are likely to evade antibody recognition if those regions are relevant epitopes in vivo.\n\nIntriguingly, the two samples with multiple mutations within a single B cell epitope each possess a distinct short stretch littered with T-to-C transitions, a phenomenon also observed in Tong et al. [4] . Excessive T-to-C and A-to-G mutation of virus genomes has been observed previously as a result of adenosine deaminases acting on RNA (ADARs) [33] [34] [35] . When acting on viral genomic RNA, ADARs cause a pattern of excess A-to-G transitions that are represented by T-to-C transitions in our dataset. These transitions are known to occur either promiscuously within 200 nucleotide stretches or in a sequence-specific manner; therefore, we investigated both possibilities.\n\nWhile only five of the 318 sequences in our dataset contained obvious T-to-C stretches, we showed that the top 5% of sequences by sequence divergence, excluding the five sequences with T-to-C stretches, were also moderately enriched for T-to-C transitions across the genome. The remaining 95% of sequences appeared to show no enrichment. We do not know whether this phenomenon is caused by ADAR acting upon genomic RNA, as we cannot exclude the possibility of bias by the EBOV RNA polymerase or other effects. Additionally, it is yet unclear whether these T-to-C muta-tions have an anti-viral or other effect on viral fitness. These questions open avenues of research into molecular mechanisms shaping EBOV evolution.\n\nThe results of some of the specific genome analysis methods that we introduced here, while promising, will require denser EBOV genome sampling to yield sufficient information to influence the EVD outbreak response. Among these methods is transmission analysis, which could prove valuable for improved understanding of hospital-based transmissions and therefore for im- Unfortunately, long delays of shipping samples from the field and required changes to the EBOV inactivation protocol caused severe degradation of many samples, which prevented identification of variants and transmission analysis. This loss should serve as a reminder that standardized and optimized protocols for sample collection, virus deactivation, and shipment are crucial for a rapid worldwide response to any new infectious disease outbreak. An important future research effort will be aimed at understanding which certified EVD sample deactivation protocols are best suited for high-quality genomic sequencing. Complications with sample shipment also emphasize the need for establishing in-country sequencing capabilities either before or at the onset of future EVD outbreaks [36] .\n\nBeyond coordinated field and experimental responses, a culture of rapid data sharing is critical for teams around the world to have the best current information about a circulating virus or ongoing disease [37] . In light of this need, we released all data discussed in this paper publicly as they were generated, beginning in December 2014, well in advance of our own analysis. We have previously described our high-depth sequencing protocols [19] , and we have also made available our computational analysis pipeline, in the hope that they will assist the many laboratories engaged in viral genomic research. More EBOV data will allow the scientific community to together obtain a broader picture of transmission and evolution of EBOV Makona during the EVD epidemic. \n\nHost ribosomal and carrier poly(rA) RNA depletion, randomly primed cDNA synthesis, Nextera XT library construction, and 101-bp paired-end Illumina sequencing were performed as described previously [3, 19] .\n\nEBOV Makona genomes were assembled from high-throughput sequencing data using an updated bioinformatics pipeline based on our previously described methods [3, 19] . Of the collected samples, 150 KGH and 82 CDC samples had sufficient EBOV genome sequencing coverage for highquality de novo genome assembly.\n\nThe viral assembly pipeline began by depleting paired-end reads from each sample of human and other contaminants using best match tagger (BMTagger) [38] and the nucleotide basic local alignment search tool (BLASTN) [39] . PCR duplicates were removed using a custom modification to Vicuna, M-Vicuna [40] . The resulting \"de-identified\" metagenomic datasets were deposited in sequence read archive (BioProjects PRJNA257197 and PRJNA283385). Next, reads were filtered to all members of the Ebolavirus genus (all ebolaviruses including EBOV) using LASTAL [41] , qualitytrimmed with Trimmomatic [42] , and further de-duplicated with PRINSEQ [43] .\n\nThe filtered and trimmed reads were subsampled to 100,000 pairs, if available, and de novo assembled using Trinity [44] . Subsequently, reference-assisted assembly improvements (contig scaffolding, gap-filling, etc.) were performed with virtual file application table [45] , which relies on MOSAIK [46] and multiple sequence comparison by log expectation (MUSCLE) [47] . Each sample's reads were aligned to its de novo assembly using Novoalign [48] , and any remaining duplicates were removed using Picard with MarkDuplicates command [49] . Variant positions in each assembly were identified using genome analysis toolkit [50] insertions and deletions realinger (In-delRealigner) and UnifiedGenotyper [51, 52] on the read alignments. The assembly was refined to represent the major allele at each variant site, and any positions supported by fewer than three reads were changed to N (nonsynonymous sites). This align-call-refine cycle was iterated twice, to minimize reference bias in the assembly.\n\nOur Linux-based software pipeline is publicly available at https://github.com/ broadinstitute/viral-ngs [20] . This pipeline includes command-line tools for each of the above steps and optional Snakemake workflows [21] to automate them either sequentially or in parallel. Most of the third-party tools used are either included or can be downloaded and installed automatically, except for GATK and Novoalign, which must be provided by the user due to licensing restrictions.\n\nThe assembly pipeline is also available via the DNAnexus cloud platform. RNA paired-end reads from either HiSeq or MiSeq instruments (Illumina) can be securely uploaded in FASTQ or BAM format and processed through the pipeline using graphical and command-line interfaces.\n\nInstructions for the cloud analysis pipeline are available at https://github.com/dnanexus/ viral-ngs/wiki. To reconstruct the EBOV Makona transmission history within Sierra Leone, we grouped samples into sets of one or more genetically identical viruses based on their consensus sequences.\n\nWe then identified relationships between these groups, progressing from the Guinean reference genome (KJ660346.2) and ending with nine viruses sampled in Freetown (eight from our KGH and CDC cohorts and one sequenced in Italy).\n\niSNVs were called from each sample's read alignments using V-Phaser 2.0 [54] and subjected to an initial set of filters: variant calls with fewer than five forward or reverse reads or more than a 10-fold strand bias were eliminated. iSNVs were also removed if there was more than a five-fold difference between the strand bias of the variant call and the strand bias of the reference call. Variant calls that passed these filters were additionally subjected to a 0.5% frequency filter. The final list of iSNVs contains only variant calls that passed all filters in two separate library preparations. These data infer 100% allele frequencies for all samples at an iSNV position without intrahost variation within the sample, but a clear consensus call during assembly. Annotations were computed with the effect of single nucleotide polymorphisms (SnpEff) program [55] .\n\nEvolutionary distances between pairs of phylogeny tips were computed from the posterior sample of trees produced by Bayesian evolutionary analysis by sampling trees (BEAST) [56] analysis. This calculation integrates across phylogenetic uncertainty and produces a temporal evolutionary distance between phylogeny tips. We used this distance matrix to calculate the average distance between pairs of phylogeny tips that share an iSNV and compared the result to the average distance between random pairs of tips. We calculated a p value for the observed average distance by conducting a randomization test. In each random replicate, we sampled the same distribution of iSNV possessing tips as observed in the empirical data and calculated the average distance between these pairs of tips. We calculated a p value by comparing the empirical mean distance to the mean distances observed over 10,000 random replicates.\n\nData were obtained from the NIAID Virus Pathogen Database and Analysis Resource (ViPR) online through the web site at http://www.viprbrc.org [27] . Analyses of rates, phylogenies, and evolution were performed on all three datasets in BEAST [56] . Synonymous and nonsynonymous counts were mapped onto the molecular phylogenies using robust counting [57, 58] by specifying independent Hasegawa, Kishno, Yano (HKY) nucleotide substitution models [59] for all three codon-position partitions. Substitutions in intergenic regions were modeled according to HKY with \u03934-distributed rate heterogeneity [59, 60] . A relaxed molec-ular clock with log-normal rate distribution categories [61] and a non-parametric Bayesian skygrid [62] tree prior were used. A reference prior [63] was used on the molecular clock.\n\nWe estimated the ratio of nonsynonymous substitutions over synonymous substitutions, (GP \u0394MLD ). This split is due due to concern that the GP MLD is highly disorganized [66] and thus is under little constraint at the amino acid level. To date, only linear epitopes in GP MLD are known to be targeted by antibodies [28] , due to its extensive O-and N-linked glycosylation. We employed independent codon models for all eight partitions, parameterized with independent strict molecular clocks. A reference prior [63] was used on the evolutionary rate. Substitutions in the ninth partition, with concatenated noncoding intergenic regions, was modeled using the HKY + \u03934 [59, 60] model. The non-parametric Bayesian skygrid was used as the tree prior [62] for both long-term and current datasets.\n\nWe thank KGH staff who died of EVD (including M. \n\nThe Ebola virus genomes described in the previous chapter allowed us to better understand viral exchange across the Sierra Leonean border, as well as to identify patterns of selection on the virus during a period of sustained human-to-human transmission. We also identified a number of within-host variants from these data and concluded that low-frequency variants could be transmitted between hosts. As suggested in Gire et al. [1] , these shared iSNVs can suggest transmission links; because low-frequency variants generally disappear or fix in a population over time [2] , individuals sharing an iSNV are more likely to be close in a transmission chain, assuming the transmission bottleneck is large enough to allow transmission of multiple viruses [3] . However, as noted by Worby et al. [3] , transmission dynamics and bottleneck sizes differ among viruses, and the extent to which transmission can be reconstructed using iSNVs needs to be tested on a per-virus basis. Evaluating these methods is often difficult if details about the 'true' transmission chain are unknown.\n\nThe Ebola virus (EBOV) outbreak in Nigeria presented a unique opportunity to compare conclusions obtained from genomic data to those determined from detailed contact tracing. When We also identified 31 intrahost single-nucleotide variants (iSNVs) in five of the 12 EBOV genomes from Nigeria (five synonymous, five nonsynonymous, five noncoding SNPs, and 16 insertions/deletions). We sequenced each of the five samples with iSNVs at least twice from replicate libraries, and iSNV calls were concordant between libraries. Eight of these iSNVs were shared by \u22652 samples, and two iSNVs (positions 7,551 and 10,503), both found in sample E027, were also consensus variants in sample E030. The presence and number of iSNVs found correlated roughly with sample coverage; only samples with >100\u00d7 coverage had >1 iSNV call that passed our basic filters. \n\nTo better understand the evolutionary relationship between the EVD outbreak in Nigeria and the West African outbreak as a whole, we created a maximum likelihood tree (Figure 3.1) . The tree confirms that the EVD outbreak in Nigeria was due to a single introduction from Liberia, as suggested by contact tracing. More specifically, the EBOV genomes from Nigeria are descendants of the LB5 clade in Liberia [7] . No EBOV sequences yet sampled outside Nigeria descend from the Nigerian EBOV isolates [1, [8] [9] [10] [11] [12] , indicating containment of EVD cases in Nigeria within the larger outbreak, as also suggested by contact tracing.\n\nGiven the phylogenetic tree of the sampled viruses, along with their dates, it is possible to infer at least the outlines of the chain of transmission from one patient to another ( Contact tracing provides more precise information, but is not always available. Samples were collected in Lagos, Nigeria, unless otherwise identified. Each case is labeled with its sample collection date; cases not connected to sequenced samples are labeled with date of hospitalization. Samples are colored by consensus sequence (i.e., samples with identical viral genomes are similarly colored). Cases in gray are those for which genetic data are not available.\n\nSeptember, and these patients therefore may have been infected by one of the earlier case patients.\n\nThe presence of additional SNPs in the viral genomes corresponding to cases 2 and 9 make it difficult to place these samples within the transmission chain. However, case 6 has an iSNV at each of the two case 9 SNP positions (position 7,551, 21% minor allele frequency; position 10,503, 16% minor allele frequency), suggesting that these two cases are closely linked.\n\nIn the limited Nigeria EVD outbreak, it was also possible to reconstruct a nearly complete transmission chain based on contact tracing alone ( \n\nThe 2014 Nigeria outbreak is unusual for an EVD outbreak in the detailed information available about its development: we have both a good reconstruction of the transmission chain of 20 patients, and viral genomic data from most cases in the chain. The completeness of the record reflects the public health situation: Nigeria was prepared for the arrival of EBOV and was able to im-plement thorough contact tracing promptly after the index case was diagnosed, while the number of cases was still small. That effort was critical in containing the outbreak, but it is also very helpful in reconstructing its details afterward. Combined with sequence data, the transmission chain helps us interpret the changes occurring in the virus, because it generally lets us pinpoint where in the chain each new mutation actually occurred.\n\nViewed by itself, sequence data can serve to provide a broad picture of an outbreak, and that is true of this EVD outbreak. This capability is obviously useful when contact tracing is absent or incomplete, as is usually the case with epidemics. In the 2014 Nigeria outbreak, sequencing alone makes it clear that the entire outbreak stemmed from a single introduction of EBOV into the country. It also places the Nigerian outbreak in its larger context, identifying a particular branch of the Liberian LB5 lineage of EBOV as the source and showing that the Nigerian lineage did not spread into other countries.\n\nIdentifying individual links in the transmission chain is usually beyond the resolution of sequence data, however, and requires contact tracing in the field. The resolution of genomic data is limited because new variants arise less often than new cases, meaning that many cases will be genetically indistinguishable. This can be seen in our data in Within-host variants (iSNVs) that are shared between patients can provide a more detailed picture of transmission routes, but our data point out some important caveats about their usefulness. First, detection of iSNVs requires deep sequencing of good-quality samples, and that is not always possible: deep enough sequencing could be achieved for only two-thirds of our sequenced samples. Second, even when iSNV data are available, it may not all be meaningful. Some of the iS-NVs we observed have previously been documented in unrelated data sets from Sierra Leone and Liberia [1, 5, 7] ; these included all eight of the shared iSNVs. Most of our iSNVs, including most shared iSNVs, were low-frequency frameshift insertions or deletions. Because they can disrupt protein structure, they are unlikely to be transmitted. More likely, these iSNVs represent either recurring mutations in highly mutable regions of the EBOV genome or sequencing errors, especially because many of them occur in homopolymer regions. In either case, their value for determining transmission chains is uncertain. More research is necessary to fully make use of within-host genomic data in understanding transmission, including better sequencing coverage for all samples and improved methods to identify false-positives.\n\nOne aspect of our genomic data that is slightly surprising is the distribution of new variants, which is not at all uniform. Our sequenced samples include the results of 11 transmissions from the index case. Nine of these produced no new consensus SNPs, one produced four new SNPs, and one produced two (Figure 3.2A) . This clustering of mutations in certain samples suggests the possibility that the mutation rate was not uniform across all of the cases. This is no more than a possibility, though, because the clustering is not statistically significant (p=0.07).\n\nAlso puzzling is a pair of variants that were seen twice, once as consensus SNPs (in case 9) and once as iSNVs (in case 6). Based on sample dates and contact data, both of these patients were infected by the index patient, so presumably they inherited these variants from that patient. We do not, however, find them in the sample from the index patients, either as consensus SNPs or as iSNVs, despite high sequencing depth. Nor do they appear as consensus SNPs in the other cases derived from the index case, or as iSNVs in the one other case that was deeply sequenced and was sampled around the same time as samples 6 and 9. The explanation may simply be that the variants were present in the index patient but at too low a frequency for us to detect. It is also possible that their frequency changed in the index patient between the time he was sampled and transmission to the other cases, or that they differed across tissues within the patient. Better understanding of the dynamics of within-host evolution and transmission, and of our power to detect iSNVs, would help clarify this issue.\n\nThe genomic data were invaluable in revealing what was happening to the virus during the outbreak, but it would have been even more informative had samples been of uniformly high quality. Many samples did not produce whole-genome assemblies because of poor sample quality, and a third of those that did could not be used to detect iSNVs. This highlights the importance of rapid sequencing in clinical settings during outbreaks, with well-established sample collection and processing protocols. Although at the time of the outbreak sequencing was not yet ready on site, sequencing capability is now becoming increasingly available throughout many regions. With highthroughput deep sequencing now being routinely performed by ACEGID at RUN, high-resolution pathogen information can now be generated to elucidate outbreak dynamics and response, both in Nigeria and throughout West Africa.\n\nData handling could similarly benefit from good protocols established in advance. In the case of the data presented here, clinical and contact data were separated from sequence data, and the correspondence between them had to be established post hoc, a process that was both laborious and uncertain. In an outbreak setting, keeping track of different kinds of data is not the highest priority, but valuable information can be lost as a result. Having a system for collecting and maintaining both clinical and laboratory data established in advance would be very helpful. Injectables and invasive procedures were avoided unless patients were too ill or weak to take oral rehydration solution.\n\nInfection prevention and control procedures and protocols were strictly adhered to in patient management. Before discharge, patients were confirmed negative for EVD by RT-PCR. When discharged, they were decontaminated before being allowed to leave the ETC and were not allowed to take clothing or other personal items. Replacement clothes, footwear, and basic personal effects were provided by family or the ETC, depending on individual circumstances. \n\nwhere S t is the total number of new SNPs, S s is the number of new SNPs seen in a single case, and N t is the number of transmissions. The first probability is the Poisson probability density function, p(S t | \u03bc s ) = ((\u03bc s N t ) St e \u2212\u03bc s Nt )/S t ! , and the second is the cumulative distribution function,\n\nWe would like to thank Mike Lin and Yifei Men at DNAnexus for their engineering work to assist with analysis of data generated by ACEGID at RUN. We also thank the RUN management for the support provided to ACEGID staff during the 2014 EVD outbreak in Nigeria, members of the Emergency Operations Center in Nigeria during the outbreak, and the federal government of Nigeria. In the process of analyzing ZIKV and responding to the outbreak, we performed an in-depth exploration of sequencing methods and iSNV identification; these are analyses I was primarily responsible for, and they are summarized in Figure 4 .1A, Figure 4 .4, and \n\nAlthough the recent ZIKV epidemic in the Americas and its link to birth defects have attracted a great deal of attention [5, 6] , much remains unknown about ZIKV disease epidemiology and ZIKV evolution, in part owing to a lack of genomic data. Here we address this gap in knowledge by using multiple sequencing approaches to generate 110 ZIKV genomes from clinical and mosquito samples from 10 countries and territories, greatly expanding the observed viral genetic diversity from this outbreak. We analyzed the timing and patterns of introductions into distinct geographic regions; our phylogenetic evidence suggests rapid expansion of the outbreak in Brazil and multiple introductions of outbreak strains into Puerto Rico, Honduras, Colombia, other Caribbean islands, and the continental United States. We find that ZIKV circulated undetected in multiple regions for many months before the first locally transmitted cases were confirmed, highlighting the importance of surveillance of viral infections. We identify mutations with possible functional implications for ZIKV biology and pathogenesis, as well as those that might be relevant to the effectiveness of diagnostic tests.\n\nSince its introduction into the Americas, mosquito-borne ZIKV (family: Flaviviridae) has spread rapidly, causing hundreds of thousands of cases of ZIKV disease, as well as ZIKV congenital syndrome and probably other neurological complications [5, 6, 12] . Phylogenetic analysis of ZIKV can reveal the trajectory of the outbreak and detect mutations that may be associated with new disease phenotypes or affect molecular diagnostics. Despite the 70 years since its discovery and the scale of the recent outbreak, however, fewer than 100 ZIKV genomes have been sequenced directly from clinical samples. This is due in part the technical challenge of sequencing ZIKV, which is difficult because the viral load is relatively low compared to viruses with acute infections like Ebola virus (EBOV) [13] , or even compared to other flaviviruses such as Dengue [14] .\n\nSpecifically, while EBOV titers often range from 10 5 -10 7 cp/ml [13] , ZIKV titer typically ranges from 10 2 -10 5 cp/ml [15, 16] (possibly up to 10 5 cp/ml in urine [16] ). Additionally, ZIKV titer in most bodily fluids drops rapidly during the first week of symptomatic infection [16, 17] , making it difficult to detect and sequence unless a patient is diagnosed shortly after he or she becomes symptomatic. Since ZIKV often causes short-lived, non-specific, flu-like symptoms [ (Table D. 1). In one patient we detected no ZIKV sequence but did assemble a complete genome from dengue virus (type 1), one of the viruses that co-circulates with and presents similarly to ZIKV [20] .\n\nTo capture sufficient ZIKV content for genome assembly, we turned to two targeted approaches for enrichment before sequencing: multiplex PCR amplification [21] and hybrid capture [22] . We sequenced and assembled complete or partial genomes from 110 samples from across the epidemic, out of 229 attempted (221 clinical samples from confirmed and possible ZIKV disease cases and eight mosquito pools; Table 4 .1). This dataset, which we used for further analysis, includes 110 genomes produced using multiplex PCR amplification (amplicon sequencing) and a subset of 37 genomes produced using hybrid capture (out of 66 attempted). Because these approaches amplify any contaminant ZIKV content, we relied heavily on negative controls to detect artefac- tual sequence, and we established stringent, method-specific thresholds on coverage and completeness for calling high-confidence ZIKV assemblies (Figure 4.1A) . We typically include a water sample (or a sample from a completely different virus, such as EBOV) when preparing samples for sequencing, and thus used ZIKV content in these negative controls as a proxy for the level of contamination present in that particular batch of samples. We used this principle to define inclusion thresholds for ZIKV assemblies generated in this study. Given the significant differences between the hybrid capture and amplicon sequencing methods, we developed a different set of criteria for each method.\n\nFor the hybrid capture method, we identified the highest number of unambiguous (non-'N') bases observed in any negative control across all sequencing runs, B NC \u2248 2,000, and set the thresh- old at approximately 2 * B NC = 4,000. We could have set a unique threshold for every run, but opted for this more conservative measure. For the amplicon sequencing method, we noticed that sequencing assemblies were roughly binary in their median sequencing depth: the best samples had a depth well over 1,000\u00d7, and poor samples generally had a coverage under 100\u00d7. To set the exact threshold of 275\u00d7, we used data from positive controls and ensured that the assembly of every positive control would pass our threshold. As an additional precaution, we required 2,500 unambiguous bases (roughly 25% of the genome) in sequences produced via amplicon sequencing (see also Methods, Section 4.5).\n\nWhile using negative controls to set inclusion thresholds for assemblies may seem a simple and practical task, it is important to note that we had not previously employed such thresholds for sequencing EBOV or other viruses. This is largely because contamination is much less of an issue in the absence of amplification prior to sequencing, or when dealing with samples with higher viral content. That said, we have since used inclusion thresholds (the method, not the specific numerical values) when sequencing other viruses, such as mumps, as described in the following chapter.\n\nAfter applying these thresholds to select genomes for downstream analysis, we calculated completeness and coverage, shown in Figure 4 .1B-C; the median fraction of the genome with unambiguous base calls was 93%. Per-base discordance between genomes produced by the two methods was 0.017% across the genome, 0.15% at polymorphic positions, and 2.2% for minor allele base calls. Patient sample type (urine, serum, or plasma) made no significant difference to sequencing success in our study (Figure D . This is 1.3-5 times higher than reported rates for other flaviviruses [23] , but is measured over a short sampling period, and therefore may include a higher proportion of mildly deleterious mutations that have not yet been removed through purifying selection.\n\nDetermining when ZIKV arrived in specific regions helps to elucidate the spread of the outbreak and track rising incidence of possible complications of ZIKV infection. The majority of the ZIKV genomes from our study fall into four major clades from different geographic regions, for which we estimated a likely date for ZIKV arrival. In each case, the date was months earlier than the first confirmed, locally transmitted case, indicating ongoing local circulation of ZIKV before its detection. In Puerto Rico, the estimated date was 4.5 months earlier than the first confirmed local case [3] ; it was 8 months earlier in Honduras [2] , 5.5 months earlier in Colombia [24] , and 9 months earlier for the Caribbean-continental U.S. clade [4] . In each case, the arrival date rep- Table D .2 for details). Similar temporal gaps between the tMRCA of local transmission chains and the earliest detected cases were seen when chikungunya virus emerged in the Americas [25] . We also observed evidence for several in- . Adaptive mutations are more likely to be found at high frequency or to be seen multiple times, although both effects can also occur by chance. We observed five positions with nonsynonymous mutations at more than 5% minor allele frequency that occurred on two or more branches of the tree (Figure 4 .3B); two of these (at positions 4,287 and 8,991) occurred together and might represent incorrect placement of a Brazil branch in the tree. The remaining three are more likely to represent multiple nonsynonymous mutations; one (at 9,240) appears to involve nonsynonymous mutations to two different alleles.\n\nTo assess the possible biological significance of these mutations, we looked for evidence of selection in the ZIKV genome. Viral surface glycoproteins are known targets of positive selection, and mutations in these proteins can confer adaptation to new vectors [32] or aid immune escape [33, 34] . We therefore searched for an excess of nonsynonymous mutations in the ZIKV envelope glycoprotein (E). However, the nonsynonymous substitution rate in E proved to be similar to that in the rest of the coding region (Figure 4 .3C, left); moreover, amino acid changes were significantly more conservative in that region than elsewhere (Figure 4 .3C, middle and right). Any diversifying selection occurring in the surface protein thus appears to be operating under selective constraint.\n\nWe also found evidence for purifying selection in the ZIKV 3' UTR (Figure 4 .3D), which is important for viral replication [35] .\n\nWhile the transition-to-transversion ratio (6.98) was within the range seen in other viruses [36] , we observed a considerably higher frequency of C-to-T and T-to-C substitutions than other performance in this outbreak [37] . To assess the potential influence of ongoing viral evolution on diagnostic function, we compared eight published qRT-PCR-based primer/probe sets to our data.\n\nWe found numerous sites at which the probe or primer did not match an allele found among the 174 ZIKV genomes from the current dataset (Figure 4 .3E). In most cases, the discordant allele was shared by all outbreak samples, presumably because it was present in the Asian lineage that entered the Americas. These mismatches could affect all uses of the diagnostic assay in the outbreak.\n\nWe also found mismatches from new mutations that occurred after ZIKV entry into the Americas.\n\nMost of these were present in less than 10% of samples, although one was seen in 29%. These observations suggest that genome evolution has not caused widespread degradation of diagnostic performance during the course of the outbreak, but that mutations continue to accumulate and ongoing monitoring is needed.\n\nViral within-host variants can be useful for understanding transmission between hosts, and for elucidating viral dynamics and evolutionary processes. This variation comes from the presence of viral quasispecies [38, 39] , viruses that are very similar but not identical within a single host. Quasispecies are particularly common in RNA viruses, which have high mutation rates, short generation times, and large population sizes [40, 41] . The coexistence of multiple variants may be advantageous to viruses, and provide opportunities for immune and vaccine escape [42, 43] . Therefore, studying these variants may reveal functionally-important mutations, while also providing clues about within-host dynamics and viral pathogenesis [44] . If multiple viral quasispecies are passed between hosts during an infection event [45] [46] [47] , they may also be useful in understanding transmission patterns, especially if between-host viral variation is otherwise limited.\n\nGiven the potential of within-host variation to inform transmission and biology, we attempted to identify intrahost variants (iSNVs) in our ZIKV samples. Since we had sequencing data from two independent sequencing methods (amplicon sequencing and hybrid capture), we also wanted to test concordance between these methods and potentially improve upon our method [45] for filtering out low-frequency iSNVs likely due to contamination or sequencing error.\n\nWe called iSNVs on genomes generated by each sequencing method. We initially used the frequency filters established in Gire et al. [45] , which require an iSNV to be identified in five forward and reverse reads and limits strand bias (the ratio between reads with the variant on the two strands) to 10\u00d7. Using this method, we quickly noticed that very few iSNVs were identified in samples generated by the amplicon sequencing method. Given the substantial differences in sequencing preparation methods, we hypothesized that a different set of requirements may be necessary to accurately filter data generated using this method. Therefore, we removed all filters from both methods and compared the resulting variants, assuming variants generated by hybrid capture and passing the Gire et al. filters to be correct (we call these 'verified' iSNVs). We additionally required a minimum read depth at each iSNV position, with the aim of eliminating difference in coverage as a reason for unmatched calls (see Chapter 2, Figure B .3).\n\nThe results of this analysis, summarized in Figure 4 .4, show that high-frequency (>20%) iS-NVs are consistently identified by both methods. At lower frequencies, however, the amplicon sequencing method misses variants that we accept as true (Figure 4 .4A and Table D .3A, which shows that 25% of verified iSNVs are not identified using amplicon sequencing). Additionally, we find that amplicon sequencing identifies a large number of presumably spurious mutations not identified in hybrid capture replicates, or even in a second amplicon sequencing replicate (Table D. 3).\n\nThis investigation also demonstrates the importance of filtering within-host variant calls, regardless of method. Low levels of contamination and sequencing error are common contributors to low-frequency variants; these issues are unavoidable with current technologies and must be addressed by computational filtering. Figures 4.4B and Table D .3B show that nearly 75% of variants from hybrid capture sequences failed to replicate, but that this problem disappears when implementing a strand bias filter. However, the low number of variants passing this filter (n=8) suggests that this method may be too conservative, that the samples were not sequenced to high enough read depth for iSNV identification, or that ZIKV may lack substantial within-host variation. Opti-mizing methods and further testing on ZIKV will be required to better understand this result and potentially generate iSNVs useful for studying ZIKV evolution or transmission. Identifying thresholds that successfully filter out spurious variants will be especially useful for amplicon sequencing, and will hopefully generate reliable iSNVs from this method. Notably, in a cultured ZIKV sample with high viral content, both methods produced the same results and were internally consistent at nearly all frequencies (Figure 4 .4C), again suggesting that issues in iSNV identification may be exacerbated by enrichment of low quality samples.\n\nAlthough this analysis did not produce enough validated variants for ZIKV transmission analysis, it clearly demonstrates the need for appropriate filtering of variants, and cautions against drawing conclusions from within-host variants without replication and validation. It also adds to our understanding of amplicon and hybrid capture sequencing, both of which are still widely used.\n\nSequencing low-titer viruses such as ZIKV directly from clinical samples presents several challenges that are likely to have contributed to the paucity of genomes available from the current outbreak. While the development of technical and analytical methods will surely continue, we note that factors upstream in the process, including collection site and cohort, were strong predictors of sequencing success in our study (Figure D.1 \n\nRNA was isolated following the manufacturer's standard operating protocol for 0.14-1 ml samples [8] using the QIAamp Viral RNA Minikit (Qiagen), except that in some cases 0.1-M final concentration of \u03b2-mercaptoethanol (as a reducing agent) or 40 \u03bcg/ml final concentration of linear acrylamide (Ambion) (as a carrier) were added to AVL buffer before inactivation. Extracted RNA was resuspended in AVE buffer or nuclease-free water. In some cases, viral samples were concentrated using Vivaspin-500 centrifugal concentrators (Sigma-Aldrich) before inactivation and extraction.\n\nIn these cases, 0.84 ml of sample was concentrated to 0.14 ml by passing through a 30 kDa filter and discarding the flow-through.\n\nIn a subset of human samples, carrier poly(rA) RNA and host rRNA were depleted from RNA samples using RNase H selective depletion [22, 48] . ZIKV amplicons were prepared as described [10, 21] , similarly to 'RNA jackhammering' for preparing low-input viral samples for sequencing [49] , with slight modifications. After PCR amplification, each amplicon pool was quantified on a 2200 Tapestation \n\nVirus hybrid capture was performed as previously described [22] . Probes were created to target ZIKV and chikungunya virus (CHIKV). Candidate probes were created by tiling across publicly available sequences for ZIKV and CHIKV on NCBI GenBank [50] . Probes were selected from among these candidate probes to minimize the number used while maintaining coverage of the observed diversity of the viruses. Alternating universal adapters were added to allow two separate PCR amplifications, each consisting of non-overlapping probes.\n\nThe probes were synthesized on a 12k array (CustomArray). The synthesized oligos were am- \n\nWe assembled reads from all sequencing methods into genomes using viral-ngs v1.13.3 [46, 51] . We taxonomically filtered reads from amplicon sequencing against a ZIKV reference, GenBank acces- For each sample, we ran Kraken on data from unbiased sequencing replicates (not including hybrid capture data) and searched its output reports for viral taxa with more than 100 reported reads. We manually filtered the results, removing ZIKV, bacteriophages, and known laboratory contaminants. For each sample and its associated taxa, we assembled genomes using viral-ngs as described above; the results are in Table D reporting sequence identity of an assembly to its taxon, we used BLASTN43 to determine the identity between the sequence and the reference used for its assembly.\n\nTo focus on metagenomics of mosquito pools (Table D. 1B), we considered unbiased sequencing data from eight mosquito pools (not including hybrid capture data). We first ran the depletion pipeline of viral-ngs on raw data and then ran the viral-ngs Trinity [57] assembly pipeline on the depleted reads to assemble them into contigs. We pooled contigs from all mosquito pool samples and identified all duplicate contigs with sequence identity >95% using CD-HIT [58] . Additionally, we used predicted coding sequences from Prodigal 2.6.3 [59] to identify duplicate protein sequences at >95% identity. We classified contigs using BLASTN [60] against nt and BLASTX [60] against nr (as of February 2017) and discarded all contigs with an E value greater than 1 \u00d7 10 \u22124 .\n\nWe define viral contigs as contigs that hit a viral sequence, and we manually removed all reversetranscriptase-like contigs owing to their similarity to retrotransposon elements within the Aedes aegypti genome. We categorized viral contigs with less than 80% amino acid identity to their best hit as likely novel viral contigs. We treated samples with type 'Plasma EDTA' as having type 'Plasma' . We treated the collection interval variable as categorical (0-1, 2-3, 4-6, and 7+ days).\n\nWith a single model we underfit the zero counts, possibly because many zeros (samples without a replicate that passed ZIKV assembly) are truly ZIKV-negative. We thus view the data as coming from two processes: one determining whether a sample is ZIKV-positive or ZIKV-negative, and another that determines, among the observed passing samples, how much of a ZIKV genome we are able to sequence. We modelled the first process, predicting whether a sample is passing, with logistic regression (in R using GLM [61] with binomial family and logit link); here, the observed passing samples are the samples S for which Y S \u2265 2, 500. For the second, we performed a beta regression, using only the observed passing samples, of Y S divided by ZIKV genome length on the predictor variables. We implemented this in R using the betareg package [62] and transformed fractions from the closed unit interval to the open unit interval as the authors suggest.\n\nTo test the significance of predictor variables, we used a likelihood ratio test. For variable X i we compared a full model (with all predictors) against a model that used all predictors except X i .\n\nThe results of these tests are shown in Figure D .1A,D. We explored the effects of sample type and collection interval on obtaining a passing assembly in Figure D .1B and C, respectively. Error bars are 95% confidence intervals derived from binomial distributions. We explored the effects of these same two variables on Y S (in passing samples only) in Figure D .1E-F.\n\nWe attempted to sequence one or more replicates of each sample and attempted to assemble a genome from each replicate. We discarded data from any replicates whose assembly showed high sequence similarity, in any part of the genome, to our assembly of the genome in a sample consisting of an African (Senegal) lineage (strain HD78788) of ZIKV. We used this sample as a positive control throughout this study, and considered its presence in the assembly of a clinical or mosquito pool sample to be evidence of contamination. Similarly, we discarded data from four replicates belonging to samples from the Dominican Republic because they yielded assemblies that were unexpectedly identical or highly similar to our assembly of the ZIKV isolate PE243 genome, another positive control used in this study. We also discarded data from replicates that showed evidence of contamination, at the RNA stage, by the baits used in hybrid capture; we detected these by looking for adapters that were added to these probes for amplification.\n\nFor amplicon sequencing, we considered an assembly of a replicate to be 'passing' if it contained at least 2,500 unambiguous base calls and had a median depth of coverage of at least 275\u00d7 over its unambiguous bases (depth includes duplicate reads). For the unbiased and hybrid capture approaches, we considered an assembly of a replicate 'passing' if it contained at least 4,000 unambiguous base calls. For each approach, the unambiguous base threshold was based on an observed density of negative controls below the threshold (Figure 4.1A) . For amplicon sequencing assemblies, we added a coverage depth threshold because coverage depth was roughly binary across replicates, with negative controls falling in the lower class. On the basis of these thresholds, zero of 99 negative controls used throughout our sequencing runs yielded passing assemblies and 32 of 32 positive controls yielded passing assemblies.\n\nWe considered a sample to have a passing assembly if any of its replicates, by either method, yielded an assembly that passed the above thresholds. For each sample with at least one passing assembly, we pooled read data across replicates for each sample, including replicates with assemblies that did not pass the assembly thresholds. When data were available from both amplicon sequencing and unbiased/hybrid capture approaches, we pooled amplicon sequencing data separately from data produced by the unbiased and hybrid capture approaches, the latter two of which were pooled together (henceforth, the 'hybrid capture' pool). We then assembled a genome from each set of pooled data. When assemblies on pooled data were available from both approaches, we selected for downstream analysis the assembly from the hybrid capture approach if it had at least 10,267 unambiguous base calls (95% of the reference genome used, GenBank accession: KX197192.1); when this condition was not met, we selected the one that had more unambiguous base calls.\n\nThe number of ZIKV genomes publicly available before this study was the result of an NCBI GenBank [50] search for ZIKV in February 2017. We filtered any sequences with length <4,000 nt, excluded sequences that are being published as part of this study or in Faria et al. [9] or Grubaugh et al. [10] , excluded sequences from non-human hosts, and excluded sequences labelled as having been passaged. We counted fewer than 100 sequences, the precise number depending on details of the count.\n\nFor amplicon sequencing data, we plotted coverage across the 110 samples that yielded a passing assembly by amplicon sequencing (Figure 4.1B) . With viral-ngs, we aligned depleted reads to the reference sequence KX197192.1 using the novoalign aligner with options '-r Random -l 40 -g 40 -x 20 -t 100 -k' . Because of the nature of amplicon sequencing, duplicates were not identified or removed.\n\nWe binarized depth at each nucleotide position, showing red if depth of coverage was at least 100\u00d7.\n\nRows (samples) are hierarchically clustered to ease visualization.\n\nFor hybrid capture sequencing data, we plotted depth of coverage across the 37 samples that yielded a passing assembly (Figure 4 .1C). We aligned reads as described above for amplicon sequencing data, except we removed duplicates. For each sample, we calculated the depth of coverage at each nucleotide position. We then scaled the values for each sample so that each would have a mean depth of 1.0. At each nucleotide position, we calculated the median depth across the samples, as well as the 20th and 80th percentiles. We plotted the mean of each of these metrics within a 200-nt sliding window.\n\nWe aligned ZIKV consensus genomes using MAFFT v7.221 [63] with the following parameters:\n\n'--maxiterate 1000 --ep 0.123 --localpair' .\n\nTo measure overall per-base discordance between consensus genomes produced by amplicon sequencing and hybrid capture, we considered all sites at which base calls were made in both the amplicon sequencing and hybrid capture consensus genomes of a sample, and we calculated the fraction in which the bases were not in agreement. To measure discordance at polymorphic sites, we searched for positions with a polymorphism in all genomes generated in this study that we selected for downstream analysis (see Section 4.5.11 for choosing among the amplicon sequencing and hybrid capture genome when both are available). We then looked at these positions in genomes that were available from both methods, and we calculated the fraction in which the alleles were not in agreement.\n\nTo measure discordance at minor alleles, we searched for minor alleles in all genomes generated in this study that we selected for downstream analysis. We then looked at all sites at which there was a minor allele and for which genomes from both methods were available, and we calculated the fraction in which the alleles were not in agreement. For these calculations, we tolerated partial ambiguity (for example, 'Y' is concordant with 'T'). If one genome had full ambiguity ('N') at a position and the other genome had an indel, we counted the site as discordant; otherwise, if one genome had full ambiguity, we did not count the site.\n\nAfter assembling genomes, we identified within-sample variants by running V-Phaser 2.0 via viral-ngs [51] on all pooled reads mapping to each sample assembly. When determining per-library allele counts at each variant position, we modified viral-ngs to require a minimum base (Phred) quality score of 30 for all bases, discard anomalous read pairs, and use per-base alignment quality (BAQ) in its calls to SAMtools [64] mpileup. This is particularly helpful for filtering spurious ampli- to eliminate lack of coverage as a reason for discrepancy between two methods. When comparing allele frequencies across sequencing replicates within a method, we imposed only a minimum read depth (275\u00d7 for amplicon sequencing and 100\u00d7 for hybrid capture), but required this depth in both libraries. In samples with more than two replicates, we considered only the two replicates with the highest depth at each variant position.\n\nWe considered allele frequencies from hybrid capture sequencing 'verified' if they passed the strand bias and frequency filters described in Gire et al. [45] , with the exception that we imposed a minimum allele frequency of 1% and allowed a variant identified in only one library if its frequency was \u22655%. In Figure 4 .4B and Table D We called SNPs on the aligned genomes using Geneious version 9.1.7 [65] . We converted all fully or partially ambiguous calls, which are treated by Geneious as variants, into missing data. We then removed all sites that were no longer polymorphic from the SNP set and re-calculated allele frequencies.\n\nA nonsynonymous mutation is shown on the tree ( We quantified the effect of nonsynonymous mutations using the original BLOSUM62 scoring matrix for amino acids [66] , in which positive scores indicate conservative amino acid changes and negative scores unlikely or extreme substitutions. We assessed statistical significance for equality of proportions by \u03c7 2 test (Figure 4 .3C, middle), and for difference of means by two-sample t-test with Welch-Satterthwaite approximation of d.f. (Figure 4 .3C, right). Error bars are 95% confidence intervals derived from binomial distributions (Figure 4 .3C, left and middle; Figure 4 .3D) or Student's t distributions (Figure 4 .3C, right).\n\nWe generated a maximum likelihood tree using a multiple sequence alignment that included genomes generated in this study, as well as a selection of other available sequences from the Americas, Southeast Asia, and the Pacific. We ran PhyML [67] with the GTR substitution model and four gamma substitution rate categories; for the tree search operation, we used 'BEST' (best of NNI and SPR). In FigTree v1.4.2 [68] , we rooted the tree on the oldest sequence used as input (GenBank accession: EU545988.1).\n\nWe used TempEst v1.5 [69] , which selects the best-fitting root with a residual mean squared function, to estimate root-to-tip distances. We performed regression in R with the lm function [61] of distances on dates. The relationship between root-to-tip divergence and sample dates ( Figure   D .2) supports the use of a molecular clock analysis in this study.\n\nFor molecular clock phylogenetics, we made a multiple sequence alignment from the genomes generated in this study combined with a selection of other available sequences from the Americas.\n\nWe did not use sequences from outside the outbreak in the Americas. Among ZIKV genomes published and publicly available on NCBI GenBank [50] , we selected 32 from the Americas that had at least 7,000 unambiguous bases, were not labelled as having been passaged more than once, and had location metadata. We also used 32 genomes from Brazil published in Faria et al. [9] that met the same criteria.\n\nWe used BEAST v1.8.4 to perform molecular clock analyses [70] . We used sampled tip dates to handle inexact dates [71] . Because of sparse data in non-coding regions, we used only the CDS as input. We used the SRD06 substitution model on the CDS, which uses HKY with gamma site heterogeneity and partitions codons into two partitions (positions (1+2) and 3) [72] . To perform model selection, we tested three coalescent tree priors: a constant-size population, an exponential growth population, and a Bayesian Skyline tree prior (ten groups, piecewise-constant model) [73] .\n\nFor each tree prior, we tested two clock models: a strict clock and an uncorrelated relaxed clock with log-normal distribution (UCLN) [74] . In each case, we set the molecular clock rate to use a continuous time Markov chain rate reference prior [75] . For all six combinations of models, we performed path-sampling (PS) and stepping-stone sampling (SS) to estimate marginal likelihood [76, 77] . We sampled for 100 path steps with a chain length of 1 million, with power posteriors determined from evenly spaced quantiles of a Beta(alpha=0.3; 1.0) distribution. The Skyline tree prior provided a better fit than the two other (baseline) tree priors (Table D. 2), so we used this tree prior for all further analyses. Using a constant or exponential tree prior, a relaxed clock provides a better model fit, as shown by the log Bayes factor when comparing the two clock models. Using a Skyline tree prior, the log Bayes factor comparing a strict and relaxed clock is smaller than it is using the other tree priors, and it is similar to the variability between estimated log marginal likelihood from PS and SS methods. We chose to use a relaxed clock for further analyses, but we also report key findings using a strict clock.\n\nFor the tree and tMRCA estimates in Figure 4 .2, as well as the clock rate reported in main text, we ran BEAST with 400 million MCMC steps using the SRD06 substitution model, Skyline tree prior, and relaxed clock model. We extracted clock rate and tMRCA estimates, and their distributions, with Tracer v1.6.0 and identified the maximum clade credibility (MCC) tree using TreeAnnotator v1.8.4. We visualised the tree in FigTree v1.4.2 [68] . The reported credible intervals around estimates are 95% highest posterior density (HPD) intervals. When reporting substitution rate from a relaxed clock model, we give the mean rate (mean of the rates of each branch weighted by the time length of the branch). Additionally, for the tMRCA estimates in Figure 4 .2C with a strict clock, we ran BEAST with the same specifications (also with 400M steps) except using a strict clock model. The resulting data are also used in the more comprehensive comparison shown in Figure   D .3.\n\nFor the data with an outgroup in Figure D. 3, we ran BEAST as specified above (with strict and relaxed clock models), except with 100 million steps and with outgroup sequences in the input alignment. The outgroup sequences were the same as those used to make the maximum likelihood tree. For the data excluding sample DOM_2016_MA-WGS16-020-SER in Figure D. 3, we ran BEAST as specified above (with strict and relaxed clocks), except we removed the sequence of this sample from the input and ran 100 million steps.\n\nWe used BEAST v1.8.4 to estimate transition and transversion rates within the CDS and noncoding regions. The model was the same as above except that we used the Yang96 substitution model on the CDS, which uses GTR with gamma site heterogeneity and partitions codons into three partitions [78] ; for the non-coding regions, we used a GTR substitution model with gamma site heterogeneity and no codon partitioning. There were four partitions in total: one for each codon position and another for the non-coding region (5' and 3' UTRs combined). We ran this for 200 million steps. At each sampled step of the MCMC, we calculated substitution rates for each partition using the overall substitution rate, the relative substitution rate of the partition, the relative rates of substitutions in the partition, and base frequencies. In Figure D .4, we plot the means of these rates over the steps; the error bars shown are 95% HPD intervals of the rates over the steps.\n\nWe used BEAST v1.8.4 to reconstruct ancestral state at the root of the tree using CDS and noncoding regions. The model was the same as above except that, on the CDS, we used the HKY substitution model with gamma site heterogeneity and codons partitioned into three partitions (one per codon position). On the non-coding regions we used the same substitution model without codon partitioning. We ran this for 50 million steps and used TreeAnnotator v1.8.4 to find the state with the MCC tree. We selected the ancestral state corresponding to this state.\n\nIn all BEAST runs, we discarded the first 10% of states from each run as burn-in.\n\nWe carried out principal components analysis using the R package FactoMineR [79] . We imputed missing data with the package missMDA [80] and we show the results in Figure 4 .2D.\n\nWe extracted primer and probe sequences from eight published RT-qPCR assays [26] [27] [28] [29] [30] [31] and aligned them to our ZIKV genomes using Geneious version 9.1.7 [65] . We then tabulated matches and mismatches to the diagnostic sequence for all outbreak genomes, allowing multiple bases to match where the diagnostic primer and/or probe sequence contained nucleotide ambiguity codes (Figure 4 .3E).\n\nSequence data that support findings of this study have been deposited in NCBI GenBank [50] un- we observed multiple co-circulating clades within individual universities as well as spillover into the local community. We also used publicly available sequences from a single gene to estimate migration between world regions and to place this outbreak in a global context, but found this short sequence to be inadequate for tracing detailed transmission. Our findings suggest continuous, often undetected circulation of mumps both locally and nationally, and highlight the value of combining genomic and epidemiological data to track viral disease transmission at high resolution.\n\nAn unusually large number of mumps cases were reported in the United States in 2016 and 2017, despite high rates of vaccination [2, 3] . Mumps incidence declined by more than 98% after introduc- (Table E. 3).\n\nWe used genomic epidemiology to investigate the spread of MuV in Massachusetts and on national and international scales to better understand routes of mumps transmission, as well as to determine whether mutations associated with vaccine escape could be identified. We generated 203 whole genomes (160 from Massachusetts and 43 from 15 other states) using a combination of unbi-ased and capture-based sequencing approaches [1, 7] (see Methods, Section 5.5, and Table E .1). The genomes had a median of 99.48% unambiguous base calls and a median depth of 176\u00d7 (Figure E.1) .\n\nWe also sequenced a set of 29 PCR-negative samples from patients with suspected mumps to determine if we could detect MuV or other viruses that may explain their symptoms. We saw evidence\n\nfor MuV in one sample and identified four other viruses known to cause upper respiratory symptoms (at least two of which are known to cause parotitis) in four separate samples [8] [9] [10] (Table   E. 2).\n\nTo understand how the Massachusetts outbreak fit into the larger context of mumps in the United States, we performed a phylogenetic analysis on our dataset together with all genotype G whole genomes available from NCBI GenBank [11] (n=25). The resulting phylogeny ( Table E . 3 ).\n\n(C) Probability distributions for the tMRCA of selected sequences labeled in (A) (see Table E .4 for additional clades). Dotted lines indicate the mean of each distribution. Thus, what appeared to be a single outbreak across multiple institutions was shown by sequence data to be multiple overlapping outbreaks.\n\nSequence data also allowed us to identify a spillover event from one institution into the larger community. Before genomic data was available, cases associated with Institution A and with a geographic community (clade II-community) were inferred to be separate outbreaks due to the dif-ferent populations affected (mostly students versus adults with no reported university connection) and an apparent five month gap between the two sets of cases. From the phylogeny, however, it is clear that these two groups of cases are related and that the community-associated cases represent a spillover from Institution A into the broader population ( That R E (t=0) is well above one has implications for the required reach of reactive vaccination campaigns in at-risk populations: in this case, vaccination would need to reach 59% (52-67%) to effectively curtail transmission.\n\nWe next investigated the usefulness of sequence data to supplement epidemiological data at the finest analysis scale: reconstructing individual transmission chains. To determine the extent to which genetic data can be used to infer epidemiological links, we examined the genetic distance between samples with a known epidemiological link (i.e., samples likely part of the same transmission chain). All samples with a known link were genetically similar (Figure E .5A), and genetic distance was a good predictor of epidemiological linkage (Figure E.5B) . Given this, we used genetic data (along with sampling dates) to reconstruct transmission chains during the outbreak ( Figure   5 .2C), focusing on samples within clade II-outbreak. This analysis correctly inferred all known epidemiological links.\n\nWithin-host variants (intrahost variants, or iSNVs) shared between samples can provide additional information about transmission chains during viral outbreaks [12] [13] [14] . In our dataset, we identified iSNVs in 52% of samples. Most of the samples without iSNVs had low sequencing coverage, though we did observe this lack of iSNVs in samples with high sequencing coverage as well.\n\nUltimately, however, iSNVs proved uninformative in understanding MuV transmission. We did not find evidence for shared iSNVs in the five pairs of known direct contacts or above 0.1% frequency between any samples. These data suggest that the MuV transmission bottleneck may be small enough to preclude shared within-host variation. We note, however, that in two patients for whom we had multiple samples (in both cases collected nine days apart), the MuV genomes from the same patient differed at one nucleotide position. In one pair, different alleles were fixed in the two genomes; in the other, the genome from the second time point had an iSNV at one position (56% alternate allele, 44% matching the other genome).\n\nWe also looked for variation between genomes in our dataset that could be associated with vaccine We also investigated changes in immunogenic regions of the mumps genome, particularly the hemagglutinin (HN) protein, which is the primary target of neutralizing antibodies (NAb). We identified 32 fixed amino acid substitutions in HN between the strains in our data set and the Jeryl Lynn vaccine strain (the vaccine strain used in the United States). These included substitutions within one putative and two experimentally-defined NAb epitopes and both a gain and a loss of potential N-linked glycosylation site (Figure 5 .3D) [15] [16] [17] [18] [19] . We also observed eight fixed amino acid substitutions between our sequences and the Jeryl Lynn strain in the hypervariable hydrophilic Cterminus of the nucleoprotein (NP), which has also been demonstrated to contain NAb targets [20] .\n\nAt all but two of these sites in HN and NP, the allele in our sequences is the same as in a strain isolated in Iowa in 2006 (GenBank accession: JX287385.1); MuV from this outbreak has been shown to be neutralized by sera from both vaccinated and naturally-infected individuals [21] . Our strains differed from the Iowa 2006 strain at HN positions 336 (in a known NAb epitope) and 474 (in a putative NAb epitope). At both of these positions, most genotype G sequences share the same allele . This genotype appears to have stopped circulating (previously noted in Jin et al. [24] ), raising the possibility that vaccination contributed to the disappearance of that genotype.\n\nWe performed a phylogeographic analysis on these SH sequences to understand movement Each panel shows a posterior probability density, taken across resampled input, of the fraction of all reconstructed migrations that occur to the destination from each of the other three sources (see Methods, Section 5.5, for details regarding geographic and temporal resampling of sequences). (D) Identical genotype G sequences over time. Each dot represents a sample and each row contains samples with identical SH sequences, except the bottom, which includes samples whose sequences are distinct from those in the above five categories. Numbers on the right of each row are the percentage of all genotype G samples found in that row. also found that recent sequences from the United States have primarily European ancestry, and vice versa (Figure E.6D) . Notably, recent sequences from East Asia and South/Southeast Asia have minimal external ancestry, indicating relatively little spread to these regions (Figure 5 .4C).\n\nIt is important to note that the SH gene is less than 3% of the MuV genome and represents \n\nHere we show the importance of pairing genomics with epidemiology to understand the spread of a mumps outbreak. Despite detailed epidemiological data collected in the outbreak, MuV sequence data were required to identify multiple co-circulating strains within Massachusetts (and even within single institutions), to connect two outbreaks previously thought to be separate, and to accurately estimate R E within one institution. Additionally, while SH gene sequencing reveals global trends in MuV circulation, analysis based on a single, short gene would not be adequate for producing this kind of detailed picture of how mumps is spreading in the United States ( Figure   E.7) . Not only do these findings reveal transmission patterns in this particular outbreak, they also suggest the presence of undetected mumps circulating within Massachusetts and the United States, and a possible high asymptomatic burden despite widespread immunization. This has implications for vaccination use and recommendations, already under consideration by the U.S. Centers for Disease Control and Prevention [25] .\n\nAlthough mumps is not deadly, it does serve as a good model for the use of genomic data in a more severe viral outbreak, in which reconstructing transmission and analyzing spread may have great public health significance. We expect that applying detailed genomic and epidemiological data to viral outbreaks will play an increasingly important role in surveillance and response. \n\nViral hybrid capture was performed as previously described [1] using two different probe sets. In one case, probes were created to target MuV and measles virus (MeV), and in one case, probes were created to target 356 species of viruses known to infect humans (V-All probe set [7] ). Capture using V-All was used to enrich viral sequences primarily in samples in which we could not detect MuV, as well as in other samples. As described in Siddle et al. [7] , the probe sets were designed to capture the diversity across all publicly available sequences on GenBank [11] for the relevant viruses.\n\nWe used viral-ngs v1.18.1 [28] to assemble reads form all sequencing runs. We used a set of MuV \n\nWe used the V-All capture method on all samples from suspected mumps cases with a negative\n\nMuV PCR result (n=29). A subset of PCR-positive samples were also sequenced with this probe set (n=145) or without capture ('unbiased' , n=111). We used the mock Enders strain MuV sample as a positive control on a sequencing run containing all PCR-negative samples, and used a water sample as a negative control. We obtained a partial mumps virus assembly using the viral-ngs method described above [28] in six PCR-negative samples, but observed high sequence similarity between some parts of these assemblies and the mock sample, which we considered evidence of contamination. Therefore, we prepared new sequencing libraries from all samples with evidence of other viruses and sequenced these replicates in the absence of the mock mumps sample. We required both replicates to contain reads matching any virus detected in the sample, and we found no evidence of other viruses in PCR-positive samples.\n\nWe used the metagenomic tool Kraken [29] to identify the source genera of the reads in each sample. We required total raw read count for any sample-genus pair to be twice (in practice, seven times) that in any negative control (water) from the same run. For any sample that had one or more pathogenic viral genera that passed this filter and had de-duplicated reads well distributed across the relevant viral genome, we attempted contig assembly: we filtered all sample reads against all GenBank [11] nucleotide entries matching the identified species and then de novo assembled reads and scaffolded against the GenBank RefSeq [30] genome for the identified virus,\n\nusing the viral-ngs [28] pipeline described above. We report all viruses identified via this method in We ignored all fully or partially ambiguous base calls, and excluded sequences that did not descend from the USA_2006 clade (Figure 5 .1A) from this analysis. When examining amino acid changes in HN given vaccination status, we ignored sequences from patients with unknown vaccination history.\n\nWe used BEAST v1.8.4 [32] to perform both the site-specific and per-gene d N /d S analyses ( MuV genome, excluding the portion of the V protein after the insertion site [34] . We partitioned the CDS alignment into codon positions and used a separate HKY [35] substitution model and uncorrelated relaxed clock with log-normal distribution (UCLN) [36] on each partition. We used the trees generated when running BEAST on the full dataset (see Section 5.5.16) and ran 10 million MCMC steps to generate site-specific counts, sampling every 10,000 states.\n\nFor the per-gene analysis, we partitioned the alignment by gene (again excluding the ambiguous portion of the V protein) and ran BEAST as described in Park et al. [13] . We ran 200 million MCMC steps (sampling every 10,000 states) using a Bayesian Skyline tree prior [37] and an independent GY94 codon substitution model [38] for each gene partition. We used a HKY substitution model with \u0393 4 -distributed rate heterogeneity for the noncoding region [35, 39] ) and parameterized all partitions with independent strict molecular clocks. For samples without exact dates, we used sampled tip dates [40] .\n\nWe generated a maximum likelihood tree using the whole genome genotype G multiple sequence alignment. The tree was created using IQ-TREE v1.3.13 [41] with a GTR substitution model [42] . We rooted the tree on the oldest sequence in this dataset (GenBank accession: KF738113.1) in FigTree v1.4.2 [43] .\n\nTo estimate root-to-tip distance of samples in the primary U.S. lineage, we subsetted the full genotype G alignment to include only samples descending from the USA_2006 clade ( Figure 5 .1A).\n\nWe rooted this tree on the USA_2006 node and used TempEst v1.5 [44] to estimate distance from the root. We used scikit-learn in Python [45] to perform linear regression of distances on dates.\n\nWe also generated maximum likelihood trees using the SH gene only (full 316-nucleotide mRNA), HN (CDS only), F (CDS only), and a concatenation of the aforementioned SH, HN, and F regions. For each tree, we started with the whole genome genotype G alignment (225 sequences) and extracted the relevant region. We then removed any sequence with two or more consecutive ambiguous bases ('N's) in any of SH, HN, or F, leaving 209 sequences in each alignment. We used IQ-TREE v1.5.5 [41] with a GTR substitution model to generate maximum likelihood trees.\n\nWe used all 225 MuV genotype G genomes for molecular clock phylogenetics using BEAST v1.8.4 [32] . On the coding regions, we used the SRD06 substitution model, which partitions codons into two partitions (positions 1+2, position 3) and uses an HKY model [35] with gamma site heterogeneity. On the noncoding regions, we used a HKY substitution model also with gamma site heterogeneity. We again removed codon sites in the V protein after the insertion site [34] because of reading frame ambiguity in that region. For samples without exact dates, we used sampled tip dates [40] .\n\nWe performed model selection as described in Metsky et al. [46] . We tested two clock models (a strict clock and an uncorrelated relaxed clock with log-normal distribution (UCLN) [36] ) and three coalescent tree priors (a constant size population, an exponential growth population, and a\n\nBayesian Skygrid tree prior with 20 groups [47] ). We performed path-sampling (PS) and steppingstone sampling (SS) on all six combinations of models and sampled for 100 path steps with a chain length of 2 million. The Skygrid tree prior in combination with a relaxed clock provided the best model fit, as shown by the log Bayes factor when comparing this to other models (Table E. 4), so we used these parameters for all other analyses. In all BEAST runs, we discarded the first 10% of states from each run as burn-in.\n\nTo obtain the tree and tMRCA estimates shown in Figure 5 .1 and Table E [50] ; and 'shared activity links' , individuals who participated in the same extra-curricular activity (i.e., a sports team or university club) or frequented a specific residence or athletic facility. When analyzing the relationship between genetic and epidemiological data, we grouped both types of links.\n\nWe calculated pairwise genetic distance between all pairs of samples in the whole genome genotype G alignment. For each pair, the genetic distance score is s/n, where s is the number of unambiguous differing sites (both sequences must have an unambiguous base at the site, and the called bases must differ) and n is the number of sites at which both sequences have unambiguous base calls.\n\nTo calculate a multidemensional scaling (MDS) from the genomic distance matrix, we used the R package cmdscale [51] . To determine the ability of genetic distance to predict epidemiological linkage, we looked specifically within the II-outbreak (Figure 5 .1A) clade, which is comprised of mostly Institution A, and the related community outbreak, cases. We constructed a receiver operating characteristic (ROC) curve using pairwise distance between II-outbreak cases as the predictor variable and presence or absence of an epidemiological link as the binary response variable.\n\nWe developed a stochastic model for mumps transmission accounting for the natural history of infection, vaccination status, and control measures implemented in response to the outbreak at Institution A. We used previous estimates of the effectiveness and waning rate of mumps vaccination [52] , and of the vaccination status distribution of individuals on a university campus [53] , to account for susceptibility to infection among the Institution A population (N = 22, 000). Risk for mumps virus infection, given exposure, was scaled to time since receipt of the last vaccine dose, yielding the hazard ratio \u03be i = e \u03c90 \u03c4 \u03c91 i for an individual i who received his/her last dose \u03c4 i years previously, relative to an unvaccinated individual. For fitted values from [52] , estimates were below one for individuals vaccinated since 1967, when the Jeryl Lynn vaccine was introduced. We plot the resulting susceptibility distribution in Figure E .4D.\n\nOur stochastic model of mumps virus transmission included three stages after initial infection, the durations of which we inferred using data from previous clinical studies Figure E .4A-C.\n\nThese included the gamma-distributed incubation period from infection to onset of mumps virus shedding in saliva [54] ; the gamma-distributed period of latent infection from shedding onset to parotitis onset [54, 55] ; and the log-normally distributed time from parotitis onset to the cessation of shedding [56] . For asymptomatic cases, we defined the total duration of shedding (\u03b3) as the sum of independent random draws from the durations of shedding before and after parotitis onset, based on the lack of any reported difference in durations of shedding for symptomatic and asymptomatic cases [54] . To account for case isolation precautions implemented by Institution A, we modeled the removal of symptomatic individuals one day after onset of parotitis. In comparison to the 70% probability for symptoms given infection among unvaccinated individuals [57] , we modeled the probability of symptoms given infection as uniformly distributed between 27.3% and 38.3% [52, 58] .\n\nGiven the instantaneous hazard of infection for an as-yet uninfected individual i exposed to \n\nWe resampled according to f{x i | R E (0)} to define the distribution of the cumulative number of cases (Z) resulting from Y introductions, conditioned on R E (0):\n\nFor simulations where Z \u2265 66, we drew k = 66 cases at random to determine the number of distinct lineages (S, defined by the index infection) expected to be present within such a sample.\n\nThe probability of obtaining 66 sequences, and observing S = s m lineages among them, is\n\nThe posterior density of our model also accounted for the probability of observing 71 symptomatic cases in total. Defined in terms of the number of introductions and the initial reproductive number, the model posterior was proportional to\n\nWe measured this probability from 100,000 iterates for each pairing of R E (0) \u2208 0.10, 0.11, ..., 2.50\n\nand Y \u2208 1, 2, ..., 200.\n\nWe used the R package outbreaker [59] to reconstruct transmission for samples included in clade II-outbreak. We infer the distribution of the generation interval length using data from ten cases in our dataset with known exposure sources. A gamma distribution fitted by maximum likelihood (Figure E .4E) recovers mean and dispersion estimates nearly identical to those reported in earlier mumps outbreaks [60] . We ran outbreaker six times in parallel, each with 1 million MCMC steps, and discarded the first 10% of states as the burn-in.\n\nTo analyze all published SH and HN MuV sequences, we searched NBCI GenBank in July 2017 for all nucleotide sequences with organism 'Mumps rubulavirus' . We performed a pairwise alignment between each sequence result and a reference genome (GenBank accession: JX287389.1) using MAFFT [31] with parameters: '--localpair --maxiterate 1000 --preservecase' . We then extracted the SH sequence from each pairwise alignment, removing all sequences without the full 316-nucleotide region and all sequences with an insertion or deletion ('indel') relative to the reference. We then used MAFFT with parameters '--localpair --maxiterate 1000 --retree 2 --preservecase' to create a multiple sequence alignment of the extracted SH gene sequences and removed any sequences with indels in this final alignment. We repeated the same process for the HN region, requiring the full 1749-nucleotide coding region.\n\nIn both the SH and HN alignments, we removed sequences from vaccine strains (i.e., genotype N, or another genotype marked as \"(VAC)\" or \"vaccine\"). We also removed sequences with GenBank records indicating extensive passaging. In the SH alignment only, we removed sequences with no reported collection date or country of origin, as these data are required for phylogeographic analyses. In samples with a collection decade (e.g., 1970s) but not a specific year, we as- In both the SH and HN alignments, we relabeled outdated genotypes (M, E, and any subgenotypes) and constructed a maximum likelihood tree (using IQ-TREE with a GTR substitution model, as described in Section 5.5.15) to assign a genotype if one was not reported on GenBank. We preserved genotypes designated as 'Unclassified' [61] .\n\nTo each alignment, we appended all SH or HN sequences from individual patients generated in this study, except those with two or more consecutive ambiguous bases ('N's) in the SH or HN region.\n\nTo perform phylogenetic and phylogeographic analyses of the SH gene sequence, we first sampled trees using BEAST v1.8.4 [32] . We used constant size population and strict clock models. Other demographic and clock models would have likely provided a better fit to the data, but it appeared that using them (especially the use of a relaxed clock) would have made it impractical to achieve convergence and sufficient sampling of trees and all parameters on the full dataset. We used the HKY substitution model [35] with four rate categories and no codon partitioning. We ran BEAST in four replicates, each for 500 million states with sampling every 50,000 states, and removed the first 150 million states as burn-in. We verified convergence of all parameters across the four replicates.\n\nFinally, we combined the four replicates using LogCombiner and resampled to obtain the final sam- This dataset has large temporal and spatial sampling biases that affect estimates of migration and, to a lesser extent, ancestry. Using a structured coalescent to model migration and co-alescent processes could alleviate these biases, but might face practical limitations on this large dataset [63] . Here, we used resampling on the input sequences to construct distributions of esti- available for resampling, which may lead to an underestimate on the relative rates of migration involving these regions. Moreover, the high sequence similarity between SH gene sequences from In Chapter 2, we found there was limited movement of Ebola virus (EBOV) across the Sierra Leonean border, made the case for purifying selection in EBOV during the course of the epidemic, and identified two pieces of evidence for host effects on the viral genome. These findings not only answered questions about the outbreak, but also suggested new hypotheses that may aid in combating future EBOV outbreaks. For example, we identified many new variants shared across EBOV genomes. As shown by a recent study [1] on the mutation defining the SL3 EBOV lineage [2] , mutations identified in this type of large sequencing study, especially ones deemed of particular in-terest by phylogenetic relationships, can focus experimental investigation and lead to important advances in our understanding of the virus.\n\nOur work on EBOV in Sierra Leone also shows that within-host variants are likely passed between individuals infected with EBOV, and may be useful in understanding transmission. We used this observation in Chapter 3, in which I explain how we used both between-and within-host variants to reconstruct the EBOV transmission tree during the outbreak in Nigeria. In addition to highlighting the ability of genomic data to reconstruct transmission patterns, variation in the virus allowed us to place the outbreak in Nigeria in the context of the larger EBOV epidemic. We showed that the virus likely entered the country from Liberia, and that EBOV did not spread from Nigeria to other countries, thus confirming the success of the public health response. The data presented in this chapter also underscore the need for better understanding of within-host dynamics of EBOV, which could aid in both transmission reconstruction and identifying variants of potential functional or therapeutic interest. This is already an area of active interest [3] , but further experimental studies are needed. Although the high risk of working with EBOV, a BSL-4 virus, makes this a challenging endeavor, it is an important step in gaining a more complete picture of this deadly virus.\n\nWhen studying Zika virus (ZIKV), we were most interested in understanding viral entry into the Americas, and found evidence for ZIKV in four different countries several months before it was diagnosed. Genomic data made it clear that many ZIKV cases were likely missed by traditional diagnostics, and that this is a key area of improvement for combating the spread of the virus. Future studies should not only focus on improving our ability to detect ZIKV (an area of research already rapidly evolving [4] [5] [6] ), but also on methods that can be used to diagnose viral infection without a priori knowledge of the infecting pathogen. In Chapter 4, I also describe our investigation of within-host variation in ZIKV using two different sequencing methods. This analysis demonstrates the need for improved sequencing methods for low-titer viruses, as well as better ways to distinguish true within-host variants from ones caused by sequencing error or contamination.\n\nWhile genomic analysis of EBOV and ZIKV produced important conclusions related to country-level viral origins and spread, the analysis of mumps virus (MuV) described in Chapter 5\n\nhighlights the ability of genomic data to inform public health on a variety of geographic scales. In this chapter, genomic data suggest ongoing MuV transmission in the United States, a conclusion not obvious from the pattern of sporadic outbreaks throughout the country. Additionally, we were able to link the outbreaks in two very different communities within Massachusetts, suggesting transmission of the virus outside of universities (where most cases were identified) into the local community. Despite an apparent lack of informative within-host variation in MuV, we were able to reconstruct transmission at high resolution. We also addressed the hypothesis of vaccine escape during the outbreak, and found no strong evidence of this occurring. That said, we identified two amino acid sites that differ between the dataset we generated and published genomes from a large outbreak in the United States in 2006. These sites should be carefully investigated to determine if they affect virus neutralization by vaccine-induced antibodies.\n\nThis dissertation covers many topics in viral genomics and the potential of sequencing to inform public health, yet one theme that emerges in all chapters is the potential of using variation -including within-host variation -to inform transmission analysis. [10] or contamination during a particular process), potentially providing a tool for improving sequencing methods [9] .\n\nOf course, some viruses do not have meaningful within-host variation, as we saw in Chapter 5. In these cases, between-host variation may be sufficient for understanding transmission, or additional data may be required. For example, it may be possible to take advantage of metagenomic sequencing to bolster transmission analysis; other organisms may be transmitted with a viral pathogen and have the potential to provide valuable information about the individuals involved.\n\nThis type of approach will rely heavily on the previous and ongoing work in the microbiome field, and will require careful consideration of metagenomic techniques to filter out background and contaminants from the sequencing data.\n\nIf we can confidently detect iSNVs, it is important that we know how to use them. This entails improving our understanding of virus-specific within-host dynamics and developing new computational methods that use within-host variation in transmission reconstruction. These aims are connected: an accurate model of within-host viral dynamics is essential to building computational methods that make estimates from these models. As discussed briefly in the introductory chapter, within-host dynamics have been explored in more detail in some pathogens already (e.g., HIV [11] ), and within-host dynamics have been incorporated into recent recent transmission reconstruction methods [12] [13] [14] . Most, however, assume a bottleneck size (the number of viruses transmitted from one individual to another) of one, and relaxing this assumption may be essential to proper transmission reconstruction of viruses like Ebola, which have been hypothesized to have a substantially large bottleneck [15, 16] .\n\nFinally, we also need to consider how these methods could and should be used in real time.\n\nDuring an ongoing outbreak, there may be missing cases or only partial transmission chains, and it is important to consider how transmission reconstruction methods account for missing data, and also how they should be optimized to answer key questions during an outbreak. These important questions include identification of super-spreaders -individuals who transmitted a virus to an exceptional number of secondary cases -and identification of groups or demographics most likely to be involved in pathogen transmission. For example, during the MuV outbreak, sports teams were widely considered to be a hotbed of MuV transmission and infection. Determining the individuals likely to be most affected by an outbreak can help focus monitoring and containment methods.\n\nIt is important to note that genomic data, even without iSNVs, has already been shown to greatly improve our understanding of a number of viral outbreaks. While continued efforts to understand within-host variation and dynamics may increase our ability to draw conclusions from se-quencing data, widespread adoption of sequencing for pathogen surveillance is of equal, or greater, importance. Throughout my work on EBOV, ZIKV, and MuV, I have seen how conclusions drawn from genomic data can have a direct impact on our understanding of a viral outbreak and can be used to inform public health. Additionally, these projects suggest the importance of combining genomic and epidemiological data to most effectively trace and address outbreaks. In Chapter 5, for example, information about institutions most affected by MuV allowed us to explore spread into and within these specific communities, and epidemiological information about individuals affected will be essential to achieving the transmission-related goals (such as identifying demographics prone to viral transmission) stated above.\n\nGenomic epidemiology and surveillance, despite mounting evidence of its value, remains challenging. Issues related to sample transfer, research approvals, and metadata were common across the EBOV, ZIKV, and MuV projects, highlighting the need for a better framework for real-time genomic surveillance everywhere. Sequencing technology has improved enough to make real-time genomic sequencing possible, and building the systems to facilitate sample collection, transfer, and sequencing are the necessary next step. Improving the experimental, computational, and logistical methods for genomic analysis of viral outbreaks will continue to expand our ability to inform public health measures and respond to viral disease outbreaks. Samples collected seven or more days after symptom onset produced, on average, the fewest unambiguous bases, though these observations are based on a limited number of data points. While the sample site variable accounts for differences in cohort composition, the observed effects of gender and collection interval might be due to confounders in composition that span multiple cohorts. These results illustrate the effects of variables on sequencing outcome for the samples in this study; they are not indicative of ZIKV titer more generally. A>T  T>A  C>G  G>C  G>T  T>G  A>G  G>A  C>T  T>C  A>C  C>A  A>T  T>A  C>G  G>C  G>T  T>G  A>G  G>A  C>T  T>C  A>C  C>A  A>T  T>A  C>G  G>C  G>T  T>G  A>G  G>A  C>T  T>C  A>C  C>A  A>T  T>A  C>G  G>C  G>T  T>G  A>G Tapestation) is highly predictive of amplicon sequencing outcome. On each axis, 1+primer pool concentration is plotted on a log scale. Each point is a technical replicate of a sample and colours denote observed sequencing outcome of the replicate. If a replicate is predicted to be passing when at least one primer pool concentration is \u22650.8 ng/\u03bc l, then sensitivity is 98.71% and specificity is 90.34%. An accurate predictor of sequencing success early in the sample processing workflow can save resources. Genome assembly statistics of samples before hybrid capture (grey), and after one (blue) or two (red) rounds of hybrid capture. Nine individual libraries (eight unique samples) were sequenced all three ways, had more than one million raw reads in each method, and generated at least one passing assembly. Raw reads from each method were downsampled to the same number of raw reads (8.5 million) before genomes were assembled. (A) Percent of the genome identified, as measured by number of unambiguous bases. (B) Median sequencing depth of ZIKV genomes, taken over the assembled regions. \n\n(A) Viral species other than Zika were found by unbiased sequencing of 38 samples. Column 3, number of reads in a sample belonging to a species as a raw count and a percent of total reads. Column 4, per cent genome assembled based on the number of unambiguous bases called. We identified cell fusing agent virus (a flavivirus) and deformed wing virus-like genomes in mosquito pools, and dengue virus type 1, JC polyomavirus, and JC polyomavirus-like genomes in clinical samples. All assemblies had \u226595% sequence identity to a reference sequence for the listed species, except cell fusing agent virus in USA_2016_FL-06-MOS (91%) and dengue virus type 1 in BLM_2016_MA-WGS16-006-SER (92%). The dengue virus type 1 genome showed \u226595% sequence identity to other available isolates of the virus. (B) Contigs assembled from unbiased sequencing data of eight mosquito pools. Column 2, number of contigs assembled. Column 3, number of contigs classified by BLASTN/BLASTX. Column 4, number of contigs hitting a viral species. Column 5, number of contigs hitting a viral species with <80% amino acid identity to the best hit. Each column is a subset of the previous column. Contigs in column 5 are considered to be likely to be novel. Last row lists counts, after removing duplicate contigs, for all mosquito pools combined. Marginal likelihoods calculated with path-sampling (PS) and stepping-stone sampling (SS) for combinations of three coalescent tree priors (constant size population, exponential growth population, and Skyline) and two clock models (strict clock and uncorrelated relaxed clock with log-normal distribution). The Bayes factor is calculated against the baseline model, a constant size tree prior and strict clock. (B) Mean estimates and 95% credible intervals across evaluated models for the clock rate, date of tree root, and tMR-CAs of the four regions shown in Figure 4 .2C. Under a Skyline tree prior, the use of strict and relaxed clock models yields similar estimates. Table E .4. The HN protein sequence does a significantly better job at capturing the epidemiologically-relevant clades than the SH gene, and the tree created from SH+HN+F (nearly 25% of the genome) closely resembles the tree created from whole genome sequences. "}