{
    "paper_id": "PMC3986061",
    "metadata": {
        "title": "Advantages and Limitations of Anticipating Laboratory Test Results from Regression- and Tree-Based Rules Derived from Electronic Health-Record Data",
        "authors": [
            {
                "first": "Fahim",
                "middle": [],
                "last": "Mohammad",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Jesse",
                "middle": [
                    "C."
                ],
                "last": "Theisen-Toupal",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Ramy",
                "middle": [],
                "last": "Arnaout",
                "suffix": "",
                "email": null,
                "affiliation": {}
            },
            {
                "first": "Pal",
                "middle": [
                    "Bela"
                ],
                "last": "Szecsi",
                "suffix": "",
                "email": null,
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "Laboratory testing is the single highest-volume medical activity [1]. Its main role is to help adjust the level of clinical suspicion of a diagnosis to help rule it in or out; it is also used for disease monitoring. In practice, the level of clinical suspicion and the probability of a given test result can be correlated: the higher the suspicion, the more likely it is that the result will confirm the diagnosis. Information that feeds into the clinical suspicion\u2014including the age and gender of the patient, prior diagnoses, and prior laboratory results\u2014thus may also influence the test result.",
            "cite_spans": [
                {
                    "start": 65,
                    "end": 68,
                    "mention": "[1]",
                    "ref_id": "BIBREF2"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "In principle, this relationship can be used to improve laboratory testing by making it possible to estimate the pre-test probability of getting a given test result before ordering the test, and, in the limit, to reduce test utilization without adversely affecting patient outcomes. Indeed, ordering fewer tests, where warranted, might benefit outcomes by saving the patient the burden of following up false positives (or negatives) [2]\u2013[4].",
            "cite_spans": [
                {
                    "start": 432,
                    "end": 435,
                    "mention": "[2]",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 436,
                    "end": 439,
                    "mention": "[4]",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "Conceptually, the relationship between clinical suspicion and pre-test probability is used routinely to help set guidelines regarding when and when not to order a given test. For example, the pre-test probability of Lyme serology being positive given a targetoid rash is high enough that, given the test's sensitivity and specificity, ordering the test is contraindicated [5]. Because of the large number of tests and clinical scenarios that exist, and in light of evidence from across medicine that utilization of laboratory testing can be improved [1], [6], it is of interest to understand whether analyzing large clinical databases using the robust application of standard statistical techniques can turn this relationship into actionable decision-support rules\u2014or whether progress toward better laboratory utilization might instead lie elsewhere.",
            "cite_spans": [
                {
                    "start": 372,
                    "end": 375,
                    "mention": "[5]",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 550,
                    "end": 553,
                    "mention": "[1]",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 555,
                    "end": 558,
                    "mention": "[6]",
                    "ref_id": "BIBREF3"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We sought to test the limits of rule-mining for this purpose. To what extent can laboratory results be anticipated computationally based on data available to the clinician, or a clinical decision support system, at the time of the order? We addressed this question using generalized linear modeling (GLM), a generalized form of linear regression [7], and, for comparison, classification trees (CT) [8], [9].",
            "cite_spans": [
                {
                    "start": 346,
                    "end": 349,
                    "mention": "[7]",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 398,
                    "end": 401,
                    "mention": "[8]",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 403,
                    "end": 406,
                    "mention": "[9]",
                    "ref_id": "BIBREF12"
                }
            ],
            "section": "Introduction",
            "ref_spans": []
        },
        {
            "text": "We sought to identify simple, robust subsets of our input data to evaluate as linear predictors (\u201crules\u201d) for whether a test result would be normal or abnormal. To do this, we used GLM twice: first to find rules based on a particular training set and a second time to find rules based on just those items that were common to rules found from a number of different training sets (to avoid overfitting any one training set). We did this as follows, for each test of interest (the response variable or \u201cresponse\u201d).",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We first excluded those input variables (\u201cfeatures\u201d) that appeared with fewer than 5 percent of the response. We then temporarily set aside the most common features (those of the complete blood count and basic metabolic panel) as well as age and gender, and searched the remaining items for frequent featuresets (using the Apriori algorithm [10], [11]). We then added back to each resulting featureset the common features, age, and gender (which are frequent items by definition, since they appear in all instances) with a support threshold of 0.60 (i.e., itemsets for which all items were present with at least 60 percent of instances of the response variable). This set-aside/add-back approach sped the search for featuresets without loss of comprehensiveness.",
            "cite_spans": [
                {
                    "start": 341,
                    "end": 345,
                    "mention": "[10]",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 347,
                    "end": 351,
                    "mention": "[11]",
                    "ref_id": "BIBREF1"
                }
            ],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We used each featureset to create a model for the test of interest using R's glm function (with the family argument set to \u201cBinomial\u201d). We used backward feature elimination to remove non-significant features one at a time from the featureset (using a significance threshold p-value of 1\u00d710\u22125; see below) until the only features that remained were all significantly correlated with the response. We also removed features that are used to calculate the result for the test of interest\u2014e.g., CD4 and CD8 count for T-cell count, which is the sum of CD4 and CD8\u2014for all but proof-of-principle runs.",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The significance threshold was corrected for multiple comparisons by dividing the traditional threshold of p = 0.05 by the product of the total number of tests considered and the average number of rules generated for each test. The combined total number of features (in-house tests plus sendout tests plus diagnoses) was 170+81+434 = 685. The average number of rules after application of GLM for the first time for each test is 6. Thus our threshold p-value was 0.05/(6*685) = 1.2\u00d710\u22125, which we rounded to 1\u00d710\u22125.",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We constructed a model for the result by running glm a second time on a training set (see below) based on this reduced featureset. Of note, there was no guarantee that any feature would be significantly correlated (p\u22641\u00d710\u22125) or that there would be enough instances (glm's threshold was 200) of the test appearing with all features of even the reduced featureset for glm to produce a model. When feature elimination resulted in no significant features or too few instances, no model was constructed. We scored models using PPV, NPV, and ROC AUC.",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "We were interested only in models that were robust to the size and choice of training set. Therefore we repeated the above process for a range of training set-test set splits (80-20, 70-30, 60-40, 50-50, 40-60, 30-70, and 20-80 percent). For each split, we ran the above process 10 times and found the number of rules with AUC\u22650.75. We decided on using a 60-40 split for downstream analyses as this split generated a total number of rules comparable to 70-30 and 80-20 splits but with less training data (Fig. 1).",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": [
                {
                    "start": 505,
                    "end": 511,
                    "mention": "Fig. 1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "Finally, for each test of interest, we selected features that appeared in a strict majority of rules for that test and reran glm using only those features. This made rules both simpler and more robust by removing features whose presence was contingent on a particular choice of training or test set.",
            "cite_spans": [],
            "section": "Generalized linear modeling (GLM) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For each of the inhouse and sendout tests we used CART, implemented as RPART in R (rpart v3.1-50; CRAN.R-project.org/package = rpart), to predict the response from all input features, using 60\u223640 training\u2236test-set splits. We fixed some of the metrics (see below) that were used in building the final tree. The CART grows classification tree in two stages. In stage one, a tree is grown by finding a feature which best splits the data into two groups. Splitting is done only if the overall \u201cimpurity,\u201d the number of outcomes different from the majority (e.g., a \u201clow\u201d response alongside many \u201cnormal\u201d responses), decreases, above some threshold (the \u201ccomplexity parameter;\u201d 0.01). Then, in top-down fashion, these two subgroups are further divided in a recursive manner until the subgroups reach a minimum size (minsplit = 20 records) or until no further improvement can be made. The resulting tree may overfit the training data. To avoid this, cross-validation (xval = 10; 10-fold cross-validation) was used in the second stage by pruning the tree. We fixed the maximum depth (maxdepth) of the tree, i.e., the maximum number of branchings from stem to leaf, to be 20. The final models were tested on the test data and performance statistics are found. We repeated model-building 10 times for each test and summarized the statistics.",
            "cite_spans": [],
            "section": "Classification and Regression Trees (CART) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Data-processing was performed in Python (Enthought Canopy Python version 2.7.3. R (version 2.15.3) was used for statistical analysis and reports generation.",
            "cite_spans": [],
            "section": "Classification and Regression Trees (CART) ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To determine how well sendout and in-house test results can be anticipated based on basic information available in the medical record, we used two independent methods\u2014generalized linear modeling (GLM) and classification and regression trees (CART)\u2014to build simple, robust test-result predictors and then evaluated the performance of these predictors according to the standard clinical metrics of positive predictive value (PPV) and negative predictive value (NPV), as well as sensitivity and specificity via the receiver-operator curve (ROC) area under the curve (AUC).",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "As proof of principle for GLM, we first tested it on the anion gap, a result calculated by subtracting the serum concentrations of the anions chloride and bicarbonate from those of the cations sodium and potassium, and confirmed that our methods found a rule for elevated anion gap based on these four items.",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "We next applied GLM to 81 sendout tests ordered regularly at our hospital. GLM generated rules for just 11 of these tests. For the remaining tests, either no recent diagnosis or in-house test result (or age or gender) was sufficiently correlated with the sendout test result, or there were not enough instances in which correlated items appeared with the result, to generate a rule. Only two tests\u2014for high corticotropin (ACTH) and for low ceruloplasmin\u2014had NPV\u22650.95. Of these, ceruloplasmin had a PPV\u22650.94. The mean AUC for all rules was 0.69, with models for only three tests having an average AUC\u22650.75 over 10 repeat runs. Removal of features that did not appear in a majority of rules had essentially no effect on these AUCs (difference in mean AUC\u22640.02).",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "CART generated rules for 60 tests. However, the AUC for most of these rules was low, with only five tests having AUC\u22650.75: free T3, alpha-macroglobulin, CA27-29, hyaluronic acid, and alpha fetoprotein (AUC 0.75\u20130.79).",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "We next applied GLM to in-house tests. A total of 170 in-house tests were analyzed. A number of rules exhibited a high PPV (the probability of seeing an abnormal value given a prediction of an abnormal value by the rule) or NPV (the probability of seeing a normal value given prediction of a normal value). These were mostly components of the complete blood count (CBC) and metabolic panels. Interestingly, the predictive power of these rules was almost exclusively based on a previous measurement of the test in question: in other words, the best rules were for repeat tests, and the best predictor of a result being normal or abnormal was whether it had been normal or abnormal within the previous seven days. For example, the NPV for a low red blood cell count was 0.95 (with PPV = 0.75), with a rule that depended most on the previous red blood cell count also having been low, and the PPV for high total calcium was 0.98 (NPV = 0.76) and based exclusively on the previous total calcium having been high.",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": []
        },
        {
            "text": "For comparison, we applied CART to in-house tests, again including in the input data the most recent result for that test if performed within a week of the order. Again, a number of rules exhibited a high PPV (\u22650.95), and again these were often tests of the CBC and metabolic panels, with rules based almost exclusively on a previous abnormal value. Examples included low white blood cell count (WBC; PPV = 0.97, NPV = 0.79), platelet count (0.95, 0.88), and serum sodium (0.96, 0.65), and high total calcium (0.99, 0.67), mean corpuscular volume (0.98, 0.84), and iron (0.97, 0.56) all of which were determined almost exclusively from the previous value being low or high (Table 1). Overall, there was good agreement in PPV between GLM and CART for tests for which both methods found rules, but CART outperformed GLM noticeably in NPV (Fig. 2).",
            "cite_spans": [],
            "section": "Results",
            "ref_spans": [
                {
                    "start": 837,
                    "end": 843,
                    "mention": "Fig. 2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 674,
                    "end": 681,
                    "mention": "Table 1",
                    "ref_id": null
                }
            ]
        },
        {
            "text": "The growing availability of large clinical databases has raised interest in the possibility of using systematic rule-mining for clinical decision support [12]\u2013[15]. One popular and well characterized approach has been logistic regression [16]\u2013[18], a special case of generalized linear modeling (GLM). Researchers have applied these approaches for diverse health-related purposes including prediction of cardiovascular risk [19], mortality in head trauma [18], texture analysis of magnetic resonance images [16], and many other applications [17], [20], [21]. However, we note that GLM does not easily incorporate missing values, as it removes records with missing features; a feature will be \u201cmissing\u201d for any record in which that test (the feature) was not performed.",
            "cite_spans": [
                {
                    "start": 154,
                    "end": 158,
                    "mention": "[12]",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 159,
                    "end": 163,
                    "mention": "[15]",
                    "ref_id": "BIBREF4"
                },
                {
                    "start": 238,
                    "end": 242,
                    "mention": "[16]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 243,
                    "end": 247,
                    "mention": "[18]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 424,
                    "end": 428,
                    "mention": "[19]",
                    "ref_id": "BIBREF11"
                },
                {
                    "start": 455,
                    "end": 459,
                    "mention": "[18]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 507,
                    "end": 511,
                    "mention": "[16]",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 541,
                    "end": 545,
                    "mention": "[17]",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 547,
                    "end": 551,
                    "mention": "[20]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 553,
                    "end": 557,
                    "mention": "[21]",
                    "ref_id": "BIBREF23"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Other methods, such as classification and regression trees (CART) and artificial neural networks [18], have also been applied. Most of these studies were limited in scope to predicting risk of a particular diagnosis. Harper [22] compared four classification techniques (regression, CART, artificial neural networks, and discriminant analysis) on four different datasets and concluded that there was no obvious best choice for their data; while CART performed best, regression was fastest and nearly as good. Similar comparative studies on coronary artery disease [20] and Alzheimer disease [23] indicated that newer algorithms such as ANN and random forests [24] have little advantage over simpler, more traditional approaches. Also, the utility and limitations of these approaches for predicting laboratory results (as opposed to diagnoses) are unclear. However, while CART is both a top performer and overcomes GLM's problem with missing values, it is also more computationally intensive and potentially less sensitive to simple algebraic relationships among features (e.g., among sodium, chloride and bicarbonate and the anion gap). Therefore we chose GLM as a well-understood approach with strong performance and excellent speed, and CART as the best-performing complementary approach for purposes of comparison.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 101,
                    "mention": "[18]",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 224,
                    "end": 228,
                    "mention": "[22]",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 563,
                    "end": 567,
                    "mention": "[20]",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 590,
                    "end": 594,
                    "mention": "[23]",
                    "ref_id": "BIBREF14"
                },
                {
                    "start": 658,
                    "end": 662,
                    "mention": "[24]",
                    "ref_id": "BIBREF5"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Given the importance of laboratory testing, we asked how much information regression- or classification tree-based rules could provide in assessing the pre-test probability of a test result being abnormal for 251 commonly ordered in-house and sendout tests at our hospital.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Data-mining can sometimes find spurious correlations, artifacts of the particular partitioning of the data into training and test set. To avoid such artifacts, we repeated our regression on multiple independent partitions of the data and kept only items that appeared in a majority of the resulting rules. This safeguard also had the effect of simplifying rules by making each rule dependent on a smaller number of items. As expected, the effect on performance was negligible and dependence on the resulting items was more often clinically and pathophysiologically plausible than rules derived from each run.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "When data-mining it is also important to consider the setting. The rules we found do not exist in a vacuum but are \u201ccontingent\u201d in the sense that they depend on current clinical practice. Certain tests and panels are ordered in patterns. In a sense, contingency is a form of selection bias: there may well be other diagnoses or test result results that correlate with the result for the test of interest that are not routinely measured according to current best practices. However, as long as the setting in which such rules would be applied is the substantially similar to that in which they were found, selection bias would have little if any effect on finding rules. As long as one is clear that one is looking for relationships in a current practice process, and not among all things that could possibly be measured, any rules that are discovered will by construction be setting-appropriate.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "But while our rules appear to be plausible and setting-appropriate, the motivating question behind this study is whether the rules we found could be useful clinically. One way to approach this question is by considering the positive and negative predictive value of each rule (PPV and NPV). These metrics are in contrast to sensitivity and specificity, by which rules are often measured but which do not incorporate disease prevalence in spite of its importance to clinical decision-making. A PPV of 0.95 means that when a rule suggests that the test result will be abnormal, the result actually will be abnormal 95 percent of the time. A NPV of 0.95 means that when a rule suggests that the test result will be normal, the result actually will be normal 95 percent of the time.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "We found rules with PPV and/or NPV\u22650.95 (by GLM) for only two tests that are sendouts at our hospital\u2014one of which is ceruloplasmin, which we have previously suggested is overordered via chart review [25]. In contrast, for in-house tests we found over a dozen such rules. Interestingly, the main determinant for rules for in-house tests was a normal or abnormal result for the same test within the previous seven days. Although in this study we did not set out explicitly to make a statement about repeat laboratory testing, the appropriateness of which has been investigated elsewhere [4], these results suggest that repeat laboratory testing within one week does not always add information that could not have been anticipated from the previous result. Refining this observation using the same unbiased approach we have followed here is potentially an area for future investigation.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 204,
                    "mention": "[25]",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 586,
                    "end": 589,
                    "mention": "[4]",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "Our results should not be taken as a categorical criticism of repeat testing. First, while the PPV was \u22650.95 in several cases, the NPV was more typically 0.70\u20130.85. Thus, while prediction that a result will be abnormal may be correct 95 percent of the time, which may be good enough to discourage repeat ordering, prediction that a result will be normal may not be so dependable. Therefore use of a rule depends on the subtle distinction of whether the clinical question is \u201cwill the result be abnormal\u201d vs. \u201cwill the result be normal.\u201d Second, we note that no rules with such strong performance were found for the majority of our sendout or in-house tests by either of our two complementary approaches. Thus while the rules we found can inform clinical decision-makers, the information they provide rarely replaces the information obtained from actually performing these tests.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        },
        {
            "text": "It is interesting to note that on average, our simple rules yielded a PPV of 0.84 and an NPV of 0.75. This means that on average, rules will correctly predict an abnormal laboratory result 5 times out of 6 (5/6\u22480.84) and correctly predict a normal result 3 times out of 4. While not good enough to replace testing (especially for rules that depend on previous test results), these observations raise the question of how much better prediction can get. Integration of information not considered in the present study, including vital signs, chief complaints, and physical findings, may improve prediction by these methods.",
            "cite_spans": [],
            "section": "Discussion",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "FIGREF0": {
            "text": "Figure 1: A 60-40 split generated a total number of rules comparable to 70-30 and 80-20 splits but with less training data.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Figure 2: Both linear modeling (GLM) and classification trees (CART) were better at finding rules with high positive predictive value (PPV; panels a and b), with good agreement between the methods, than negative predictive value (PPV; panels c and d).",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF1": {
            "title": "Fast Discovery of Association Rules",
            "authors": [],
            "year": 1996,
            "venue": "Advances in knowledge discovery and data mining",
            "volume": "12",
            "issn": "",
            "pages": "307-328",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF2": {
            "title": "Big Data in Clinical Pathology",
            "authors": [],
            "year": 2011,
            "venue": "Critical Values",
            "volume": "4",
            "issn": "",
            "pages": "15-19",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Elementary, my dear Doctor Watson",
            "authors": [],
            "year": 2012,
            "venue": "Clin Chem",
            "volume": "58",
            "issn": "",
            "pages": "986-988",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF4": {
            "title": "Predictive data mining in clinical medicine: current issues and guidelines",
            "authors": [],
            "year": 2008,
            "venue": "Int J Med Informatics",
            "volume": "77",
            "issn": "",
            "pages": "81-97",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF5": {
            "title": "Random forests",
            "authors": [],
            "year": 2001,
            "venue": "Machine Learning",
            "volume": "45",
            "issn": "",
            "pages": "5-32",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF6": {
            "title": "Electronic health record surveillance algorithms facilitate the detection of transfusion-related pulmonary complications",
            "authors": [],
            "year": 2012,
            "venue": "Transfusion",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF7": {
            "title": "Comparison of artificial neural network and logistic regression models for prediction of mortality in head trauma based on initial clinical data",
            "authors": [],
            "year": 2005,
            "venue": "BMC Medical Informatics and Decision Making",
            "volume": "5",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF8": {
            "title": "Textural analysis of contrast-enhanced MR images of the breast",
            "authors": [],
            "year": 2003,
            "venue": "Magnetic Resonance in Medicine",
            "volume": "50",
            "issn": "",
            "pages": "92-98",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF9": {
            "title": "A review and comparison of classification algorithms for medical decision making",
            "authors": [],
            "year": 2005,
            "venue": "Health Policy",
            "volume": "71",
            "issn": "",
            "pages": "315-331",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF10": {
            "title": "The dangers of false-positive and false-negative test results: false-positive results as a function of pretest probability",
            "authors": [],
            "year": 2008,
            "venue": "Clin Lab Med",
            "volume": "28",
            "issn": "",
            "pages": "305-319, vii",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "Improved cardiovascular risk prediction using nonparametric regression and electronic health record data",
            "authors": [],
            "year": 2013,
            "venue": "Medical Care",
            "volume": "51",
            "issn": "",
            "pages": "251-258",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF12": {
            "title": "Using classification trees to assess low birth weight outcomes",
            "authors": [],
            "year": 2006,
            "venue": "Artificial intelligence in medicine",
            "volume": "38",
            "issn": "",
            "pages": "275-289",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF13": {
            "title": "Comparing performances of logistic regression, classification and regression tree, and neural networks for predicting coronary artery disease",
            "authors": [],
            "year": 2008,
            "venue": "Expert Syst Appl",
            "volume": "34",
            "issn": "",
            "pages": "366-374",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF14": {
            "title": "Application and comparison of classification algorithms for recognition of Alzheimer's disease in electrical brain activity (EEG)",
            "authors": [],
            "year": 2007,
            "venue": "Journal of neuroscience methods",
            "volume": "161",
            "issn": "",
            "pages": "342-350",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF15": {
            "title": "Data mining and clinical data repositories: Insights from a 667,000 patient data set",
            "authors": [],
            "year": 2006,
            "venue": "Computers in Biology and Medicine",
            "volume": "36",
            "issn": "",
            "pages": "1351-1377",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF16": {
            "title": "Generalized linear models",
            "authors": [],
            "year": 1972,
            "venue": "Journal of the Royal Statistical Society Series A (General)",
            "volume": "",
            "issn": "",
            "pages": "370-384",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "The Ulysses syndrome",
            "authors": [],
            "year": 1972,
            "venue": "Can Med Assoc J",
            "volume": "106",
            "issn": "",
            "pages": "122-123",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Mining association rules from clinical databases: an intelligent diagnostic process in healthcare",
            "authors": [],
            "year": 2001,
            "venue": "Studies in Health Technology and Informatics",
            "volume": "",
            "issn": "",
            "pages": "1399-1403",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF19": {
            "title": "The overuse of serum ceruloplasmin measurement",
            "authors": [],
            "year": 2013,
            "venue": "Am J Med",
            "volume": "126",
            "issn": "",
            "pages": "e921-925",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "Laboratory evaluation in the diagnosis of Lyme disease",
            "authors": [],
            "year": 1997,
            "venue": "Ann Intern Med",
            "volume": "127",
            "issn": "",
            "pages": "1109-1123",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF21": {
            "title": "Establishing a clinical decision rule of severe acute respiratory syndrome at the emergency department",
            "authors": [],
            "year": 2004,
            "venue": "Ann Emerg Med",
            "volume": "43",
            "issn": "",
            "pages": "17-22",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF22": {
            "title": "Machine Learning for Personalized Medicine: Predicting Primary Myocardial Infarction from Electronic Health Records",
            "authors": [],
            "year": 2012,
            "venue": "AI Magazine",
            "volume": "33",
            "issn": "",
            "pages": "33-45",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Predicting improvement in urinary and bowel incontinence for home health patients using electronic health record data",
            "authors": [],
            "year": 2011,
            "venue": "J Wound Ostomy Continence Nurs",
            "volume": "38",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF24": {
            "title": "The Landscape of Inappropriate Laboratory Testing: A 15-Year Systematic Review and Meta-Analysis",
            "authors": [],
            "year": 2013,
            "venue": "PLoS One",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        }
    }
}