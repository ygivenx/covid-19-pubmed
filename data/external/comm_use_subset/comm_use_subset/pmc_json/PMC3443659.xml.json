{
    "paper_id": "PMC3443659",
    "metadata": {
        "title": "A novel hierarchical clustering algorithm for gene sequences",
        "authors": [
            {
                "first": "Dan",
                "middle": [],
                "last": "Wei",
                "suffix": "",
                "email": "dan.wei@siat.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Qingshan",
                "middle": [],
                "last": "Jiang",
                "suffix": "",
                "email": "qs.jiang@siat.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Yanjie",
                "middle": [],
                "last": "Wei",
                "suffix": "",
                "email": "yj.wei@siat.ac.cn",
                "affiliation": {}
            },
            {
                "first": "Shengrui",
                "middle": [],
                "last": "Wang",
                "suffix": "",
                "email": "shengrui.wang@usherbrooke.ca",
                "affiliation": {}
            }
        ]
    },
    "body_text": [
        {
            "text": "With the development of advanced biotechnology, more and more biological sequence information has been generated. The amount of genetic data is growing faster than the rate at which it can be analyzed. Clustering techniques provide a viable solution for handling and analyzing such rapidly growing genetic data. Clustering algorithms partition sequences into different biologically meaningful groups, facilitating therefore the prediction of functions of genes [1]. When a new gene is assigned to a cluster, the biological function of this cluster can be attributed to this gene with high confidence. On the other hand, clustering gene sequences into groups may also help with analyzing evolutionary relationships among the sequences in a cluster [2].",
            "cite_spans": [
                {
                    "start": 462,
                    "end": 463,
                    "mention": "1",
                    "ref_id": "BIBREF0"
                },
                {
                    "start": 748,
                    "end": 749,
                    "mention": "2",
                    "ref_id": "BIBREF11"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Clustering of gene sequences requires calculation of similarity between sequences. There are two clustering approaches according to the similarity measure used in a clustering method. One is based on sequence alignment. The similarity between two gene sequences is measured by the scores obtained from an alignment algorithm such as BLAST [3] or FASTA [4]. Although sequence alignment gives good solutions, it is relatively difficult to cluster a large number of sequences because of its computational complexity. Moreover, if the sequences in the set vary in length, a satisfactory alignment is hard to achieve, resulting in a low accuracy of clustering.",
            "cite_spans": [
                {
                    "start": 340,
                    "end": 341,
                    "mention": "3",
                    "ref_id": "BIBREF22"
                },
                {
                    "start": 353,
                    "end": 354,
                    "mention": "4",
                    "ref_id": "BIBREF33"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "The other approach for similarity measure is to use alignment-free methods [5-10]. In recent years, several alignment-free measures have been proposed. The word-based measure is one of the most widely used methods [11-14]. This method chooses a short word length k, maps each sequence onto an n-dimensional vector according to its k-length tuple (also called k-tuple or k-word) properties, and then assesses the similarity of any two vectors by measures such as Euclidean distance [15], Mahalanobis distance [16], Kullback\u2013Leibler discrepancy [17], cosine distance [18] or Pearson\u2019s correlation coefficient [19]. In recent years, several novel alignment-free measures [20,21] have been designed for DNA sequences analysis. Yang et al. [22] extended the k-tuple distance, which is based on the difference in tuple frequencies, to clustering gene sequences. Their tuple-based method determines the similarity of sequences by considering only tuple frequencies and ignoring the positional information within a sequence.",
            "cite_spans": [
                {
                    "start": 76,
                    "end": 77,
                    "mention": "5",
                    "ref_id": "BIBREF44"
                },
                {
                    "start": 78,
                    "end": 80,
                    "mention": "10",
                    "ref_id": "BIBREF1"
                },
                {
                    "start": 215,
                    "end": 217,
                    "mention": "11",
                    "ref_id": "BIBREF2"
                },
                {
                    "start": 218,
                    "end": 220,
                    "mention": "14",
                    "ref_id": "BIBREF5"
                },
                {
                    "start": 482,
                    "end": 484,
                    "mention": "15",
                    "ref_id": "BIBREF6"
                },
                {
                    "start": 509,
                    "end": 511,
                    "mention": "16",
                    "ref_id": "BIBREF7"
                },
                {
                    "start": 544,
                    "end": 546,
                    "mention": "17",
                    "ref_id": "BIBREF8"
                },
                {
                    "start": 566,
                    "end": 568,
                    "mention": "18",
                    "ref_id": "BIBREF9"
                },
                {
                    "start": 608,
                    "end": 610,
                    "mention": "19",
                    "ref_id": "BIBREF10"
                },
                {
                    "start": 669,
                    "end": 671,
                    "mention": "20",
                    "ref_id": "BIBREF12"
                },
                {
                    "start": 672,
                    "end": 674,
                    "mention": "21",
                    "ref_id": "BIBREF13"
                },
                {
                    "start": 736,
                    "end": 738,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Major algorithms used in gene sequence clustering can be divided into two categories according to the result format: hierarchical clustering algorithms and partitional clustering algorithms [23]. Hierarchical clustering is widely used for detecting clusters in genomic data. It generates a set of partitions forming a cluster hierarchy. According to linkage criteria, there are three hierarchical clustering methods including single-linkage clustering (SL), complete-linkage clustering (CL) and average-linkage clustering (AL) [24]. With SL, clusters may be merged together due to single sequences being close to each other, even though many of the sequences in each cluster may be very distant to each other [25]. CL tends to find compact clusters of approximately equal diameters [25]. With CL, all objects in a cluster are similar to each other. AL can be seen as an intermediate between single and complete linkage clustering, resulting in more homogeneous clusters than those obtained by the single-linkage method [26]. For instance, BlastClust [27] and GeneRage [28] employ single linkage clustering approach; SWORDS [29] is based on word frequencies as profiles to merge clusters hierarchically; and Uchiyama [30] use average linkage clustering algorithm to classify genes. Hierarchical approaches may yield fairly good results, but they require the similarity of all pairs of sequences and quickly arrive at a bottleneck in terms of computational time and memory usage for large-scale data sets [31].",
            "cite_spans": [
                {
                    "start": 191,
                    "end": 193,
                    "mention": "23",
                    "ref_id": "BIBREF15"
                },
                {
                    "start": 528,
                    "end": 530,
                    "mention": "24",
                    "ref_id": "BIBREF16"
                },
                {
                    "start": 710,
                    "end": 712,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 783,
                    "end": 785,
                    "mention": "25",
                    "ref_id": "BIBREF17"
                },
                {
                    "start": 1020,
                    "end": 1022,
                    "mention": "26",
                    "ref_id": "BIBREF18"
                },
                {
                    "start": 1051,
                    "end": 1053,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 1069,
                    "end": 1071,
                    "mention": "28",
                    "ref_id": "BIBREF20"
                },
                {
                    "start": 1124,
                    "end": 1126,
                    "mention": "29",
                    "ref_id": "BIBREF21"
                },
                {
                    "start": 1217,
                    "end": 1219,
                    "mention": "30",
                    "ref_id": "BIBREF23"
                },
                {
                    "start": 1504,
                    "end": 1506,
                    "mention": "31",
                    "ref_id": "BIBREF24"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Partitioning algorithms have also been used. Partitional clustering obtains a partition of data objects by optimizing some clustering criterion. Partitional clustering algorithms are simple and well-suited for clustering large datasets [32]. K-means (KM) [33,34] is a commonly used method of partitional clustering methods. KM has a lower order of computational complexity and demands less physical memory than the hierarchical method. It is suitable for clustering large gene data. Some KM-based algorithms, such as those introduced by Wan et al. [33], Kelarev et al. [34], Tseng et al. [35] and Ashlock et al. [36], have been developed to group DNA sequences. The major drawback of KM compared to hierarchical clustering algorithms is the lack of hierarchical relationships in its results. To remedy the problem, bisecting K-means (BKM), a hierarchical variation of KM, was proposed to build a tree of clusters in a top-down fashion by splitting the least homogeneous cluster into two more homogeneous ones. BKM can produce either a flat clustering or a hierarchical clustering by recursively applying KM. It has a linear complexity and is relatively efficient and scalable. Recent study [37] concluded that BKM outperforms KM and performs equally well or better than hierarchical methods when it partitions the dataset based on a homogeneity criteria. The bisecting approach is very attractive for genomic studies [38].",
            "cite_spans": [
                {
                    "start": 237,
                    "end": 239,
                    "mention": "32",
                    "ref_id": "BIBREF25"
                },
                {
                    "start": 256,
                    "end": 258,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 259,
                    "end": 261,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 549,
                    "end": 551,
                    "mention": "33",
                    "ref_id": "BIBREF26"
                },
                {
                    "start": 570,
                    "end": 572,
                    "mention": "34",
                    "ref_id": "BIBREF27"
                },
                {
                    "start": 589,
                    "end": 591,
                    "mention": "35",
                    "ref_id": "BIBREF28"
                },
                {
                    "start": 613,
                    "end": 615,
                    "mention": "36",
                    "ref_id": "BIBREF29"
                },
                {
                    "start": 1191,
                    "end": 1193,
                    "mention": "37",
                    "ref_id": "BIBREF30"
                },
                {
                    "start": 1418,
                    "end": 1420,
                    "mention": "38",
                    "ref_id": "BIBREF31"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Hierarchical clustering produces a nested series of partitions, where the results are usually depicted as a dendrogram while partitional clustering produces a flat partition. BlastClust [27] is a hierarchical clustering method based on BLAST scores as the measure of sequence similarity. BlastClust computes pairwise similarity of all sequences by BLAST alignment and then clusters sequences by the single linkage clustering method which produces clusters of linear topology. The performance of BlastClust is limited by the size of the input data. CD-HIT-EST [39], a partitional approach, is also widely used to cluster DNA sequences. CD-HIT-EST uses an incremental clustering process and avoids the unnecessary alignments by a short word filtering mechanism, which detects similar sequences by counting the number of identical short words between them. The purpose of filters is to decide whether the identity between two sequences is above or below a threshold without aligning them, therefore speeding up the clustering process. Though CD-HIT-EST is based on alignment, it can avoid too many pairwise alignments by using a filter, thus it is faster than BlastClust, and can handle larger datasets.",
            "cite_spans": [
                {
                    "start": 187,
                    "end": 189,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 560,
                    "end": 562,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "Recent studies reveal also that BlastClust is less effective for clustering divergent sequences [40], and its performance strongly depends on the choice of optimal BLAST parameters including similarity threshold, percent identity, and alignment length [41]. CD-HIT-EST, on the other hand, does not provide hierarchical relationships between clusters of sequences. In many situations both CD-HIT-EST and BlastClust yield clusters with only one sequence [41]. All the traditional clustering methods based on sequence alignment encounter computational difficulties in dealing with large biological databases.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 99,
                    "mention": "40",
                    "ref_id": "BIBREF34"
                },
                {
                    "start": 253,
                    "end": 255,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                },
                {
                    "start": 453,
                    "end": 455,
                    "mention": "41",
                    "ref_id": "BIBREF35"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "The approach presented in this paper involves a new alignment-free distance measure based on k-tuples, DMk (Distance Measure based on k-tuples) [42], and a modified bisecting K-means clustering algorithm, mBKM (modified Bisecting K-Means algorithm). mBKM aims to speed up the clustering process by using the alignment-free similarity measure, and is able to produce either a hierarchical clustering or a partition clustering result. We have applied mBKM with DMk in clustering gene sequences and performing phylogenetic analysis. DMk shows better performance than the k-tuple distance in our experiments, and mBKM outperforms SL, CL, AL, BKM and KM when tested on public gene sequence datasets. Furthermore, the proposed method also outperforms alignment-based methods such as BlastClust and CD-HIT-EST.",
            "cite_spans": [
                {
                    "start": 145,
                    "end": 147,
                    "mention": "42",
                    "ref_id": "BIBREF36"
                }
            ],
            "section": "Background",
            "ref_spans": []
        },
        {
            "text": "In this section, we introduce a new similarity measure which takes into account the occurrence, location and order relation of k-tuple in a DNA sequence.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Sequences are numerically transformed to feature vectors that can be processed by data mining algorithms. Let \u03a3 be the alphabet set of nucleotides (\u03a3 = {A, C, G, T}). A sequence of length s, S, is defined as a linear succession of s symbols from \u03a3. A segment of k consecutive symbols in sequence S (k \u2264 s) is designated as a k-tuple. There is a set of 4k possible k-tuples, Wk. The number of occurrences of a k-tuple w, Nw, is counted by moving a sliding window of length k over the sequence with k - 1 bp overlapping step size.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To explore the correlation properties of DNA, Nair et al. [47] provided a presentation of genomic data using the inter-nucleotide distance sequence. Based on a similar idea, we utilize the gaps between the locations where k-tuple occur in the sequence to explore the sequence structure. For a DNA sequence Spr is the location of the rth occurrence of k-tuple w, where p0 = 0. And \u03b1r is given as,",
            "cite_spans": [
                {
                    "start": 59,
                    "end": 61,
                    "mention": "47",
                    "ref_id": "BIBREF41"
                }
            ],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(2)\u03b1r=1pr\u2212pr\u22121,1\u2264r\u2264m",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "in which m stands for the number of occurrences of w. \u03b1r reflects the density of w and is closely related to the location where w occurs in the sequence. Each w begins at the 1/\u03b11 position, and {\u03b11,\u03b12,\u2026,\u03b1m} for repetition of w forms an array whose rth element indicates the relative position of two neighboring w in the sequence. This array allows us to find all subsequent repeats of w.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To characterize the order of \u03b1r, we define \u03b2j as a partial sum of {\u03b1r}. \u03b2j is calculated by the following formula:",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(3)\u03b2j=\u2211r=1j\u03b1r,1\u2264j\u2264m",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "{\u03b1r} is a list of non-negative real numbers, and \u03b2j is totally ordered by \u2264, so \u03b21, \u03b22, \u2026, \u03b2m is also an ordered set. {\u03b11, \u03b12,\u2026, \u03b1m} and {\u03b21, \u03b22,\u2026, \u03b2m} determine each other uniquely. \u03b2j is only dependent of the number and positions of w and independent on other k-tuples. Given the set of {\u03b21, \u03b22,\u2026, \u03b2m}, one can obtain where w occurs and how many times w occurs in the sequence.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Shannon\u2019s entropy[48], which illuminates the total information measure of source on the average, is a measure of order/disorder. According to [49], when using the totally ordered set {\u03b21\u03b22,\u2026, \u03b2m} to calculate the probabilities, the Shannon entropy reflects the degree of importance of position in a sequence. We construct a discrete probability distribution Q=(q1,q2,\u2026,qm), where qi=\u03b2i/\u2211i=1m\u03b2i, and \u2211i=1mqi=1. The Shannon entropy of the discrete probability distribution is calculated by",
            "cite_spans": [
                {
                    "start": 18,
                    "end": 20,
                    "mention": "48",
                    "ref_id": "BIBREF42"
                },
                {
                    "start": 143,
                    "end": 145,
                    "mention": "49",
                    "ref_id": "BIBREF43"
                }
            ],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(4)H=\u2212\u2211i=1mqilog2qi",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For each k-tuple w in the sequence, not only the information of tuple numbers but also the information of tuple positions is involved in the definition of H. We take H as the feature of w in the sequence, and then construct a vector consisted of H of all possible k-tuples in the given sequence.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "For a fixed k, there are 4k distinct k-tuples to be considered. These k-tuples in a fixed 4k-dimension feature vector are denoted by (H1,H2,\u2026,H4k), where Hi means the feature representation of the ith k-tuple. This feature vector based on H can be regarded as an index for its corresponding sequence.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Cluster analysis algorithms partition objects into groups based on the distances between objects. Euclidean distance is the square root of the summation of the squares of the differences between all pairs of corresponding objects. The k-tuple distance is the sum of the differences in frequency over all possible k-tuples; on the other hand, we use Euclidean distance between Shannon entropy of k-tuples in sequences to measure the similarity. This distance measure method is referred as DMk. For any two sequences X and Y, DMk can be calculated as:",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(5)dDMk(X,Y)=\u2211i=14k(hwiX\u2212hwiY)2",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "where hwiX and hwiY represent the Shannon entropy values of the ithk-tuple in sequences X and Y, respectively. DMk can be calculated from following algorithm:",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Algorithm Name: DMk for similarity measure",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Input: sequences {S1, S2,\u2026, SN}.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Output: similarity matrix, (d(X,Y))N*N.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Steps:",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1. For each sequence, search and locate each k-tuple;",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1.1 For each k-tuple, use Equation (1) to calculate \u03b1r(1\u2264r\u2264m)",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1.2 For each k-tuple, use Equation (2) to calculate \u03b2j(1\u2264j\u2264m);",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1.3 For each k-tuple, use Equation (3) to calculate H;",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "2. For each sequence, construct 4k -component vector by H of all k-tuples.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3. For any two sequences, use Equation (4) to calculate the distance between the two sequences.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "4. Return {d}.",
            "cite_spans": [],
            "section": "A new similarity measure: DMk ::: Methods",
            "ref_spans": []
        },
        {
            "text": "KM can be used to obtain a hierarchical clustering solution using a repeated bisecting approach [50,51]. BKM is such an algorithm and it can produce either a partitional or a hierarchical clustering.",
            "cite_spans": [
                {
                    "start": 97,
                    "end": 99,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                },
                {
                    "start": 100,
                    "end": 102,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                }
            ],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "BKM has a linear time complexity in each bisecting step. Recent study [51] concludes BKM outperforms KM as well as the agglomerative approach in terms of accuracy and efficiency. Consequently, the bisecting approach is very attractive in many applications for clustering and genomic data analysis.",
            "cite_spans": [
                {
                    "start": 71,
                    "end": 73,
                    "mention": "51",
                    "ref_id": "BIBREF46"
                }
            ],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "BKM initially regards the whole data set as a cluster, and splits one cluster into two subclusters at each bisecting step using KM until singleton clusters are obtained at the leafs or until K clusters are obtained. The outcome is structured as a binary tree. There are two key steps in a typical BKM. The first one is the selection of initial centroids. Generally the initial centroids are chosen randomly in BKM. The second key step is the rule, \u03b6, for selection of a existing cluster to be split in each bisecting step. \u03b6 is typically given by the following three approaches [50]:",
            "cite_spans": [
                {
                    "start": 579,
                    "end": 581,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                }
            ],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1) Choosing the cluster with largest size;",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "2) Selecting the cluster with the overall similarity",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(6)1C2\u2211s\u2208Cs'\u2208Cd(s,s')",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "The overall similarity is either minimized or maximize, depending on the definition of d(s, s\u2019). C is a cluster;",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3) Using a criterion based on both size and overall similarity.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Because the differences between these methods are small in terms of the final clustering result, the way of splitting the largest remaining cluster is recommended [50].",
            "cite_spans": [
                {
                    "start": 164,
                    "end": 166,
                    "mention": "50",
                    "ref_id": "BIBREF45"
                }
            ],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "There are two problems in BKM algorithm:",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1. Randomly choosing the initial centroids in BKM may result in too adjacent elements selected. If the initial centroids are too close, the algorithm will reach a local optimization. Moreover, different sets of initial cluster centroids can lead to different final clustering results.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "2. The algorithm for choosing one existing cluster to split in each bisecting step usually selects the cluster with the largest size. Although this leads to reasonably good and balanced clustering solution, it cannot gracefully work for datasets where the natural clusters are of different sizes, as it will tend to partition larger clusters first. In real biological data, the number of elements in every cluster may not always be similar.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "To address the above two problems and obtain more natural hierarchical solutions, we develop a modified bisecting K-means, mBKM, which choose the initial centroids by the maximum and minimum principle and select the cluster to split based on the compactness of clusters.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1) Selecting Initial Cluster Centroids",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "In order to achieve stable and reliable clustering results, we use the maximum distance, which can avoid obtaining adjacent elements, to select the initial centroids. For a set of sequences, {s1, s2, \u2026, sN}, let d(si, sj)(i, j = 1, 2, \u2026N) be the distance between any two sequences in the dataset. We choose the sequence sc1 and sc2 as the cluster centroid according the following rule:",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(7)dc1,c2=maxi,j=1,2,\u2026,Nd(si,sj)",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "2) Selecting the Cluster to Split",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "BKM algorithm usually partitions the largest size cluster into two smaller ones and yields clusters with similar size. However, a cluster with large number is not always the loose one. If one existing cluster is a loose one, in which its members are not closely related to each other, the cluster will be selected to be split.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Variance is a measure of how far a set of numbers are spread out from each other, and it can measure the compactness of the clusters. So we select the cluster to split on the basis of the compactness of clusters measured by variance. The variance of cluster Cj is defined as following:",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "(8)\u03c3j=\u2211si\u2208Cjd2(si,\u03bcj)nj,1\u2264i,j\u2264N",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "where \u03bcj is the centroid of sequences in Cj, d (si, \u03bcj) is the distance between si and \u03bcj, and nj is the number of sequences in the cluster.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "A small variance of a cluster indicates that the members in the cluster tend to be closely related to the mean. In other words, the smaller the variance is, the more compact the cluster is, and vice versa.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Based on the above idea, we outline mBKM algorithm as follows.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Algorithm Name: mBKM for clustering sequences",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Input: sequences {s1, s2, \u2026, sN}, a distance function d between sequences, the number of clusters K.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Output: Set of K clusters.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Steps:",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "1. Initialization: Regard the whole dataset {s1, s2, \u2026, sN} as a single cluster.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "2. Pick a cluster to split.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3. Find two sub-clusters:",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3.1 Select two initial centroids using Equation (6);",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3.2 Assign the sequences to the closest centroid;",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3.3 Recalculate two centroids based on the sequences assigned to the cluster;",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "3.4 Repeat steps 3.2 and 3.3 until no change in cluster centroid calculation.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "4. Calculate the variance of each cluster according Equation (7) and take the split that produces the clustering result with the highest variance.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "5. Repeat steps 2, 3 and 4 until the desired number K is reached.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "This algorithm outputs a binary tree of sequences, where each leaf represents a sequences and each node represents a sequence collection.",
            "cite_spans": [],
            "section": "A new clustering algorithm: mBKM ::: Methods",
            "ref_spans": []
        },
        {
            "text": "Genes of the same family usually share similar sequences, functional domains, and even interacting partners. When a new gene is assigned to a cluster, the biological function of this cluster can be attributed to this gene with high confidence.",
            "cite_spans": [],
            "section": "Gene sequences clustering ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "Four data sets are extracted from different gene repositories as shown in Table 1. The sequences of DS1 are downloaded from NCBI (http://www.ncbi.nlm.nih.gov). The other three datasets, DS2, DS3 and DS4, are taken from PBIL (http://pbil.univ-lyon1.fr/). DS2 is taken from HOVERGEN of PBIL, a database of homologous vertebrate genes. DS3 is taken from HOGENOM, which contains homologous gene families from microbial organisms. DS4 is randomly selected from HOMOLENS, a database of homologous genes from Ensembl organisms and Ensembl families.",
            "cite_spans": [],
            "section": "Gene sequences clustering ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 80,
                    "end": 81,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "Four widely used clustering algorithms, including KM, single-linkage clustering (SL), complete-linkage clustering (CL) and average-linkage clustering (AL), have been chosen in the experiments. For comparison, we perform the clustering tests on all data sets using the k-tuple distance and DMk distance. In this paper, we set k value to 3. For protein coding genes, a tuple size of 3 is a good choice according to reference [22]. We also tested the clustering performance on different k values, and the result confirms that a small k value is preferred, see Additional file 1: Table S1. For larger k values, there are more tuples with zero frequencies and less information is captured by the algorithm.",
            "cite_spans": [
                {
                    "start": 424,
                    "end": 426,
                    "mention": "22",
                    "ref_id": "BIBREF14"
                }
            ],
            "section": "Gene sequences clustering ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "KM algorithm would yield different results during multiple executions due to its stochastic feature for initialization. We examine KM in ten runs and report the average performance. The AL, CL and SL hierarchical algorithms generate one solution for each of them. We obtain the result of hierarchical clustering algorithms by analyzing the hierarchical tree using the expected number of cluster as input parameters.",
            "cite_spans": [],
            "section": "Gene sequences clustering ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "According to Table 2, the F-measure values for each of the data sets using DMk are clearly higher than those obtained with the k-tuple distance. In our experiments, on average, the value of the F-measure given by DMk is 18% better than by the k-tuple distance (p = 0.0165, one-sided paired t-test) in KM, 49.7% better in SL (p = 0.0028), 24.9% better in CL (p = 0.016), and 35.8% better in AL (p = 0.01885). Clearly, DMk provides a significant improvement in clustering sequences. On the four data sets, the F-measure of DMk is improved more than 20% compared with that of the k-tuple distance during the same clustering process in most cases. DMk outperforms the k-tuple distance in the experiments. This is because DMk considers the occurrence, location and order relation of tuples in sequence and can capture more information in the sequence, while the k-tuple distance considers frequency alone and ignore the position of tuples in a sequence. In addition, we have tested DMk and k-tuple measures on protein sequences with a k value of 2, and the results indicate that DMk performs better than k-tuple distance (data not shown). Thus in practical DMk measure can also be applied in clustering protein sequences after tuning current algorithm.",
            "cite_spans": [],
            "section": "Gene sequences clustering ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 19,
                    "end": 20,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "In this experiment, the proposed similarity measure DMk is further tested by phylogenetic analysis. In order to evaluate the similarity measures, we use UPGMA in the PHYLIP package, a widely used clustering algorithm in phylogenetic analysis. The tree is drawn by TREEVIEW program [53].",
            "cite_spans": [
                {
                    "start": 282,
                    "end": 284,
                    "mention": "53",
                    "ref_id": "BIBREF48"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "The selected data set includes the full \u03b2-globin gene sequences of 10 species reported by Feng et al. [54], which are downloaded from NCBI (http://www.ncbi.nlm.nih.gov). Their names, accession numbers, locations and lengths are listed in the Additional file 1: Table S2. The similarity/dissimilarity matrices for the full sequences of \u03b2-globin gene of the 10 species using DMk are shown in Table 3, respectively. The smaller the distance is, the more similar the two sequences are.",
            "cite_spans": [
                {
                    "start": 103,
                    "end": 105,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 396,
                    "end": 397,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "In Table 3, the most similar species pairs are human-gorilla, human-chimpanzee and gorilla-chimpanzee, which are expected from their evolutionary relationship. A slightly less similar species pair is goat-bovine. On the other hand, gallus is separated from the rest, this coincides with the fact that gallus is the only nonmammalian species among these 10 species. We can also find that opossum is far away from the remaining mammals. These results are consistent with biological morphology.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 9,
                    "end": 10,
                    "mention": "3",
                    "ref_id": "TABREF2"
                }
            ]
        },
        {
            "text": "The quality of the constructed tree shows the quality of the distance matrix and the method of abstracting information from DNA sequences. In Figure 1v, we show the phylogenetic tree of 10 \u03b2-globin gene sequences based on DMk, generated by UPGMA. For comparison, the phylogenetic tree of the k-tuple distance is shown in Figure 1(a).",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 149,
                    "end": 150,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 328,
                    "end": 329,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "The tree in Figure 1 (a) has some consistencies with biological morphology. Although it supports the separation of gallus relative to other species, its obvious drawback is that it fails to separate (mouse, rat) and (goat, bovine) from opossum. From Figure 1 (b), gallus is separated from the rest and opossum is far away from the other species. This topology is in good agreement with that presented by Feng et al. [54] and Cao et al. [55] except for the relative position of rodents.",
            "cite_spans": [
                {
                    "start": 417,
                    "end": 419,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 437,
                    "end": 439,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 19,
                    "end": 20,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                },
                {
                    "start": 257,
                    "end": 258,
                    "mention": "1",
                    "ref_id": "FIGREF0"
                }
            ]
        },
        {
            "text": "DMk measures the similarity between DNA sequences more effective than the k-tuple distance. This is because DMk measures the distance between DNA sequences based on sequence structure and composition. Through evaluation on gene families and constructing phylogenetic trees of full gene sequences of 10 species, we find that DMk gives more competitive results compared to the k-tuple distance.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of similarity measure ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "In order to illustrate the efficiency of mBKM in gene sequence clustering, we ran mBKM with the k-tuple distance and DMk on real data sets listed in Table 1. The clustering results are compared with those of KM, SL, CL, AL and BKM algorithms. For BKM, the number of iterations for each bisecting step is set to 5. We ran BKM 10 times to obtain the average F-measure. By combing the six clustering algorithms with two similarity measures, we have 12 combinations of clustering algorithm for performance assessment. The combinations are KM with k-tuple, SL with k-tuple, CL with k-tuple, AL with k-tuple, BKM with k-tuple, mBKM with k-tuple, KM with DMk, SL with DMk, CL with DMk, AL with DMk, BKM with DMk and mBKM with DMk.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 155,
                    "end": 156,
                    "mention": "1",
                    "ref_id": "TABREF0"
                }
            ]
        },
        {
            "text": "The clustering performance of different clustering methods is the result of a combination of factors, including the types of sequence distances used for clustering and the choice of clustering algorithms. Table 2 shows the clustering performance on the data sets for all 12 clustering methods. For each data set, we set the number of cluster as the real number of class during the clustering run. For example, the real number of cluster is 8 in DS1 and 6 in DS2.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 211,
                    "end": 212,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "From Table 2, we observe that mBKM using DMk achieves best result and clearly outperforms other methods for the four data sets. The average F-measure of mBKM with k-tuple is about 2.2% higher than KM with k-tuple (p = 0.036), 45% higher than SL with k-tuple (p = 0.00195), 11.4% higher than CL with k-tuple (p = 0.0424), 19% higher than AL with k-tuple (p = 0.08615) and 2.3% higher than BKM (p = 0.0141). For mBKM with DMk, F-measures for DS1, DS2, DS3, and DS4 are 0.808, 0.9645, 0.9143, and 0.9587 respectively. On average, the value of F-measure given by mBKM is 14.2% better than KM (p = 0.00025), 21.3% better than SL (p = 0.0105), 15.4% better than CL (p = 0.02835), 10.1% better in AL (p = 0.0686), and 2.3% higher than BKM (p = 0.0015) respectively. These results show that our method, combining mBKM with DMk, is able to achieve high quality results on all the data sets.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 11,
                    "end": 12,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Because the clustering methods listed in Table 2 use the numbers of cluster as input parameters, we analyze the effects of varying the number of clusters on the clustering performance. This analysis is applied to DS1, DS2, DS3 and DS4 datasets and all 12 combinations. Figures 2 and 3 show the results of these runs based on the k-tuple distance and DMk, respectively. The data used for generating these figures are included in Additional file 1: Tables S3-S10.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 277,
                    "end": 278,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 283,
                    "end": 284,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                },
                {
                    "start": 47,
                    "end": 48,
                    "mention": "2",
                    "ref_id": "TABREF1"
                }
            ]
        },
        {
            "text": "Figure 2 illustrates the results of the six clustering algorithms with the k-tuple distance. From Figure 2 and Additional file 1: Tables S3-S6, mBKM achieves better F-measures than other five clustering algorithms for the real number of clusters on all the data sets. Although the other clustering algorithms give slightly better results in terms of F-measure in some cases, mBKM performs better than the other clustering algorithms in terms of the average of the F-measures values (average values are shown in Additional file 1: Tables S3-S6). This result shows that on average, mBKM performs better than other clustering algorithms for a range of cluster numbers, in the vicinity of real number of clusters. It also implies that varying the number of clusters as input for these clustering algorithms does not affect the performance.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 105,
                    "end": 106,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                }
            ]
        },
        {
            "text": "Figure 3 shows the results of clustering algorithms with DMk. mBKM obtains the highest F-measure values among the six clustering algorithms at the real number of clusters. On average, mBKM achieves better results than the other clustering algorithms for DS2, DS3, and DS4. For DS1, the average value of mBKM is very close to that of AL and higher than those of the other clustering algorithms. Overall mBKM produces consistently high quality clusters in the neighborhood of the real number of cluster (data shown in Additional file 1: Tables S7-S10). The F-measures given by mBKM are higher than those of other clustering methods at the corresponding number of clusters in most cases.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "From Figures 2 and 3, we can see that DMk achieves better cluster quantity than the k-tuple distance in terms of F-measure. Using same clustering algorithm on the same data set, DMk achieves higher average of the F-measure values than the k-tuple distance, and DMk also obtains higher F-measures at corresponding number of clusters (data shown in Additional file 1: Tables S3-S10). From both Figures, we find that F-measure changes as the number of cluster changes. As it is known, F-measure is a balanced measure of precision and recall. It is an ideal condition when the number of cluster is equal to the real number. When the number of cluster is greater than or less than the real number, the F-measure will be affected.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 13,
                    "end": 14,
                    "mention": "2",
                    "ref_id": "FIGREF1"
                },
                {
                    "start": 19,
                    "end": 20,
                    "mention": "3",
                    "ref_id": "FIGREF2"
                }
            ]
        },
        {
            "text": "With regard to clustering algorithms, SL performs poorly in many cases, and this may be because that SL uses the nearest pair of sequences and may lead to bad splits of one cluster if two or more clusters show different pattern densities. For KM and BKM, the results of many runs are lower than those of mBKM. On the whole, mBKM achieves better results than other clustering algorithms, and mBKM combining with DMk achieves best results among these clustering methods in our experiments.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "The task of sequence clustering is to group given sequences into clusters. The similarity measure, DMk, measures the similarity between DNA sequences based solely on the k-tuple. It is more effective than the k-tuple distance, which is one of the most widely used methods. The clustering algorithm, mBKM, can obtain better clustering results and can reveal the relationships among clusters in hierarchical manner. In the next experiments, we combine mBKM with DMk to clustering DNA sequences.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "In order to further illustrate the efficiency of our method, combining mBKM and DMk, we compare mBKM with DMk to two other clustering programs: BlastClust [27] and CD-HIT-EST [39]. BlastClust is an alignment-dependent clustering algorithm. BlastClust is from NCBI Blast package. BlastClust accepts a number of parameters that can be used to control the clustering stringency including thresholds for score density (\u2212S parameter), and alignment length (\u2212L parameter). CD-HIT-EST is a popular DNA clustering program based on greedy incremental clustering method. CD-HIT-EST groups DNA sequences into clusters that meet a user-defined similarity threshold (\u2212c parameter) and uses short-word filters to rapidly determine that if two sequences are similar, which reduces the number of full alignments necessary.",
            "cite_spans": [
                {
                    "start": 156,
                    "end": 158,
                    "mention": "27",
                    "ref_id": "BIBREF19"
                },
                {
                    "start": 176,
                    "end": 178,
                    "mention": "39",
                    "ref_id": "BIBREF32"
                }
            ],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "We perform tests using BlastClust and CD-HIT-EST on the data sets listed in Table 1. In order to obtain the best possible performance of BlastClust, we set -p as F (input type is nucleotide sequence) and vary the input parameters, -S and \u2013L, to evaluate the results. The score density, \u2013S parameter, varies between 10 and 90 with step size 10, and the alignment length, \u2013L parameter, varies between 0.1 and 0.9 with step size 0.1. Other parameters are kept default. For CD-HIT-EST, because the sequence identity threshold, -c parameter, should be greater than or equal to 0.8 in the program, we vary -c parameter between 0.8 and 1 with step size 0.02, and set the word length as default value. The best results from different parameter combination are recorded. For mBKM with DMk, we set the size of k-tuple as 3 and use the real number of clusters as input. As BlastClust and CD-HIT-EST do not use the number of clusters as input, we choose the resulting class i, which has the max F(i,j) for cluster j, to calculate the F-measures. The results, which contain the corresponding F-measures and the execution time, are summarized in Table 4.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 82,
                    "end": 83,
                    "mention": "1",
                    "ref_id": "TABREF0"
                },
                {
                    "start": 1138,
                    "end": 1139,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "Table 4 demonstrates that mBKM with DMk produces good results relative to each original cluster set in terms of F-measure. Every F-measure of mBKM with DMk is higher than 0.8 and the highest is 0.9645. It is also seen in the table that mBKM with DMk outperforms BlastClust and CD-HIT-EST on all the data sets. BlastClust and CD-HIT-EST tend to give more clusters than the real numbers of classes, therefore, BlastClust and CD-HIT-EST give high precision and low recall value. But neither of these two performs well in terms of F-measure. The execution times reported in Table 4 for algorithm comparison show mBKM with DMk is faster than BlastClust and CD-HIT-EST.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 6,
                    "end": 7,
                    "mention": "4",
                    "ref_id": "TABREF3"
                },
                {
                    "start": 576,
                    "end": 577,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "For the cases that the real number of clusters is unknown, the performance of our algorithm will be affected. In order to compare with BlastClust and CD-HIT-EST on a relatively fair ground, we can vary the number of clusters and take the average of the F-measure values over the different numbers of clusters. For instance, we run mBKM with DMk with the range of 3\u201320 numbers and the average values of F-measure are 0.7065, 0.8533, 0.8205 and 0.8429 for DS1, DS2, DS3 and DS4, respectively. As shown in Additional file 1: Tables S7-S10, these values are also higher than the corresponding F-measure of BlastClust and CD-HIT-EST.",
            "cite_spans": [],
            "section": "Performance comparison of clustering methods ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "In this experiment, we used mBKM with DMk to construct phylogenetic trees.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "1) The clustering result of 10 species",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "We apply mBKM with DMk to the 10 DNA sequences of \u03b2-globin gene in Table 4. The clustering result is shown in Figure 4(a). Using the same data set, we also build the phylogenetic tree using CLUSTALW [56] and MUSCLE [57] for alignment, and UPGMA and Maximum Likelihood (ML) method (in the PHYLIP package) for presenting the tree. Figure 4(b) and 4(c) shows the tree built by CLUSTALW with UPGMA and MUSCLE with ML respectively. The trees built by MUSCLE with UPGMA and CLUSTALW with ML are provided in Figure 1 of Additional file 1.",
            "cite_spans": [
                {
                    "start": 200,
                    "end": 202,
                    "mention": "56",
                    "ref_id": "BIBREF51"
                },
                {
                    "start": 216,
                    "end": 218,
                    "mention": "57",
                    "ref_id": "BIBREF52"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 117,
                    "end": 118,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 336,
                    "end": 337,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 345,
                    "end": 346,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 73,
                    "end": 74,
                    "mention": "4",
                    "ref_id": "TABREF3"
                }
            ]
        },
        {
            "text": "In Figure 4(a), human, gorilla, chimpanzee and lemur are closer to bovine and goat than to mouse and rat, this topology is in complete agreement with Feng et al. [54] and Cao et al. [55] confirming the outgroup status of rodents relative to ferungulates and primates. Moreover, the tree in Figure 4(a) is identical to the tree in Figure 4(b)4(c) and the tree built MUSCLE with UPGMA. In experiment, the branch (bovine, goat) is not classified well by CLUSTALW with ML. Furthermore, it took about 0.1 second for our method. However, UPGMA with CLUSTALW and MUSCLE for the same data set took 5.1 and 1.2 seconds to build the tree, respectively, and ML with CLUSTALW and MUSCLE took 8 and 4.1 seconds to build the tree, respectively.",
            "cite_spans": [
                {
                    "start": 163,
                    "end": 165,
                    "mention": "54",
                    "ref_id": "BIBREF49"
                },
                {
                    "start": 183,
                    "end": 185,
                    "mention": "55",
                    "ref_id": "BIBREF50"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 10,
                    "end": 11,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 297,
                    "end": 298,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 337,
                    "end": 338,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                },
                {
                    "start": 341,
                    "end": 342,
                    "mention": "4",
                    "ref_id": "FIGREF3"
                }
            ]
        },
        {
            "text": "2) The Clustering result of 60 H1N1 viruses",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "H1N1 is subtype of the influenza A virus which can cause illness in humans and many other animal species. Analysis of H1N1 is critical for preparing a strategy to prevent and to control influenza epidemics and pandemics. The H1N1 avian influenza is characterized by its continuous antigen variation, which is mainly caused by the HA and NA proteins in which HA protein has highest rate of mutation. HA protein plays a critical role in identifying and adsorbing the host cell receptor in the infection process, and it is the decisive factor of host specific. We use our method to verify the phylogenetic relationships of H1N1, and the result is included in Additional file 1. The clustering result using mBKM with DMk is shown in Figure 5(a). As a comparison, we also use CLUSTALW with UPGMA and MUSCLE with ML to construct the phylogenetic tree and they are presented in Figure 5(b) and 5(c).",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 736,
                    "end": 737,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 878,
                    "end": 879,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 887,
                    "end": 888,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "As is seen from Figure 5(a), 60 H1N1 viruses are distinctly divided into four main groups using our method. The four groups, include European swine older than 2009 (G1), the avian older than 2009 (G2), American swine older than 2009 (G3) and the new 2009 viruses from human, swine and avian (G4). The result shows that the new 2009 human H1N1 viruses have closer relationship with old American swine than old avian and European swine. This grouping result is generally consistent with the topology given by CLUSTALW with UPGMA, which is shown in Figure 5(b), and the one presented by MUSCLE with UPGMA, which is provided in the Additional file 1, as well as the result suggested by zhao et al. [58]. Figure 5(c), built by MUSCLE using ML method, also shows the new 2009 human H1N1 viruses have close relationship with old American swine except the position of the group (old avian swine, European swine) is different from the positions in Figure 5(a) and 5(b). CLUSTALW with ML (in Additional file 1) also classifies the 60 H1N1 viruses into four groups except that swine/Wisconsin/1961 and swine/Wisconsin/1961 are not classified well.",
            "cite_spans": [
                {
                    "start": 695,
                    "end": 697,
                    "mention": "58",
                    "ref_id": "BIBREF53"
                }
            ],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 23,
                    "end": 24,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 553,
                    "end": 554,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 707,
                    "end": 708,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 946,
                    "end": 947,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                },
                {
                    "start": 955,
                    "end": 956,
                    "mention": "5",
                    "ref_id": "FIGREF4"
                }
            ]
        },
        {
            "text": "Our method analyzed the 60 H1N1 viruses within 1 second, while UPGMA with CLUSTALW and MUSCLE of the same data set took 460 and 60.1 seconds to build the tree, and ML with CLUSTALW and MUSCLE took 571 and 188.1 seconds to build the tree, respectively.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "Our method, mBKM with DMk, performs well when clustering 10 species and 60 H1N1 viruses. It obtains similar results to the alignment-based method. Furthermore, our method is much faster than the alignment-based methods.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "In order to compare the speed of our method with the multiple sequence alignment based methods, CLUSTALW and MUSCLE, we performed the test on two sets of sequences. The first set consists of six datasets. All the six datasets include 100 sequences. The lengths of all sequences in the six datasets are around 1000, 2000, 3000, 4000, 5000 and 6000 respectively. Another set also consists of six datasets. The number of sequences in each dataset is 20, 40, 60, 80, 100, 120 respectively; the lengths of all the sequences are around 3000. Because ML method is slower than UPGMA, we use UPGMA to build the phylogenetic tree of the results from CLUSTALW and MUSCLE and record the time used for each method. The results in Figure 6 show that our method is much faster than the other two methods. The actual time differences are much higher than the visual differences in the figure since we are using the log(time) as the label of y-axis.",
            "cite_spans": [],
            "section": "Phylogenetic analysis ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 724,
                    "end": 725,
                    "mention": "6",
                    "ref_id": "FIGREF5"
                }
            ]
        },
        {
            "text": "For DMk, the time complexity of transforming the gene sequence s1\u22efsl to a vector is O (l4K), thus the time complexity of generating the vectors for the whole sequence database is O(Nl\u00af4k), where l\u00af is the average length of the sequences and N is the number of sequences. The value of k set to 3 yields good results in our experiments, and we fix k to 3 as the size of k-tuple. DMk have linear time complexity with respect to both l\u00af and N.",
            "cite_spans": [],
            "section": "Scalability test ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "The time consumed for mBKM calculation is primarily determined by choosing the initial cluster centroids. For N sequences, this step has a time complexity of O (N2). The time complexity of clustering step in mBKM is O (N log K). The following scalability test on our method, mBKM with DMk, confirms that our method has linear time complexity with respect to the average length of the sequences. The scalability test uses theoretical model sequences composed of the four symbols \u2018A\u2019, \u2018C\u2019, \u2019G\u2019 and \u2018T\u2019. The method is implemented in Java and on a computer with 3.00 GHz CPU and 2 GB RAM.",
            "cite_spans": [],
            "section": "Scalability test ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": []
        },
        {
            "text": "Figure 7(a) illustrates the relationships between the runtime and the number of sequences (implemented on a computer with 8 GB RAM). To test the scalability with respect to the number of sequences, we use five data sets which consist of 5000, 10000, 15000, 20000, 25000, 30000, 35000 and 40000 sequences. Each data set contains 10 clusters and all the sequences have the same length, 100. The curve in Figure 7(a) is primarily consistent with the time complexity of mBKM with O (N2). The scalability with respect to the length of sequences was tested on five datasets with five different sequence lengths: 10000, 20000, 30000, 40000, 50000 and each set consists of 4 clusters and 100 sequences. The sensitivity with respect to the length of the sequence is illustrated in Figure 7(b), from which we can see that the time of our method increases linearly when the length of sequences increases.",
            "cite_spans": [],
            "section": "Scalability test ::: Evaluation of clustering methods ::: Results and discussion",
            "ref_spans": [
                {
                    "start": 7,
                    "end": 8,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 409,
                    "end": 410,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                },
                {
                    "start": 779,
                    "end": 780,
                    "mention": "7",
                    "ref_id": "FIGREF6"
                }
            ]
        },
        {
            "text": "In this paper, we presented a novel approach for DNA sequence clustering, mBKM, based on a new sequence similarity measure, DMk, which is extracted from DNA sequences based on the position and composition of oligonucleotide pattern. The experimental results show the method of combining mBKM with DMk is effective in classifying DNA sequences with similar biological characteristics and in discovering the underlying relationship among the sequences. In addition, DMk can achieve comparable or better accuracy than the frequency-based distance measure. Our proposed method can be applied to study gene families and it can also help with the prediction of novel genes. Furthermore, mBKM with DMk can generate cluster trees that are useful to understand the processes governing the gene evolution. In addition, our method may be extended for protein sequence analysis and metagenomics of identifying source organisms of metagenmic data. Our method has limitations too. For example, the method did not consider edge length, and has not address problems with long repeated sequences or long insertions. In future we will try to address these problems.",
            "cite_spans": [],
            "section": "Conclusions",
            "ref_spans": []
        },
        {
            "text": "The authors declare that there are no competing interests.",
            "cite_spans": [],
            "section": "Competing interests",
            "ref_spans": []
        },
        {
            "text": "DW designed the algorithm, conducted the experiments, and wrote the manuscript. QJ supervised the project and proposed data mining algorithm. YW guided the experiments, wrote the manuscript and analyzed the results. SW guided the experiment analysis, and proposed ideas for sequence clustering algorithm. All authors read and approved the final manuscript.",
            "cite_spans": [],
            "section": "Authors\u2019 contributions",
            "ref_spans": []
        }
    ],
    "ref_entries": {
        "TABREF0": {
            "text": "Table 1: Description for the Data Sets\n",
            "type": "table"
        },
        "TABREF1": {
            "text": "Table 2: The F-measures of the Data Sets\n",
            "type": "table"
        },
        "TABREF2": {
            "text": "Table 3: The similarity/dissimilarity matrix for the 10 full \u03b2-globin gene sequences based on DMk\n",
            "type": "table"
        },
        "TABREF3": {
            "text": "Table 4: Clustering results on the data sets listed in Table 1\n(Time contains the time of similarity measuring and clustering)",
            "type": "table"
        },
        "FIGREF0": {
            "text": "Figure 1: The phylogenetic trees for 10 species using the full DNA sequences of \u03b2-globin.",
            "type": "figure"
        },
        "FIGREF1": {
            "text": "Figure 2: The distribution of F-measure as a function of the number of clusters based on the k-tuple distance (The real numbers of DS1, DS2, DS3 and DS4 are 8, 6, 6, and 6, respectively).",
            "type": "figure"
        },
        "FIGREF2": {
            "text": "Figure 3: The distribution of F-measure as a function of the number of clusters based on DMk (The real numbers of DS1, DS2, DS3 and DS4 are 8, 6, 6, and 6, respectively).",
            "type": "figure"
        },
        "FIGREF3": {
            "text": "Figure 4: The phylogenetic trees for 10 species using the full DNA sequences of \u03b2-globin.",
            "type": "figure"
        },
        "FIGREF4": {
            "text": "Figure 5: The phylogenetic trees for 60 H1N1 viruses.",
            "type": "figure"
        },
        "FIGREF5": {
            "text": "Figure 6: The time comparison of three methods.",
            "type": "figure"
        },
        "FIGREF6": {
            "text": "Figure 7: The relationship between the runtime and different numbers of sequences and length of sequences.",
            "type": "figure"
        }
    },
    "back_matter": [],
    "bib_entries": {
        "BIBREF0": {
            "title": "The evolution of mammalian gene families",
            "authors": [],
            "year": 2006,
            "venue": "PLoS One",
            "volume": "1",
            "issn": "",
            "pages": "1-10",
            "other_ids": {
                "DOI": [
                    "10.1371/journal.pone.0000001"
                ]
            }
        },
        "BIBREF1": {
            "title": "CLUSS: Clustering of protein sequences based on a new similarity measure",
            "authors": [],
            "year": 2007,
            "venue": "BMC Bioinformatics",
            "volume": "8",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-8-286"
                ]
            }
        },
        "BIBREF2": {
            "title": "Alignment-free sequence comparison (I): statistics and power",
            "authors": [],
            "year": 2009,
            "venue": "JComput Biol",
            "volume": "16",
            "issn": "12",
            "pages": "1615-1634",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF3": {
            "title": "Numerical characteristics of word frequencies and their application to dissimilarity measure for sequence comparison",
            "authors": [],
            "year": 2011,
            "venue": "JTheor Biol",
            "volume": "276",
            "issn": "1",
            "pages": "174-180",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jtbi.2011.02.005"
                ]
            }
        },
        "BIBREF4": {
            "title": "An improved string composition method for sequence comparison",
            "authors": [],
            "year": 2008,
            "venue": "BMC Bioinformatics",
            "volume": "9",
            "issn": "Suppl 6",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-9-S6-S15"
                ]
            }
        },
        "BIBREF5": {
            "title": "A mathematical consideration of the word-composition vector method in comparison of biological sequences",
            "authors": [],
            "year": 2011,
            "venue": "BioSystems",
            "volume": "106",
            "issn": "",
            "pages": "67-75",
            "other_ids": {
                "DOI": [
                    "10.1016/j.biosystems.2011.06.009"
                ]
            }
        },
        "BIBREF6": {
            "title": "A measure of the similarity of sets of sequences not requiring sequence alignment",
            "authors": [],
            "year": 1986,
            "venue": "ProcNatlAcad Sci USA",
            "volume": "83",
            "issn": "",
            "pages": "5155-5159",
            "other_ids": {
                "DOI": [
                    "10.1073/pnas.83.14.5155"
                ]
            }
        },
        "BIBREF7": {
            "title": "A measure of DNA sequence dissimilarity based on Mahalanobis distance between frequencies of words",
            "authors": [],
            "year": 1997,
            "venue": "Biometrics",
            "volume": "53",
            "issn": "4",
            "pages": "1431-1439",
            "other_ids": {
                "DOI": [
                    "10.2307/2533509"
                ]
            }
        },
        "BIBREF8": {
            "title": "Statistical measures of DNA dissimilarity under Markov chain models of base composition",
            "authors": [],
            "year": 2001,
            "venue": "Biometrics",
            "volume": "57",
            "issn": "2",
            "pages": "441-448",
            "other_ids": {
                "DOI": [
                    "10.1111/j.0006-341X.2001.00441.x"
                ]
            }
        },
        "BIBREF9": {
            "title": "Integrated gene and species phylogenies from unaligned whole genome protein sequences",
            "authors": [],
            "year": 2002,
            "venue": "Bioinformatics",
            "volume": "18",
            "issn": "1",
            "pages": "100-108",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/18.1.100"
                ]
            }
        },
        "BIBREF10": {
            "title": "Statistical method for predicting protein coding regions in nucleic acid sequences",
            "authors": [],
            "year": 1987,
            "venue": "ComputAppl Biosci",
            "volume": "3",
            "issn": "4",
            "pages": "287-295",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF11": {
            "title": "A novel clustering method via nucleotide-based Fourier power spectrum analysis",
            "authors": [],
            "year": 2011,
            "venue": "JTheor Biol",
            "volume": "279",
            "issn": "",
            "pages": "83-89",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jtbi.2011.03.029"
                ]
            }
        },
        "BIBREF12": {
            "title": "WSE, a new sequence distance measure based on word frequencies",
            "authors": [],
            "year": 2008,
            "venue": "Math Biosci",
            "volume": "215",
            "issn": "1",
            "pages": "78-83",
            "other_ids": {
                "DOI": [
                    "10.1016/j.mbs.2008.06.001"
                ]
            }
        },
        "BIBREF13": {
            "title": "A Poisson model of sequence comparison and its application to coronavirus phylogeny",
            "authors": [],
            "year": 2009,
            "venue": "Math Biosci",
            "volume": "217",
            "issn": "2",
            "pages": "159-166",
            "other_ids": {
                "DOI": [
                    "10.1016/j.mbs.2008.11.006"
                ]
            }
        },
        "BIBREF14": {
            "title": "Performance comparison of gene family clustering methods with expect curated gene family data set in Arabidposis thaliana",
            "authors": [],
            "year": 2008,
            "venue": "Planta",
            "volume": "228",
            "issn": "",
            "pages": "439-447",
            "other_ids": {
                "DOI": [
                    "10.1007/s00425-008-0748-7"
                ]
            }
        },
        "BIBREF15": {
            "title": "Classification, clustering, features and distances of sequence Data",
            "authors": [],
            "year": 2007,
            "venue": "Sequence Data Mining",
            "volume": "33",
            "issn": "",
            "pages": "47-65",
            "other_ids": {
                "DOI": [
                    "10.1007/978-0-387-69937-0_3"
                ]
            }
        },
        "BIBREF16": {
            "title": "",
            "authors": [],
            "year": 1995,
            "venue": "Biometry: The Principles and Practice of Statistics in Biological Research",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF17": {
            "title": "",
            "authors": [],
            "year": 2001,
            "venue": "Cluster Analysis",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF18": {
            "title": "Efficient algorithms for accurate hierarchical clustering of huge datasets: tackling the entire protein space",
            "authors": [],
            "year": 2008,
            "venue": "Bioinformatics",
            "volume": "24",
            "issn": "13",
            "pages": "i41-i49",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btn174"
                ]
            }
        },
        "BIBREF19": {
            "title": "",
            "authors": [],
            "year": null,
            "venue": "Documentation of the BLASTCLUST-algorithm",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF20": {
            "title": "GeneRAGE: a robust algorithm for sequence clustering and domain detection",
            "authors": [],
            "year": 2000,
            "venue": "Bioinformatics",
            "volume": "16",
            "issn": "5",
            "pages": "451-457",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/16.5.451"
                ]
            }
        },
        "BIBREF21": {
            "title": "SWORDS: A statistical tool for analyzing large DNA sequences",
            "authors": [],
            "year": 2002,
            "venue": "J Biosci",
            "volume": "27",
            "issn": "1",
            "pages": "1-6",
            "other_ids": {
                "DOI": [
                    "10.1007/BF02703678"
                ]
            }
        },
        "BIBREF22": {
            "title": "A basic local alignment search tool",
            "authors": [],
            "year": 1990,
            "venue": "JMol Biol",
            "volume": "215",
            "issn": "",
            "pages": "403-410",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF23": {
            "title": "Hierarchical clustering algorithm for comprehensive orthologous-domain classification in multiple genomes",
            "authors": [],
            "year": 2006,
            "venue": "Nucleic Acids Res",
            "volume": "34",
            "issn": "2",
            "pages": "647-658",
            "other_ids": {
                "DOI": [
                    "10.1093/nar/gkj448"
                ]
            }
        },
        "BIBREF24": {
            "title": "Clustering 16 S rRNA for OTU prediction: a method of unsupervised Bayesian clustering",
            "authors": [],
            "year": 2011,
            "venue": "Bioinformatics",
            "volume": "27",
            "issn": "5",
            "pages": "611-618",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btq725"
                ]
            }
        },
        "BIBREF25": {
            "title": "JACOP: a simple and robust method for the automated classification of protein sequences with modular architecture",
            "authors": [],
            "year": 2005,
            "venue": "BMC Bioinformatics",
            "volume": "6",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-6-216"
                ]
            }
        },
        "BIBREF26": {
            "title": "Interactive Clustering for Exploration of Genomic Data",
            "authors": [],
            "year": 2002,
            "venue": "SmartEng Design",
            "volume": "12",
            "issn": "",
            "pages": "753-758",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF27": {
            "title": "Clustering Algorithms for ITS Sequence Data with Alignment Metrics",
            "authors": [],
            "year": 2006,
            "venue": "Lect Notes ComputSci",
            "volume": "4304",
            "issn": "",
            "pages": "1027-1031",
            "other_ids": {
                "DOI": [
                    "10.1007/11941439_116"
                ]
            }
        },
        "BIBREF28": {
            "title": "Penalized and weighted K-means for clustering with scattered objects and prior information in high-throughput biological data",
            "authors": [],
            "year": 2007,
            "venue": "Bioinformatics",
            "volume": "23",
            "issn": "17",
            "pages": "2247-2255",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btm320"
                ]
            }
        },
        "BIBREF29": {
            "title": "Classifying Synthetic and Biological DNA Sequences with Side Effect Machines",
            "authors": [],
            "year": 2008,
            "venue": "IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology: 15\u201317 Sept. 2008; Sun Valley, ID",
            "volume": "",
            "issn": "",
            "pages": "22-29",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF30": {
            "title": "",
            "authors": [],
            "year": 2001,
            "venue": "Criterion functions for document clustering: experiments and analysis",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF31": {
            "title": "Enhanced bisecting k-means clustering using intermediate cooperation",
            "authors": [],
            "year": 2009,
            "venue": "Pattern Recognit",
            "volume": "42",
            "issn": "11",
            "pages": "2557-2569",
            "other_ids": {
                "DOI": [
                    "10.1016/j.patcog.2009.03.011"
                ]
            }
        },
        "BIBREF32": {
            "title": "Cd-hit: a Fast Program for Clustering and Comparing Large Sets of Protein or Nucleotide Sequences",
            "authors": [],
            "year": 2006,
            "venue": "Bioinformatics",
            "volume": "22",
            "issn": "13",
            "pages": "1658-1659",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btl158"
                ]
            }
        },
        "BIBREF33": {
            "title": "Improved tools for biological sequence comparison",
            "authors": [],
            "year": 1988,
            "venue": "ProcNatlAcad Sci USA",
            "volume": "85",
            "issn": "8",
            "pages": "2444-2488",
            "other_ids": {
                "DOI": [
                    "10.1073/pnas.85.8.2444"
                ]
            }
        },
        "BIBREF34": {
            "title": "A Methodology for Comparative Functional Genomics",
            "authors": [],
            "year": 2007,
            "venue": "JIntegr Bioinform",
            "volume": "4",
            "issn": "3",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF35": {
            "title": "EasyCluster: a fast and efficient gene-oriented clustering tool for large-scale transcriptome data",
            "authors": [],
            "year": 2009,
            "venue": "BMC Bioinformatics",
            "volume": "10",
            "issn": "Suppl 6",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-10-S6-S10"
                ]
            }
        },
        "BIBREF36": {
            "title": "A DNA Sequence Distance Measure Approach for Phylogenetic Tree Construction",
            "authors": [],
            "year": 2010,
            "venue": "5th IEEE International Conference on Bio-Inspired Computing: Theories and Applications: 23\u201326 Sept. 2010 Changsha",
            "volume": "",
            "issn": "",
            "pages": "204-212",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF37": {
            "title": "The gene is dead-Long live the gene: Conceptualizing genes the constructionist way",
            "authors": [],
            "year": 1999,
            "venue": "Sociobiology and Bioeconomics: the Theory of Evolution in Biological and Economic Theory",
            "volume": "",
            "issn": "",
            "pages": "105-137",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF38": {
            "title": "Alignment and clustering of phylogenetic markers- implications for microbial diversity studies",
            "authors": [],
            "year": 2010,
            "venue": "BMC bioinformatics",
            "volume": "11",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": [
                    "10.1186/1471-2105-11-152"
                ]
            }
        },
        "BIBREF39": {
            "title": "",
            "authors": [],
            "year": 1995,
            "venue": "Introduction to Computational Biology: Maps, Sequences, and Genomes",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF40": {
            "title": "",
            "authors": [],
            "year": 1998,
            "venue": "Biological Sequence Analysis: probabilistic models of proteins and nucleic acids",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF41": {
            "title": "Visualization of genomic data using inter-nucleotide distance signals",
            "authors": [],
            "year": 2005,
            "venue": "Proceedings of IEEE Genomic Signal Processing: 11\u201313 July 2005; Bucharest",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF42": {
            "title": "Estimating the Entropy of DNA sequences",
            "authors": [],
            "year": 1997,
            "venue": "JTheor Biol",
            "volume": "188",
            "issn": "3",
            "pages": "369-377",
            "other_ids": {
                "DOI": [
                    "10.1006/jtbi.1997.0493"
                ]
            }
        },
        "BIBREF43": {
            "title": "Relative entropy of DNA and its application",
            "authors": [],
            "year": 2005,
            "venue": "Physica A: Stat Mech Appl",
            "volume": "347",
            "issn": "",
            "pages": "465-471",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF44": {
            "title": "Alignment-free sequence comparison-a review",
            "authors": [],
            "year": 2003,
            "venue": "Bioinformatics",
            "volume": "19",
            "issn": "4",
            "pages": "513-523",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btg005"
                ]
            }
        },
        "BIBREF45": {
            "title": "A comparison of document clustering techniques",
            "authors": [],
            "year": null,
            "venue": "KDD Workshop on Text Mining: 20\u201323 August 2000; Boston",
            "volume": "",
            "issn": "",
            "pages": null,
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF46": {
            "title": "Hierarchical Clustering Algorithms for Document Datasets",
            "authors": [],
            "year": 2005,
            "venue": "Data Mining Knowl Discov",
            "volume": "10",
            "issn": "",
            "pages": "141-168",
            "other_ids": {
                "DOI": [
                    "10.1007/s10618-005-0361-3"
                ]
            }
        },
        "BIBREF47": {
            "title": "Fast and effective text mining using linear-time document clustering",
            "authors": [],
            "year": 1999,
            "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining: 15\u201318 August 1999; San Diego",
            "volume": "",
            "issn": "",
            "pages": "16-22",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF48": {
            "title": "TreeView: an application to display phylogenetic trees on personal computers",
            "authors": [],
            "year": 1996,
            "venue": "Bioinformatics",
            "volume": "12",
            "issn": "",
            "pages": "357-358",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/12.4.357"
                ]
            }
        },
        "BIBREF49": {
            "title": "New method for comparing DNA primary sequences based on a discrimination measure",
            "authors": [],
            "year": 2010,
            "venue": "JTheor Biol",
            "volume": "266",
            "issn": "4",
            "pages": "703-707",
            "other_ids": {
                "DOI": [
                    "10.1016/j.jtbi.2010.07.040"
                ]
            }
        },
        "BIBREF50": {
            "title": "Conflict among individual mitochondrial proteins in resolving the phylogeny of eutherian orders",
            "authors": [],
            "year": 1998,
            "venue": "JMol Evol",
            "volume": "47",
            "issn": "3",
            "pages": "307-322",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF51": {
            "title": "Multiple sequence alignment with the Clustal series of programs",
            "authors": [],
            "year": 2003,
            "venue": "Nucleic Acids Res",
            "volume": "31",
            "issn": "13",
            "pages": "3497-3500",
            "other_ids": {
                "DOI": [
                    "10.1093/nar/gkg500"
                ]
            }
        },
        "BIBREF52": {
            "title": "MUSCLE: multiple sequence alignment with high accuracy and high throughput",
            "authors": [],
            "year": 2004,
            "venue": "Nucleic Acids Res",
            "volume": "32",
            "issn": "5",
            "pages": "1792-1797",
            "other_ids": {
                "DOI": [
                    "10.1093/nar/gkh340"
                ]
            }
        },
        "BIBREF53": {
            "title": "A new distribution vector and its application in genome clustering",
            "authors": [],
            "year": 2011,
            "venue": "MolPhylogenet Evol",
            "volume": "59",
            "issn": "2",
            "pages": "438-443",
            "other_ids": {
                "DOI": []
            }
        },
        "BIBREF54": {
            "title": "Alignment-free estimation of nucleotide diversity",
            "authors": [],
            "year": 2011,
            "venue": "Bioinformatics",
            "volume": "27",
            "issn": "4",
            "pages": "449-455",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btq689"
                ]
            }
        },
        "BIBREF55": {
            "title": "A novel feature-based method for whole genome phylogenetic analysis without alignment: application to HEV genotyping and subtyping",
            "authors": [],
            "year": 2008,
            "venue": "Biochem Biophys Res Commun",
            "volume": "368",
            "issn": "2",
            "pages": "223-230",
            "other_ids": {
                "DOI": [
                    "10.1016/j.bbrc.2008.01.070"
                ]
            }
        },
        "BIBREF56": {
            "title": "Efficient estimation of pairwise distances between genomes",
            "authors": [],
            "year": 2009,
            "venue": "Bioinformatics",
            "volume": "25",
            "issn": "24",
            "pages": "3221-3227",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btp590"
                ]
            }
        },
        "BIBREF57": {
            "title": "Alignment-free detection of local similarity among viral and bacterial genomes",
            "authors": [],
            "year": 2011,
            "venue": "Bioinformatics",
            "volume": "27",
            "issn": "11",
            "pages": "1466-1472",
            "other_ids": {
                "DOI": [
                    "10.1093/bioinformatics/btr176"
                ]
            }
        }
    }
}